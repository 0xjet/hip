{
    "id": "f46d708d-ab03-4009-b716-a2110c7320d1",
    "created_at": "2022-10-25T16:48:23.441548Z",
    "updated_at": "2025-03-27T02:16:39.983201Z",
    "deleted_at": null,
    "sha1_hash": "ada0fc17b33be6fd19b66d7406794adda4f1fbb4",
    "title": "",
    "authors": "",
    "file_creation_date": "2015-05-15T17:20:06Z",
    "file_modification_date": "2015-05-15T17:20:06Z",
    "file_size": 790310,
    "plain_text": "### David Fifield*, Chang Lan, Rod Hynes, Percy Wegmann, and Vern Paxson\n# Blocking-resistant communication through domain fronting\n\n\n**Abstract:** We describe “domain fronting,” a versatile\ncensorship circumvention technique that hides the remote endpoint of a communication. Domain fronting\nworks at the application layer, using HTTPS, to communicate with a forbidden host while appearing to communicate with some other host, permitted by the censor. The key idea is the use of different domain names at\ndifferent layers of communication. One domain appears\non the “outside” of an HTTPS request—in the DNS request and TLS Server Name Indication—while another\ndomain appears on the “inside”—in the HTTP Host\nheader, invisible to the censor under HTTPS encryption. A censor, unable to distinguish fronted and nonfronted traffic to a domain, must choose between allowing circumvention traffic and blocking the domain entirely, which results in expensive collateral damage. Domain fronting is easy to deploy and use and does not require special cooperation by network intermediaries. We\nidentify a number of hard-to-block web services, such as\ncontent delivery networks, that support domain-fronted\nconnections and are useful for censorship circumvention.\nDomain fronting, in various forms, is now a circumvention workhorse. We describe several months of deployment experience in the Tor, Lantern, and Psiphon circumvention systems, whose domain-fronting transports\nnow connect thousands of users daily and transfer many\nterabytes per month.\n\n**Keywords: censorship circumvention**\n\nDOI 10.1515/popets-2015-0009\nReceived 2015-02-15; revised 2015-02-15; accepted 2015-05-15.\n\n***Corresponding Author: David Fifield: University of**\nCalifornia, Berkeley, E-mail: fifield@eecs.berkeley.edu\n**Chang Lan: University of California, Berkeley, E-mail:**\nclan@eecs.berkeley.edu\n**Rod Hynes: Psiphon Inc, E-mail: r.hynes@psiphon.ca**\n**Percy Wegmann: Brave New Software, E-mail:**\nox.to.a.cart@gmail.com\n**Vern Paxson: University of California, Berkeley and**\nthe International Computer Science Institute, E-mail:\nvern@berkeley.edu\n\n\n## 1 Introduction\n\nCensorship is a daily reality for many Internet users.\nWorkplaces, schools, and governments use technical and\nsocial means to prevent access to information by the network users under their control. In response, those users\nemploy technical and social means to gain access to the\nforbidden information. We have seen an ongoing conflict\nbetween censor and censored, with advances on both\nsides, more subtle evasion countered by more powerful\ndetection.\nCircumventors, at a natural disadvantage because\nthe censor controls the network, have a point working\nin their favor: the censor’s distaste for “collateral damage,” incidental overblocking committed in the course of\ncensorship. Collateral damage is harmful to the censor,\nbecause the overblocked content has economic or social\nvalue, so the censor tries to avoid it. (Any censor not\nwilling to turn off the Internet completely must derive\n_some benefit from allowing access, which overblocking_\nharms.) One way to win against censorship is to entangle circumvention traffic with other traffic whose value\nexceeds the censor’s tolerance for overblocking.\nIn this paper we describe “domain fronting,”\na general-purpose circumvention technique based on\nHTTPS that hides the true destination of a communication from a censor. Fronting works with many web\nservices that host multiple domain names behind a frontend server. These include such important infrastructure as content delivery networks (CDNs) and Google’s\npanoply of services—a nontrivial fraction of the web.\n(Section 4 is a survey of suitable services.) The utility\nof domain fronting is not limited to HTTPS communication, nor to accessing only the domains of a specific web\nservice. It works well as a domain-hiding component of\na larger circumvention system, an HTTPS tunnel to a\ngeneral-purpose proxy.\nThe key idea of domain fronting is the use of different domain names at different layers of communication.\nIn an HTTPS request, the destination domain name appears in three relevant places: in the DNS query, in the\nTLS Server Name Indication (SNI) extension [18, §3],\nand in the HTTP Host header [20, §14.23]. Ordinarily, the same domain name appears in all three places.\n\n\n-----\n\nIn a domain-fronted request, however, the DNS query\nand SNI carry one name (the “front domain”), while the\nHTTP Host header, hidden from the censor by HTTPS\nencryption, carries another (the covert, forbidden destination).\n\n**Fig. 1. Domain fronting uses different domain names at different**\nlayers. At the plaintext layers visible to the censor—the DNS\nrequest and the TLS Server Name Indication—appears the front\ndomain allowed.example. At the HTTP layer, unreadable to the\ncensor, is the actual, covert destination forbidden.example.\n\nThe censor cannot block on the contents of the DNS\nrequest nor the SNI without collaterally blocking the\nfront domain. The Host header is invisible to the censor,\nbut visible to the frontend server receiving the HTTPS\nrequest. The frontend server uses the Host header internally to route the request to its covert destination;\nno traffic ever reaches the putative front domain. Domain fronting has many similarities with decoy routing [29, 35, 69, 70]; it may be understood as “decoy\nrouting at the application layer.” A fuller comparison\nwith decoy routing appears in Section 3.\nThis Wget command demonstrates domain fronting\non Google, one of many fronting-capable services. Here,\nthe HTTPS request has a Host header for maps.google.\ncom, even though the DNS query and the SNI in the\nTLS handshake specify www.google.com. The response\ncomes from maps.google.com.\n```\n  $ wget -q -O - https://www.google.com/ \\\n    --header 'Host: maps.google.com' | \\\n    grep -o '<title>.*</title>'\n  <title>Google Maps</title>\n\n```\nA variation is “domainless” fronting, in which there is no\nDNS request and no SNI. It appears to the censor that\nthe user is browsing an HTTPS site by its IP address,\nor using a web client that does not support SNI. Domainless fronting can be useful when there is no known\nfront domain with sufficiently high collateral damage; it\nleaves the censor the choice of blocking an entire IP address (or blocking SNI-less connections entirely), rather\nthan blocking only a single domain. According to our\n\n\ncommunication with the International Computer Science Institute’s certificate notary [32], which observes\non the order of 50 million TLS connections daily, 16.5%\nof TLS connections in June 2014 lacked SNI, which is\nenough to make it difficult for a censor to block SNI-less\nTLS outright.\nDomain fronting works with CDNs because a CDN’s\nfrontend server (called an “edge server”), on receiving\na request for a resource not already cached, forwards\nthe request to the domain found in the Host header\n(the “origin server”). (There are other ways CDNs may\nwork, but this “origin pull” configuration is common.)\nThe client issues a request that appears to be destined\nfor an unrelated front domain, which may be any of\nthe CDN’s domains that resolve to an edge server; this\nfronted request is what the censor sees. The edge server\ndecrypts the request, reads the Host header and forwards the request to the specified origin, which in the\ncircumvention scenario is a general-purpose proxy. The\norigin server, being a proxy, would be blocked by the\ncensor if accessed directly—fronting hides its address\nfrom the censor.\nOn services that do not automatically forward requests, it is usually possible to install a trivial “reflector” web application that emulates an origin-pull CDN.\nIn this case, fronting does not protect the address of\nthe origin per se; rather it protects the address of the\nreflector application, which in turn forwards requests to\nthe origin. Google App Engine is an example of such\na service: against a censor that blocks the App Engine\ndomain appspot.com but allows other Google domains,\ndomain fronting enables access to a reflector running on\nappspot.com.\nNo matter the specifics of particular web services, as\na general rule they do not forward requests to arbitrary\ndomains—only to domains belonging to one of their customers. In order to deploy a domain-fronting proxy, one\nmust become a customer of the CDN (or Google, etc.)\nand pay for bandwidth. It is the owner of the covert\ndomain who pays the bandwidth bills, not the owner of\nthe front domain, which need not have any relation to\nthe covert domain beyond using the same web service.\nThe remainder of this paper is devoted to a deep\nexploration of domain fronting as we have deployed\nit in practice. Section 2 explains our threat model\nand assumptions. Section 3 gives general background\non the circumvention problem and outlines its three\ngrand challenges: address-based blocking, content-based\nblocking, and active probing. Domain-fronting systems\nare capable of meeting all three challenges, forcing censors to use more expensive, less reliable censorship tech\n\n-----\n\nniques that have heretofore not been seen in practice.\nSection 4 is a survey of CDNs and other services that\nare usable for fronting; we identify general principles as\nwell as idiosyncrasies that affect implementation. The\nfollowing sections are three case studies of deployment:\nSection 5 for Tor, Section 6 for Lantern, and Section 7\nfor Psiphon. Section 8 sketches domain fronting’s resistance to statistical traffic analysis attacks. Section 9 has\ngeneral discussion and Section 10 summarizes.\n\n## 2 Threat model\n\nOur threat model includes four actors: the censor, the\ncensored client, the intermediate web service, and the\ncovert destination (a proxy server). Circumvention is\nachieved when the client reaches the proxy, because the\nproxy grants access to any other destination. The client\nand proxy cooperate with each other. The intermediate\nweb service need not cooperate with either, except to\nthe extent that it does not collude with the censor.\nThe censor controls a (generally national) network\nand the links into and within it. The censor can inspect\ntraffic flowing across all links under its control and can\nblock or allow any packet. The censor can inject and replay traffic, and operate its own clients and servers. The\nclient lies within the censor’s network, while the intermediate web service and proxy lie outside. The censor\nblocks direct communication between the client and the\nproxy, but allows HTTPS between the client and at least\none front domain or IP address on the intermediate web\nservice.\nThe client, intermediate web service, and destination proxy are uncontrolled by the censor. The censor\ndoes not control a trusted certificate authority: it cannot\nman-in-the-middle TLS without being caught by ordinary certificate validation. The client is able to obtain\nthe necessary circumvention software.\n\n## 3 Background and related work\n\nBroadly speaking, there are three main challenges in\nproxy-based circumvention: blocking by content, blocking by address, and active probing. Blocking by content\nis based on what you say, blocking by address is based on\n_whom you talk to, and active probing means the censor_\n_acts as a client. A savvy censor will employ all these_\ntechniques, and effective circumvention requires countering them all.\n\n\nA content-blocking censor inspects packets and payloads, looking, for example, for forbidden protocols or\nkeywords. Content-based blocking is sometimes called\ndeep packet inspection (DPI). An address-blocking censor forbids all communication with certain addresses,\nfor example IP addresses and domain names, regardless of the contents of the communication. An activeprobing censor does not limit itself to observation and\nmanipulation of user-induced traffic only. It sends its\nown proxy requests (active probes), either proactively or\non demand in response to observed traffic, and blacklists\nany proxies it finds thereby. Active probing is a precise\nmeans of identifying proxy servers, even when a protocol\nis hard to detect on the wire—it can regarded as a way\nof reducing accidental overblocking. Winter and Lindskog [66] confirmed an earlier discovery of Wilde [64]\nthat China’s Great Firewall discovers secret Tor bridges\nby issuing followup scans after observing a suspected\nTor connection.\nThere are two general strategies for countering\ncontent-based blocking. The first is to look unlike anything the censor blocks; the second is to look like something the censor allows. Following the first strategy are\nthe so-called “look-like-nothing” transports whose payloads look like a uniformly random byte stream. Examples of look-like-nothing transports are obfuscatedopenssh [41] and its string of successors: obfs2 [33],\nobfs3 [34], ScrambleSuit [67], and obfs4 [4]. They all\nwork by re-encrypting an underlying stream so that\nthere remain no plaintext components, not even in the\nhandshake and key exchange. obfs2, introduced in early\n2012 [14], used a fairly weak key exchange that can be\ndetected passively; it is now deprecated and little used.\nobfs3 is Tor’s most-used transport [61] as of May 2015. It\nimproves on obfs2 with a Diffie–Hellman key exchange,\npublic keys being encoded so as to be indistinguishable\nfrom random binary strings. ScrambleSuit and obfs4\nadd resistance to active probing: the server accepts a\nTCP connection but does not send a reply until the\nclient proves knowledge of a secret shared out of band.\nScrambleSuit and obfs4 additionally obscure the traffic\nsignature of the underlying stream by modifying packet\nlengths and timing.\nThe other strategy against DPI is the steganographic one: look like something the censor allows.\nfteproxy [17] uses format-transforming encryption to encode data into strings that match a given regular expression, for example a regular-expression approximation of\nHTTP. StegoTorus [63] transforms traffic to look like\na cover protocol using a variety of special-purpose encoders. Code Talker Tunnel (formerly SkypeMorph) [47]\n\n\n-----\n\nmimics a Skype video call. FreeWave [30] encodes a\nstream as an acoustic signal and sends it over VoIP to a\nproxy. Dust [65] uses encryption to hide static byte patterns and then shapes statistical features such as packet\nlengths and byte frequencies to match specified distributions.\nHoumansadr et al. [28] evaluate “parrot” systems\nthat imitate another protocol and conclude that unobservability by imitation is fundamentally flawed. To\nfully mimic a complex and sometimes proprietary protocol like Skype is difficult, because the system must\nimitate not only the normal operation of the protocol,\nbut also its reaction to errors, its typical traffic patterns, and quirks of implementations. Geddes et al. [23]\ndemonstrate that even non-parrot systems may be vulnerable to attacks that disrupt circumvention while having little effect on ordinary traffic. Their examination\nincludes VoIP protocols, in which packet loss and duplication are acceptable. The censor may, for example,\nstrategically drop certain packets in order to disrupt a\ncovert channel, without much harming ordinary calls.\nThe challenge of address-based blocking is a difficult one that has inspired various creative circumvention ideas. Tor has long faced the problem of the\nblocking of its relays, the addresses of which appear\nin a public directory. In response, Tor began to reserve a portion of its relays as secret “bridges” [15]\nwhose addresses are not publicly known. BridgeDB [9],\nthe database of secret bridges, carefully distributes addresses so that it is easy to learn a few bridges, but hard\nto enumerate all of them. BridgeDB uses CAPTCHAs\nand other rate-limiting measures, and over short time\nperiods, always returns the same bridges to the same\nrequester, preventing enumeration by simple repeated\nqueries. BridgeDB is also capable of distributing the\naddresses of obfuscated bridges (currently obfs3, obfs4,\nScrambleSuit, and fteproxy), granting IP-blocking resistance to DPI-resistant systems that otherwise lack it.\nCensorSpoofer [62] decouples upstream and downstream data channels. The client sends data to a CensorSpoofer proxy over a low-bandwidth covert channel\nsuch as email. The proxy sends data back over a UDP\nchannel, all the time spoofing its source address so the\npackets appear to originate from some other “dummy”\nhost. The censor has no IP address to block, because\nthe proxy’s true address never appears on the wire.\nClient and server have the challenge of agreeing on a\ndependable covert upstream channel that must remain\nunblocked, and the client must carry on a believable\nUDP conversation with the dummy host—a VoIP call,\nfor example.\n\n\nFlash proxy [22] resists address blocking by\nconscripting web users as temporary proxies. Each\nJavaScript-based proxy lasts only as long as a user stays\non a web page, so the pool of proxies is constantly changing. If one of them is blocked, there is soon another\nto replace it. Flash proxy’s approach to address blocking is the opposite of domain fronting’s: where flash\nproxy uses many cheap, disposable, individually blockable proxies, domain fronting uses just a few high-value\nfront domains on hard-to-block network infrastructure.\nA drawback with flash proxy’s use of the browser is\nthat the client must be able to receive a TCP connection; in particular it must not be behind network address\ntranslation (NAT), which limits flash proxy’s usefulness.\nPart of the flash proxy protocol requires the client to\nsend a small amount of unblockable data in a process\ncalled rendezvous. The default rendezvous mechanism\nhas used domain fronting through Google App Engine\nsince 2013 [58]. Flash proxy itself does nothing to defend\nagainst DPI. Connections between censored clients and\nbrowser-based proxies use WebSocket, a meta-protocol\nrunning on HTTP, but inside the WebSocket framing is\nthe ordinary TLS-based Tor protocol.\nDecoy routing [35] is a technique that puts proxies\nin the middle of network paths, rather than at the ends.\nFor this reason, it is also called end-to-middle proxying. Realizations of decoy routing include Telex [70],\nCirripede [29], and TapDance [69]. Decoy routing asks\nfriendly ISPs to deploy special routers that reside on\nnetwork paths between censored users and uncensored\n“decoy” Internet destinations. Circumvention traffic is\n“tagged” in a way that is detectable only by the special\nrouters, and not by the censor. On receiving a tagged\ncommunication, the router shunts it away from its apparent, overt destination and toward a censored, covert\n_destination. Domain fronting is similar in spirit to de-_\ncoy routing: think of domain fronting as decoy routing\nat the application layer. In place of a router, domain\nfronting has a frontend server; in place of the overt destination is the front domain. Both systems tag flows in\na way that is invisible to the censor: decoy routing uses,\nfor example, a hash embedded in a client nonce, while\nfronting uses the HTTP Host header, encrypted inside\nof HTTPS. Fronting has the advantage of not requiring\ncooperation by network intermediaries.\nSchuhard et al. [54] introduce the idea of a rout_ing adversary against decoy routing, and show that the_\nconnectivity of the Internet enables censors to force network users onto paths that do not include participating\nrouters. Simulations by Houmansadr et al. [31] show\nthat even though such alternate paths exist, they are\n\n\n-----\n\nmany times more costly to the censor, especially when\nparticipating routers are placed strategically.\nCollage [11] makes a covert channel out of web sites\nthat accept user-generated content, like photos. Both\nsender and receiver rendezvous through one of these\nsites in order to exchange messages. The design of Collage recognizes the need for the proxy sites to be resistant to blocking, which it achieves through the wide\navailability of suitable sites.\nCloudTransport [10] uses cloud storage services, for\nexample Amazon S3, as a communication channel by encoding sends and receives as reads and writes to shared\nremote files. CloudTransport has much in common with\ndomain fronting: it hides the true endpoint of a communication using HTTPS, and it sends traffic through a domain with high collateral damage. In CloudTransport,\nthe hidden destination, which is a storage bucket name\nrather than a domain, is hidden in the path component of a URL. For example, in the S3 URL https://s3.\namazonaws.com/bucketname/filename, the censor only\ngets to “see” the generic domain part, “s3.amazonaws.\ncom”. The path component “/bucketname/filename”,\nwhich would reveal the use of CloudTransport, cannot be used for blocking because it is encrypted under\nHTTPS.\nIn a prescient 2012 blog post [8], Bryce Boe described how to gain access to Google HTTPS services\nthrough a single whitelisted Google IP address, by manual editing of a hosts file. He observed that Google App\nEngine could serve as a general-purpose proxy, and anticipated the countermeasure of SNI filtering, noting\nthat sending a false SNI could defeat it.\nTo our knowledge, the earliest use of domain\nfronting for circumvention was by GoAgent [24], a tool\nbased on Google App Engine and once widely used in\nChina. Users of GoAgent upload a personal copy of the\nproxy code to App Engine, where it runs on a subdomain of appspot.com. In order to reach appspot.com,\nGoAgent fronts through a Google IP address, using the\n“domainless” model without SNI. GoAgent does not\nuse an additional general-purpose proxy after fronting;\nrather, it fetches URLs directly from the App Engine\nservers. Because of that, GoAgent does not support protocols other than HTTP and HTTPS, and the end-toend security of HTTPS is lost, as web pages exist in\nplaintext on the App Engine servers before being reencrypted back to the client. According to a May 2013\nsurvey [53], GoAgent was the most-used circumvention\ntool in China, with 35% of survey respondents having\nused it in the previous month. It ranked higher than\npaid (29%) and free VPNs (18%), and far above special\n\npurpose tools like Tor (2.9%) and Psiphon (2.5%).\nGoAgent was disrupted in China starting in the beginning of June 2014, when all Google services were\nblocked [2, 25]. The block also affected our prototype\nsystems when used in China with App Engine, though\nthey continued to work in China over other web services.\n\n## 4 Fronting-capable web services\n\nIn this section we survey a variety of web services and\nevaluate their suitability for domain fronting. Most of\nthe services we evaluated support fronting in one form\nor another, but they each have their own quirks and\nperformance characteristics. The survey is not exhaustive but it includes many of the most prominent content\ndelivery networks. Table 1 is a summary.\nPricing across services varies widely, and depends on\ncomplicated factors such as geographical region, bandwidth tiers, price breaks, and free thresholds. Some services charge per gigabyte or per request, some for time,\nand some for other resources. Most services charge between $0.10 and $0.20 per GB; usually bandwidth is\ncheaper in North America and Europe than in the rest\nof the world.\nRecall that even services that support domain\nfronting will front only for the domains of their own\ncustomers. Deployment on a new service typically requires becoming a customer, and an outlay of time and\nmoney. Of the services surveyed, we have at some time\ndeployed on Google App Engine, Amazon CloudFront,\nMicrosoft Azure, Fastly, and CloudFlare. The others we\nhave only tested using manually crafted HTTP requests.\n\n**Google App Engine [26] is a web application**\nplatform. Users can upload a web app needing nothing\nmore than a Google account. Each application gets a\nuser-specified subdomain of appspot.com, for which almost any Google domain can serve as a front, including\ngoogle.com, gmail.com, googleapis.com, and many others. App Engine can run only web applications serving\nshort-lived requests, not a general-purpose proxy such\nas a Tor bridge. For that reason we use a tiny “reflector”\napplication that merely forwards incoming requests to\na long-lived proxy running elsewhere. Fronting through\nApp Engine is attractive in the case where the censor blocks appspot.com but at least one other Google\ndomain is reachable. App Engine costs $0.12/GB and\n$0.05 for each “instance hour” (the number of running\ninstances of the app is adjusted dynamically to meet\nload, and you pay for each instance after the first).\n\n\n-----\n\n**Fig. 2. Architecture of meek. The client sends an HTTP request to the Tor bridge by way of an intermediate web service such as a**\nCDN. The client protects the bridge’s domain name forbidden.example from the censor by fronting it with another name, here al**lowed.example. The intermediate web server decrypts the TLS layer and forwards the request to the bridge according to the Host**\nheader. The bridge sends data back to the client in the HTTP response. meek-client and meek-server are the interface between Tor\nand the pluggable transport; from Tor’s point of view, everything between meek-client and meek-server is an opaque data transport.\nThe host at allowed.example does not participate in the communication.\n\n\n**Table 1. Summary of fronting-capable services. The table does**\nnot include other kinds of services, such as shared web hosting,\nthat may work for domain fronting. Bandwidth charges usually\nvary by geographic region. Many services offer price breaks starting around 10 TB/month. Prices are current as of May 2015 and\nare rounded to the nearest cent.\n\n**service** **$/GB** **$/10K reqs.** **$/hour** **$/month**\n**App Engine[1]** **0.12** **–** **0.05** **–**\n**CloudFront** **0.09–0.25** **0.01–0.02** **–** **–**\n**Azure** **0.09–0.14** **–** **–** **–**\n**Fastly[2]** **0.12–0.19** **0.01** **–** **–**\n**CloudFlare[3]** **–** **–** **–** **200**\n**Akamai[4]** **–** **–** **–** **400**\n**Level 3[5]** **0.10–0.25** **–** **–** **–**\n\n1 App Engine dynamically scales the number of “instances” of the application code in order to handle\nchanging load. Every instance after the first costs\n$0.05/hour.\n2 Fastly has a minimum monthly charge of $50.\n3 CloudFlare charges $200/month for its “business”\nplan. It has other plans that cost more and less.\n4 Akamai does not publish pricing information; the\nprices here are from a reseller called Cache Simple,\nwhich quotes $400/month for 1000 GB transfer, and\n$0.50/GB for overages.\n5 Level 3 does support domain fronting per se, but\npaths under the secure.footprint.net domain can\nlikely serve the same purpose. Level 3 does not publish pricing information; the prices here are from\na reseller called VPS.NET, which quotes $0.10–\n0.25/GB.\n\n\nApplications are free of charge if they stay below certain usage thresholds, for example 1 GB of bandwidth\ndaily, making possible a distributed, upload-your-ownapp model in the style of GoAgent.\n**Amazon CloudFront [3] is the CDN of Ama-**\nzon Web Services. A CloudFront “distribution,” as a\nCDN configuration is called, associates an automatically generated subdomain of cloudfront.net with an\norigin server. The front domain may be any other\ncloudfront.net subdomain (all of which support HTTPS\nthrough a wildcard certificate), or any other DNS alias\nfor them. CloudFront is easy to set up: one must only set\nthe origin domain and no reflector app is needed. Pricing per GB ranges from $0.085 for the United States\nand Europe, up to $0.25 for South America, with price\nbreaks starting at 10 TB/month. There is an additional charge per 10,000 HTTPS requests, ranging from\n$0.0075 in the United States to $0.0160 in South America. CloudFront has a usage tier that is free of charge for\na year, subject to a bandwidth limit of 50 GB/month.\n**Microsoft Azure [46] is a cloud computing plat-**\nform that features a CDN. Like CloudFront, Azure\nassigns automatically generated subdomains of vo.\nmsecnd.net. any of which can front for any other.\nThere are other possible front domain names, like ajax.\naspnetcdn.com, that are used as infrastructure by many\nweb sites, lending them high collateral damage. Unlike CloudFront’s, Azure’s CDN forwards only to Azureaffiliated domains, so as with App Engine, it is necessary\nto run a reflector app that forwards requests to some external proxy. Bandwidth costs $0.087–0.138/GB, with\nprice breaks starting at 10 TB/month.\n**Fastly [19] is a CDN. Unlike most CDNs, Fastly**\nvalidates the SNI: if SNI and Host do not match, the\n\n\n-----\n\nedge server returns an HTTP 400 (“Bad Request”) error. However, if the TLS ClientHello simply omits SNI,\nthen the Host may be any Fastly domain. Fastly therefore requires the “domainless” fronting style. Fastly’s\npricing model is similar to CloudFront’s. They charge\nbetween $0.12 and $0.19 per GB and $0.0075 and $0.009\nper 10,000 requests, depending on the region.\n**CloudFlare [12] is a CDN also marketed as pro-**\ntection against denial-of-service attacks. Like Fastly,\nCloudFlare checks that the SNI matches the Host\nheader and therefore requires sending requests without\nSNI. CloudFlare charges a flat fee per month and does\nnot meter bandwidth. There is a no-cost plan intended\nfor small web sites, which is adequate for a personal\ndomain-fronting installation. The upgraded “business”\nplan is $200/month.\n**Akamai [1] is a large CDN. Requests may be**\nfronted through the special HTTPS domain a248.e.\nakamai.net, or other customer-configured DNS aliases,\nthough it appears that certain special domains get special treatment and do not work as fronts. Akamai has\nthe potential to provide a lot of cover: in 2010 it carried\n15–20% of all web traffic [49]. Akamai does not publish pricing details, but it is reputed to be among the\npricier CDNs. We found a reseller, Cache Simple, that\ncharges $400 for 1000 GB/month, and $0.50/GB after\nthat. The special domain a248.e.akamai.net began to\nbe DNS-poisoned in China in late September 2014 [27]\n(possibly because it had been used to mirror blocked\nweb sites), necessitating an alternative front domain in\nthat country.\n**Level 3 [42] is a tier-1 network operator that has**\na CDN. Unlike other services in this section, Level 3\ndoes not appear to support domain fronting. However,\nwe mention it because it may be possible to build similar functionality using distinct URL paths under the\ndomain secure.footprint.net (essentially using the path,\nrather than the Host header, as a hidden tag). Level 3\ndoes not publish pricing data. We found a reseller,\nVPS.NET, that quotes $34.95 for the first 1000 GB and\n$0.10–0.25/GB thereafter. Level 3’s special HTTPS domain secure.footprint.net is also now DNS-poisoned in\nChina.\n\nThere are other potential deployment models apart\nfrom CDNs. For example, there are cheap web hosts\nthat support both PHP and HTTPS (usually with a\nshared certificate). These features are enough to support a reflector app written in PHP, which users can\nupload under their own initiative. In this do-it-yourself\nmodel, blocking resistance comes not from a strong front\n\n\ndomain, but from the diffuseness of many proxies, each\ncarrying only a small amount of traffic. The URLs of\nthese proxies could be kept secret, or could be carefully disseminated by a proxy-distribution service like\nBridgeDB [9]. Psiphon uses this approach when in “unfronted” mode.\nAnother alternative is deployment with the cooperation of an existing important web site, the blocking\nof which would result in high collateral damage. It is a\nnice feature of domain fronting that it does not require\ncooperation by the intermediate web service, but if you\nhave cooperation, you can achieve greater efficiency. The\nimportant web site could, for example, reserve a magic\nURL path or domain name, and forward matching requests to a proxy running locally. The web site does\ntwo jobs: its ordinary high-value operations that make\nit expensive to block, and a side job of handling circumvention traffic. The censor cannot tell which is which\nbecause the difference is confused by HTTPS.\n\n## 5 Deployment on Tor\n\nWe implemented domain fronting as a Tor pluggable\ntransport [5] called meek. meek combines domain\nfronting with a simple HTTP-based tunneling proxy.\nDomain fronting enables access to the proxy; the proxy\ntransforms a sequence of HTTP requests into a Tor data\nstream.\nThe components of the system appear in Figure 2.\nmeek-client acts as an upstream proxy for the client’s\nTor process. It is essentially a web client that knows how\nto front HTTPS requests. When meek-client receives\nan outgoing chunk of data from a client Tor process,\nit bundles the data into a POST request and fronts the\nrequest through the web service to a Tor bridge. The Tor\nbridge runs a server process, meek-server, that decodes\nincoming HTTP requests and feeds their data payload\ninto the Tor network.\nThe server-to-client stream is returned in the bodies of HTTP responses. After receiving a client request,\nmeek-server checks for any pending data the bridge\nhas to send back to the client, and sends it back in\nthe HTTP response. When meek-client receives the response, it writes the body back into the client Tor.\nThe body of each HTTP request and response carries a small chunk of an underlying TCP stream (up\nto 64 KB). The chunks must be reassembled, in order,\nwithout gaps or duplicates, even in the face of transient failures of the intermediate web service. meek uses\n\n\n-----\n\n```\nPOST / HTTP/1.1\nHost: forbidden.example\nX-Session-Id: cbIzfhx1HnR\nContent-Length: 517\n\\x16\\x03\\x01\\x02\\x00\\x01\\x00\\x01\\xfc\\x03\\x03\\x9b\\xa9...\n  HTTP/1.1 200 OK\n  Content-Length: 739\n  \\x16\\x03\\x03\\x00\\x3e\\x02\\x00\\x00\\x3a\\x03\\x03\\x53\\x75...\nPOST / HTTP/1.1\nHost: forbidden.example\nX-Session-Id: cbIzfhx1HnR\nContent-Length: 0\n  HTTP/1.1 200 OK\n  Content-Length: 75\n  \\x14\\x03\\x03\\x00\\x01\\x01\\x16\\x03\\x03\\x00\\x40\\x06\\x84...\n\n```\n**Fig. 3. Requests and responses in the meek HTTP proto-**\ncol. The session ID is randomly generated by the client. Request/response bodies contain successive chunks of a Tor TLS\nstream (\\x16\\x03\\x01 is the beginning of a TLSv1.0 ClientHello\nmessage). The second POST is an empty polling request. The\nmessages shown here are encrypted inside HTTPS until after\nthey have been fronted, so the censor cannot use the Host and\nX-Session-Id headers for classification.\n\na simple approach: requests and responses are strictly\nserialized. The client does not send a second chunk of\ndata (i.e., make another request) until it has received\nthe response to its first. The reconstructed stream is\nsimply the concatenation of bodies in the order they\narrive. This technique is simple and correct, but less efficient because it needs a full round-trip between every\nsend. See Sections 6 and 7 for alternative approaches\nthat increase efficiency.\nmeek-server must be able to handle many simultaneous clients. It maintains multiple connections to a\nlocal Tor process, one for each active client. The server\nmaps client requests to Tor connections by “session ID,”\na token randomly generated by the client at startup.\nThe session ID plays the same role in the meek protocol\nthat the (source IP, source port, dest IP, dest port) tuple\nplays in TCP. The client sends its session ID in a special\nX-Session-Id HTTP header. meek-server, when it sees a\nsession ID for the first time, opens a new connection\nto the local Tor process and adds a mapping from ID\nto connection. Later requests with the same session ID\nreuse the same Tor connection. Sessions are closed after\na period of inactivity. Figure 3 shows a sample of the\nprotocol.\nHTTP is fundamentally a request-based protocol.\nThere is no way for the server to “push” data to the\nclient without having first received a request. In order\n\n\nAzure\n\nGoogle\n\nCloudFront\n\n\n14 s 48 s\n\n21 s 60 s\n\n31 s 101 s\n\n0 30 60 90\nTime (seconds)\n\n|Col1|21 s|Col3|6|0 s|Col6|Col7|\n|---|---|---|---|---|---|---|\n||3|1 s||||10|\n\n\n**Fig. 4. Time to download an 11 MB file through Tor, with and**\nwithout meek. Text labels indicate the mean of 10 measurements.\nBulk-download times increase by about a factor of 3 when meek\nis activated. The middle and exit nodes are constant across all\nmeasurements; only the entry varies according to the service. The\nwithout-meek circuits use an entry node located at the same IP\naddress as the corresponding with-meek circuits.\n\nto enable the server to send back data, meek-client sends\noccasional empty polling requests even when it has no\ndata to send. The polling requests simply give the server\nan opportunity to send a response. The polling interval\nstarts at 100 ms and grows exponentially up to a maximum of 5 s.\nThe HTTP-based tunneling protocol adds overhead. Each chunk of data gets an HTTP header, then\nthe HTTP request is wrapped in TLS. The HTTP\nheader adds about 160 bytes [59], and TLS adds another 50 bytes or so (the exact amount depends on\nthe ciphersuite chosen by the intermediate web service).\nThe worst-case overhead when transporting a single encrypted Tor cell of about 540 bytes is about 40%, and\nless when more than one cell is sent at once. We can\nestimate how much overhead occurs in practice by examining CDN usage reports. In April 2015, the Amazon CloudFront backend for meek received 3,231 GB\nin 389 M requests [21], averaging about 8900 bytes\nper request. If the overhead per request is 210 bytes,\nthen the average overhead is 210/(8900 − 210) ≈ 2.4%.\nmeek-client reuses the same TLS connection for many\nrequests, so the TLS handshake’s overhead is amortized.\nPolling requests also use bandwidth, but they are sent\nonly when the connection is idle, so they do not affect\nupload or download speed.\nFigure 4 measures the effect of meek’s overhead on\ndownload speed. It shows the time taken to download\na 11,536,384-byte file (http://speedtest.wdc01.softlayer.\ncom/downloads/test10.zip) with and without meek,\nover the three web services on which we have deployed.\nWe downloaded the file 10 times in each configuration.\n\n\n-----\n\n**App Engine** **CloudFront** **Azure (est.)**\n**GB** **cost** **GB** **cost** **GB** **cost**\n**early 2014** **71** **$8** **67** **$8** **47** **$5**\n**Oct 2014** **289** **$41** **479** **$130** **296** **$31**\n**Nov 2014** **1,375** **$225** **1,269** **$363** **499** **$53**\n**Dec 2014** **2,132** **$327** **1,579** **$417** **511** **$64**\n**Jan 2015** **2,944** **$464** **2,449** **$669** **637** **$68**\n**Feb 2015** **4,114** **$651** **2,369** **$605** **614** **$65**\n**Mar 2015** **5,316** **$690** **3,385** **$816** **736** **$78**\n**Apr 2015** **6,304** **$886** **3,231** **$785** **1,982** **$210**\n**total** **22,545 $3,292** **14,828 $3,793** **5,328** **$565**\n\n**Fig. 5. Concurrent users of the meek pluggable transport with**\nmonth-by-month transfer and cost. The Azure columns are estimates of what we would pay if we did not have a special research\ngrant. User counts come from the Tor Metrics Portal [43, 60].\n\nThe time to download the file increases by about a factor of 3 when meek is in use. We attribute this increase\nto the added latency of an indirect path through the\nCDN, and the latency-bound nature of meek’s naive serialization.\nmeek’s primary deployment vehicle is Tor\nBrowser [51], a derivative of Firefox that is preconfigured to use a built-in Tor client. Tor Browser features\nan easy interface for enabling meek and other pluggable\ntransports. Deployment began in earnest in October\n2014 with the release of Tor Browser 4.0 [50], the first\nrelease to include meek as an easy selectable option.\nIt runs over Google App Engine, Amazon CloudFront,\nand Microsoft Azure. Figure 5 shows the daily average\nnumber of concurrent users. (A value of 1,000, for example, means that there were on average 1,000 users of\nthe system at any time during the day.) Also in Figure 5 is a table of monthly costs broken down by web\nservice. Our Azure service is currently running on a free\nresearch grant, which does not provide us with billing\ninformation. We estimate what Azure’s cost would\nbe by measuring the bandwidth used at the backing\nTor bridge, and assuming bandwidth costs that match\nthe geographic traffic mix we observe for CloudFront:\nroughly 62% from North America and Europe, and 38%\nfrom other regions.\n\n\n### 5.1 Camouflage for the TLS layer\n\nWithout additional care, meek would be vulnerable to\nblocking by its TLS fingerprint. TLS, on which HTTPS\nis based, has a handshake that is largely plaintext [13,\n§7.4] and leaves plenty of room for variation between\nimplementations. These differences in implementation\nmake it possible to fingerprint TLS clients [44]. Tor itself was blocked by China in 2011 because of the distinctive ciphersuites it used at the time [57]. Figure 12a in\nAppendix A shows how meek-client’s fingerprint would\nappear natively; it would be easy to block because not\nmuch other software shares the same fingerprint. Figures 12b and 12c show the fingerprints of two web\nbrowsers, which are more difficult to block because they\nalso appear in much non-circumvention traffic.\nIn order to disguise its TLS fingerprint, meek-client\nproxies all its HTTPS requests through a real web\nbrowser. It looks like a browser, because it is a browser.\nWe wrote extensions for Firefox and Chrome that enable\nthem to make HTTPS requests on another program’s\nbehalf. The browser running the extension is completely\nseparate from the Tor Browser the user interacts with.\nIt runs in the background in a separate process, does\nnot display a user interface, and shares no state with\nthe user’s browser. The extra cost of this arrangement\nis negligible in terms of latency, because communication\nwith the headless browser occurs over a fast localhost\nconnection, and in terms of CPU and RAM it is the\nsame as running two browsers at once.\nThe client’s Tor process starts both meek-client and\nthe headless browser, then configures meek-client to\nproxy its requests through the browser. The headless\nbrowser is the only component that actually touches\nthe network. It should be emphasized that the headless browser only makes domain-fronted requests to the\nfront domain; the URLs it requests have no relation to\nthe pages the user browses.\n\n## 6 Deployment on Lantern\n\nLantern [40] is a free circumvention tool for casual web\nbrowsing. It does not employ onion routing and focuses more on performance and availability than on\nanonymity. Lantern encompasses a network of shared\nHTTPS proxy servers, and client software that allows\ncensored users to find and use those proxy servers with\ntheir existing web browsers. The Lantern client also\nallows uncensored users to host proxy servers (“peer\n\n-----\n\n**Fig. 6. MB/s served by domain-fronted Lantern proxies (both**\nLantern-hosted and peers).\n\nhosted” servers) for use by others. Lantern aims to\nprovide a secure mechanism for distributing knowledge about both Lantern-hosted and peer-hosted proxy\nservers using a trust network–based distribution mechanism such as Kaleidoscope [56]. In the meantime,\nLantern also randomly assigns users to Lantern-hosted\nproxy servers.\nLantern has a centralized infrastructure for authenticating users and assigning them proxies. Its threat\nmodel assumes that the centralized infrastructure may\nbe blocked by censors. Therefore, users must have a priori access to an unblocked proxy (a “fallback”) which\nthey use to bootstrap into the rest of the network.\nLantern originally distributed the IP addresses of\nfallbacks by embedding them in customized software installers that we sent to users via email autoresponder.\nThis method prevented users from directly downloading\nLantern from our website and would have made it easy\nfor censors to discover proxies simply by signing up for\nLantern (though in practice we never saw this happen).\n\n### 6.1 Implementation\n\nWe rolled out domain fronting in July 2014, allowing\nusers to download Lantern directly for the first time.\nThe directly downloaded clients proxied all their traffic\nvia domain fronting. After initial testing with Fastly, we\nchanged to a different CDN, which has proven attractive\nbecause it has many unblocked front domains, it does\nnot charge for bandwidth, and its API enables us to\neasily register and unregister proxies.\nFigure 6 shows user bandwidth since deployment.\nAfter experiencing steady growth, in October 2014 we\nstarted randomly assigning direct HTTPS proxies to\nusers who had direct-downloaded Lantern. This diverted some traffic from domain fronted servers to more\nefficient direct servers. In December 2014 and January 2015, there was a dramatic surge in domain-fronted\ntraffic, which jumped from 1 MB/s to 100 MB/s within\n\n\nthose two months. Activity has remained at around the\n100 MB/s level since then.\nLantern’s domain fronting support is provided by\nan application called flashlight [38], which uses library\nlayers called enproxy [37] and fronted [39]. enproxy provides an abstract network connection interface that encodes reads and writes as a sequence of HTTP requests\nvia a stateful enproxy proxy. enproxy allows flashlight\nto proxy any streaming-oriented traffic like TCP. Unlike\nTor’s implementation of meek, enproxy supports fullduplex transfer, which is handy for bidirectional protocols like XMPP, which Lantern uses for P2P signaling. fronted uses domain fronting to transmit enproxy’s\nHTTP requests in a blocking-resistant manner. In practice, we configure fronted with several hundred host domains that are dialed via IP address (no DNS lookup).\nDomain-fronted Lantern requests go to domain\nnames, such as fallbacks.getiantem.org, that represent\npools of servers. The CDN distributes requests to the\nservers in round-robin fashion. The domain-fronting\nprotocol is stateful, so subsequent HTTP requests for\nthe same connection are routed to the original responding proxy using its specific hostname (sticky routing),\nwhich the client obtains from a custom HTTP header.\nThe proxy hostname serves the same request-linking\npurpose as the session ID does in meek.\n\n### 6.2 Mitigations for increased latency\n\nThe encoding of a stream as a sequence of HTTP requests introduces additional latency beyond that of\nTCP. In the case of flashlight with our chosen CDN,\nthe additional latency has several causes. We describe\nthe causes and appropriate mitigations.\nDomain fronting requires the establishment of additional TCP connections. The client, the CDN, and\nthe proxy between themselves introduce three additional TCP connections between the client and the destination. To reduce latency, the CDN pools and reuses\nconnections to the Lantern proxy. Unfortunately, the\nLantern client cannot do the same for its connections\nto the CDN because the CDN seems to time out idle\nconnections fairly aggressively. We mitigate this by aggressively pre-connecting to the CDN when we detect\nactivity [36].\nThough enproxy is mostly full duplex, reads cannot begin until the first request and its response with\nthe sticky-routing header have been processed. This is\na basic limitation.\n\n\n-----\n\ndirect\n\nnon-fronted proxy\n\nfronted proxy\n\n\n5 s\n\n6 s\n\n9 s\n\n0.0 2.5 5.0 7.5 10.0 12.5\nTime (seconds)\n\n|Col1|Col2|Col3|Col4|6|s|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n||||||||9 s|||\n\n\n**Fig. 7. Time to download an 11 MB file direct, through a one-**\nhop proxy, and through Lantern with domain fronting. The enproxy transport and extra CDN hop increase download times by\nabout a factor of 2. Text labels indicate the mean of 10 measurements.\n\nenproxy does not pipeline HTTP requests. Even\nthough reads and writes are full duplex, a write cannot proceed until the flush of previous writes has been\nacknowledged with an HTTP response—a full roundtrip is necessary between each flush. In the future,\nHTTP/2’s request pipelining will potentially improve\non latency.\nenproxy has no way of knowing when the client is\ndone writing. If the data were streaming directly from\nthe client through to the proxy, this would not be a\nproblem, but CDNs buffer uploads: small uploads aren’t\nactually forwarded to the proxy until the HTTP request\nis finished. enproxy assumes that a writer is finished\nif it detects inactivity for more than 35 ms, at which\npoint it flushes the write by finishing the HTTP request.\nThis introduces at least 35 ms of additional latency, and\npotentially more if the guess is wrong and the write is\nnot actually finished, since we now have to wait for a\nfull round trip before the next write can proceed. This\nlatency is particularly noticeable when proxying TLS\ntraffic, as the TLS handshake consists of several small\nmessages in both directions.\nThis last source of latency can be eliminated if enproxy can know for sure when a writer is finished. This\ncould be achieved by letting enproxy handle the HTTP\nprotocol specifically. Doing so would allow enproxy to\nknow when the user agent is finished sending an HTTP\nrequest and when the destination is finished responding.\nHowever, doing the same for HTTPS would require a\nlocal man-in-the-middle attack on the TLS connection\nin order to expose the flow of requests. Furthermore,\nthis approach would work only for HTTP clients. Other\ntraffic, like XMPP, would require additional support for\nthose protocols.\nFigure 7 compares download speeds with and\nwithout a domain fronting proxy. It is based on\n10 downloads of the same 11 MB file used in the\n\n\nTor bandwidth test in the previous section, however\nlocated on a different server close to the Lantern\nproxy servers: http://speedtest.ams01.softlayer.com/\ndownloads/test10.zip. The fronted proxy causes download times to approximately double. Because of\nLantern’s round-robin rotation of front domains, the\nperformance of the fronted proxy may vary over time\naccording to the route to the CDN.\n\n### 6.3 Direct domain fronting\n\nThe Lantern network includes a geolocation server. This\nserver is directly registered on the CDN and the Lantern\nclient domain-fronts to it without using any proxies, reducing latency and saving proxy resources. This sort of\ndirect domain fronting technique could in theory be implemented for any web site simply by registering it under\na custom domain such as facebook.direct.getiantem.org.\nIt could even be accomplished for HTTPS, but would\nrequire the client software to man-in-the-middle local\nHTTPS connections between browser and proxy, exposing the plaintext not only to the Lantern client but also\nto the CDN. In practice, web sites that use the CDN\nalready expose their plaintext to the CDN, so this may\nbe an acceptable solution.\n\n## 7 Deployment on Psiphon\n\nThe Psiphon circumvention system [52] is a centrally\nmanaged, geographically diverse network of thousands\nof proxy servers. It has a performance-oriented, one-hop\narchitecture. Much of its infrastructure is hosted with\ncloud providers. As of January 2015, Psiphon has over\ntwo million daily unique users. Psiphon client software\nruns on popular platforms, including Windows and Android. The system is designed to tunnel a broad range of\n\n300,000\n\n200,000\n\n100,000\n\n0\n\nDec 15 Jan 01 Jan 15 Feb 01\n2014 2015 2015 2015\n\n**Fig. 8. Daily unique users of meek with Psiphon. Clients send a**\ncoarse-grained “last connected” timestamp when connecting. A\nunique user is counted whenever a user connects with a timestamp before the day under consideration.\n\n\n-----\n\nhost traffic: web browsing, video streaming, and mobile\napp data transfer. Client software is designed for ease of\nuse; users are not asked to perform any configuration.\nPsiphon has faced threats including blocking by\nDPI—both blacklisting and whitelisting—and blocking\nby address. For example, in 2013, Psiphon circumvented\nHTTP-whitelisting DPI by sending an “HTTP prefix”\n(the first few bytes of an HTTP request) before the start\nof its regular upstream flow [6].\nPsiphon strives to distribute its server addresses in\nsuch a way that most clients discover enough servers\nto have several options in the case of a server being blocked, while making it difficult to enumerate all\nservers. In February 2014, Psiphon was specifically targeted for address-based blocking, and this blocking was\naggressive enough to have a major impact on our user\nbase, though not all users were blocked. As part of\nour response we integrated and deployed meek-based\ndomain fronting, largely based on Tor’s implementation, with some modifications. It was fully deployed in\nJune 2014. Figure 8 shows the number of unique daily\nusers of fronted meek with Psiphon.\nIn addition, Psiphon also employs meek in what\nwe call “unfronted” mode. Unfronted meek omits the\nTLS layer and the protocol on the wire is HTTP. As\nfully compliant HTTP, unfronted meek supersedes the\n“HTTP prefix” defense against HTTP whitelisting. Unfronted meek is not routed through CDNs, and as such is\nonly a defense against DPI whitelisting and not against\nproxy address enumeration. We envision a potential future fronted HTTP protocol with both properties, which\nrequires cooperating with CDNs to route our HTTP requests based on, for example, some obfuscated HTTP\nheader element.\n\n### 7.1 Implementation\n\nPsiphon’s core protocol is SSH. SSH provides an encryption layer for communication between Psiphon clients\nand servers; the primary purpose of this encryption is\nto frustrate DPI. On top of SSH, we add an obfuscatedopenssh [41] layer that transforms the SSH handshake\ninto a random stream, and add random padding to the\nhandshake. The payload within the meek transport appears to be random data and lacks a trivial packet size\nsignature in its initial requests and responses. Psiphon\nclients authenticate servers using SSH public keys obtained out of band, a process that is bootstrapped with\nserver keys embedded in the client binaries.\n\n\nPsiphon uses a modified version of the meek protocol described in Section 5. The session ID header contains extra information: a protocol version number and\nthe destination Psiphon server address. As this cookie\nwill be visible to the censor in unfronted mode, its\nvalue is encrypted in a NaCl crypto_box [7] using the\npublic key of the destination meek-server; then obfuscated; then formatted as an innocuous-seeming cookie\nwith a randomly selected key. meek-server uses the protocol version number to determine if the connecting\nmeek-client supports Psiphon-specific protocol enhancements. The destination address is the SSH server to\nwhich meek-server should forward traffic.\nIn Psiphon, meek-client transmits its chosen session ID on its first HTTP request, after which\nmeek-server assigns a distinct ID to be used on subsequent requests. This change allows meek-server to distinguish new and existing sessions when a client sends a\nrequest after a long delay (such as after an Android device awakes from sleeping), when meek-server may have\nalready expired and discarded its session.\nWe ported meek-client, originally written in Go, to\nJava for Android. On Android, we make HTTP and\nHTTPS requests using the Apache HttpClient component, in order to have a TLS fingerprint like those of\nother Android apps making web service requests.\nThe Psiphon meek-server inspects CDN-injected\nheaders, like X-Forwarded-For, to determine the client’s\nIP address. The address is mapped to a geographic region that is used in recording usage statistics.\n\n### 7.2 Server selection\n\nWhen a user starts a Psiphon client, the client initiates connections to up to ten different servers simultaneously, keeping the first to be fully established. Candidate servers are chosen at random from cached lists\nof known servers and a mix of different protocols, both\nfronted and non-fronted, are used. The purpose of the\nsimultaneous connections is to minimize user wait time\nin case certain protocols are blocked, certain servers are\nblocked by address, or certain servers are at capacity\nand rejecting new connections. This process also tends\nto pick the closest data center, and the one with lowest\ncost, as it tends to pick lower-latency direct connections\nover domain-fronted connections.\nWe made two modifications to server selection in\norder to accommodate fronting. First, we changed the\nnotion of an “established connection” from TCP connection completion to full SSH handshake completion.\n\n\n-----\n\nwithout meek\n\nmeek (streaming)\n\nmeek (no streaming)\n\n\n5 s\n\n11 s\n\n21 s\n\n0 10 20\nTime (seconds)\n\n|Col1|1|1 s|Col4|Col5|\n|---|---|---|---|---|\n||||2|1 s|\n\n\n**Fig. 9. Time to download an 11 MB file through Psiphon over**\nthree transports: one-hop obfuscated-openssh proxy without\nmeek; meek with streaming downloads; and meek without\nstreaming downloads. Text labels indicate the mean of the 50\nfastest measurements out of 100. Bulk-download times increase\nby about a factor of 3 when meek is activated. With fixed-size\nHTTP bodies, meek costs about a factor of 4 in download time.\nWith the optimization of unlimited-size HTTP bodies, the download time decreases to about a factor of 2.\n\nThis ensures that both hops are measured in the fronted\ncase. Second, we adjusted our protocol selection schedule to ensure that, while we generally favor the fastest\nconnection, we do not expose the system to an attack\nthat would force us to use a degraded protocol. For example, a censor could use a DPI attack that allows all\nconnections to establish, but then terminate or severely\nthrottle non-whitelisted protocols after some short time\nperiod. If the client detects such degraded conditions,\nit begins to favor fronted and unfronted protocols over\nthe faster obfuscated SSH direct connections.\n\n### 7.3 Performance\n\nWe identified a need to improve the video streaming and\ndownload performance of meek-tunneled traffic. In addressing this, we considered the cost per HTTP request\nof some candidate CDNs, a lack of support for HTTP\npipelining in our components, and a concern about the\nDPI signature of upstream-only or downstream-only\nHTTP connections. As a compromise between these\nconsiderations, we made a tweak to the meek protocol: instead of sending at most 64 KB in each HTTP\nresponse, responses stream as much as possible, as long\nas there is data to send and for up to 200 ms.\nThis tweak yielded a significant performance improvement, with download speeds increasing by up to\n4–5×, and 1080p video playback becoming smooth. Under heavy downstream conditions, we observe response\nbodies up to 1 MB, 300 KB on average, although the exact traffic signature is highly dependent on the tunneled\napplication. We tuned the timeout parameter through\n\n\nsubjective usability testing focused on latency while web\nbrowsing and simultaneously downloading large files.\nFigure 9 compares the time taken to download a\nfile both with and without meek, and with and without\nthe streaming download optimization. The target is the\nsame speedtest.wdc01 URL used in the Tor performance\ntests in Section 5. The performance effect of meek is\nabout a factor-4 increase in download time; streaming\ndownloads cut the increase in half.\n\n## 8 Traffic analysis\n\nIn developing domain fronting circumvention systems,\nwe hope to deprive the censor of easy distinguishers\nand force the use of more expensive, less reliable classification tests—generally, to increase the cost of censorship. We believe that domain fronting, implemented\nwith care, meets the primary challenges of proxy-based\ncircumvention. It defeats IP- and DNS-based blocking\nbecause the IP- and DNS-layer information seen by the\ncensor are not those of the proxy; content-based blocking because content is encrypted under HTTPS; and\nactive probing because though a censor may be able to\ndiscover that a web service is used for circumvention,\nit cannot block the service without incurring significant\ncollateral damage.\nOur experiences with deploying circumvention systems has led us to conclude that other potential means\nof censorship—e.g., identifying circumventing content\nby analyzing packet length distributions—do not currently have relevance when considering the practices of\ntoday’s censors. We speculate that censors find such\ntests unattractive because they require storing significant state and are susceptible to misclassification. More\nbroadly, we are not aware of any nation-level censorship\nevent that made use of such traffic features.\nNevertheless, we expect censors to adapt to a changing environment and to begin deploying more sophisticated (but also more expensive and less reliable) tests.\nThe issue of traffic analysis is a general one [68], and\nmostly separable from domain fronting itself. That is,\ndomain fronting does not preclude various traffic shaping techniques and algorithms, which can be developed\nindependently and plugged in when the censors of the\nworld make them necessary. This section contains a\nsketch of domain fronting’s resistance to certain traffic analysis features, though a case study of meek with\nTor and a trace of non-circumvention traffic. While we\nidentify some features that may give a censor leverage\n\n\n-----\n\n1.00\n\n0.75\n\n0.50\n\n0.25\n\n0.00\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|||LBL Goo|gle H|TTP|S|||||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n||||me|ek o|n|Ap|p Engi|ne||||\n|||||||||||||\n|||||||||||||\n|||||||||||||\n\n\n5 10 24 60 120180 300 600900 1800 3600\nDuration (seconds)\n\n\n**LBL Google HTTPS** **meek on App Engine**\n**0 bytes** **37.6%** **1418 bytes** **40.5%**\n**1430 bytes** **9.1%** **0 bytes** **37.7%**\n**1418 bytes** **8.5%** **1460 bytes** **7.2%**\n**41 bytes** **6.1%** **396 bytes** **2.0%**\n**1416 bytes** **3.1%** **196 bytes** **1.8%**\n**1460 bytes** **2.9%** **1024 bytes** **1.5%**\n\n**Fig. 10. Comparison of TCP payload length distributions in or-**\ndinary HTTPS connections to Google services from the LBL\ntraffic trace, and meek running on App Engine, fronted through\nwww.google.com.\n\n\n**Fig. 11. CDF of connection duration. The x-axis is logarithmic.**\n\nthe two are not grossly different. In both cases, about\n38% of packets are empty (mostly ACKs), with many\npackets near the usual TCP Maximum Segment Size of\n1460 bytes. Conspicuous in the meek trace are a small\npeaks at a few specific lengths, and a lack of short payloads of around 50 bytes. Both of characteristics are\nprobably reflections of the fixed cell size of the underlying Tor stream.\n\n\nin distinguishing circumvention traffic, we believe that\nthe systems we have deployed are sufficiently resistant\nto the censors of today, and do not block the way to\nfuture enhancements to traffic analysis resistance.\nAs domain fronting is based on HTTPS, we evaluate\ndistinguishability from “ordinary” HTTPS traffic. We\ncompare two traffic traces. The first is HTTPS traffic\nfrom Lawrence Berkeley National Laboratory (LBL), a\nlarge (≈ 4K users) research lab, comprising data to and\nfrom TCP port 443 on any Google server. Its size is\n313 MB (packet headers only, not payloads) and it lasts\n10 minutes. The IP addresses in this first trace were\nmasked, replaced by a counter. The second trace is of\nmeek in Tor Browser, browsing the home pages of the\ntop 500 Alexa web sites over Google and App Engine.\nIt is 687 MB in size and covers 4.5 hours.\n\n### 8.1 Packet length distribution\n\n\nA censor could attempt to block an encrypted tunnel\nby its distribution of packet lengths, if it is distinctive\nenough. Figure 10 compares the packet length distributions of the sample traces. Keeping in mind that the\nLBL trace represents many users, operating systems,\nand web browsers, and the meek trace only one of each,\n\n\n### 8.2 Connection lifetime\n\nThe total duration of TCP connections is another potential distinguisher. Figure 11 shows the cumulative\nprobability of connection durations in the two traces.\nThe LBL trace has interesting concentrations on certain round numbers: 10/60/120/180/240 seconds. We\nhypothesize that they are caused by keepalive timeouts\nin web browsers and servers and periodic polling by web\napps. The small rise at 600 seconds is an artifact caused\nby the 10-minute duration of the trace. We do not know\nhow much longer than 10 minutes those connections\nlasted, but they are only 8% of observed connections.\nThe meek trace shows a propensity for longer connections. In 4.5 hours, there were only 10 connections,\nthree of them lasting for an hour. The long connections are caused by the client browser extension’s aggressive use of long-lived HTTP keepalive connections,\nand by its being constantly busy, giving every opportunity for connection reuse. 60% of meek’s connections\nlasted five minutes or longer, while only 13% of ordinary\ntraffic’s did. meek had essentially no connections lasting\nless than 24 seconds, but such short connections were\nover 42% of the LBL trace. 30% (3 out of 10) of meek’s\nconnections lasted almost exactly one hour, evidently\nreflecting a built-in keepalive limit in either the client\nbrowser extension or in App Engine.\nIn light of these measurements, the censor may decide simply to terminate long-lived HTTPS connections.\n\n\n-----\n\nAccording to our traffic trace, doing so will not disrupt\nmore than 8% of ordinary connections (although such\nlong connections may be valuable large transfers with\nhigher collateral damage). The censor can lower the timing threshold, at the cost of more false positives. In order\nto be effective, then censor must cut off the client completely; otherwise the client may start a new connection\nwith the same session ID and begin where it left off.\n\nWe do not know of any obvious traffic characteristics that reliably distinguish domain fronting from\nother HTTPS traffic. Long-lived connections and packet\nlengths are potential targets for a more concerted attack. We are fundamentally trying to solve a problem\nof steganography, to make circumvention traffic fit some\nmodel of “normal” traffic. However, this can be regarded\nas an advantage. What is a challenge for the evaluator\nis also a challenge for the censor, simply because it is\ndifficult to characterize just what normal traffic is, especially behind a CDN that may host variety of services\nsuch as software updates, video streaming, and ordinary web pages. Circumvention traffic need not be perfectly indistinguishable, only indistinguishable enough\nthat that blocking it causes more and costlier false positives than the censor can accept.\n\n## 9 Discussion\n\nDomain fronting derives its strength from the collateral damage that results from blocking the front domain. It should not—nor should any other circumvention technique—be thought of as unblockable; rather,\none should think of what it costs the censor to block\nit. What is unblockable by one censor may be blocked\nby another that has different resources and incentives.\nBlocking resistance depends on the strength of the front\ndomain and on the censor’s cost calculus, which has\nboth economic and social components.\nWe can at least roughly quantify the cost of blocking any domain fronting system in general. It is the\nminimum cost of: blocking a domain; deploying traffic\nanalysis to distinguish circumvention from other traffic;\nor conducting some attack outside our threat model,\nfor example physical surveillance of Internet users. A\ncensor could also, for example, block HTTPS entirely,\nbut that is likely to be even more damaging than targeted blocking of a domain. The cost of blocking a\ndomain—and the benefit of blocking circumvention—\nwill vary by censor. For example, China can afford to\nblock twitter.com and facebook.com partly because it\n\n\nhas domestic replacements for those services, but not\nall censors have the same resources. In June 2014, the\nGreat Firewall of China took the unprecedented step\nof blocking all Google services [2, 25], including all potential fronts for App Engine. It is not clear whether\nthe blocking targeted domain fronting systems like GoAgent; our own systems were only prototypes at that\npoint. Since then, domain fronting to App Engine has\nbeen effectively stamped out in China, though it continues to work over other web services.\nA censor could directly confront the operators of an\nintermediate web service and ask them to disable domain fronting (or simply get rid of customers like us who\nfacilitate circumvention). The censor could threaten to\nblock the service entirely, costing it business. Whether\nsuch an attack succeeds again depends on specific costs\nand motivations. A powerful censor may be able to carry\nout its threat, but others will harm themselves more by\nblocking a valuable service than the circumvention traffic is worth.\nReliance on paid web services creates the potential for a “financial denial of service” attack against\ndomain fronting systems, in which the censor uses the\nservice excessively in an attempt to drive up the operators’ costs. In March 2015, the anticensorship group\nGreatFire, which had used various cloud services for\ncensorship circumvention in China, was the target of\na distributed denial of service attack against their hosting on Amazon Web Services [55]. The attack lasted\nfor days and incurred tens of thousands of dollars in\nbandwidth charges. The attack against Amazon was followed shortly by one against GitHub, the largest in\nthe site’s history [48]. The second attack specifically\ntargeted GreatFire’s accounts there. The available evidence indicates that both attacks were coordinated from\nwithin China, using an offensive network system dubbed\nthe “Great Cannon” [45]. Such an attack could be mitigated by active defenses that shut down a service when\nit is being used excessively, though this only protects\nagainst ruinous costs and will not defeat a long-term\nattack. It is noteworthy that a world-class censor’s first\nreaction was a disruptive, unsubtle denial of service\nattack—though we cannot say for sure that the censor did not have something better up its sleeve. GreatFire speculated that the attacks were precipitated by\nthe publication of an article in the Wall Street Jour_nal [16] that described in detail domain fronting and_\nother “collateral freedom” techniques. The interview associated with the article also caused CloudFlare to begin\nmatching SNI and Host header, in an apparent attempt\nto thwart domain fronting.\n\n\n-----\n\nFronting shares a potential weakness with decoy\nrouting, which is that the network paths to the overt and\ncovert destinations diverge. The difference in paths may\ncreate side channels—different latencies for instance—\nthat distinguish domain-fronted traffic from the traffic\nthat really arrives at its apparent destination. For example, a CDN can be expected to have responses to\nsome fraction of requests already in cache, and respond\nto those requests with low latency, while domain-fronted\nrequests always go all the way to the destination with\nhigher latency. Schuhard et al. [54, §5] applied latency\nmeasurement to decoy routing. The authors of TapDance [69, §5.1] observe that such an attack is difficult\nto carry out in practice, because it requires knowledge\nof the performance characteristics of many diverse resources behind the proxy, some of which are not accessible to the censor (login-protected web pages, for example). Domain fronting favors the circumventor even\nmore, because of the variety of resources behind a CDN.\nThe intermediate web service has a privileged network position from which it may monitor domainfronted traffic. Even though the censor does not know\nwhich client IP addresses are engaging in circumvention, the CDN knows. The risk is especially acute when\nclient browses a web site of the same entity that controls the intermediate web server, for example browsing YouTube while fronting through www.google.com.\nWhen this happens, the web service gets to see both\nentry and exit traffic, and is in a better position to attempt to correlate flows by timing and volume, even\nwhen the underlying channel is an encrypted protocol\nlike Tor. This phenomenon seems hard to counter, because the front domain needs to be a popular one in\norder to have high collateral damage, but popular domains are also the ones that users tend to want to visit.\nIt is in theory possible to dynamically switch between\nmultiple fronts, so as to avoid the situation where the\ndestination and front are under the same control, at the\ncost of leaking information about where the user is not\ngoing at a given moment.\nA censor that can man-in-the-middle HTTPS connections can detect domain fronting merely by removing\nencryption and inspecting the Host header. Unless the\ncensor controls a certificate authority, this attack falls to\nordinary HTTPS certificate validation. Against a censor\nthat controls a trusted certificate authority, certificate\npinning is an effective defense. If the underlying transport is an authenticated and encrypted one like Tor,\nthen the destination and contents of a user’s connection will remain secret, even if the user is outed as a\ncircumventor.\n\n\n## 10 Summary\n\nWe have presented domain fronting, an applicationlayer censorship circumvention technique that uses different domain names at different layers of communication in order to hide the true destination of a message.\nDomain fronting resists the main challenges offered by\nthe censors of today: content blocking, address blocking, and active probing. We have implemented domain\nfronting in three popular circumvention systems: Tor,\nLantern, and Psiphon, and reported on the experience\nof deployment. We begin an investigation into the more\ndifficult, less reliable means of traffic analysis that we\nbelieve will be necessary to block domain fronting.\n\n## Code and acknowledgments\n\n[The meek pluggable transport has a home page at https:](https://trac.torproject.org/projects/tor/wiki/doc/meek)\n[//trac.torproject.org/projects/tor/wiki/doc/meek and](https://trac.torproject.org/projects/tor/wiki/doc/meek)\n[source code at https://gitweb.torproject.org/pluggable-](https://gitweb.torproject.org/pluggable-transports/meek.git)\n[transports/meek.git. The source code of Lantern’s](https://gitweb.torproject.org/pluggable-transports/meek.git)\n[flashlight proxy is at https://github.com/getlantern/](https://github.com/getlantern/flashlight)\n[flashlight; other components are in sibling reposito-](https://github.com/getlantern/flashlight)\n[ries. Psiphon’s source code is at https://bitbucket.org/](https://bitbucket.org/psiphon/psiphon-circumvention-system)\n[psiphon/psiphon-circumvention-system.](https://bitbucket.org/psiphon/psiphon-circumvention-system)\nWe would like to thank Yawning Angel, George Kadianakis, Georg Koppen, Lunar, and the members of\nthe tor-dev, tor-qa, and traffic-obf mailing lists who\nresponded to our design ideas, reviewed source code,\nand tested our prototypes. Arlo Breault wrote the\nflashproxy-reg-appspot program mentioned in Section 3,\nan early application of domain fronting. Leif Ryge and\nJacob Appelbaum tipped us off that domain fronting\nwas possible. Sadia Afroz, Michael Tschantz, and Doug\nTygar were sources of inspiring conversation. Johanna\nAmann provided us with an estimate of the fraction of\nSNI-bearing TLS handshakes.\nThis work was supported in part by the National\nScience Foundation under grant 1223717. The opinions,\nfindings, and conclusions expressed herein are those of\nthe authors and do not necessarily reflect the views of\nthe sponsors.\n\n\n-----\n\n## References\n\n[1] [Akamai. http://www.akamai.com/.](http://www.akamai.com/)\n\n[2] P. Alpha. Google disrupted prior to Tiananmen anniversary; mirror sites enable uncensored access to information,\n[June 2014. https://en.greatfire.org/blog/2014/jun/google-](https://en.greatfire.org/blog/2014/jun/google-disrupted-prior-tiananmen-anniversary-mirror-sites-enable-uncensored-access)\n[disrupted-prior-tiananmen-anniversary-mirror-sites-enable-](https://en.greatfire.org/blog/2014/jun/google-disrupted-prior-tiananmen-anniversary-mirror-sites-enable-uncensored-access)\n[uncensored-access.](https://en.greatfire.org/blog/2014/jun/google-disrupted-prior-tiananmen-anniversary-mirror-sites-enable-uncensored-access)\n\n[3] [Amazon CloudFront. https://aws.amazon.com/cloudfront/.](https://aws.amazon.com/cloudfront/)\n\n[4] Y. Angel and P. Winter. obfs4 (the obfourscator), May\n[2014. https://gitweb.torproject.org/pluggable-transports/](https://gitweb.torproject.org/pluggable-transports/obfs4.git/tree/doc/obfs4-spec.txt)\n[obfs4.git/tree/doc/obfs4-spec.txt.](https://gitweb.torproject.org/pluggable-transports/obfs4.git/tree/doc/obfs4-spec.txt)\n\n[5] J. Appelbaum and N. Mathewson. Pluggable transport\n[specification, Oct. 2010. https://gitweb.torproject.org/](https://gitweb.torproject.org/torspec.git/tree/pt-spec.txt)\n[torspec.git/tree/pt-spec.txt.](https://gitweb.torproject.org/torspec.git/tree/pt-spec.txt)\n\n[6] ASL19 and Psiphon. Information controls: Iran’s presidential\n[elections. Technical report, 2013. https://asl19.org/cctr/](https://asl19.org/cctr/iran-2013election-report/)\n[iran-2013election-report/.](https://asl19.org/cctr/iran-2013election-report/)\n\n[7] D. J. Bernstein, T. Lange, and P. Schwabe. Public-key\n[authenticated encryption: crypto_box, Aug. 2010. http:](http://nacl.cr.yp.to/box.html)\n[//nacl.cr.yp.to/box.html.](http://nacl.cr.yp.to/box.html)\n\n[8] B. Boe. Bypassing Gogo’s inflight Internet authentication,\n[Mar. 2012. http://bryceboe.com/2012/03/12/bypassing-](http://bryceboe.com/2012/03/12/bypassing-gogos-inflight-internet-authentication/)\n[gogos-inflight-internet-authentication/.](http://bryceboe.com/2012/03/12/bypassing-gogos-inflight-internet-authentication/)\n\n[9] [BridgeDB. https://bridges.torproject.org/.](https://bridges.torproject.org/)\n\n[10] C. Brubaker, A. Houmansadr, and V. Shmatikov. CloudTransport: Using cloud storage for censorship-resistant\nnetworking. In Proceedings of the 14th Privacy Enhanc_ing Technologies Symposium (PETS), July 2014. http:_\n[//www.cs.utexas.edu/~amir/papers/CloudTransport.pdf.](http://www.cs.utexas.edu/~amir/papers/CloudTransport.pdf)\n\n[11] S. Burnett, N. Feamster, and S. Vempala. Chipping away at\ncensorship firewalls with user-generated content. In USENIX\n_Security Symposium, Washington, DC, USA, Aug. 2010._\n[USENIX. https://www.usenix.org/event/sec10/tech/full_](https://www.usenix.org/event/sec10/tech/full_papers/Burnett.pdf)\n[papers/Burnett.pdf.](https://www.usenix.org/event/sec10/tech/full_papers/Burnett.pdf)\n\n[[12] CloudFlare. https://www.cloudflare.com/.](https://www.cloudflare.com/)\n\n[13] T. Dierks and E. Rescorla. RFC 5246: The Transport Layer\n[Security (TLS) Protocol Version 1.2, Aug. 2008. https:](https://tools.ietf.org/html/rfc5246)\n[//tools.ietf.org/html/rfc5246.](https://tools.ietf.org/html/rfc5246)\n\n[14] R. Dingledine. Obfsproxy: the next step in the censorship\n[arms race, Feb. 2012. https://blog.torproject.org/blog/](https://blog.torproject.org/blog/obfsproxy-next-step-censorship-arms-race)\n[obfsproxy-next-step-censorship-arms-race.](https://blog.torproject.org/blog/obfsproxy-next-step-censorship-arms-race)\n\n[15] R. Dingledine and N. Mathewson. Design of a blockingresistant anonymity system. Technical Report 2006-11-001,\n[Tor Project, Nov. 2006. https://research.torproject.org/](https://research.torproject.org/techreports/blocking-2006-11.pdf)\n[techreports/blocking-2006-11.pdf.](https://research.torproject.org/techreports/blocking-2006-11.pdf)\n\n[16] E. Dou and A. Barr. U.S. cloud providers face backlash\n[from China’s censors. Wall Street Journal, Mar. 2015. http:](http://www.wsj.com/articles/u-s-cloud-providers-face-backlash-from-chinas-censors-1426541126)\n[//www.wsj.com/articles/u-s-cloud-providers-face-backlash-](http://www.wsj.com/articles/u-s-cloud-providers-face-backlash-from-chinas-censors-1426541126)\n[from-chinas-censors-1426541126.](http://www.wsj.com/articles/u-s-cloud-providers-face-backlash-from-chinas-censors-1426541126)\n\n[17] K. P. Dyer, S. E. Coull, T. Ristenpart, and T. Shrimpton. Protocol misidentification made easy with formattransforming encryption. In Proceedings of the 20th ACM\n_conference on Computer and Communications Security_\n_(CCS), Nov. 2013._ [https://kpdyer.com/publications/](https://kpdyer.com/publications/ccs2013-fte.pdf)\n[ccs2013-fte.pdf.](https://kpdyer.com/publications/ccs2013-fte.pdf)\n\n[18] D. Eastlake. RFC 6066: Transport Layer Security (TLS)\n[extensions: Extension definitions, Jan. 2011. https://tools.](https://tools.ietf.org/html/rfc6066)\n[ietf.org/html/rfc6066.](https://tools.ietf.org/html/rfc6066)\n\n\n\n[[19] Fastly. http://www.fastly.com/.](http://www.fastly.com/)\n\n[20] R. Fielding, J. Gettys, J. Mogul, H. Frystyk, L. Masinter,\nP. Leach, and T. Berners-Lee. RFC 2616: Hypertext transfer\n[protocol — HTTP/1.1, June 1999. https://tools.ietf.org/](https://tools.ietf.org/html/rfc2616)\n[html/rfc2616.](https://tools.ietf.org/html/rfc2616)\n\n[21] D. Fifield. Summary of meek’s costs, April 2015, May 2015.\n[https://lists.torproject.org/pipermail/tor-dev/2015-May/](https://lists.torproject.org/pipermail/tor-dev/2015-May/008767.html)\n[008767.html.](https://lists.torproject.org/pipermail/tor-dev/2015-May/008767.html)\n\n[22] D. Fifield, N. Hardison, J. Ellithorpe, E. Stark, R. Dingledine, P. Porras, and D. Boneh. Evading censorship with\nbrowser-based proxies. In Proceedings of the 12th Privacy\n_Enhancing Technologies Symposium (PETS). Springer, July_\n[2012. https://crypto.stanford.edu/flashproxy/flashproxy.pdf.](https://crypto.stanford.edu/flashproxy/flashproxy.pdf)\n\n[23] J. Geddes, M. Schuchard, and N. Hopper. Cover your\nACKs: Pitfalls of covert channel censorship circumvention. In Proceedings of the 20th ACM conference on Com_puter and Communications Security (CCS), Nov. 2013._\n[http://www-users.cs.umn.edu/~hopper/ccs13-cya.pdf.](http://www-users.cs.umn.edu/~hopper/ccs13-cya.pdf)\n\n[[24] GoAgent. https://github.com/goagent/goagent.](https://github.com/goagent/goagent)\n\n[25] Google. Google Transparency Report: China, all products,\n[May 31, 2014–present, July 2014. https://www.google.com/](https://www.google.com/transparencyreport/traffic/disruptions/124/)\n[transparencyreport/traffic/disruptions/124/.](https://www.google.com/transparencyreport/traffic/disruptions/124/)\n\n[[26] Google App Engine. https://cloud.google.com/appengine/.](https://cloud.google.com/appengine/)\n\n[27] GreatFire.org. https://a248.e.akamai.net is 100% blocked in\n[China. https://en.greatfire.org/https/a248.e.akamai.net.](https://en.greatfire.org/https/a248.e.akamai.net)\n\n[28] A. Houmansadr, C. Brubaker, and V. Shmatikov. The parrot\nis dead: Observing unobservable network communications.\nIn Proceedings of the 2013 IEEE Symposium on Security\n_[and Privacy, May 2013. http://www.cs.utexas.edu/~amir/](http://www.cs.utexas.edu/~amir/papers/parrot.pdf)_\n[papers/parrot.pdf.](http://www.cs.utexas.edu/~amir/papers/parrot.pdf)\n\n[29] A. Houmansadr, G. T. K. Nguyen, M. Caesar, and\nN. Borisov. Cirripede: Circumvention infrastructure using\nrouter redirection with plausible deniability. In Proceedings\n_of the 18th ACM conference on Computer and Communi-_\n_[cations Security (CCS), Oct. 2011. http://hatswitch.org/](http://hatswitch.org/~nikita/papers/cirripede-ccs11.pdf)_\n[~nikita/papers/cirripede-ccs11.pdf.](http://hatswitch.org/~nikita/papers/cirripede-ccs11.pdf)\n\n[30] A. Houmansadr, T. Riedl, N. Borisov, and A. Singer. I\nwant my voice to be heard: IP over voice-over-IP for\nunobservable censorship circumvention. In Proceed_ings of the 20th Network and Distributed System Se-_\n_curity Symposium (NDSS). Internet Society, Feb. 2013._\n[http://www.cs.utexas.edu/~amir/papers/FreeWave.pdf.](http://www.cs.utexas.edu/~amir/papers/FreeWave.pdf)\n\n[31] A. Houmansadr, E. L. Wong, and V. Shmatikov. No direction home: The true cost of routing around decoys.\nIn Proceedings of the 21st Network and Distributed Se_curity Symposium (NDSS). Internet Society, Feb. 2014._\n[http://www.cs.utexas.edu/~amir/papers/DecoyCosts.pdf.](http://www.cs.utexas.edu/~amir/papers/DecoyCosts.pdf)\n\n[[32] The ICSI certificate notary. http://notary.icsi.berkeley.edu/.](http://notary.icsi.berkeley.edu/)\n\n[33] G. Kadianakis and N. Mathewson. obfs2 (the twobfusca[tor), Jan. 2011. https://gitweb.torproject.org/pluggable-](https://gitweb.torproject.org/pluggable-transports/obfsproxy.git/tree/doc/obfs2/obfs2-protocol-spec.txt)\n[transports/obfsproxy.git/tree/doc/obfs2/obfs2-protocol-](https://gitweb.torproject.org/pluggable-transports/obfsproxy.git/tree/doc/obfs2/obfs2-protocol-spec.txt)\n[spec.txt.](https://gitweb.torproject.org/pluggable-transports/obfsproxy.git/tree/doc/obfs2/obfs2-protocol-spec.txt)\n\n[34] G. Kadianakis and N. Mathewson. obfs3 (the threebfusca[tor), Jan. 2013. https://gitweb.torproject.org/pluggable-](https://gitweb.torproject.org/pluggable-transports/obfsproxy.git/tree/doc/obfs3/obfs3-protocol-spec.txt)\n[transports/obfsproxy.git/tree/doc/obfs3/obfs3-protocol-](https://gitweb.torproject.org/pluggable-transports/obfsproxy.git/tree/doc/obfs3/obfs3-protocol-spec.txt)\n[spec.txt.](https://gitweb.torproject.org/pluggable-transports/obfsproxy.git/tree/doc/obfs3/obfs3-protocol-spec.txt)\n\n[35] J. Karlin, D. Ellard, A. W. Jackson, C. E. Jones, G. Lauer,\nD. P. Mankins, and W. T. Strayer. Decoy routing: Toward\nunblockable internet communication. In Proceedings of the\n_USENIX Workshop on Free and Open Communications on_\n\n\n-----\n\n_[the Internet (FOCI), Aug. 2011. https://www.usenix.org/](https://www.usenix.org/events/foci11/tech/final_files/Karlin.pdf)_\n[events/foci11/tech/final_files/Karlin.pdf.](https://www.usenix.org/events/foci11/tech/final_files/Karlin.pdf)\n\n[36] Lantern. connpool. [https://github.com/getlantern/](https://github.com/getlantern/connpool)\n[connpool.](https://github.com/getlantern/connpool)\n\n[[37] Lantern. enproxy. https://github.com/getlantern/enproxy.](https://github.com/getlantern/enproxy)\n\n[38] Lantern. flashlight. [https://github.com/getlantern/](https://github.com/getlantern/flashlight-build)\n[flashlight-build.](https://github.com/getlantern/flashlight-build)\n\n[[39] Lantern. fronted. https://github.com/getlantern/fronted.](https://github.com/getlantern/fronted)\n\n[[40] Lantern. https://getlantern.org/.](https://getlantern.org/)\n\n[[41] B. Leidl. obfuscated-openssh, Apr. 2010. https://github.](https://github.com/brl/obfuscated-openssh)\n[com/brl/obfuscated-openssh.](https://github.com/brl/obfuscated-openssh)\n\n[[42] Level 3. http://www.level3.com.](http://www.level3.com)\n\n[43] K. Loesing. Counting daily bridge users. Technical Report\n[2012-10-001, Tor Project, Oct. 2012. https://research.](https://research.torproject.org/techreports/counting-daily-bridge-users-2012-10-24.pdf)\n[torproject.org/techreports/counting-daily-bridge-users-2012-](https://research.torproject.org/techreports/counting-daily-bridge-users-2012-10-24.pdf)\n[10-24.pdf.](https://research.torproject.org/techreports/counting-daily-bridge-users-2012-10-24.pdf)\n\n[[44] M. Majkowski. SSL fingerprinting for p0f, June 2012. https:](https://idea.popcount.org/2012-06-17-ssl-fingerprinting-for-p0f/)\n[//idea.popcount.org/2012-06-17-ssl-fingerprinting-for-p0f/.](https://idea.popcount.org/2012-06-17-ssl-fingerprinting-for-p0f/)\n\n[45] B. Marczak, N. Weaver, J. Dalek, R. Ensafi, D. Fifield,\nS. McKune, A. Rey, J. Scott-Railton, R. Deibert, and\n[V. Paxson. China’s Great Cannon. https://citizenlab.org/](https://citizenlab.org/2015/04/chinas-great-cannon/)\n[2015/04/chinas-great-cannon/.](https://citizenlab.org/2015/04/chinas-great-cannon/)\n\n[[46] Microsoft Azure. https://azure.microsoft.com/.](https://azure.microsoft.com/)\n\n[47] H. M. Moghaddam, B. Li, M. Derakhshani, and I. Goldberg. SkypeMorph: Protocol obfuscation for Tor bridges.\nIn Proceedings of the 19th ACM conference on Com_puter and Communications Security (CCS), Oct. 2012._\n[https://cs.uwaterloo.ca/~iang/pubs/skypemorph-ccs.pdf.](https://cs.uwaterloo.ca/~iang/pubs/skypemorph-ccs.pdf)\n\n[[48] J. Newland. Large scale DDoS attack on github.com. https:](https://github.com/blog/1981-large-scale-ddos-attack-on-github-com)\n[//github.com/blog/1981-large-scale-ddos-attack-on-github-](https://github.com/blog/1981-large-scale-ddos-attack-on-github-com)\n[com.](https://github.com/blog/1981-large-scale-ddos-attack-on-github-com)\n\n[49] E. Nygren, R. K. Sitaraman, and J. Sun. The Akamai network: A platform for high-performance Internet applications. ACM SIGOPS Operating Systems Review, 44(3):2–19,\n[2010. http://www.akamai.com/dl/technical_publications/](http://www.akamai.com/dl/technical_publications/network_overview_osr.pdf)\n[network_overview_osr.pdf.](http://www.akamai.com/dl/technical_publications/network_overview_osr.pdf)\n\n[[50] M. Perry. Tor Browser 4.0 is released, Oct. 2014. https:](https://blog.torproject.org/blog/tor-browser-40-released)\n[//blog.torproject.org/blog/tor-browser-40-released.](https://blog.torproject.org/blog/tor-browser-40-released)\n\n[51] M. Perry, E. Clark, and S. Murdoch. The design and implementation of the Tor Browser. Technical report, Tor Project,\n[Mar. 2013. https://www.torproject.org/projects/torbrowser/](https://www.torproject.org/projects/torbrowser/design/)\n[design/.](https://www.torproject.org/projects/torbrowser/design/)\n\n[52] Psiphon Team. A technical description of Psiphon, Mar.\n[2014. https://psiphon.ca/en/blog/psiphon-a-technical-](https://psiphon.ca/en/blog/psiphon-a-technical-description)\n[description.](https://psiphon.ca/en/blog/psiphon-a-technical-description)\n\n[53] D. Robinson, H. Yu, and A. An. Collateral freedom:\nA snapshot of Chinese users circumventing censorship.\nTechnical report, Open Internet Tools Project, May 2013.\n[https://openitp.org/pdfs/CollateralFreedom.pdf.](https://openitp.org/pdfs/CollateralFreedom.pdf)\n\n[54] M. Schuchard, J. Geddes, C. Thompson, and N. Hopper.\nRouting around decoys. In Proceedings of the 19th ACM\n_conference on Computer and Communications Security_\n_[(CCS), Oct. 2012. http://www-users.cs.umn.edu/~hopper/](http://www-users.cs.umn.edu/~hopper/decoy-ccs12.pdf)_\n[decoy-ccs12.pdf.](http://www-users.cs.umn.edu/~hopper/decoy-ccs12.pdf)\n\n[[55] C. Smith. We are under attack, Mar. 2015. https://en.](https://en.greatfire.org/blog/2015/mar/we-are-under-attack)\n[greatfire.org/blog/2015/mar/we-are-under-attack.](https://en.greatfire.org/blog/2015/mar/we-are-under-attack)\n\n[56] Y. Sovran, J. Li, and L. Submaranian. Unblocking the Internet: Social networks foil censors. Technical Report TR2008918, Computer Science Department, New York University,\n[Sept. 2009. http://kscope.news.cs.nyu.edu/pub/TR-2008-](http://kscope.news.cs.nyu.edu/pub/TR-2008-918.pdf)\n\n\n[918.pdf.](http://kscope.news.cs.nyu.edu/pub/TR-2008-918.pdf)\n\n[57] Tor Project. #4744: GFW probes based on Tor’s SSL cipher\n[list, Dec. 2011. https://bugs.torproject.org/4744.](https://bugs.torproject.org/4744)\n\n[58] Tor Project. #8860: Registration over App Engine, May\n[2013. https://bugs.torproject.org/8860.](https://bugs.torproject.org/8860)\n\n[59] Tor Project. #12778: Put meek HTTP headers on a diet,\n[Aug. 2014. https://bugs.torproject.org/12778.](https://bugs.torproject.org/12778)\n\n[60] Tor Project. Bridge users using transport meek, May 2015.\n[https://metrics.torproject.org/userstats-bridge-transport.](https://metrics.torproject.org/userstats-bridge-transport.html?graph=userstats-bridge-transport&end=2015-05-15&transport=meek)\n[html?graph=userstats-bridge-transport&end=2015-05-15&](https://metrics.torproject.org/userstats-bridge-transport.html?graph=userstats-bridge-transport&end=2015-05-15&transport=meek)\n[transport=meek.](https://metrics.torproject.org/userstats-bridge-transport.html?graph=userstats-bridge-transport&end=2015-05-15&transport=meek)\n\n[61] Tor Project. Bridge users using transport obfs3, May 2015.\n[https://metrics.torproject.org/userstats-bridge-transport.](https://metrics.torproject.org/userstats-bridge-transport.html?graph=userstats-bridge-transport&end=2015-05-15&transport=obfs3)\n[html?graph=userstats-bridge-transport&end=2015-05-15&](https://metrics.torproject.org/userstats-bridge-transport.html?graph=userstats-bridge-transport&end=2015-05-15&transport=obfs3)\n[transport=obfs3.](https://metrics.torproject.org/userstats-bridge-transport.html?graph=userstats-bridge-transport&end=2015-05-15&transport=obfs3)\n\n[62] Q. Wang, X. Gong, G. T. K. Nguyen, A. Houmansadr, and\nN. Borisov. CensorSpoofer: Asymmetric communication\nusing IP spoofing for censorship-resistant web browsing.\nIn Proceedings of the 19th ACM conference on Computer\n_[and Communications Security (CCS), Oct. 2012. https://](https://netfiles.uiuc.edu/qwang26/www/publications/censorspoofer.pdf)_\n[netfiles.uiuc.edu/qwang26/www/publications/censorspoofer.](https://netfiles.uiuc.edu/qwang26/www/publications/censorspoofer.pdf)\n[pdf.](https://netfiles.uiuc.edu/qwang26/www/publications/censorspoofer.pdf)\n\n[63] Z. Weinberg, J. Wang, V. Yegneswaran, L. Briesemeister, S. Cheung, F. Wang, and D. Boneh. StegoTorus:\nA camouflage proxy for the Tor anonymity system. In\n_Proceedings of the 19th ACM conference on Computer_\n_and Communications Security (CCS), Oct. 2012._ http:\n[//www.owlfolio.org/media/2010/05/stegotorus.pdf.](http://www.owlfolio.org/media/2010/05/stegotorus.pdf)\n\n[64] T. Wilde. Great Firewall Tor probing circa 09 DEC 2011.\n[Technical report, Team Cymru, Jan. 2012. https://gist.](https://gist.github.com/da3c7a9af01d74cd7de7)\n[github.com/da3c7a9af01d74cd7de7.](https://gist.github.com/da3c7a9af01d74cd7de7)\n\n[65] B. Wiley. Dust: A blocking-resistant internet transport\nprotocol. Technical report, School of Information, University\n[of Texas at Austin, 2011. http://blanu.net/Dust.pdf https:](http://blanu.net/Dust.pdf)\n[//github.com/blanu/Dust/blob/master/hs/README.](https://github.com/blanu/Dust/blob/master/hs/README)\n\n[66] P. Winter and S. Lindskog. How the Great Firewall of China\nis blocking Tor. In Proceedings of the USENIX Work_shop on Free and Open Communications on the Internet_\n_[(FOCI), Aug. 2012. https://www.usenix.org/system/files/](https://www.usenix.org/system/files/conference/foci12/foci12-final2.pdf)_\n[conference/foci12/foci12-final2.pdf.](https://www.usenix.org/system/files/conference/foci12/foci12-final2.pdf)\n\n[67] P. Winter, T. Pulls, and J. Fuss. ScrambleSuit: A polymorphic network protocol to circumvent censorship. In Proceed_ings of the Workshop on Privacy in the Electronic Society_\n_[(WPES). ACM, Nov. 2013. http://www.cs.kau.se/philwint/](http://www.cs.kau.se/philwint/pdf/wpes2013.pdf)_\n[pdf/wpes2013.pdf.](http://www.cs.kau.se/philwint/pdf/wpes2013.pdf)\n\n[68] C. Wright, S. Coull, and F. Monrose. Traffic morphing: An efficient defense against statistical traffic analysis. In Proceedings of the 16th Network and Distributed\n_Security Symposium (NDSS). IEEE, Feb. 2009._ https:\n[//www.internetsociety.org/sites/default/files/wright.pdf.](https://www.internetsociety.org/sites/default/files/wright.pdf)\n\n[69] E. Wustrow, C. M. Swanson, and J. A. Halderman. TapDance: End-to-middle anticensorship without flow blocking. In Proceedings of the 23rd USENIX Security Sym_posium, San Diego, CA, Aug. 2014. USENIX Association._\n[https://jhalderm.com/pub/papers/tapdance-sec14.pdf.](https://jhalderm.com/pub/papers/tapdance-sec14.pdf)\n\n[70] E. Wustrow, S. Wolchok, I. Goldberg, and J. A. Halderman.\nTelex: Anticensorship in the network infrastructure. In Pro_ceedings of the 20th USENIX Security Symposium, Aug._\n[2011. https://www.usenix.org/events/sec/tech/full_papers/](https://www.usenix.org/events/sec/tech/full_papers/Wustrow.pdf)\n[Wustrow.pdf.](https://www.usenix.org/events/sec/tech/full_papers/Wustrow.pdf)\n\n\n-----\n\n## A Sample TLS fingerprints\n\n\n**(a) Go 1.4.2’s crypto/tls library**\n```\nCiphersuites (13):\n TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n TLS_ECDHE_RSA_WITH_RC4_128_SHA\n TLS_ECDHE_ECDSA_WITH_RC4_128_SHA\n TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\n TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\n TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\n TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\n TLS_RSA_WITH_RC4_128_SHA\n TLS_RSA_WITH_AES_128_CBC_SHA\n TLS_RSA_WITH_AES_256_CBC_SHA\n TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA\n TLS_RSA_WITH_3DES_EDE_CBC_SHA\nExtensions (6):\n server_name\n status_request\n elliptic_curves\n ec_point_formats\n signature_algorithms\n renegotiation_info\n\n```\n\n**(b) Firefox 31**\n```\nCiphersuites (23):\n TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\n TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\n TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\n TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\n TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA\n TLS_ECDHE_ECDSA_WITH_RC4_128_SHA\n TLS_ECDHE_RSA_WITH_RC4_128_SHA\n TLS_DHE_RSA_WITH_AES_128_CBC_SHA\n TLS_DHE_DSS_WITH_AES_128_CBC_SHA\n TLS_DHE_RSA_WITH_CAMELLIA_128_CBC_SHA\n TLS_DHE_RSA_WITH_AES_256_CBC_SHA\n TLS_DHE_DSS_WITH_AES_256_CBC_SHA\n TLS_DHE_RSA_WITH_CAMELLIA_256_CBC_SHA\n TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA\n TLS_RSA_WITH_AES_128_CBC_SHA\n TLS_RSA_WITH_CAMELLIA_128_CBC_SHA\n TLS_RSA_WITH_AES_256_CBC_SHA\n TLS_RSA_WITH_CAMELLIA_256_CBC_SHA\n TLS_RSA_WITH_3DES_EDE_CBC_SHA\n TLS_RSA_WITH_RC4_128_SHA\n TLS_RSA_WITH_RC4_128_MD5\nExtensions (8):\n server_name\n renegotiation_info\n elliptic_curves\n ec_point_formats\n SessionTicket TLS\n next_protocol_negotiation\n status_request\n signature_algorithms\n\n```\n\n**(c) Chrome 40**\n```\nCiphersuites (18):\n TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\n TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\n TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\n TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\n TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\n TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\n TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\n TLS_ECDHE_ECDSA_WITH_RC4_128_SHA\n TLS_ECDHE_RSA_WITH_RC4_128_SHA\n TLS_DHE_RSA_WITH_AES_128_CBC_SHA\n TLS_DHE_DSS_WITH_AES_128_CBC_SHA\n TLS_DHE_RSA_WITH_AES_256_CBC_SHA\n TLS_RSA_WITH_AES_128_GCM_SHA256\n TLS_RSA_WITH_AES_128_CBC_SHA\n TLS_RSA_WITH_AES_256_CBC_SHA\n TLS_RSA_WITH_3DES_EDE_CBC_SHA\n TLS_RSA_WITH_RC4_128_SHA\n TLS_RSA_WITH_RC4_128_MD5\nExtensions (10):\n server_name\n renegotiation_info\n elliptic_curves\n ec_point_formats\n SessionTicket TLS\n next_protocol_negotiation\n Application Layer Protocol Negotiation\n Channel ID\n status_request\n signature_algorithms\n\n```\n\n**Fig. 12. Selected differences in ClientHello messages in three different TLS implementations. Even though the contents of application**\ndata records are hidden by encryption, the plaintext headers of TLS reveal information about the implementation. This figure illustrates the need to disguise the TLS fingerprint so that it is not easily identified as pertaining to a circumvention tool.\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "99fdc3ef-333d-48f5-a4a1-becd788c7b80",
            "created_at": "2022-10-25T15:28:29.802983Z",
            "updated_at": "2022-10-25T15:28:29.802983Z",
            "deleted_at": null,
            "name": "MITRE",
            "url": "https://github.com/mitre-attack/attack-stix-data",
            "description": "MITRE ATT&CK STIX Data",
            "reports": null
        }
    ],
    "references": [
        "http://www.icir.org/vern/papers/meek-PETS-2015.pdf"
    ],
    "report_names": [
        "meek-PETS-2015.pdf"
    ],
    "threat_actors": [
        {
            "id": "2864e40a-f233-4618-ac61-b03760a41cbb",
            "created_at": "2023-12-01T02:02:34.272108Z",
            "updated_at": "2025-03-27T02:02:10.209072Z",
            "deleted_at": null,
            "main_name": "WildCard",
            "aliases": [],
            "source_name": "ETDA:WildCard",
            "tools": [
                "RustDown",
                "SysJoker"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "256a6a2d-e8a2-4497-b399-628a7fad4b3e",
            "created_at": "2023-11-30T02:00:07.299845Z",
            "updated_at": "2025-03-27T02:00:03.257794Z",
            "deleted_at": null,
            "main_name": "WildCard",
            "aliases": [],
            "source_name": "MISPGALAXY:WildCard",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        }
    ],
    "ts_created_at": 1666716503,
    "ts_updated_at": 1743041799,
    "ts_creation_date": 1431710406,
    "ts_modification_date": 1431710406,
    "files": {
        "pdf": "https://archive.orkl.eu/ada0fc17b33be6fd19b66d7406794adda4f1fbb4.pdf",
        "text": "https://archive.orkl.eu/ada0fc17b33be6fd19b66d7406794adda4f1fbb4.txt",
        "img": "https://archive.orkl.eu/ada0fc17b33be6fd19b66d7406794adda4f1fbb4.jpg"
    }
}