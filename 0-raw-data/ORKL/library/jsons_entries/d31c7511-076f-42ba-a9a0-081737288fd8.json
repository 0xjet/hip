{
    "id": "d31c7511-076f-42ba-a9a0-081737288fd8",
    "created_at": "2023-01-12T15:02:52.421888Z",
    "updated_at": "2025-03-27T02:13:58.301369Z",
    "deleted_at": null,
    "sha1_hash": "26e9445c7405669fcb5905440a8d4ce3dac863bf",
    "title": "2021-10-04 - Threat hunting in large datasets by clustering security events",
    "authors": "",
    "file_creation_date": "2022-05-28T17:31:11Z",
    "file_modification_date": "2022-05-28T17:31:11Z",
    "file_size": 1989318,
    "plain_text": "# Threat hunting in large datasets by clustering security events\n\n**[blog.talosintelligence.com/2021/10/threat-hunting-in-large-datasets-by.html](https://blog.talosintelligence.com/2021/10/threat-hunting-in-large-datasets-by.html)**\n\nWatch Video At:\n\nhttps://youtu.be/5nFIgAN5Nq4\n\n\n-----\n\n_By_ _[Tiago Pereira.](https://www.google.com/url?q=https://twitter.com/t14g0p&sa=D&source=editors&ust=1633368289972000&usg=AOvVaw0KZ3ZY2zV3jK39qbIqBhe0)_\n\nSecurity tools can produce very large amounts of data that even the most sophisticated\norganizations may struggle to manage.\nBig data processing tools, such as spark, can be a powerful tool in the arsenal of\nsecurity teams.\nThis post walks through threat hunting on large datasets by clustering similar events to\nreduce search space and provide additional context.\n\n## Introduction\n\nCisco Talos processes 1.5 million new pieces of malware each day. Converting such a large\ndataset into actionable intelligence requires a combination of automated tools to process the\ndata, and human ingenuity to spot the data that stands out.\n\nThere is a limit to the amount of information that humans can process. Even the most\nsophisticated organizations may struggle with the sheer amount of data generated from\nmodern security systems — hence the need for data-processing tools to reduce this vast\namount of data into manageable information that can be processed manually, if required.\n\nAs an example, we'd like to walk through how we hunted for new threats by using a big data\n[processing tool/library — Apache Spark — to group large amounts of suspicious events into](https://www.google.com/url?q=https://spark.apache.org/&sa=D&source=editors&ust=1633368289975000&usg=AOvVaw2l5aRkTvFHtfLgIGBYItuK)\nmanageable groups. This technique may be useful to organizations of all sizes to handle\nlarge amounts of security events efficiently, or it could inspire some ideas to improve existing\ntools.\n\n## Clustering data\n\nWe'll describe a technique Cisco Talos uses to create clusters from a dataset of suspicious\nevent logs that have been generated by our own tools. This will group items into clusters of\nsimilar, but not identical, events or sequences of events. Since we are searching only for\nsimilar items, the events don't need to be exactly the same to be grouped together. Small\ndifferences such as usernames, paths, capitalization and commands are accepted in similar\nevents. This allows us to transform a massive list of security event logs into a manageable\nlist of groups of similar events that can be processed by an analyst. The groups won't always\nbe perfect. However, they are better for human processing, and humans are very good at\ndealing with imperfect data.\n\n\n-----\n\nAlthough we use data generated by our own tools, the method described is generic and can\nbe used by organizations of all sizes and on varying datasets from several sources, such as\nWindows logs, security solution logs (e.g., SIEM, Cisco Secure Endpoint) or proxy logs. The\nonly requirements for this method are an available Spark cluster and data stored in a\nmedium that is appropriate for Spark, such as CSV or JSON files in a cloud or a large\nphysical storage system.\n\nIt is also worth mentioning that the method shown here is not the only clustering option.\nHowever, it uses algorithms that are suited for processing very large volumes of data, is\ngeneric enough to be easily adapted to different datasets, and only makes use of the free\nand convenient available Spark libraries.\n\n### Preparing data for clustering\n\nThe base concept of the system is very simple: We represent each of our items as a set of\n\"tokens\" and then compare how similar the set derived from one item is to the other sets.\nThis allows us to find items that are most similar to each other, even if they are subtly\ndifferent.\n\nThese \"tokens\" are very similar to words in a book. If you represent a book as a set of words,\nyou can identify books that can be grouped together based on the words they share. With\nthis technique, you could group together English-language books or ones written in Spanish\nor German. By applying tighter criteria for clustering, we can identify groups of books that\nmention \"malware,\" \"computer,\" or \"vulnerability\" separate from another group that may\nmention \"Dumbledore,\" \"Hagrid\" or \"Voldemort.\"\n\nHowever, before any clustering takes place, we need to load and prepare the data for\nprocessing.\n\nThe first step is to load pyspark and import a few necessary libraries:\n\n[CountVectorizer is part of the machine learning package pyspark.ml and transforms the data](https://www.google.com/url?q=https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.CountVectorizer.html&sa=D&source=editors&ust=1633368289980000&usg=AOvVaw2JGvMCfdyI4fevihEHvGu5)\n[into a format that is used by many ML algorithms. The MinhashLSH package will be used to](https://www.google.com/url?q=https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.MinHashLSH.html%23pyspark.ml.feature.MinHashLSH&sa=D&source=editors&ust=1633368289981000&usg=AOvVaw1bVT3S2gLAv08QqWKoO9oY)\nreduce the amount of data that needs to be processed and to calculate a set of similar event\npairs. The [graphframes library is a pyspark graph library based on Spark dataframes.](https://www.google.com/url?q=https://graphframes.github.io/graphframes/docs/_site/index.html&sa=D&source=editors&ust=1633368289981000&usg=AOvVaw21yQm3ehy4qSp3BRt13QM6)\n\nOnce the environment is ready, we will start by loading the data and immediately start\ntransforming it. The first commands follow:\n\n\n-----\n\nOn the first line, the data is read from its path using Spark's read method. On the second\nline, a set of functions are called that concatenate the short_description and argv fields, then\nsplit them by each non-alphanumeric character and finally \"explode\" them, creating one row\nper word. Finally, on the third line, the words are grouped by the system where they were\nseen. As a result, for each agent, we will have an array of words that were used in its\ncommand line.\n\nThe following image shows the resulting table, with the systems and the commands broken\ndown into words:\n\n### Encoding the data in the correct format\n\nMost algorithms from machine learning libraries need the input data in a specific format. In\nthis case, the Minhash algorithm requires a numeric vector of features, so we'll use the\nCountVectorizer function, which will transform all the unique words present in each of the\nevents into columns. Events are described by having the count of occurrences of each word\non the column corresponding to that word. For example, imagine that each event contains\nonly one word as in the following table:\n\n\n-----\n\nAfter using the \"CountVectorizer\" encoder, the event table would look like this:\n\nStarting from the tocluster dataframe shown above, the following code shows the operation\nof transforming this data into a vector of numeric events by adding a column called \"features\"\nto the dataframe.\n\nThis will result in a massive vector, with a huge amount of information that will require a lot of\nprocessing power. Luckily, this can be solved or at least improved using Locality Sensitive\nHashing, as we will see in the next section.\n\nThe following image shows the resulting data frame, with a features column, that contains a\nnumeric vector of the counts of each word:\n\n\n-----\n\n### MinHash LSH\n\nLocality Sensitive Hashing (LSH) is a fairly complex topic. However, Spark has some nice\nmachine learning libraries that allow users to use the power of the technique without having\nto know all the details.\n\nUnlike other hash algorithms, LSH seeks to maximize, rather than minimize, hash collisions.\nTherefore, the user can compute a set of LSH hashes for each event where the number of\ncommon words or features between different events we use to cluster similar events is\nrepresented by the number of common hashes. This means that we can then discard the\nenormous amount of feature columns and use only a few hashes to calculate the similarity.\n\nThe use of Minhash LSH makes it possible to calculate the similarity of very large datasets,\nusing Spark's power for distributed processing in a large number of systems. Trying to use a\nsimple machine learning library on a single system to cluster this amount of information\nwould be almost impossible.\n\nThe code section using MinHash LSH is shown below:\n\n\n-----\n\nIn this case, we used 10 hashes. There is a tradeoff between the processing time and the\naccuracy of the system by increasing the number of computed hashes per record. More\nhashes would return more accurate results but require more processing time, while fewer\nhashes would return less accurate results while requiring less processing time.\n\nAlthough there are complex ways to select the optimal value, for manual threat hunting, a bit\nof trial and error is usually good enough to arrive at a number of hashes that work. The\nfollowing image shows the data frame with the resulting hashes. Now, instead of a huge\nvector, we have only an array of length 10 on each row that needs to be analysed.\n\n### Computing similarity\n\n\n-----\n\nThere are many techniques to calculate the similarity between events. In this case, we\ncalculate the Jaccard distance between events to determine what is similar and what is not.\nThis means calculating the number of words that A and B have in common and dividing it by\nthe set of words they don't have in common. The code to compute the similarity between all\nthe events is:\n\nWe have to decide how similar two events have to be before we cluster them together. In this\ncase, we use the value of 0.2 as the maximum Jaccard distance between events that we\nrequire to consider them similar. Zero would mean exactly equal events and one would mean\ncompletely different events.\n\nAfter selecting this value and preparing the data, we must define the quality of the clusters. If\nthey're too small we will have too many clusters to work with. If they're too large, we'll have a\nfew low-quality clusters containing relatively different events. The choice of value is\ndependent on the nature of the dataset and the objectives of clustering. Again, some trial\nand error is required to create the most useful clusters for each case. The following image\nshows the resulting table of similar pairs:\n\n\n-----\n\n### Grouping similar events\n\nAfter calculating the similarity between events, which essentially cross-joints the table, we\nhave a huge table with pairs of similar events. We can query for events similar to any\nparticular event very quickly. However, what we are really after is a limited number of groups\nof similar commands.\n\nThere are many ways to do this (as there are with all parts of the presented method), but\nhere we'll identify communities of connected points in a graph.\n\nWe used a very powerful Spark library called \"Graphframes\". This library works with the\nrelationship between nodes (or vertices) and their connections (or edges) and executes\nknown graph algorithms to extract information from these relations.\n\nIn this case, we used its connectedComponents algorithm to group sets of similar nodes.\nThe image below shows a theoretical example of how this would look.\n\n\n-----\n\nIn the example above, there are two communities and a singleton. The blue community is\nvery straightforward as all the nodes are similar to each other. The H node is only similar to\nitself, so it is a community of one. And finally, the yellow community shows that although\nthere is no specific similarity between B and C, they are part of the same community since\nthere are similarities between other members of the same community.\n\nThe following code computed the communities of similar event pairs calculated in the\nprevious step.\n\nThe \"v\" variable contains all the node IDs, and the variable \"e\" contains all the similar pairs\ncalculated in the previous step. After creating a GraphFrame object with these values,\ncalculating the communities is as simple as invoking the connectedComponents method of\nthe Graph object.\n\nThe following graph shows the communities that the described methodology generated.\nEach dot represents one system and, as expected, the communities are not connected to\neach other. There are various communities with different sizes and colors. Either way, the\nmost important aspect is that the search space for a human researcher was reduced\ndramatically.\n\n\n-----\n\n## Digging into the results\n\nNow that we have a set of clusters to research, we'll perform a deeper analysis on three\nrepresentative clusters that highlight different attributes of the described method.\n\nSummarising the examples that follow:\n\nThe first example shows how a community of similar but not identical commands was\nfound that contained one common strange word.\n\n\n-----\n\nThe second example shows how a choice made previously in the way the data was\nprepared for clustering produced a useful result with \"better\" clusters and how the\nsystem was able to isolate a few attack patterns.\nFinally, by limiting the clusters to only those that have only recent occurrences, we\ndetected a relevant change in the behaviour of a known financially motivated threat\nactor.\n\n### Example 1: Who IsErik?\n\nThis cluster stood out to us:\n\nThe first thing that's unique is there's a common string to all the events — \"--IsErik\" — that\nbegs the question: Who is Erik and what is he doing in these systems?\n\nIdentifying the threat that these events relate to is pretty straightforward. A quick Google\nsearch for the \"isErik\" string reveals numerous articles describing it as an artifact of a known\npersistent adware family.\n\nWhat is interesting about this cluster, and the reason it was selected as an example, is that\nthe name and path of the file that wscript.exe executes, as well as the hex values that follow,\nare different for each event. This shows that the clustering system is doing its job of\naccepting small differences.\n\n\n-----\n\n### Example 2: Hiding a miner on Exchange Servers\n\nWe'll also look at a set of clustered events that demonstrates the importance of selecting and\npreparing data before clustering. It may seem strange that a set of completely different\ncommands were joined into the clustered events below.\n\nHowever, this was intended — all the commands run in a host started the clustering, not a\ncommand-line event. So we'll group the words with the same commands on a host. As a\nresult, the clusters contain not only groups of similar commands but also groups of similar\ncommand combinations.\n\nThis has one big advantage: context. Different attacks are grouped separately. Even when\nsome of the events are similar between groups as, for example, when multiple unrelated\nattacks are exploiting a common vulnerability.\n\nSo, what attack does this cluster reveal? Performing further analysis on one of the affected\nsystems we observed the following sequence of events:\n\n\n-----\n\n**Attack vector**\n\nW3wp.exe's execution of a cmd.exe was our first sign of suspicious activity. W3wp is the IIS\nworker process also used in Exchange servers. Knowing that this server has an internetexposed Exchange Server and the numerous critical vulnerabilities recently published and\nwidely exploited, we can assume that this is the attack vector: exploiting one of the\nExchange vulnerabilities.\n\n**Installation stealth and persistence**\n\nThe first command executes a base64-encoded PowerShell payload that can be decoded\ninto:\n\nThe command contains minimal obfuscation and appears to be attempting to download and\nexecute something from the URL https://122[.]10[.]82[.]109:8080/connect, taking special care\nto set a specific user agent, possibly to evade automatic analysis of the URL.\n\nThe additional PowerShell code is responsible for the remaining installation and execution.\n\n\n-----\n\nIt downloads the final paypload and writes it to the file system as\nC:\\ProgramData\\Microsoft\\conhost.exe. In the following steps, the script deletes PowerShell\nlogs and registers the final payload as a Windows service using a long command line with\nsome strange permission settings:\n\n[A quick Google search reveals that these are used to make the](https://www.google.com/url?q=https://www.sans.org/blog/red-team-tactics-hiding-windows-services/&sa=D&source=editors&ust=1633368290017000&usg=AOvVaw3O9VjoBVshgKOItEuyeBU_) service hidden and\nunremovable using the regular Windows administration tools, without some additional\nactions.\n\n**Final payload**\n\nFinally, the C:\\ProgramData\\Microsoft\\conhost.exe file (the same one that is used for the\nhidden service) is executed by the process powershell.exe.\n\nThis file (sha256:\n81A6DE094B78F7D2C21EB91CD0B04F2BED53C980D8999BF889B9A268E9EE364C) is\n[XMRig, a known cryptocurrency miner. We can confirm this by looking at its communications](https://www.google.com/url?q=https://blog.talosintelligence.com/2020/01/vivin-cryptomining-campaigns.html&sa=D&source=editors&ust=1633368290019000&usg=AOvVaw0rirrHSjKs-oGJu32oiBE1)\nand pool login ID.\n\nWhile this attack is not particularly sophisticated, it uses some interesting tricks to hide and\npersist its execution in the system.\n\nIt's interesting that these events were grouped even though they were not very similar and\nother attacks against Exchange Servers that have similar initial access commands were not.\nBy looking at this technique, a researcher could identify the different types of ongoing attacks\nagainst Exchange Server.\n\n### Threat actor TA551's Bazar\n\nThis final example highlights how clustering can be done in a time-bounded way, to reveal\nonly attacks happening on a restricted time frame. In this case, clustering was limited only to\nwhat happened in the last few days. As a result, the recent clusters brought our attention to a\n\n\n-----\n\nmalware campaign by a known threat actor that, once further researched, showed some\nchanges in the actor's usual activities. We started by looking at the following listing of the\ncluster events:\n\nThe cluster contained a suspicious combination of commands, using DLL files with a JPEG\nextension and registering them as services (IcedID is known to have been distributed this\nway). However, after analysis, we concluded that BazarBackdoor was the actual malware\nbeing distributed in this campaign, and, based on the TTPs, that TA551 was the probable\nadversary in this case.\n\nAdditional analysis on one of the affected systems uncovered the following sequence of\nrelevant events:\n\n\n-----\n\n**Attack vector**\n\nAs can be seen in Steps 1 and 2, the attack started with an email with an attachment. A ZIP\nfile named \"request.zip,\" containing a .doc file named \"official paper,08.21.doc.\" This ZIP file\nis encrypted to avoid detection by email protection systems.\n\n**Malware installation**\n\nWhen the .doc file was opened, a macro wrote an .hta file to disk and used mshta.exe to\nexecute its contents.\n\nThe .hta file is the downloader that connects to the server on 185[.]53[.]46[.]33, downloads a\nfile and writes it to disk as a .jpg file and registers it as a service using regsrv32.exe. The .jpg\nfile is actually a .dll file that contains the backdoor to be installed on the system.\n\n**Bazarbackdoor**\n\nOnce executed, the DLL (devDivEx.jpg) connects to the host 167[.]172[.]37[.]20.\n\nWith OSINT, we found several samples with similar names (devDivEx.jpg) that connect to the\nsame host. We identified these with memory analysis rules and by the use of a DGA with the\n.bazar TLD as being Bazarbackdoor. The following are examples of these samples:\n\nC96ee44c63d568c3af611c4ee84916d2016096a6079e57f1da84d2fdd7e6a8a3\n\n\n-----\n\nf7041ccec71a89061286d88cb6bde58c851d4ce73fe6529b6893589425cd85da\n\n**The Trickbot installation**\n\nAround one hour after the Bazar infection occurred, the svchost.exe process started\nperforming additional suspicious activities:\n\nAs shown, svchost.exe (which was running the Bazarbackdoor process) writes a DLL file to\ndisk, and starts it using rundll32.exe. A few seconds later, the rundll32.exe process starts\nconnecting to the IP addresses 103[.]140[.]207[.]110, 103[.]56[.]207[.]230,\n45[.]239[.]232[.]200.\n\nThese IPs are easily identifiable through OSINT as Trickbot C2 IP addresses, and multiple\nTrickbot samples can be found connecting to them on public sandbox execution reports.\n\n**The threat actor**\n\nWe found that there are several of the attacker's TTPs that are similar to those of the threat\nactor TA551, leading us to believe with moderate confidence that this is the threat actor. For\nexample:\n\nThe request.zip file name.\nUse of email with encrypted ZIP attachment.\nUse of Microsoft Word macros.\nUse of an HTA file as downloader.\nUse of DLL with a .jpg extension in the c:\\users\\public directory.\nRegistering the JPEG file as a service.\nThe format of the commands used to perform each of these activities.\n\n\n-----\n\nWhile searching for a match for the observed TTPs and IOCs, we found that this has also\n[been observed by other researchers who recently tweeted and](https://www.google.com/url?q=https://twitter.com/malware_traffic/status/1417905207682510849&sa=D&source=editors&ust=1633368290045000&usg=AOvVaw3D-IhmBm9iNJfXsKtHCaNj) [blogged about TA551 starting](https://www.google.com/url?q=https://isc.sans.edu/diary/rss/27738&sa=D&source=editors&ust=1633368290046000&usg=AOvVaw2R-iMUoPVPOlG1CIgPFz5K)\nto drop BazarBackdoor and Trickbot.\n\nTA551 is a known, financially motivated attacker that distributes several other malware\nfamilies in the past (e.g., Ursnif, Valak, IcedID). Distributing BazarBackdoor is a fairly recent\nchange that deserves network defenders' attention. This example demonstrates how, by\nselecting only recent clusters, it is possible to identify threats that are happening recently.\n\n## Conclusion\n\nAs attacks become more frequent and impactful, one of the most powerful weapons that\norganizations have is data. Security is not something that you can master by purchasing a\nsingle software or hardware solution. Several layers of defense are needed, and still, attacks\nwill get through occasionally. Without data, security teams are blind to ongoing and past\nattacks that may have passed the existing layers of protection.\n\nSecurity tools can produce very large amounts of data. Thankfully, there are several great\ntools that help with querying large volumes of data with more or less flexibility. At Talos,\nSpark is one of the tools we use, for its flexibility and ability to handle very large data sets..\nThese data processing tools should be a powerful tool in the arsenal of security teams and\nthis blog post walks through one technique we use that we find particularly useful. Hopefully\nafter reading it, it helps \"spark\" a new idea for data processing or \"spark\" the interest to use\nand explore these tools.\n\n## IOC's in this post\n\n**Samples:**\n\nXMRig Miner:\n\n81A6DE094B78F7D2C21EB91CD0B04F2BED53C980D8999BF889B9A268E9EE364C\n\nBazarBackdoor:\n\nC96ee44c63d568c3af611c4ee84916d2016096a6079e57f1da84d2fdd7e6a8a3\n\nf7041ccec71a89061286d88cb6bde58c851d4ce73fe6529b6893589425cd85da\n\n**Network IOC's:**\n\n\n-----\n\nIP and url for miner downloader:\n\n122[.]10[.]82[.]109\n\nhttps://122[.]10[.]82[.]109:8080/connect\n\nBazar backdoor downloaded from:\n\n185[.]53[.]46[.]33\n\nBazarBackdor C2:\n\n167[.]172[.]37[.]20\n\nTrickbot C2:\n\n103[.]140[.]207[.]110, 103[.]56[.]207[.]230, 45[.]239[.]232[.]200\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2021/2021-10-04 - Threat hunting in large datasets by clustering security events.pdf"
    ],
    "report_names": [
        "2021-10-04 - Threat hunting in large datasets by clustering security events.pdf"
    ],
    "threat_actors": [
        {
            "id": "26a04131-2b8c-4e5d-8f38-5c58b86f5e7f",
            "created_at": "2022-10-25T15:50:23.579601Z",
            "updated_at": "2025-03-27T02:00:55.50292Z",
            "deleted_at": null,
            "main_name": "TA551",
            "aliases": [
                "TA551",
                "GOLD CABIN",
                "Shathak"
            ],
            "source_name": "MITRE:TA551",
            "tools": [
                "QakBot",
                "IcedID",
                "Valak",
                "Ursnif"
            ],
            "source_id": "MITRE",
            "reports": null
        },
        {
            "id": "04e34cab-3ee4-4f06-a6f6-5cdd7eccfd68",
            "created_at": "2022-10-25T16:07:24.578896Z",
            "updated_at": "2025-03-27T02:02:10.286803Z",
            "deleted_at": null,
            "main_name": "TA551",
            "aliases": [
                "Gold Cabin",
                "Monster Libra",
                "Shathak",
                "TA551"
            ],
            "source_name": "ETDA:TA551",
            "tools": [
                "BokBot",
                "CRM",
                "Gozi",
                "Gozi CRM",
                "IceID",
                "IcedID",
                "Papras",
                "Snifula",
                "Ursnif",
                "Valak",
                "Valek"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "90f216f2-4897-46fc-bb76-3acae9d112ca",
            "created_at": "2023-01-06T13:46:39.248936Z",
            "updated_at": "2025-03-27T02:00:03.031127Z",
            "deleted_at": null,
            "main_name": "GOLD CABIN",
            "aliases": [
                "Shakthak",
                "TA551",
                "ATK236",
                "G0127",
                "Monster Libra"
            ],
            "source_name": "MISPGALAXY:GOLD CABIN",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        },
        {
            "id": "40b623c7-b621-48db-b55b-dd4f6746fbc6",
            "created_at": "2024-06-19T02:03:08.017681Z",
            "updated_at": "2025-03-27T02:05:17.342829Z",
            "deleted_at": null,
            "main_name": "GOLD CABIN",
            "aliases": [
                "TA551 ",
                "Shathak"
            ],
            "source_name": "Secureworks:GOLD CABIN",
            "tools": [
                ""
            ],
            "source_id": "Secureworks",
            "reports": null
        }
    ],
    "ts_created_at": 1673535772,
    "ts_updated_at": 1743041638,
    "ts_creation_date": 1653759071,
    "ts_modification_date": 1653759071,
    "files": {
        "pdf": "https://archive.orkl.eu/26e9445c7405669fcb5905440a8d4ce3dac863bf.pdf",
        "text": "https://archive.orkl.eu/26e9445c7405669fcb5905440a8d4ce3dac863bf.txt",
        "img": "https://archive.orkl.eu/26e9445c7405669fcb5905440a8d4ce3dac863bf.jpg"
    }
}