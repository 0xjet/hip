{
    "id": "a7131486-d1b8-4c39-90c5-4a71fedf2a76",
    "created_at": "2023-01-12T15:10:43.110993Z",
    "updated_at": "2025-03-27T02:15:45.42936Z",
    "deleted_at": null,
    "sha1_hash": "2ab2d23d82a3db432c2455283395e5c2c7b31c6c",
    "title": "2021-09-16 - Pointer- Hunting Cobalt Strike globally",
    "authors": "",
    "file_creation_date": "2022-05-28T19:26:46Z",
    "file_modification_date": "2022-05-28T19:26:46Z",
    "file_size": 150671,
    "plain_text": "# Pointer: Hunting Cobalt Strike globally\n\n**[medium.com/@shabarkin/pointer-hunting-cobalt-strike-globally-a334ac50619a](https://medium.com/@shabarkin/pointer-hunting-cobalt-strike-globally-a334ac50619a)**\n\nPavel Shabarkin November 21, 2021\n\nPav\nel\n\n\n[Pavel Shabarkin](https://shabarkin.medium.com/?source=post_page-----a334ac50619a--------------------------------)\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9284dc7b6da6&operation=register&redirect=https%3A%2F%2Ftowardsaws.com%2Fpointer-hunting-cobalt-strike-globally-a334ac50619a&user=Pavel+Shabarkin&userId=9284dc7b6da6&source=post_page-9284dc7b6da6----a334ac50619a---------------------follow_byline-----------)\nSep 16, 2021\n\n\n14 min read\n\n## Introduction\n\nCobalt Strike is a commercial, full-featured, remote access tool that bills itself as “adversary\nsimulation software designed to execute targeted attacks and emulate the post-exploitation\nactions of advanced threat actors”. Cobalt Strike’s interactive post-exploit capabilities cover the\nfull range of ATT&CK tactics, all executed within a single, integrated system.\n\nIn addition to its own capabilities, Cobalt Strike leverages the capabilities of other well-known\ntools such as Metasploit and Mimikatz.\n\nCobalt Strike is a legitimate security tool used by penetration testers and red teamers to\nemulate threat actor activity in a network. However, lately, this tool has been hijacked and\nabused by cybercriminals.\n\nOur goal was to develop a tool to help identify default Cobalt Strike servers exposed on the\nInternet. We strongly believe that understanding and mapping adversaries and their use of\nCobalt Strike can improve defenses and boost organization detection & response controls.\nBlocking, mapping and tracking adversaries is a good start.\n\n\n-----\n\nPointer logo\n\n## Tool Development\n\nA review of existing Cobalt Strike detection tools and public research showed that current tools\ncan only scan a small number of potential Cobalt Strike instances (1–5k hosts). Our goal was to\nincrease the scanning capabilities and validate several million potential Cobalt instances in less\nthan an hour.\n\nTo achieve the above goal within a reasonable timeframe and on a small budget, it was\nnecessary to adapt and scale the current understanding of the Cobalt Strike hunting\nmethodology. The following content assumes an understanding of what Cobalt Strike is and\n\n\n-----\n\nhow to locate and identify Cobalt strike instances. Before going into the details of the tool and\ntheir components, let’s take a look at the general architecture.\n\n## Architecture review\n\nScanning a large number of hosts in a reasonable amount of time does not scale and has\nphysical, cost and power limitations. Unless you have a great home lab and the bandwidth to\nsupport it, personal computing cannot really solve the scaling problem, so the decision was\nmade to use AWS to affordably scale and achieve the desired goals.\n\n## General architecture review\n\nThe tool is developed and heavily based on AWS SQS, Lambda and DynamoDB.\n\n\n-----\n\nThe Pointer client parses the local json file with a list of IPs, optimally splits them into packets\n(10–20 IPs), and then adds the packets to be processed to the SQS queue.\n\nThe SQS queue is setup to invoke a lambda function for each packet in the queue. The lambda\nfunction (Pointer server) performs the actual scanning of the provided packet of IPs and saves\nresults to DynamoDB.\n\nIn cases where Lambda fails or throws an error, packets are returned to the SQS queue and will\nwait for a retry.\n\nIf the packet fails a second time, a new Lambda function is launched that logs the failed packet\nto DynamoDB for further analysis and rescan each IP individually to locate the failed IPs.\n\n\n-----\n\n## Code Review\n\nThe scan functionality of the “Pointer server” consists of 4 parts:\n\n1. Port Scanning (Port Workers)\n2. HTTP Webservice scan (HTTP Workers)\n\nCertificate parsing\nJARM parsing\n\n3. HTTPS Webservice scan (HTTPS Workers)\n\n4. Beacon Parsing (Beacon Workers)\n\nThe tool was designed with an asynchronous approach to IP processing. Each scan probe\nstands as an independent unit, which is then processed by a Worker. The probes include a port\nscanning, Certificate Issuer parsing, JARM parsing, webservice scanning, and Beacon parsing.\nOnce each probe is completed, the result is sent to the corresponding controller, which writes\nthe result to the global map. After all scan workers are done, the data is sorted and ordered\nbefore being combined into the `Target structure. Overall, this reduces the number of delays`\nsince each service(ip:port) has its own scan pipeline.\n\n\n-----\n\nInternal architecture of Lambda function\n**Detailed review**\n\nInitially the lambda function launches Port, HTTP, HTTPS, and Beacon workers. The number of\nworkers depends on the level of internal concurrency (Internal concurrency is the controllable\nCLI parameter). Each type of worker is portioned accordingly to the required power resources.\nPortioning has been calculated based on the number of probes each worker performs in\naverage.\n\nEach targeted IP address is scanned for 27 predefined ports, this list includes common ports on\nwhich Cobalt Strike beacons are hosted. The “launcher” sends service (ip:port) to the Port\nWorkers through `portChannel Golang channel.`\n\n\n-----\n\nCode snippet of the Service Launcher\nPort workers then scan the individual ports. If a port is open, the worker sends the service to the\nHTTP Worker and Output controller through `httpChannel and` `outputChannel Golang`\nchannels. If the port is closed the Port Worker exits the function.\n\n\n-----\n\nCode snippet of Port Worker\nAll workers send results through a single Golang channel, `outputChannel, which are then`\nprocessed by the output controller and saved to the global map ( Sorter struct).\n\nEach result produced by the workers has its own type tag (Ex:\n```\n\"Service|\", \"Certificate|\", \"Jarm|\", … ), ensuring that the ValidateOutput function\n\n```\ncan sort the results based on their types.\n\n\n-----\n\nCode snippet of Output Controller\nThe HTTP worker waits for IP and port tuple (service) to be provided by the Port Worker via the\n```\nhttpChannel . If the HTTP Worker receives port 50050 it attempts the following actions:\n\n```\nParse the certificate issuer -> identifying the default self-signed Cobalt certificate\nParse the JARM signature -> detecting malicious JARM signatures\n\nFor other services, it performs a web request to analyse response behaviour. Beacon’s\nHTTP/HTTPS indicators are controlled by a malleable C2 profile, if the server uses the default\nmalleable C2 profile, it responds with a 404 status code and 0 content-length for requests made\nto the root web endpoint. (http://domain.com/)\n\n\n-----\n\nIf the request to the targeted web service fails, HTTP Worker sends the service through\n```\nhttpsChannel channel further to the HTTPS Worker to perform the web request through\n\n```\nHTTPS protocol.\n\nCode snippet of the HTTP Worker\nBeing inspired by the “Analyzing Cobalt Strike for Fun and Profit” research and its\ncorresponding tool for cobalt strike beacon parsing (developed using Python), we integrated the\nsimilar logic into our tool for beacon parsing (developed using Golang).\n\n_The guy, who researched how the beacon is packed, how to parse the beacon, how to decrypt_\n_the beacon, and how to work with that in general, you did the good job a big thank you!_\n\n\n-----\n\nAll identified web services that have been configured with default malleable C2 profile are sent\nto the Beacon Workers. The Beacon Worker attempts to parse the beacon config. If the parsing\nsucceeds, Beacon Worker sends the `CobaltStrikeBeaconStruct struct to the Beacon`\ncontroller through `beaconStructChannel channel, and the beacon location URI to the output`\ncontroller through `outputChannel channel.`\n\nCode snippet of Beacon Worker\n\n\n-----\n\nCode snippet of Beacon Controller\nWhen all workers finish the scans, the `Sort method maps all gathered scan results sent to`\nthe output controller into the array of `CobaltStrikeStruct type:`\n\n\n-----\n\nCode snippet of `CobaltStrikeStruct data type`\nThe `Probability field is assigned when the` `Voter function calls the internal method`\n```\nVote for each CobaltStrikeStruct object within the array.\n\n```\nIn case the certificate issuer matches the default Cobalt Strike self-signed certificate, the `Vote`\nmethod gives 100% probability that it is the Cobalt Strike server. The same applies if the\nBeacon Worker successfully parses the beacon config hosted on the web service.\n\nDefault web service response and malicious JARM signature results cannot give us confidence\nin assigning the probability rate. Because other web services can respond with 0 content length\nand 404 status code, and servers can be configured with the same TLS options (if you don’t\n\n\n-----\n\nunderstand what JARM is). If the `Vote method matches only those two indicators, it assigns`\nthe 70% probability to the object.\n\nIf none of these meet our requirements, it is probably not a Cobalt Strike server. But, again, this\ntool targets only Cobalt Strike servers with default malleable C2 profile configurations.\n\nCode snippet of the Vote method\n\n## DynamoDB Component\n\n\n-----\n\nWe chose DynamoDB service to store scan results. DynamoDB can handle more than 10\ntrillion requests per day and support peaks of more than 20 million requests per second. That is\nwhat we needed 100%! We wanted to scan 20000–25000 targets per 60 seconds, which is\nabout 40k-50k writing requests to the database.\n\nOn the first implementation, the Output and Beacon workers exceeded DynamoDB rate limits\nbecause they performed a write request to the DynamoDB table for each target object\nseparately, and, in addition, we used the default DynamoDB configuration. The default capacity\nconfiguration could not handle that many requests, but by increasing the capacity we would pay\nmore money for autoscaling during constant scanning. Further examination of the AWS\ndocumentation revealed that AWS had implemented the batch write to DynamoDB. For each\nlambda invocation, we have 10–20 targets (depending on the packet size) to scan, so this\nshould reduce the number of requests to DynamoDB tables by a factor of 10–20.\n\nWe found that DynamoDB’s `BatchWriteItemInput function allows writing up to 25 items and`\nup to 16 Mb in one request. The batch write implementation significantly decreased the number\nof requests and removed the rate limiting issue at the default configuration level. We did not\nhave to pay for unnecessary autoscaling.\n\nThis method has the disadvantage that if one of the items in the batch fails to be written, the\nwhole batch will not be saved. (The partition key must be unique and not exist in the table, but\nthis is suitable in our case, as we filter our targets by unique values before launching the\nscans).\n\n\n-----\n\nCode snippet of the WriteBatchTarget function\nAlso, for unpredictable cases where the default capacity configuration cannot handle a large\nnumber of requests, we configure autoscaling:\n\nAWS Console → DynamoDB → choose the Table → Edit Capacity → Read / Write Capacity\nincrease to 10–15. To enable autoscaling we should give the required permissions for the\nDynamoDB service role .\n\n## Lambda Component\n\n\n-----\n\nAWS Lambda is an interesting service. We wanted to try Lambda as a core service for our\nscans, however we did not want to get a crazy paycheck at the end of the month, so we had\nseveral things to figure out:\n\n1. How much memory to allocate for Lambda execution\n2. What default timeout to set for Lambda execution\n3. How to manage Lambda concurrency\n4. What internal concurrency would be suited for our model;\n5. What request timeouts would be suited for our model\n6. What packet size would be suited for our model\n\nAnd the most difficult question — How to setup everything the way it would be efficient, cheap,\nand with minimum loss rate?\n\n**Lambda memory allocation**\n\nIt was interesting to research how AWS allocates memory and CPU for Lambda functions,\nbecause it is physically impossible to divide 1/10 of the CPU. But it can allocate 1/10 of the time\nof the CPU to a single function, and you can have 10 of them working at the same time to share\n[the same CPU core (check this research, it explains how AWS Lambda allocates CPU).](https://engineering.opsgenie.com/how-does-proportional-cpu-allocation-work-with-aws-lambda-41cd44da3cac)\n\nThe only controllable parameter in AWS for Lambda functions is memory usage:\n\n\n-----\n\nExample of the memory configuration in AWS Console\nWe designed our model with a multithreaded architecture — the more cores we have, the better\nperformance we can potentially obtain. But the nasty thing here is what the price of this luxury\nis.)))\n\nWe cannot directly control the number of cores we want to use. The CPU performance scales\nwith the memory configuration. Lambda functions used to always have 2 vCPU cores,\nregardless of the allocated memory. The rest of the cores are throttled at certain memory\nconfigurations. By increasing the memory allocation, we obtain more cores. I found the\n[research that discovered how the number of vCPUs and multithreaded computation power vary](https://www.sentiatechblog.com/aws-re-invent-2020-day-3-optimizing-lambda-cost-with-multi-threading)\ndepending on the memory configuration.\n\n\n-----\n\nThe price for using the AWS Lambda function is based on the function runtime (in milliseconds)\nmultiplied by the allocated memory (fixed prices per Mb). So, allocating 3008MB for the Lambda\nfunction, we get 2 vCPUs, and allocating 3009MB we get 3vCPUs. By allocating 3009MB\nmemory, we could gain more performance, at almost the same price.)))\n\nAccording to the research, we get better performance gain for multithreading with each spike\ntransition (jump in cores). But for our model we do not need more than 3 cores, 3009MB is\nenough for our purposes.\n\nCorrelation between Lambda memory configuration and number of cores\nBy the way, we decided to measure the computational power ourselves, and the practical tests\nshowed that the power spike between 3008–3009 MB is bigger than between 5307–5308 MB.\nThis once again confirms that the 3009MB memory configuration is the best choice for us.\n\n\n-----\n\nCode snippet for measuring the multithreaded computation power\n**Internal parameter tuning**\n\nAs we got a solid understanding of how many resources to use, we started tuning and suiting\nother parameters for our model.\n\nIn my opinion, the lifetime of the Pointer lambda function should not be more than 60 seconds,\nbecause otherwise it will not be a true server-less tool with easy management, stable to errors\nand autoscaled architecture.\n\nWith a memory configuration of 3009 Mb and a default timeout of 60 seconds for one Lambda\nexecution, we could scan from 10–20 targets in a single packet.\n\n\n-----\n\nIn case the lambda execution fails, we do not want to rescan all the targets inside the packet\nagain. By having less number of targets in the packet, we minimize the probability that the\npacket will be crashed. Therefore, the optimal size, in my opinion, is 10–20 targets.\n\nBy having less number of targets inside the packet, we are minimising the chance that the\npacket will be crashed. In case the lambda execution fails, we must rescan all targets inside the\npacket (even those that have been successfully scanned).\n\nWhen we defined lambda configuration parameters, the rest of the parameters were tuned with\na big number of tests:\n```\nTargets/per packet 20  (Items)Concurrency     140  (Items)Lambda Memory   \n3009 (Mb)Lambda Timeout   60  (sec)Http timeout    4   (sec)Port timeout  \n2   (sec)Beacon timeout   10  (sec)\n\n```\n**Lambda Concurrency**\n\nAWS Lambda provides autoscaling for function instances. But we simply cannot deploy as\nmany instances as we want. We are limited by the AWS region quota (All the lambda functions\nof an account can use the pool of 1000 unreserved concurrent executions). Thus, having only 1\ndeployed function, we could get 1000 concurrent executions at the same time.\n\nDorking potential Cobalt Strike servers through Shodan, we could retrieve around 200–300k\npotential targets. However, we are designing the tool to scan 2M-10M targets. For example, 2M\ntargets is about 100k packets (20 targets per packet), which means 100k lambda function\ninvocations. If Lambda function is invoked 100k times, the lambda puller would process only 1k\nrequests at a time, and the rest would be just throttled. So even if we increase the AWS region\nquota, it will not be enough.\n\nSo the question arises — how we can manage the invocation process? The answer is simple —\nSQS.\n\n## SQS Component\n\n**Configuration**\n\nAmazon Simple Queue Service (SQS) is a fully managed message queuing service that\nenables you to decouple and scale microservices, distributed systems, and serverless\napplications. This means we can send all our packets to the queue and SQS will manage the\nprocess of lambda invocation. The SQS management can be configured according to our\nneeds:\n\n1. We can control the number of retries.\n\nWe can configure the maximum number of retries the SQS would perform, if the batch of\nmessages(packets) fails\n\n\n-----\n\nThe SQS sends messages packed in the batches, we can control the number of\nmessages inside the batch we want to pass to the Lambda function, so having 1 message\nin the batch will equal 1 message.\nIn our model we decided that If message fails more than once, it would be sent to the\n(DLQ). We designed the (DLQ) to redirect the failed messages to the Lambda function\nwith the same logic as the core one, but before scanning activities it writes the the failed\npacket to the DynamoDB table and rescans each target separately.\n\n2. Visibility timeout\n\nThe visibility timeout sets the length of time that a message received from the queue (by\none lambda function) will not be visible to the lambda function again. If the lambda\nfunction fails to process and delete the message before the visibility timeout expires, the\nmessage becomes visible to the lambda function again.\n\n3. SQS batch size\n\nWe configured SQS batch size to a single message.\n\n**SQS & Lambda autoscaling**\n\nFor standard queues, Lambda uses long polling to poll the queue until it becomes active. When\nmessages are available, Lambda reads up to 5 batches and sends them to our lambda\nfunction. If messages are still available, Lambda increases the number of processes that read\nbatches up to 60 more instances per minute. The maximum number of batches that can be\nprocessed simultaneously by event source mapping is 1000. This means that the full power we\ncan get after 16 minutes of continuous scanning.\n\n## Results\n\nAt the first launch, when we ran a scan for 160k targets, we were able to identify 1,700 Cobalt\nStrike servers and parse 1,400 of their beacon configurations within 40 minutes. The Pointer\ntool can produce best performance results if the target size exceeds 500k. Scanning 160k\ntargets took a little longer because 1000 concurrent lambda executions were achieved only\nafter 30 minutes when the tool was launched. For the current implementation, the cost of\nscanning 250k targets is about 20$, however we are looking for a solution that will make it\ncheaper.\n\n## Targets table [sample]\n\nWe have developed 2 tables, first one for identified Cobalt Strike servers, and the second for\nparsed beacon configurations. Identified Cobalt Strike servers can be described by 7 features:\n\nIP address is a unique sorting key\nprobability that it’s the actual cobalt strike server (easier filtering)\nJARM signature\n\n\n-----\n\nCertificate Issuer\nOpened Ports\nResponse behaviour\nLinks to the beacon configurations that we parsed and saved to another table\n\nThere is an example of the cobalt strike server table:\n\nTable of parsed Cobalt Strike targets\n\n## Beacons table [sample]\n\n\n-----\n\nThe Beacon configuration table has the `uri feature as a unique sorting key, when the rest of`\nthe features are the actual parsed beacon configurations.\n\nHere is an example of the table with parsed beacon configurations:\n\nThe full version of tables you can find here:\n\n## Data analysis\n\nWe are using collected data to map attackers infrastructure and understand how the attackers\noperate Cobalt Strike.\n\n\n-----\n\nWe know that threat intelligence groups are tracking specific ransomware groups with the help\nof Watermarks, For example:\n\nSodinokibi (Watermark 452436291)\nAPT 27 (Watermark 305419896).\n\nBased on the beacon’s `spawnto locations the blue teams can develop detection controls.`\n\nLocation of servers (IP) (Hosting provides)\n\n\n-----\n\nWatermarks\n\n\n-----\n\nCountries\n\n\n-----\n\nSpawn location\n\n\n-----\n\nSample of the Dork database (not completed)\n\n## Summary & Future work\n\nFor the first Pointer version, we have developed a system with a complete hunting methodology\nthat could be easily scalable up to 2–3 millions targets.\n\nThe first tests showed that the tool can scan 200k targets in 45–50 minutes with 10% packet\nloss. We strongly believe that we have taken a big step in hunting and detection system.\n\nBut, of course, we have not achieved the results we wanted, this is just the first demo version of\nthe Pointer.\n\n\n-----\n\nAny feedback is more than welcome and if you have any ideas, suggestions, and\nrecommendations, or if you want to help us improve the Cobalt Strike Hunting tool, just contact\nPavel Shabarkin and Michael Koczwara.\n\n## References\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2021/2021-09-16 - Pointer- Hunting Cobalt Strike globally.pdf"
    ],
    "report_names": [
        "2021-09-16 - Pointer- Hunting Cobalt Strike globally.pdf"
    ],
    "threat_actors": [
        {
            "id": "610a7295-3139-4f34-8cec-b3da40add480",
            "created_at": "2023-01-06T13:46:38.608142Z",
            "updated_at": "2025-03-27T02:00:02.87217Z",
            "deleted_at": null,
            "main_name": "Cobalt",
            "aliases": [
                "Cobalt Gang",
                "GOLD KINGSWOOD",
                "COBALT SPIDER",
                "G0080",
                "Mule Libra",
                "Cobalt Group"
            ],
            "source_name": "MISPGALAXY:Cobalt",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        },
        {
            "id": "5c13338b-eaed-429a-9437-f5015aa98276",
            "created_at": "2022-10-25T16:07:23.582715Z",
            "updated_at": "2025-03-27T02:02:09.875151Z",
            "deleted_at": null,
            "main_name": "Emissary Panda",
            "aliases": [
                "APT 27",
                "ATK 15",
                "Bronze Union",
                "Budworm",
                "Earth Smilodon",
                "Emissary Panda",
                "Group 35",
                "Iron Taurus",
                "Iron Tiger",
                "LuckyMouse",
                "Operation DRBControl",
                "Operation Iron Tiger",
                "Operation PZChao",
                "Operation SpoiledLegacy",
                "Operation StealthyTrident",
                "Red Phoenix",
                "TEMP.Hippo",
                "TG-3390",
                "ZipToken"
            ],
            "source_name": "ETDA:Emissary Panda",
            "tools": [
                "ASPXSpy",
                "ASPXTool",
                "Agent.dhwf",
                "AngryRebel",
                "Antak",
                "CHINACHOPPER",
                "China Chopper",
                "Destroy RAT",
                "DestroyRAT",
                "FOCUSFJORD",
                "Farfli",
                "Gh0st RAT",
                "Ghost RAT",
                "HTTPBrowser",
                "HTran",
                "HUC Packet Transmit Tool",
                "HighShell",
                "HttpBrowser RAT",
                "HttpDump",
                "HyperBro",
                "HyperSSL",
                "HyperShell",
                "Kaba",
                "Korplug",
                "LOLBAS",
                "LOLBins",
                "Living off the Land",
                "Mimikatz",
                "Moudour",
                "Mydoor",
                "Nishang",
                "OwaAuth",
                "PCRat",
                "PlugX",
                "ProcDump",
                "PsExec",
                "RedDelta",
                "SEASHARPEE",
                "Sensocode",
                "SinoChopper",
                "Sogu",
                "SysUpdate",
                "TIGERPLUG",
                "TVT",
                "Thoper",
                "Token Control",
                "TokenControl",
                "TwoFace",
                "WCE",
                "Windows Credential Editor",
                "Windows Credentials Editor",
                "Xamtrav",
                "ZXShell",
                "gsecdump",
                "luckyowa"
            ],
            "source_id": "ETDA",
            "reports": null
        }
    ],
    "ts_created_at": 1673536243,
    "ts_updated_at": 1743041745,
    "ts_creation_date": 1653766006,
    "ts_modification_date": 1653766006,
    "files": {
        "pdf": "https://archive.orkl.eu/2ab2d23d82a3db432c2455283395e5c2c7b31c6c.pdf",
        "text": "https://archive.orkl.eu/2ab2d23d82a3db432c2455283395e5c2c7b31c6c.txt",
        "img": "https://archive.orkl.eu/2ab2d23d82a3db432c2455283395e5c2c7b31c6c.jpg"
    }
}