{
    "id": "1933a681-4d36-43f0-82eb-fc8f6cba95dc",
    "created_at": "2023-01-12T15:10:55.816862Z",
    "updated_at": "2025-03-27T02:09:07.376368Z",
    "deleted_at": null,
    "sha1_hash": "92e5de1199fb48d88a41410c442dded76ea7bf4c",
    "title": "2021-05-24 - SCOTCH- A framework for rapidly assessing influence operations",
    "authors": "",
    "file_creation_date": "2022-05-27T21:38:03Z",
    "file_modification_date": "2022-05-27T21:38:03Z",
    "file_size": 866103,
    "plain_text": "# SCOTCH: A framework for rapidly assessing influence operations\n\n**[atlanticcouncil.org/blogs/geotech-cues/scotch-a-framework-for-rapidly-assessing-influence-operations/](https://www.atlanticcouncil.org/blogs/geotech-cues/scotch-a-framework-for-rapidly-assessing-influence-operations/)**\n\nMay 24, 2021\n\n[Digital Policy](https://www.atlanticcouncil.org/issue/digital-policy/) [Disinformation](https://www.atlanticcouncil.org/issue/disinformation/) [Extremism](https://www.atlanticcouncil.org/issue/extremism/) [Media](https://www.atlanticcouncil.org/issue/media/) [Politics & Diplomacy](https://www.atlanticcouncil.org/issue/politics-diplomacy/) [Technology & Innovation](https://www.atlanticcouncil.org/issue/technology-innovation/)\nBy [Sam Blazek, PhD](https://www.atlanticcouncil.org/expert/sam-blazek/)\n\nMost of humanity now engages with digital and social media, in large part through\nsmartphones. This new reality has cross-sectoral impacts and has changed the nature of\nconflict. For instance, in LikeWar: The Weaponization of Social Media, Peter Singer and\nEmerson Brooking note how the information landscape altered the dynamics of the recent\nwar in Syria:\n\n_How information was being accessed, manipulated, and spread had taken on new power._\n_Who was involved in the fight, where they were located, and even how they achieved victory_\n_had been twisted and transformed. Indeed, if what was online could swing the course of a_\n\n\n-----\n\n_battle—or eliminate the need for battle entirely—what exactly, could be considered war at_\n_all?_\n\nThe increased involvement of digital technology and media in war requires innovative\nframeworks for understanding information warfare and influence operations. Based on\nexperience assessing hundreds of influence operations across six continents over the past\nseven years, this paper offers a new framework for professionals engaged in analyzing,\nunderstanding, and countering them.\n\n## Characterizing frameworks for influence operations\n\nGeopolitical influence operations may be defined as those that i) are either coordinated or\nsupported by a state actor and ii) seek to influence an audience in the interests of said actor.\nSuch activities have been used for millennia to gain tactical or strategic advantage in combat\nand competition; however, the global proliferation of information technology has dramatically\nenhanced their scale, speed, and reach. Individuals charged with recognizing and\nforestalling such threats utilize many projects and platforms aimed at detecting, quantifying,\nforecasting, and countering influence operations. However, these efforts all rely on some\ncharacterization of what these operations are and how they work. Unfortunately, the rapid\nspeed of change of the battle space has caused practitioners and researchers alike to\nstruggle with defining threats and attacks.\n\nIn spite of the many existing tools, datasets, case studies, and processes that these teams\nhave either acquired or built, there is little consensus on how practitioners and decision\n**makers describe and address influence operations. As a result, they talk in circles with**\nvarying amounts of shared context or situational awareness and struggle to quickly adapt\nand respond as a community to social media innovations. For example, as the Atlantic\n[Council’s DFRLab recently noted, policy makers lack a cohesive strategy to combat the](https://medium.com/dfrlab/op-ed-the-next-big-wave-of-disinformation-will-be-heard-not-seen-70507fcbf79a)\nmalicious use of real-time audio and video broadcasting.\n\nIn addition, many neglect the facts that the battlespace is highly complex and that its features\nare both evolving and interdependent. Society witnesses technological evolution in real-time\nin discussions with families and peers because the platforms in question are ubiquitous, but\nit is difficult to define threats, activities, and objectives in a shared operational and analytical\nlanguage—as a result, researchers and policy makers struggle to validate and communicate\ntheir observations. For instance, understanding how both coordination by bad-faith actors\n[and organic irony poisoning can morph ironic misinformation into genuine disinformation](https://static.nytimes.com/email-content/INT_4981.html)\nacross communities is intuitive. Nonetheless, characterizing and developing practical\ncountermeasures for these mechanisms is a remarkable challenge.\n\nExisting taxonomies of influence operations tend to be incomplete—for example, the\n[Carnegie Mellon BEND framework and the earlier 4Ds framework characterize only the](https://link.springer.com/article/10.1007/s10588-020-09322-9)\nmeans and some tactical objectives of individual and mass behavioral exploitation. MITRE’s\n[ATT&CK framework, as well as AMITT, a library and clearinghouse of incidents and TTPs](https://attack.mitre.org/)\n\n\n-----\n\n[supported by fellow Atlantic Council Fellows Dr. Pablo Breuer and](https://www.atlanticcouncil.org/expert/pablo-breuer/) [Ms. SJ Terp, formally](https://www.atlanticcouncil.org/expert/sara-jayne-terp/)\ncategorizes adversarial tactics, techniques and procedures (TTPs), while specific\nframeworks such as Graphika’s [ABC(D) focus on the “who,” the “what,” and some of the](https://www.brookings.edu/techstream/adding-a-d-to-the-abc-disinformation-framework/)\n“how” of operations. These frameworks provide excellent summaries of certain key elements\nof influence operations but fail to address the big picture.\n\nA few “big picture” models do exist, nonetheless. One example is the Malign Foreign\nInfluence Campaign Cycle developed by the US Department of Justice (DOJ) Cyber Digital\nTask Force. However, given DOJ’s institutional objective of establishing a solid basis for legal\naction, the rigor and sophistication of the framework may be unwieldy for practical, timesensitive use, and for deeper social and behavioral study.\n\nAll these frameworks can provide value to those addressing influence operations. However,\nas influence operations grow in complexity and technical sophistication, operators and\nanalysts continue to lag in one key area: characterizing operations succinctly and effectively\nto colleagues and decision makers. Many describe their work using ad-hoc mental models in\nlarge part because existing classification schemes are either too simple to describe the\nnuances of complex operations or too specific to comprehensively summarize the entire\ninformation battlespace.\n\nOne key point that most will intuitively recognize, but that is too often absent from formal\nframeworks, is that the technical affordances of an information environment dictate available\nadversary tactics. Social media sites, news platforms, and mobile messaging apps form an\noperational landscape, and the features of each platform are all features that can be\noperationalized—hashtags, comment or reply capabilities, live video streams, shared-interest\nsub-groups, privacy settings, rebroadcast capabilities, advertising and ad targeting systems,\nchat rooms, and so on. Just as these features comprise the many ways that people digitally\ncommunicate with one another and browse content, they are also the means of capturing\nand refocusing attention on which bad-faith actors rely.\n\n## Introducing SCOTCH\n\nIn seeking a comprehensive yet succinct framework to serve the operational community, it is\n[important to follow a Bottom Line Up Front (BLUF) philosophy: if a framework cannot be](https://hbr.org/2016/11/how-to-write-email-with-military-precision)\nused to both quickly describe an operation and easily distinguish it from others, it does not\nwork. Based on this approach, this paper has developed and operationalized the SCOTCH\n**Framework for characterizing influence operations.**\n\nThis framework was developed in close partnership with planners and operators within the\nUnited States Government (USG) and allied governments, analysis and data science teams\nacross USG and NGO spaces, and several researchers and investigators from major news\norganizations and academic institutions. Operators examined how information is\ncommunicated to decision makers through chains of command, and how they might improve\n\n\n-----\n\nthese information flows to enhance both situational awareness and decision making.\nResearchers examined how they sought to identify, contextualize, and communicate findings\nin order to improve resource allocation in a resource-constrained, data-rich environment.\n\nThe SCOTCH framework enables analysts to comprehensively and rapidly characterize\nan adversarial operation or campaign. Further, it is built to enable researchers and policy\nmakers to explore the underlying facets and constructs of influence, propaganda, and\npsychological operations in an organized and straightforward way. In doing so, SCOTCH\nhelps to bridge the research and policy communities and to identify dimensions of these\noperations that merit greater attention. The framework may be used at both the strategic and\ntactical levels of analysis. SCOTCH can characterize both a single operation and an\noverarching campaign.\n\nThe acronym describes:\n\n**S – Source**\n\n**C – Channel**\n\n**O – Objective**\n\n**T – Target**\n\n**C – Composition**\n\n**H – Hook**\n\n## Source\n\nThe source of a campaign may be identifiable individuals associated with a state or nonstate actor, cutouts, “bots”, or a third party such as a moderator. In many cases, the source\nmay not immediately be known to an analyst. During the 2020 US presidential election, the\nsource was occasionally the platform itself, as Twitter, Facebook, and other platforms took\nmeasures to counter and limit the spread of what the managing organizations determined\ndangerous influence operations.\n\n## Channel\n\nBoth the platform and its associated features or affordances are channels. A channel may\nbe a news site, an online game and its chat features, an advertising platform, a social media\nplatform, an online forum or chat room, and so on. Features of interest may include the\navailability and searchability of hashtags or viral content (and the existence of unsupervised\n“virality” algorithms), the existence of in-platform “groups” or subcommunities, the ability to\n\n\n-----\n\nlive-stream video, the persistence and public visibility of posted content over time, and the\nease of creating a new account and/or sharing new content. Such features create what some\n[call a “dancing competitive landscape” for varied forms of attention and influence.](https://pubsonline.informs.org/doi/abs/10.1287/isre.1100.0317)\n\n## Objective\n\nAs with the source, when monitoring influence operations in real-time, the objective may be\nheavily obfuscated. However, objectives may still be indirectly inferred given prior experience\nwith an adversary and its tradecraft.\n\nSome of the most powerful influence operations are those that galvanize populations to\npursue new objectives themselves. For example, in reviewing the QAnon conspiracies, a\nplurality of hypotheses exists regarding the group’s actual objectives: an attempt to\ndestabilize civil relations within the United States, a mechanism of making sense of\nabstractions such as “federal power,” to which many have limited exposure, a religious\nmovement, or a cash grab, to name a few. Analysts must apply their experience and\nhypothesis testing abilities carefully in making a determination and must also recognize that\nobjectives may change over time within a campaign— were any of these the original\nobjective(s) of “Q”? In the case of QAnon, organizing a group with shared, extreme views\nmay be understood as an objective in and of itself; once achieved, new objectives become\nattainable, ranging from further entrenching members’ beliefs, to doxxing and harassment, or\neven to real-world violent attacks.\n\n## Target\n\nConservatively, a target can be defined as the intended audience of a campaign over a\nspecific channel. The target may be demographically and/or geographically bounded or\ncharacterized by shared beliefs. In terms of scope, the direct targets of a campaign may be\nusers of an app, players of a game, individuals who meet particular advertising criteria,\nindividuals who are characterized by social media platforms’ ad tech as members of some\ndemographic category, members of a particular online community or network, subscribers of\nparticular publications, and so on. An adversary may choose a “target-channel” pair based\non the coverage of the target population afforded by the channel, as well as the sharing\nmechanisms baked into the design of the channel. In this way, available targets are\ndetermined in large part by the available channels and features, and in some cases, they can\nbe further scoped by the personal data and metadata available to these channels about their\nusers. The feasibility of targeting a particular group may also be mediated by a channel’s\nalgorithmic capabilities, which are frequently opaque.\n\n## Composition\n\n\n-----\n\nThe composition refers broadly to the specific language or image content used in an\ninfluence operation. In many cases, it refers simply to the content being shared. However, in\nmore sophisticated operations, it can also include technical details, such as the generation\nand employment of deepfakes or synthetically generated text and the structure and\npresentation of the materials. This category is also moderated by the channel and the hook\n(below), since the social channels and exploitation mechanisms leveraged will naturally\ninform the type of media content that may be generated and shared.\n\n## Hook\n\nTypically, the hook of an influence operation represents both the technical mechanisms of\nexploitation, which are closely tied to the composition and channel(s), as well as the\npsychological phenomena being exploited. The hook relates to the tactics of persuasion and\ndiffusion leveraged in the operation or campaign. These two constructs (persuasion and\ndiffusion) are both complementary—by design, particular diffusion or injection techniques\nbest serve particular strategic objectives—and occasionally substitutionary, wherein less\nconvincing content that is more widely shared may achieve the same objective as highly\npersuasive content that is less widely shared.\n\n## SCOTCH example\n\nAt the campaign level, a hypothetical example of a SCOTCH characterization is: Non-State\n**Actors used in an attempt to Undermine the Integrity of the 2020 Presidential Election**\namong Conservative American Audiences using Fake Images and Videos and capturing\nattention by Hashtag Hijacking and Posting in Shared-Interest Facebook Groups via\n**Cutouts.**\n\nAn influence campaign may feature multiple exemplary items within each category and may\ninclude multiple sub-branches representing a series of individual operations. For instance,\nin the above campaign, a careful reader may identify and distinguish two separate\noperations, both characterizable using SCOTCH. In one, hashtag hijacking (a hook) was\nused to draw the general public (a target) to a particular narrative or shared-interest\ncommunity (an objective). In the other, extreme content (including fake images and videos –\na composition) hosted on low-quality media outlets (another channel) is injected directly\ninto this community (a different target and hook) in order to harden group beliefs through\ncollective sensemaking and social identity-building activities (a different objective). SCOTCH\nquickly and accurately summarizes both operations, as well as the broader campaign.\n\nThe benefit of this framework is twofold. First, it is lightweight: SCOTCH characterizations\nare succinct and intuitive, leading to short, comprehensive summaries that can be easily\nbriefed and/or indexed. The above campaign characterization may remind many readers of\nheadlines from major media outlets, and it takes only moments to read and interpret.\n\n\n-----\n\nSecond, SCOTCH offers decision makers the comprehensive information needed to\nunderstand an operation, and it provides sufficient information to take counteractions that\nspecifically cater to the source(s), channels, objectives, targets, composition, and\n**hooks observed. It captures the key parameters of an operation or a campaign and enables**\neasy comparison between distinct operations without becoming unwieldy. To achieve the\nsame using other frameworks, an analyst would need to draw from ABC(D), ATT&CK, and\nBEND all at once:\n\nBEND for the behavioral, social network, and narrative hooks employed\nABC(D) for the sources, channels, and content composition observed in the operation,\nas well as channel-specific technical hooks\nATT&CK for characterization of and insight into the source and its behavioral &\ntechnical patterns, including common targets and channels\n\n## Conclusion\n\nThe SCOTCH framework is both a general-purpose framework for operational analysis and\ncharacterization and a starting point for deeper study and decision making. Strategic\nplanners may use SCOTCH to frame adversarial operations as one component in a broader\noperational and sociotechnical context. From a research standpoint, SCOTCH provides a\nsingle framework for researchers to characterize influence operations to behavioral,\ntechnical, operational, political, and commercial audiences. Operationally, it seeks to\nenhance analysts’ sensemaking capabilities by covering all key points and to enable them\nto quickly and succinctly summarize their observations. However, there are still missing\npieces. For instance, the framework does not provide for a more substantial explanation of\nhow a campaign may play into existing narratives in a nation or community. But in a bottomline up front (BLUF) environment, brevity is often an advantage.\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2021/2021-05-24 - SCOTCH- A framework for rapidly assessing influence operations.pdf"
    ],
    "report_names": [
        "2021-05-24 - SCOTCH- A framework for rapidly assessing influence operations.pdf"
    ],
    "threat_actors": [
        {
            "id": "dfee8b2e-d6b9-4143-a0d9-ca39396dd3bf",
            "created_at": "2022-10-25T16:07:24.467088Z",
            "updated_at": "2025-03-27T02:02:10.241387Z",
            "deleted_at": null,
            "main_name": "Circles",
            "aliases": [],
            "source_name": "ETDA:Circles",
            "tools": [],
            "source_id": "ETDA",
            "reports": null
        }
    ],
    "ts_created_at": 1673536255,
    "ts_updated_at": 1743041347,
    "ts_creation_date": 1653687483,
    "ts_modification_date": 1653687483,
    "files": {
        "pdf": "https://archive.orkl.eu/92e5de1199fb48d88a41410c442dded76ea7bf4c.pdf",
        "text": "https://archive.orkl.eu/92e5de1199fb48d88a41410c442dded76ea7bf4c.txt",
        "img": "https://archive.orkl.eu/92e5de1199fb48d88a41410c442dded76ea7bf4c.jpg"
    }
}