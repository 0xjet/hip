{
    "id": "f9a292b1-7bef-4a46-ad29-45f7c4fde261",
    "created_at": "2023-01-12T15:08:35.459781Z",
    "updated_at": "2025-03-27T02:09:32.903436Z",
    "deleted_at": null,
    "sha1_hash": "30b241d11eb4df4b5b1ad55a8292d9eae1c95ddf",
    "title": "2021-11-02 - Hunting for potential network beaconing patterns using Apache Spark via Azure Synapse – Part 1",
    "authors": "",
    "file_creation_date": "2022-05-28T17:30:19Z",
    "file_modification_date": "2022-05-28T17:30:19Z",
    "file_size": 2562769,
    "plain_text": "# Hunting for potential network beaconing patterns using Apache Spark via Azure Synapse – Part 1\n\n**techcommunity.microsoft.com/t5/microsoft-sentinel-blog/hunting-for-potential-network-beaconing-patterns-using-**\napache/ba-p/2916179\n\nNovember 2, 2021\n\n## Introduction\n\nWe previously blogged about Detect Network beaconing via Intra-Request time delta\npatterns in Microsoft Sentinel using native KQL from Microsoft Sentinel. This KQL query is\ncomplex in nature and often needs to operate on very large datasets such as network firewall\nlogs in [CommonSecurityLogs table. Even after applying optimization and baselining to](https://docs.microsoft.com/azure/sentinel/cef-name-mapping)\nreduce the original datasets, sometimes such complex operations do not scale well from Log\nAnalytics especially when you want to analyze and compare large historical datasets with the\ncurrent days data. In such cases you can offload expensive data manipulation and\nprocessing operations to services outside Microsoft Sentinel and bring actionable results\nback to continue hunting in Microsoft Sentinel. Please review our Ignite announcement\n[about Large Scale analytics with Azure Synapse and Microsoft Sentinel Notebooks](https://techcommunity.microsoft.com/t5/microsoft-sentinel-blog/what-s-new-large-scale-security-analytics-with-azure-synapse-and/ba-p/2916265)\n\n, In this first of 2-part blog, we will do notebook code and section walkthrough to show how\n[you can leverage power of Apache Spark via Azure Synapse in](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-overview) [Azure ML Notebooks to](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-run-jupyter-notebooks)\nperform [scalable hunting on large historical datasets to find potential network beaconing](https://docs.microsoft.com/en-us/azure/sentinel/notebooks-with-synapse)\npatterns. In second part, we will take a sample dataset and run the notebook on it to find\npotential beaconing traffic resulting from simulation of PowerShell Empire C2.\n\nIn general, the data pipeline and analysis architecture look like below.\n\n\n-----\n\n## Also special thanks to @Chi_Nguyen and Zhipeng Zhao for review, feedback on the blog and testing the notebook.\n\n Apache Spark via Azure Synapse in Azure ML Notebooks\n\n Pre-requisites\n\nTo use Apache Spark with Azure ML notebooks, you must first configure your data pipeline\nand connect Azure Synapse via linked service. You can check existing docs - Large-scale\nsecurity analytics using Azure Sentinel notebooks and Azure Synapse integration for a step\n[by step process and also use notebook Configurate Azure ML and Azure Synapse Analytics](https://github.com/Azure/Azure-Sentinel-Notebooks/blob/master/Configurate%20Azure%20ML%20and%20Azure%20Synapse%20Analytics.ipynb)\nto set up Azure ML and Azure Synapse Analytics environment. Additionally, the notebook\nprovides the steps to export your data from Log Analytics workspace to an Azure Data Lake\nStorage (ADLS) Gen2.\n\n## Loading the data\n\nOnce you have exported your logs to ADLS storage, you can connect to it and load the data\nrequired for analysis. In this case, we are going to load the current day’s dataset for analysis\nto find recent beaconing attempts. When you configure your data export to ADLS, it creates\nfolder architecture per day, hour or granular 5-min buckets. Since the data is distributed\nacross various folders, we need to programmatically generate the source path and load all\nthe files underneath it.\n\nLet’s start with specifying all the required parameters to generate ADLS paths. The ADLS\nstorage file is combination of various attributes such as storage account name, container\nname, subscription id, resource group etc. We are also specifying additional input\nparameters such as device vendor, end date and lookback_days.\n\n\n-----\n\nThe below example cell shows where you can provide these details one time. Note: All the\nsubsequent synapse cells start with %%synapse magic command. If you get an error on\nmagic not found, run setup notebook and install required packages first.\n\nNow once we have base path, data is stored across various directories.\n\nBelow example shows sample folder structure in the format year/month/day/hour/minute.\n\nTo generate such paths programmatically, we have created a function which accepts date\nand lookback period as input.\n\nFinally, once we have generated current_day and historical_day paths, you can load the\ndatasets as shown in below example cell. Here we have used below with additional options.\n\n[read : To read json file with additional options to keep the header and recursiveFileLook as](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrameReader.json.html)\ntrue to recursively scan directory for matching files.\n\n\n-----\n\nNote: recursiveFileLook is only available from Spark 3.1 so make sure to create Spark pool\nwith 3.1 or above.\n\n[select: To select specified columns required for analysis.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.select.html)\n\n[filter: To filter by column value.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.filter.html)\n\nYou can use a similar cell to load the historical dataset, where you can load individual files\nand union them together to load all historical data. Historical data will help in baselining the\nenvironment effectively, if you do not have historical data exported already, you can just run\nthis on current data.\n\n[union: Union of individual days from historical days dataset.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.union.html)\n\n\n-----\n\n-----\n\n## Data Preparation\n\nAs we are looking for external network beaconing before we do the baselining, you can filter\nthe input dataset to only IP address pair from Private to Public IP addresses. To do this, you\ncan use regular expressions and the PySpark operator rlike. Once we have populated\nadditional columns stating the type of destination IP, you can then filter only for public\ndestination IP addresses.\n\nIn this cell, we have used below PySpark APIs\n\n[withcolumn : To create new column based on the condition.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html)\n\n[rlike: To match with regular expression.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Column.rlike.html)\n\n\n-----\n\n[show: To show results after regex matching and filtering.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.show.html)\n\n## Scalable Baselining across Historical Data\n\nIf you have historical dataset (lookback days from 7 to 30 depending upon the availability)\nloaded, you can perform baselining on it. Baselining can help reduce false positives by\nremoving frequently occurring public destinations which are likely benign. You can take\nvarious baselining approaches to reduce input dataset.\n\nIn this notebook, we are going to use few static thresholds. You can also use dynamic logic\nin terms of percentages across total hosts in your environments if the thresholds are too low\nor high.\n\n**daily_event_count_threshold : minimum number of events in a day. Default value set**\nto 100. Any source IP addresses below threshold will be ignored.\n**degree_of_srcip_threshold: max number of sources IP addresses per destination IP.**\nDefault value set to 25. Any destination IP addresses above this threshold will be\nignored.\n\nOnce we identified both source IP and destination IPs from the baselining, you can further\njoin this with the original current dataset to find beaconing patterns.\n\nBelow cell shows such baselining operations along with spark APIs.\n\n[groupBy : Groups dataframe using specified columns so we can run aggregations on them.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html)\n\n[orderBy: Returns new dataframe sorted by specified column.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html)\n\n[agg: Aggregate entire dataframe.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.agg.html)\n\n[countDistinct: Return distinct count of column.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.countDistinct.html)\n\n[alias: Returns column aliased with new name.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Column.alias.html)\n\n\n-----\n\n[join: Join another dataframe such as csl_srcip, csl_dstip.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.join.html)\n\n## Data Wrangling using Apache Spark\n\nOnce we have finished loading current data, historical data, prepare the dataset by filtering\nvia regex and apply baselining to it, we can perform additional transformations on the\nPySpark dataframe.\n\nSort the dataset by SourceIP.\nCalculate the time difference between first event and next event.\nPartition dataset per Source IP, Destination IP or Destination Port\nGroup dataset into consecutive 3 rows to calculate the Timedeltalistcount which is\naggregate of 3 respective timedelta counts\nCalculate percentagebeacon between TotalEventscount and Timedeltalistcount\nApply thresholds to further reduce false positives\n\n### Windowing\n\nIn the first step, we have sorted dataset and partitioned per SourceIP to calculate the time\ndifference between the first and next row.\n\n\n-----\n\n[Window: Utility function for defining window in dataframe. In this case, we have created](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Window.html)\nwindows by partitioning by SourceIp first and later by multiple fields; DeviceVendor,\nSourceIP, DestinationIP, DestinationPort.\n\n### Time Delta Calculation\n\nIn this step, we have used lag to calculate time delta on the serialized and partitioned data\nobtained from previous step.\n\n[lag : Returns value that is offset rows before current row. In this case, we are calculating time](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.lag.html)\ndelta between two consecutive events.\n\n[unix_timestamp: convert time string to Unix time stamp (in seconds) to return timedelta in](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.unix_timestamp.html)\nseconds.\n\nWe have also performed multiple aggregations to populate additional columns as shown in\nthe screenshot.\n\n[sum : Computes sum for each numeric columns for each group.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.GroupedData.sum.html)\n\n[count: Returns number of rows in dataframe.](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.count.html)\n\n\n-----\n\n### Repartition and Calculate Percentage Beaconing\n\nNow, we can create a window specification with frame boundaries of 3 rows by partitioning\nbased on DeviceVendor, SourceIP, DestinationIP, DestinationPort and order it by SourceIP in\n. This step will group dataset into consecutive 3 rows to calculate the Timedeltalistcount\nwhich is an aggregate of 3 respective timedelta counts.\n\n[Rowsbetween : Crates window spec with frame boundaries defined from start (inclusive) to](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.Window.rowsBetween.html)\nend (inclusive).\n\n[Expr : Parses expression string into column. In this case we are calculating sum values of](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.expr.html)\narray integer.\n\n[Aggregate : aggregate(expre, start, merge,finish) – applies binary operator to an initial state](https://spark.apache.org/docs/latest/api/sql/#aggregate)\nand all elements of array and reduces this to a single state. In this case, we are aggregating\nall the timedeltalist and calculating respective sum of each.\n\nWe have also calculated PercentageBeacon which will be =\n\n\n-----\n\nAggregated count per time delta or list of time delta/ Total Events for Source-DestinationIPPort * 100.\n\n## Export Results from ADLS\n\nNow you can export potential beaconing results retrieved from previous step and export it\nlocally to be used outside the Azure Synapse session. To do, we have used SPARK API\ncoalesce.\n\n\n-----\n\n[Coalesce: You can specify number of partitions (e.g. 1) to output dataframe results into](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.coalesce.html)\nsingle json file and write back to ADLS in the specified directory.\n\nYou can then stop the session by executing %synapse stop.\n\nOnce you are outside of synapse session, you can then download the output from ADLS\nlocally and perform various data analysis, enrichment operations.\n\nSpecify the input parameters for ADLS account again since it is outside synapse session, it\nwon’t recognize the variables declared inside synapse session.\n\n## Enriching entities with MSTICPy for investigation\n\nIn order to investigate the beaconing results, we can further automate the entity enrichment\ntasks such as [GeoIP lookup, Whois lookup and ThreatIntel lookups using native features of](https://msticpy.readthedocs.io/en/latest/data_acquisition/GeoIPLookups.html)\nMSTICPy library.\n\nYou can also visualize results onto geographical map using FoliumMap visualization of\nMSTICPy.\n\nPlease refer MSTICPy [data enrichment and](https://msticpy.readthedocs.io/en/latest/DataEnrichment.html) [visualization for detailed information along with](https://msticpy.readthedocs.io/en/latest/visualization/FoliumMap.html)\nexample notebooks. Once you have enriched results, you can then send those results back\n[to Sentinel as bookmark by using MSTICPy Microsoft Sentinel APIs.](https://msticpy.readthedocs.io/en/latest/data_acquisition/AzureSentinel.html)\n\n\n-----\n\n## Conclusion\n\nThe notebook implemented use case outlined previously in the blog using Apache spark,\napplied baselining methods on historical datasets to further reduce datasets and also\nenriched entities with useful information using MSTICPy features to automate common\ninvestigation steps for analysts. You can further take these results into Microsoft Sentinel to\neither combine with other datasets or alerts to increase fidelity or create incidents to\ninvestigate the reasoning behind the patterns observed. In the second part of the blog, we\nwill take simulated datasets to test this notebook and analyze the results.\n\n## References\n\nDetect Network beaconing via Intra-Request time delta patterns in Microsoft Sentinel Microsoft Tec...\n[Large Scale analytics with Azure Synapse and Microsoft Sentinel Notebooks](https://techcommunity.microsoft.com/t5/microsoft-sentinel-blog/what-s-new-large-scale-security-analytics-with-azure-synapse-and/ba-p/2916265)\n[Guided Hunting Notebook : https://aka.ms/Beacon_Synapse_Notebook](https://aka.ms/Beacon_Synapse_Notebook)\nApache PySpark SQL :\n[https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html)\n[Introducing Window Functions in Spark SQL - The Databricks Blog](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)\nhttps://stackoverflow.com/questions/47839077/pyspark-best-way-to-sum-values-incolumn-of-type-arrayi...\n[https://msticpy.readthedocs.io/en/latest/data_acquisition/GeoIPLookups.html](https://msticpy.readthedocs.io/en/latest/data_acquisition/GeoIPLookups.html)\n[https://msticpy.readthedocs.io/en/latest/data_acquisition/TIProviders.html](https://msticpy.readthedocs.io/en/latest/data_acquisition/TIProviders.html)\n[https://msticpy.readthedocs.io/en/latest/visualization/FoliumMap.html](https://msticpy.readthedocs.io/en/latest/visualization/FoliumMap.html)\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2021/2021-11-02 - Hunting for potential network beaconing patterns using Apache Spark via Azure Synapse – Part 1.pdf"
    ],
    "report_names": [
        "2021-11-02 - Hunting for potential network beaconing patterns using Apache Spark via Azure Synapse – Part 1.pdf"
    ],
    "threat_actors": [
        {
            "id": "faa4a29b-254a-45bd-b412-9a1cbddbd5e3",
            "created_at": "2022-10-25T16:07:23.80111Z",
            "updated_at": "2025-03-27T02:02:09.985067Z",
            "deleted_at": null,
            "main_name": "LookBack",
            "aliases": [
                "FlowingFrog",
                "LookBack",
                "LookingFrog",
                "TA410",
                "Witchetty"
            ],
            "source_name": "ETDA:LookBack",
            "tools": [
                "FlowCloud",
                "GUP Proxy Tool",
                "SodomMain",
                "SodomMain RAT",
                "SodomNormal"
            ],
            "source_id": "ETDA",
            "reports": null
        }
    ],
    "ts_created_at": 1673536115,
    "ts_updated_at": 1743041372,
    "ts_creation_date": 1653759019,
    "ts_modification_date": 1653759019,
    "files": {
        "pdf": "https://archive.orkl.eu/30b241d11eb4df4b5b1ad55a8292d9eae1c95ddf.pdf",
        "text": "https://archive.orkl.eu/30b241d11eb4df4b5b1ad55a8292d9eae1c95ddf.txt",
        "img": "https://archive.orkl.eu/30b241d11eb4df4b5b1ad55a8292d9eae1c95ddf.jpg"
    }
}