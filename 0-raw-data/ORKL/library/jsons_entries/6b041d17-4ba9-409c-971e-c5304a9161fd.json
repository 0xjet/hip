{
    "id": "6b041d17-4ba9-409c-971e-c5304a9161fd",
    "created_at": "2023-04-26T02:09:28.171815Z",
    "updated_at": "2025-03-27T02:09:29.686087Z",
    "deleted_at": null,
    "sha1_hash": "9cfc6215544f2e337aa59b40318da4ab4d4fb6f3",
    "title": "",
    "authors": "",
    "file_creation_date": "2022-08-29T07:48:06Z",
    "file_modification_date": "2022-08-29T07:48:06Z",
    "file_size": 4799812,
    "plain_text": "### National Security Agency Cybersecurity and Infrastructure Security Agency\n\n Cybersecurity Technical Report\n\n# Kubernetes Hardening Guide\n\n### August 2022\n\nU/OO/168286-21\n\nPP-22-0324\n\nVersion 1.2\n\n\n-----\n\n### Notices and history\n##### Document change history\n\n**Date** **Version** **Description**\nAugust 2021 1.0 Initial publication\nMarch 2022 1.1 Updated guidance based on industry feedback\nAugust 2022 1.2 Corrected automountServiceAccountToken (Authentication and\nAuthorization), clarified ClusterRoleBinding (Appendix K)\n\n##### Disclaimer of warranties and endorsement\nThe information and opinions contained in this document are provided \"as is\" and\nwithout any warranties or guarantees. Reference herein to any specific commercial\nproducts, process, or service by trade name, trademark, manufacturer, or otherwise,\ndoes not necessarily constitute or imply its endorsement, recommendation, or favoring\nby the United States Government, and this guide shall not be used for advertising or\nproduct endorsement purposes.\n\n##### Trademark recognition\nKubernetes is a registered trademark of The Linux Foundation. ▪ SELinux is a registered\ntrademark of the National Security Agency. ▪ AppArmor is a registered trademark of\nSUSE LLC. ▪ Windows and Hyper-V are registered trademarks of Microsoft Corporation.\n\n- ETCD is a registered trademark of CoreOS, Inc. ▪ Syslog-ng is a registered trademark\nof One Identity Software International Designated Activity Company. ▪ Prometheus is a\nregistered trademark of The Linux Foundation. ▪ Grafana is a registered trademark of\nRaintank, Inc. dba Grafana Labs ▪ Elasticsearch and ELK Stack are registered\ntrademarks of Elasticsearch B.V.\n\n##### Copyright recognition\n[Information, examples, and figures in this document are based on Kubernetes](https://kubernetes.io/docs/)\n[Documentation](https://kubernetes.io/docs/) [by The Kubernetes Authors, published under a Creative Commons](https://git.k8s.io/website/LICENSE)\nAttribution 4.0 license.\n\n##### Acknowledgements\nNSA and CISA acknowledge the feedback received from numerous partners and the\ncybersecurity community on the previous version of this report, and thank them for their\nhelp in making it better. Changes have been incorporated where appropriate.\n\n|Date|Version|Description|\n|---|---|---|\n|August 2021|1.0|Initial publication|\n|March 2022|1.1|Updated guidance based on industry feedback|\n|August 2022|1.2|Corrected automountServiceAccountToken (Authentication and Authorization), clarified ClusterRoleBinding (Appendix K)|\n\n\n-----\n\n### Publication information\n\n##### Author(s) \nNational Security Agency (NSA)\nCybersecurity Directorate\nEndpoint Security\n\nCybersecurity and Infrastructure Security Agency (CISA)\n\n##### Contact information\nClient Requirements / General Cybersecurity Inquiries:\n[Cybersecurity Requirements Center, 410-854-4200, Cybersecurity_Requests@nsa.gov](mailto:Cybersecurity_Requests@nsa.gov)\n\nMedia inquiries / Press Desk:\n[Media Relations, 443-634-0721, MediaRelations@nsa.gov](mailto:MediaRelations@nsa.gov)\n\nFor incident response resources, contact CISA at CISAServiceDesk@cisa.dhs.gov.\n\n##### Purpose\nNSA and CISA developed this document in furtherance of their respective cybersecurity\nmissions, including their responsibilities to develop and issue cybersecurity\nspecifications and mitigations. This information may be shared broadly to reach all\nappropriate stakeholders.\n\n\n-----\n\n### Executive summary\n\nKubernetes[®] is an open-source system that automates the deployment, scaling, and\nmanagement of applications run in containers, and is often hosted in a cloud\nenvironment. Using this type of virtualized infrastructure can provide several flexibility\nand security benefits compared to traditional, monolithic software platforms. However,\nsecurely managing everything from microservices to the underlying infrastructure\nintroduces other complexities. This report is designed to help organizations handle\nKubernetes-associated risks and enjoy the benefits of using this technology.\n\nThree common sources of compromise in Kubernetes are supply chain risks, malicious\nthreat actors, and insider threats. Supply chain risks are often challenging to mitigate\nand can arise in the container build cycle or infrastructure acquisition. Malicious threat\nactors can exploit vulnerabilities and misconfigurations in components of the\nKubernetes architecture, such as the control plane, worker nodes, or containerized\napplications. Insider threats can be administrators, users, or cloud service providers.\nInsiders with special access to an organization’s Kubernetes infrastructure may be able\nto abuse these privileges.\n\nThis guide describes the security challenges associated with setting up and securing a\nKubernetes cluster. It includes strategies for system administrators and developers of\nNational Security Systems, helping them avoid common misconfigurations and\nimplement recommended hardening measures and mitigations when deploying\nKubernetes. This guide details the following mitigations:\n\n  - Scan containers and Pods for vulnerabilities or misconfigurations.\n\n  - Run containers and Pods with the least privileges possible.\n\n  - Use network separation to control the amount of damage a compromise can\n\ncause.\n\n  - Use firewalls to limit unneeded network connectivity and use encryption to\n\nprotect confidentiality.\n\n  - Use strong authentication and authorization to limit user and administrator\n\naccess as well as to limit the attack surface.\n\n  - Capture and monitor audit logs so that administrators can be alerted to potential\n\nmalicious activity.\n\n  - Periodically review all Kubernetes settings and use vulnerability scans to ensure\n\nrisks are appropriately accounted for and security patches are applied.\n\n\n-----\n\nFor additional security hardening guidance, see the Center for Internet Security\nKubernetes benchmarks, the Docker and Kubernetes Security Technical\nImplementation Guides, the Cybersecurity and Infrastructure Security Agency (CISA)\nanalysis report, and Kubernetes documentation [1], [2], [3], [6].\n\n\n-----\n\n### Contents\n\nKubernetes Hardening Guide ............................................................................................. i\n**Executive summary ................................................................................................................. iii**\n**Contents .................................................................................................................................... v**\n**Introduction ............................................................................................................................... 1**\n\nRecommendations ................................................................................................................... 2\nArchitectural overview ............................................................................................................. 4\n**Threat model ............................................................................................................................. 6**\n**Kubernetes Pod security .......................................................................................................... 8**\n\n“Non-root” containers and “rootless” container engines ........................................................... 9\nImmutable container file systems ........................................................................................... 10\nBuilding secure container images .......................................................................................... 10\nPod security enforcement ...................................................................................................... 12\nProtecting Pod service account tokens .................................................................................. 12\nHardening container environments ........................................................................................ 13\n**Network separation and hardening ....................................................................................... 14**\n\nNamespaces ......................................................................................................................... 14\nNetwork policies .................................................................................................................... 15\nResource policies .................................................................................................................. 17\nControl plane hardening ........................................................................................................ 18\n\nEtcd ................................................................................................................................... 19\nKubeconfig Files ................................................................................................................ 19\nWorker node segmentation .................................................................................................... 19\nEncryption ............................................................................................................................. 20\nSecrets .................................................................................................................................. 20\nProtecting sensitive cloud infrastructure ................................................................................ 21\n**Authentication and authorization .......................................................................................... 22**\n\nAuthentication ........................................................................................................................ 22\nRole-based access control .................................................................................................... 23\n**Audit Logging and Threat Detection ..................................................................................... 27**\n\nLogging ................................................................................................................................. 27\n\nKubernetes native audit logging configuration .................................................................... 29\nWorker node and container logging ................................................................................... 30\nSeccomp: audit mode ........................................................................................................ 32\nSyslog ................................................................................................................................ 32\nSIEM platforms .................................................................................................................. 33\nService meshes ................................................................................................................. 34\nFault tolerance ................................................................................................................... 35\nThreat Detection .................................................................................................................... 36\n\nAlerting .............................................................................................................................. 37\nTools ..................................................................................................................................... 38\n**Upgrading and application security practices ...................................................................... 40**\n**Works cited ............................................................................................................................. 41**\n\n\n-----\n\n**Appendix A: Example Dockerfile for non-root application .................................................. 42**\n**Appendix B: Example deployment template for read-only file system ............................... 43**\n**Appendix C: Pod Security Policies (deprecated) .................................................................. 44**\n**Appendix D: Example Pod Security Policy ........................................................................... 46**\n**Appendix E: Example namespace ......................................................................................... 48**\n**Appendix F: Example network policy .................................................................................... 49**\n**Appendix G: Example LimitRange ......................................................................................... 50**\n**Appendix H: Example ResourceQuota .................................................................................. 51**\n**Appendix I: Example encryption ............................................................................................ 52**\n**Appendix J: Example KMS configuration ............................................................................. 53**\n**Appendix K: Example pod-reader RBAC Role ...................................................................... 54**\n**Appendix L: Example RBAC RoleBinding and ClusterRoleBinding.................................... 55**\n**Appendix M: Audit Policy ....................................................................................................... 57**\n**Appendix N: Example Flags to Enable Audit Logging ......................................................... 59**\n\nFigures\n\n**Figure 1: High-level view of Kubernetes cluster components .............................................. 1**\n**Figure 2: Kubernetes architecture .......................................................................................... 4**\n**Figure 3: Example of container supply chain dependencies introducing malicious code**\n**into a cluster ............................................................................................................................ 7**\n**Figure 4: Pod components with sidecar proxy as logging container ................................... 9**\n**Figure 5: A hardened container build workflow ....................................................................11**\n**Figure 6: Possible Role, ClusterRole, RoleBinding, and ClusterRoleBinding combinations**\n**to assign access .....................................................................................................................25**\n**Figure 7: Cluster leveraging service mesh to integrate logging with network security .....35**\n\nTables\n\n**Table I: Control plane ports ....................................................................................................18**\n**Table II: Worker node ports ....................................................................................................20**\n**Table III: Remote logging configuration ................................................................................31**\n**Table IV: Detection recommendations ..................................................................................36**\n**Table V: Pod Security Policy components ............................................................................44**\n\n\n-----\n\n### Introduction\n\nKubernetes, frequently abbreviated “K8s” because there are 8 letters between K and S,\nis an open-source container-orchestration system used to automate deploying, scaling,\nand managing containerized applications. As illustrated in the following figure, it\nmanages all elements that make up a cluster, from each microservice in an application\nto entire clusters. Using containerized applications as microservices provides more\nflexibility and security benefits compared to monolithic software platforms, but also can\nintroduce other complexities.\n\n**_Figure 1: High-level view of Kubernetes cluster components_**\n\nThis guide focuses on security challenges and suggests hardening strategies for\nadministrators of National Security Systems and critical infrastructure. Although this\nguide is tailored to National Security Systems and critical infrastructure organizations,\nNSA and CISA also encourage administrators of federal and state, local, tribal, and\nterritorial (SLTT) government networks to implement the recommendations in this guide.\nKubernetes clusters can be complex to secure and are often abused in compromises\n\n\n-----\n\nthat exploit their misconfigurations. This guide offers specific security configurations that\ncan help build more secure Kubernetes clusters.\n\n##### Recommendations\n\nA summary of the key recommendations from each section are:\n\n  - Kubernetes Pod security\n\n     - Use containers built to run applications as non-root users.\n\n     - Where possible, run containers with immutable file systems.\n\n     - Scan container images for possible vulnerabilities or misconfigurations.\n\n     - Use a technical control to enforce a minimum level of security including:\n\n Preventing privileged containers.\n\n Denying container features frequently exploited to breakout, such\n\nas hostPID, hostIPC, hostNetwork, allowedHostPath.\n\n Rejecting containers that execute as the root user or allow\n\nelevation to root.\n\n Hardening applications against exploitation using security services\n\nsuch as SELinux[®], AppArmor[®], and secure computing mode\n(seccomp).\n\n  - Network separation and hardening\n\n     - Lock down access to control plane nodes using a firewall and role-based\n\naccess control (RBAC). Use separate networks for the control plane\ncomponents and nodes.\n\n     - Further limit access to the Kubernetes etcd server.\n\n     - Configure control plane components to use authenticated, encrypted\n\ncommunications using Transport Layer Security (TLS) certificates.\n\n     - Encrypt etcd at rest and use a separate TLS certificate for communication.\n\n     - Set up network policies to isolate resources. Pods and services in different\n\nnamespaces can still communicate with each other unless additional\nseparation is enforced.\n\n     - Create an explicit deny network policy.\n\n     - Place all credentials and sensitive information encrypted in Kubernetes\n\nSecrets rather than in configuration files. Encrypt Secrets using a strong\nencryption method. Secrets are not encrypted by default.\n\n  - Authentication and authorization\n\n     - Disable anonymous login (enabled by default).\n\n\n-----\n\n    - Use strong user authentication.\n\n    - Create RBAC policies with unique roles for users, administrators,\n\ndevelopers, service accounts, and infrastructure team.\n\n- Audit Logging and Threat Detection\n\n    - Enable audit logging (disabled by default).\n\n    - Persist logs to ensure availability in the case of node, Pod, or container\nlevel failure.\n\n    - Configure logging throughout the environment (e.g., cluster application\n\nprogram interface (API) audit event logs, cluster metric logs, application\nlogs, Pod seccomp logs, repository audit logs, etc.).\n\n    - Aggregate logs external to the cluster.\n\n    - Implement a log monitoring and alerting system tailored to the\n\norganization’s cluster.\n\n- Upgrading and application security practices\n\n   - Promptly apply security patches and updates.\n\n    - Perform periodic vulnerability scans and penetration tests.\n\n   - Uninstall and delete unused components from the environment.\n\n\n-----\n\n##### Architectural overview\n\nKubernetes uses a cluster architecture. A Kubernetes cluster comprises many control\nplanes and one or more physical or virtual machines called “worker nodes.” The worker\nnodes host Pods, which contain one or more containers.\n\nA container is a runtime environment containing a software package and all its\ndependencies. Container images are standalone collections of the executable code and\ncontent that are used to populate a container environment as illustrated in the following\nfigure:\n\n**_Figure 2: Kubernetes architecture_**\n\nThe control plane makes decisions about the cluster. This includes scheduling\ncontainers to run, detecting/responding to failures, and starting new Pods when the\nnumber of replicas listed in a Deployment file is unsatisfied. The following logical\n\ncomponents are all part of the control plane:\n\n  - **Controller manager – Monitors the Kubernetes cluster to detect and maintain**\n\nseveral aspects of the Kubernetes environment including joining Pods to\nservices, maintaining the correct number of Pods in a set, and responding to the\nloss of nodes.\n\n\n-----\n\n  - **Cloud controller manager – An optional component used for cloud-based**\n\ndeployments. The cloud controller interfaces with the cloud service provider\n(CSP) to manage load balancers and virtual networking for the cluster.\n\n  - **Kubernetes application programming interface (API) server – The interface**\n\nthrough which administrators direct Kubernetes. As such, the API server is\ntypically exposed outside of the control plane. It is designed to scale and may\nexist on multiple control plane nodes.\n\n  - **Etcd[®] – The persistent backing store where all information regarding the state of**\n\nthe cluster is kept. Etcd is not intended to be manipulated directly but should be\n\nmanaged through the API server.\n\n  - **Scheduler – Tracks the status of worker nodes and determines where to run**\n\nPods. Kube-scheduler is intended to be accessible only from within the control\nplane.\n\nKubernetes worker nodes are physical or virtual machines dedicated to running\ncontainerized applications for the cluster. In addition to running a container engine,\nworker nodes host the following two services that allow orchestration from the control\nplane:\n\n  - **Kubelet – Runs on each worker node to orchestrate and verify Pod execution.**\n\n  - **Kube-proxy – A network proxy that uses the host’s packet filtering capability to**\n\nensure correct packet routing in the Kubernetes cluster.\n\nClusters are commonly hosted using a CSP Kubernetes service or an on-premises\nKubernetes service; CSPs often provide additional features. They administer most\naspects of managed Kubernetes services; however, organizations may need to handle\nsome Kubernetes service aspects, such as authentication and authorization, because\ndefault CSP configurations are typically not secure. When designing a Kubernetes\nenvironment, organizations should understand their responsibilities in securely\nmaintaining the cluster.\n\n_▲Return to Contents_\n\n\n-----\n\n### Threat model\n\nKubernetes can be a valuable target for data or compute power theft. While data theft is\ntraditionally the primary motivation, cyber actors seeking computational power (often for\ncryptocurrency mining) are also drawn to Kubernetes to harness the underlying\ninfrastructure. In addition to resource theft, cyber actors may target Kubernetes to cause\na denial of service. The following threats represent some of the most likely sources of\ncompromise for a Kubernetes cluster:\n\n  - **Supply Chain – Attack vectors to the supply chain are diverse and challenging**\n\nto mitigate. The risk is that an adversary may subvert any element that makes up\n\na system. This includes product components, services, or personnel that help\nsupply the end product. Additional supply chain risks can include third-party\nsoftware and vendors used to create and manage the Kubernetes cluster. Supply\nchain compromises can affect Kubernetes at multiple levels including:\n\n     - Container/application level – The security of applications and their third\nparty dependencies running in Kubernetes rely on the trustworthiness of\nthe developers and the defense of the development infrastructure. A\nmalicious container or application from a third party could provide cyber\nactors with a foothold in the cluster.\n\n     - Container runtime – Each node has a container runtime installed to load\n\ncontainer images from the repository. It monitors local system resources,\nisolates system resources for each container, and manages container\nlifecycle. A vulnerability in the container runtime could lead to insufficient\nseparation between containers.\n\n     - Infrastructure – The underlying systems hosting Kubernetes have their\n\nown software and hardware dependencies. Any compromise of systems\nused as worker nodes or as part of the control plane could provide cyber\nactors with a foothold in the cluster.\n\n\n-----\n\n**_Figure 3: Example of container supply chain dependencies introducing malicious code into a cluster_**\n\n- **Malicious Threat Actor – Malicious actors often exploit vulnerabilities or stolen**\n\ncredentials from social engineering to gain access from a remote location.\nKubernetes architecture exposes several APIs that cyber actors could potentially\nleverage for remote exploitation including:\n\n   - Control plane – The Kubernetes control plane has many components that\n\ncommunicate to track and manage the cluster. Cyber actors frequently\ntake advantage of exposed control plane components lacking appropriate\naccess controls.\n\n    - Worker nodes – In addition to running a container engine, worker nodes\n\nhost the kubelet and kube-proxy service, which are potentially exploitable\nby cyber actors. Additionally, worker nodes exist outside of the lockeddown control plane and may be more accessible to cyber actors.\n\n    - Containerized applications – Applications running inside the cluster are\n\ncommon targets. They are frequently accessible outside of the cluster,\nmaking them reachable by remote cyber actors. An actor can then pivot\nfrom an already compromised Pod or escalate privileges within the cluster\nusing an exposed application’s internally accessible resources.\n\n- **Insider Threat – Threat actors can exploit vulnerabilities or use privileges given**\n\nto the individual while working within the organization. Individuals from within the\norganization have special knowledge and privileges that can be used against\nKubernetes clusters.\n\n    - Administrator – Kubernetes administrators have control over running\n\ncontainers, including executing arbitrary commands inside containerized\nenvironments. Kubernetes-enforced RBAC authorization can reduce the\nrisk by restricting access to sensitive capabilities. However, because\n\n\n-----\n\nKubernetes lacks two-person integrity controls, at least one administrative\naccount must be capable of gaining control of the cluster. Administrators\noften have physical access to the systems or hypervisors, which could\nalso be used to compromise the Kubernetes environment.\n\n     - User – Containerized application users may know and have credentials to\n\naccess containerized services in the Kubernetes cluster. This level of\naccess could provide sufficient means to exploit either the application itself\nor other cluster components.\n\n     - Cloud service or infrastructure provider – Access to physical systems or\n\nhypervisors managing Kubernetes nodes could be used to compromise a\n\nKubernetes environment. CSPs often have layers of technical and\nadministrative controls to protect systems from privileged administrators.\n\n_▲Return to Contents_\n\n### Kubernetes Pod security\n\nPods are the smallest deployable Kubernetes unit and consist of one or more\ncontainers. Pods are often a cyber actor’s initial execution environment upon exploiting\na container. For this reason, Pods should be hardened to make exploitation more\n\ndifficult and to limit the impact of a successful compromise. The following figure\nillustrates the components of a Pod and possible attack surface.\n\n\n-----\n\n**_Figure 4: Pod components with sidecar proxy as logging container_**\n\n##### “Non-root” containers and “rootless” container engines\n\nBy default, many container services run as the privileged root user, and applications\n\nexecute inside the container as root despite not requiring privileged execution.\nPreventing root execution by using non-root containers or a rootless container engine\nlimits the impact of a container compromise. Both methods affect the runtime\nenvironment significantly, so applications should be thoroughly tested to ensure\n\ncompatibility.\n\n**Non-root containers – Container engines allow containers to run applications as a**\n\nnon-root user with non-root group membership. Typically, this non-default setting is\nconfigured when the container image is built. For an example Dockerfile that runs an\napplication as a non-root user, refer to Appendix A: Example Dockerfile for non\n**root application. Alternatively, Kubernetes can load containers into a Pod with**\n```\n  SecurityContext:runAsUser specifying a non-zero user. While the runAsUser\n\n```\ndirective effectively forces non-root execution at deployment, NSA and CISA\nencourage developers to build container applications to execute as a non-root user.\nHaving non-root execution integrated at build time provides better assurance that\napplications will function correctly without root privileges.\n\n\n-----\n\n**Rootless container engines – Some container engines can run in an unprivileged**\n\ncontext rather than using a daemon running as root. In this scenario, execution\nwould appear to use the root user from the containerized application’s perspective,\nbut execution is remapped to the engine’s user context on the host. While rootless\ncontainer engines add an effective layer of security, many are currently released as\nexperimental and should not be used in a production environment. Administrators\nshould be aware of this emerging technology and adopt rootless container engines\nwhen vendors release a stable version compatible with Kubernetes.\n\n##### Immutable container file systems\n\nBy default, containers are permitted mostly unrestricted execution within their own\ncontext. A cyber actor who has gained execution in a container can create files,\ndownload scripts, and modify the application within the container. Kubernetes can lock\ndown a container’s file system, thereby preventing many post-exploitation activities.\nHowever, these limitations also affect legitimate container applications and can\npotentially result in crashes or anomalous behavior.\n\nTo prevent damaging legitimate applications, Kubernetes administrators can mount\nsecondary read/write file systems for specific directories where applications require\nwrite access. For an example immutable container with a writable directory, refer to\n\n**Appendix B:** **Example deployment template for read-only filesystem.**\n\n##### Building secure container images\n\nContainer images are usually created by either building a container from scratch or by\n\nbuilding on top of an existing image pulled from a repository. Repository controls within\nthe developer environment can be used to restrict developers to using only trusted\nrepositories. Specific controls vary depending on the environment but may include both\nplatform-level restrictions, such as admission controls, and network-level restrictions.\nKubernetes admission controllers, third-party tools, and some CSP-native solutions can\nrestrict entry so that only digitally signed images can execute in the cluster.\n\nIn addition to using trusted repositories to build containers, image scanning is key to\nensuring deployed containers are secure. Throughout the container build workflow,\nimages should be scanned to identify outdated libraries, known vulnerabilities, or\nmisconfigurations, such as insecure ports or permissions. Scanning should also provide\nthe flexibility to disregard false positives for vulnerability detection where knowledgeable\n\n\n-----\n\ncybersecurity professionals have deemed alerts to be inaccurate. As illustrated in the\nfollowing figure, one approach to implementing image scanning is to use an admission\ncontroller. An admission controller is a Kubernetes-native feature that can intercept and\nprocess requests to the Kubernetes API prior to persistence of the object, but after the\nrequest is authenticated and authorized. A custom or proprietary webhook can be\nimplemented to scan any image before it is deployed in the cluster. This admission\ncontroller can block deployments if the image does not comply with the organization’s\nsecurity policies defined in the webhook configuration [4].\n\n**_Figure 5: A hardened container build workflow_**\n\n\n-----\n\n##### Pod security enforcement \n\nEnforcing security requirements on Pods can be accomplished natively in Kubernetes\nthrough two mechanisms:\n\n1. A beta[1] release feature called Pod Security Admission – Production Kubernetes\n\nadministrators should adopt Pod Security Admission, as the feature is enabled by\ndefault in Kubernetes version 1.23. Pod Security Admission is based around\ncategorizing pods as privileged, baseline, and restricted and provides a more\nstraightforward implementation than PSPs. More information about Pod Security\nAdmission is available in the online documentation[2].\n\n2. A deprecated feature called Pod Security Policies (PSPs) – Administrators using\n\nPSPs while transitioning to Pod Security Admission can use information in\n**Appendix C:** **Pod Security Policies to enhance their PSPs.**\n\nIn addition to native Kubernetes solutions, third-party solutions often implemented as\nKubernetes admission controllers can provide additional fine-grained policy control.\nWhile these solutions are beyond the scope of this document, administrators may\nexplore the products available for their environment to determine the best solution for\ntheir needs.\n\n##### Protecting Pod service account tokens\n\nBy default, Kubernetes automatically provisions a service account when creating a Pod\nand mounts the account’s secret token within the Pod at runtime. Many containerized\napplications do not require direct access to the service account as Kubernetes\n\norchestration occurs transparently in the background. If an application is compromised,\naccount tokens in Pods can be gleaned by cyber actors and used to further compromise\nthe cluster. When an application does not need to access the service account directly,\nKubernetes administrators should ensure that Pod specifications disable the secret\ntoken being mounted. This can be accomplished using the\n```\n“automountServiceAccountToken: false” directive in the Pod’s YAML\n\n```\nspecification.\n\nIn some cases, containerized applications use provisioned service account tokens to\nauthenticate to external services, such as cloud platforms. In these cases, it can be\n\n1 Beta releases of software have generally passed some level of quality assurance and contain most planned functionality\n\n2 https://kubernetes.io/docs/concepts/security/pod-security-admission/\n\n\n-----\n\ninfeasible to disable the account token. Instead, cluster administrators should ensure\nthat RBAC is implemented to restrict Pod privileges within the cluster. For more\ninformation on RBAC, refer to the section on authentication and authorization.\n\n##### Hardening container environments\n\nSome platforms and container engines provide additional options or tools to harden\ncontainerized environments. For example:\n\n  - **Hypervisor-backed containerization – Hypervisors rely on hardware to enforce**\n\nthe virtualization boundary rather than the operating system. Hypervisor isolation\nis more secure than traditional container isolation. Container engines running on\nthe Windows[®] operating system can be configured to use the built-in Windows\nhypervisor, Hyper-V[®], to enhance security. Additionally, some security-focused\ncontainer engines natively deploy each container within a lightweight hypervisor\nfor defense-in-depth. Hypervisor-backed containers mitigate container breakouts.\n\n  - **Kernel-based solutions – The seccomp tool, which is disabled by default, can**\n\nbe used to limit a container’s system call abilities, thereby lowering the kernel’s\nattack surface. Seccomp can be enforced through a previously described Pod\npolicy. For more information on Seccomp, refer to Audit Logging and Threat\n**Detection.**\n\n  - **Application sandboxes – Some container engine solutions offer the option to**\n\nadd a layer of isolation between the containerized application and the host\nkernel. This isolation boundary forces the application to operate within a virtual\nsandbox thereby protecting the host operating system from malicious or\n\ndestructive operations.\n\n_▲Return to Contents_\n\n\n-----\n\n### Network separation and hardening\n\nCluster networking is a central concept of Kubernetes. Communication among\ncontainers, Pods, services, and external services must be taken into consideration. By\ndefault, Kubernetes resources are not isolated and do not prevent lateral movement or\nescalation if a cluster is compromised. Resource separation and encryption can be an\neffective way to limit a cyber actor’s movement and escalation within a cluster.\n\n\n##### Key points\n####  Use network policies and firewalls to separate and isolate resources.\n\n  Secure the control plane.\n\n  Encrypt traffic and sensitive data (such as Secrets) at rest.\n\n\n##### Namespaces\n\nKubernetes namespaces are one way to partition cluster resources among multiple\nindividuals, teams, or applications within the same cluster. By default, namespaces are\n_not automatically isolated. However, namespaces do assign a label to a scope, which_\ncan be used to specify authorization rules via RBAC and networking policies. In addition\nto policies that limit access to resources by namespace, resource policies can limit\nstorage and compute resources to provide better control over Pods at the namespace\nlevel.\n\nThere are three namespaces by default, and they cannot be deleted:\n\n  - kube-system (for Kubernetes components)\n\n  - kube-public (for public resources)\n\n  - default (for user resources)\n\nUser Pods should not be placed in kube-system or kube-public, as these are reserved\nfor cluster services. A YAML file, shown in Appendix E: Example namespace, can be\n\nused to create new namespaces. Pods and services in different namespaces can still\ncommunicate with each other unless additional separation is enforced.\n\n\n-----\n\n##### Network policies\n\nEvery Pod gets its own cluster-private IP address and can be treated similarly to virtual\nmachines (VMs) or physical hosts with regard to port allocation, naming, service\ndiscovery, and load balancing. Kubernetes can shift Pods to other nodes and recreate\nPods in a Deployment that have died. When that happens, the Pod IP addresses can\nchange, which means applications should not depend on the Pod IP being static.\n\nA Kubernetes Service is used to solve the issue of changing IP addresses. A Service is\nan abstract way to assign a unique IP address to a logical set of Pods selected using a\nlabel in the Pod configuration. The address is tied to the lifespan of the Service and will\nnot change while the Service is alive. The communication to a Service is automatically\nload-balanced among the Pods that are members of the Service.\n\nServices can be exposed externally using NodePorts or LoadBalancers, and internally.\nTo expose a Service externally, configure the Service to use TLS certificates to encrypt\ntraffic. Once TLS is configured, Kubernetes supports two ways to expose the Service to\nthe Internet: NodePorts and LoadBalancers.\n\nAdding type: NodePort to the Service specification file will assign a random port to\nbe exposed to the Internet using the cluster’s public IP address. The NodePort can also\n\nbe assigned manually if desired. Changing the type to LoadBalancer can be used in\nconjunction with an external load balancer. Ingress and egress traffic can be controlled\nwith network policies. Although Services cannot be selected by name in a network\npolicy, the Pods can be selected using the label that is used in the configuration to\n\nselect the Pods for the Service.\n\n\n-----\n\nNetwork policies control traffic flow between Pods,\nnamespaces, and external IP addresses. By default,\nno network policies are applied to Pods or\nnamespaces, resulting in unrestricted ingress and\negress traffic within the Pod network. Pods become\nisolated through a network policy that applies to the\nPod or the Pod’s namespace. Once a Pod is\nselected in a network policy, it rejects any\nconnections that are not specifically allowed by any\napplicable policy object.\n\n\n####  Use a default policy to deny all ingress\n\nand egress traffic. Ensures unselected\nPods are isolated to all namespaces\nexcept kube-system\n\n\n**Network Policies Checklist**\n\n####  Use a CNI plugin that supports\n\nNetworkPolicy API\n\n####  Create policies that select Pods using\n\npodSelector and/or the\nnamespaceSelector\n\n\nTo create network policies, a container network\n\n####  Use LimitRange and ResourceQuota\n\ninterface (CNI) plugin that supports the\n\npolicies to limit resources on a\n\nNetworkPolicy API is required. Pods are selected namespace or Pod level\nusing the podSelector and/or the\n```\nnamespaceSelector options. For an example network policy, refer to Appendix F:\n\n```\n**Example network policy.**\n\nNetwork policy formatting may differ depending on the CNI plugin used for the cluster.\nAdministrators should use a default policy selecting all Pods to deny all ingress and\negress traffic and ensure any unselected Pods are isolated. Additional policies could\nthen relax these restrictions for permissible connections.\n\nExternal IP addresses can be used in ingress and egress policies using ipBlock, but\ndifferent CNI plugins, cloud providers, or service implementations may affect the order\nof NetworkPolicy processing and the rewriting of addresses within the cluster.\n\nNetwork policies can also be used in conjunction with firewalls and other external tools\nto create network segmentation. Splitting the network into separate sub-networks or\nsecurity zones helps isolate public-facing applications from sensitive internal resources.\nOne of the major benefits to network segmentation is limiting the attack surface and\nopportunity for lateral movement. In Kubernetes, network segmentation can be used to\nseparate applications or types of resources to limit the attack surface.\n\n\n-----\n\n##### Resource policies\n\nLimitRanges, ResourceQuotas, and Process ID Limits restrict resource usage for\nnamespaces, nodes, or Pods. These policies are important to reserve compute and\nstorage space for a resource and avoid resource exhaustion.\n\nA LimitRange policy constrains individual resources per Pod or container within a\nparticular namespace, e.g., by enforcing maximum compute and storage resources.\nOnly one LimitRange constraint can be created per namespace. For an example YAML\nfile, refer to Appendix G: Example LimitRange.\n\nUnlike LimitRange policies that apply to each Pod or container individually,\nResourceQuotas are restrictions placed on the aggregate resource usage for an entire\nnamespace, such as limits placed on total CPU and memory usage. For an example\nResourceQuota policy, refer to Appendix H: Example ResourceQuota. If a user tries\n\nto create a Pod that violates a LimitRange or ResourceQuota policy, the Pod creation\nfails.\n\nProcess IDs (PIDs) are a fundamental resource on nodes and can be exhausted without\nviolating other resource limits. PID exhaustion prevents host daemons (such as\n```\nkubelet and kube-proxy) from running. Administrators can use node PID limits to\n\n```\nreserve a specified number of PIDs for system use and Kubernetes system daemons.\nPod PID limits are used to limit the number of processes running on each Pod. Eviction\npolicies can be used to terminate a Pod that is misbehaving and consuming abnormal\nresources. However, eviction policies are calculated and enforced periodically and do\n\nnot enforce the limit.\n\n\n-----\n\n##### Control plane hardening\n\nThe control plane is the core of Kubernetes and allows **Steps to secure the control plane**\nusers to view containers, schedule new Pods, read 1. Set up TLS encryption\nSecrets, and execute commands in the cluster. Because 2. Set up strong authentication\n\nmethods\n\nof these sensitive capabilities, the control plane should\n\n3. Disable access to internet and\n\nbe highly protected. In addition to secure configurations\n\nunnecessary, or untrusted networks\n\nsuch as TLS encryption, RBAC, and a strong\n\n4. Use RBAC policies to restrict\n\nauthentication method, network separation can help access\nprevent unauthorized users from accessing the control 5. Secure the etcd datastore with\nplane. The Kubernetes API server runs on port 6443, authentication and RBAC policies\n\n6. Protect kubeconfig files from\n\nwhich should be protected by a firewall to accept only\n\nunauthorized modifications\n\nexpected traffic. The Kubernetes API server should not\nbe exposed to the Internet or an untrusted network.\nNetwork policies can be applied to the kube-system namespace to limit internet access\nto the kube-system. If a default deny policy is implemented to all namespaces, the\nkube-system namespace must still be able to communicate with other control plane\nsegments and worker nodes.\n\n\nThe following table lists the control plane ports and services:\n\n**_Table I: Control plane ports_**\n\n|Protocol|Direction|Port Range|Purpose|\n|---|---|---|---|\n|TCP|Inbound|6443|Kubernetes API server|\n|TCP|Inbound|2379-2380|etcd server client API|\n|TCP|Inbound|10250|kubelet API|\n|TCP|Inbound|10259|kube-scheduler|\n|TCP|Inbound|10257|kube-controller-manager|\n\n\n-----\n\n**Etcd**\n\nThe etcd backend database stores state information and cluster Secrets. It is a critical\ncontrol plane component, and gaining write access to etcd could give a cyber actor root\naccess to the entire cluster. The etcd server should be configured to trust only\ncertificates assigned to the API server. Etcd can be run on a separate control plane\nnode, allowing a firewall to limit access to only the API servers. This limits the attack\nsurface when the API server is protected\nwith the cluster’s authentication method **The etcd backend database**\nand RBAC policies to restrict users.\n\n## is a critical control plane\n\nAdministrators should set up TLS\ncertificates to enforce Hypertext Transfer **component and the most**\nProtocol Secure (HTTPS) communication\n\n## important piece to secure\n\nbetween the etcd server and API servers.\nUsing a separate certificate authority (CA) **within the control plane.**\nfor etcd may also be beneficial, as it trusts\nall certificates issued by the root CA by\ndefault.\n\n**Kubeconfig Files**\n\n\nThe kubeconfig files contain sensitive information about clusters, users, namespaces,\nand authentication mechanisms. Kubectl uses the configuration files stored in the\n```\n$HOME/.kube directory on the worker node and control plane local machines. Cyber\n\n```\nactors can exploit access to this configuration directory to gain access to and modify\nconfigurations or credentials to further compromise the cluster. The configuration files\n\nshould be protected from unintended changes, and unauthenticated non-root users\nshould be blocked from accessing the files.\n\n##### Worker node segmentation\n\nA worker node can be a virtual or physical machine, depending on the cluster’s\nimplementation. Because nodes run the microservices and host the web applications for\nthe cluster, they are often the target of exploits. If a node becomes compromised, an\nadministrator should proactively limit the attack surface by separating the worker nodes\nfrom other network segments that do not need to communicate with the worker nodes or\nKubernetes services.\n\n\n-----\n\nDepending on the network, a firewall can be used to separate internal network\nsegments from the external-facing worker nodes or the entire Kubernetes service.\nExamples of services that may need to be separated from the possible attack surface of\nthe worker nodes are confidential databases or internal services that would not need to\nbe internet accessible.\n\nThe following table lists the worker node ports and services:\n\n**_Table II: Worker node ports_**\n\n**Protocol** **Direction** **Port Range** **Purpose**\nTCP Inbound 10250 kubelet API\nTCP Inbound 30000-32767 NodePort Services\n\n##### Encryption\n\nAdministrators should configure all traffic in the Kubernetes cluster—including between\ncomponents, nodes, and the control plane—to use TLS 1.2 or 1.3 encryption.\nEncryption can be set up during installation or afterward using TLS bootstrapping,\ndetailed in the [Kubernetes documentation, to create and distribute certificates to nodes.](https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/)\nFor all methods, certificates must be distributed among nodes to communicate securely.\n\n##### Secrets\n\nKubernetes Secrets maintain sensitive information, such as passwords, OAuth tokens,\nand Secure Shell (SSH) keys. Storing sensitive information in Secrets provides greater\naccess control than storing passwords or tokens in YAML files, container images, or\nenvironment variables. By default, Kubernetes stores Secrets as unencrypted base64encoded strings that can be retrieved by anyone with API access. Access can be\nrestricted by applying RBAC policies to the secrets resource.\n\n|Table II: Worker node ports|Col2|Col3|Col4|\n|---|---|---|---|\n|Protocol|Direction|Port Range|Purpose|\n|TCP|Inbound|10250|kubelet API|\n|TCP|Inbound|30000-32767|NodePort Services|\n\n\nSecrets can be encrypted by configuring dataat-rest encryption on the API server or by\nusing an external key management service\n\n(KMS), which may be available through a\ncloud provider. To enable Secret data-at-rest\nencryption using the API server,\nadministrators should change the kube```\napiserver manifest file to execute using the\n--encryption-provider-config\n\n```\nargument.\n\n\n## By default, Secrets are\n\n stored as unencrypted\n\n base64-encoded strings and\n\n can be retrieved by anyone\n\n with API access.\n\n\n-----\n\nFor an example encryption-provider-config file, refer to Appendix I: Example\n**encryption. Using a KMS provider prevents the raw encryption key from being stored**\n\non the local disk. To encrypt Secrets with a KMS provider, the encryption```\nprovider-config file should specify the KMS provider. For an example, refer to\n\n```\n**Appendix J: Example KMS configuration.**\n\nAfter applying the encryption-provider-config file, administrators should run the\nfollowing command to read and encrypt all Secrets:\n```\n  kubectl get secrets --all-namespaces -o json | kubectl replace -f \n##### Protecting sensitive cloud infrastructure\n\n```\nKubernetes is often deployed on VMs in a cloud environment. As such, administrators\nshould carefully consider the attack surface of the VMs on which the Kubernetes worker\nnodes are running. In many cases, Pods running on these VMs have access to\nsensitive cloud metadata services on a non-routable address. These metadata services\nprovide cyber actors with information about the cloud infrastructure and possibly even\nshort-lived credentials for cloud resources.\n\nCyber actors abuse these metadata services for privilege escalation [5]. Kubernetes\nadministrators should prevent Pods from accessing cloud metadata services by using\n\nnetwork policies or through the cloud configuration policy. Because these services vary\nbased on the cloud provider, administrators should follow vendor guidance to harden\nthese access vectors.\n\n_▲Return to Contents_\n\n\n-----\n\n### Authentication and authorization\n\nAuthentication and authorization are the primary mechanisms to restrict access to\ncluster resources. Cyber actors can scan for well-known Kubernetes ports and access\nthe cluster’s database or make API calls without being authenticated if the cluster is\nmisconfigured. Several user authentication mechanisms are supported but not enabled\nby default.\n\n##### Authentication\n\nKubernetes clusters have two types of users:\n\n  - Service accounts\n\n  - Normal user accounts\n\nService accounts handle API requests on behalf of Pods. Authentication is typically\nmanaged automatically by Kubernetes through the ServiceAccount Admission\nController using bearer tokens. When the admission controller is active, it checks\nwhether Pods have an attached service account. If the Pod definition does not specify a\nservice account, the admission controller attaches the default service account for the\nnamespace. The admission controller will not attach the default service account if the\nPod definition prohibits the addition of the service token by setting\n```\nautomountServiceAccountToken or automountServiceAccounttoken to\nfalse. Service accounts can also be individually created to grant specific permissions.\n\n```\nWhen Kubernetes creates the service account, it creates a service account Secret and\nautomatically modifies the Pod to use the Secret. The service account token Secret\n\ncontains credentials for accessing the API. If left unsecured or unencrypted, service\naccount tokens could be used from outside of the cluster by attackers. Because of this\nrisk, access to Pod Secrets should be restricted to those with a need to view them,\nusing Kubernetes RBAC.\n\nFor normal users and admin accounts, there is not an automatic authentication method.\nAdministrators must implement an authentication method or delegate authentication to a\n\n\n-----\n\nthird-party service. Kubernetes assumes that a cluster-independent service manages\n[user authentication. The Kubernetes documentation](https://kubernetes.io/docs/reference/access-authn-authz/authentication) lists several ways to implement\nuser authentication including X509 client certificates, bootstrap tokens, and OpenID\ntokens. At least one user authentication method should be implemented. When multiple\nauthentication methods are implemented, the first module to successfully authenticate\nthe request short-circuits the evaluation.\nAdministrators should not use weak\n\n## Kubernetes assumes that a\n\nmethods, such as static password files, as\nweak methods could allow cyber actors to **cluster-independent service**\nauthenticate as legitimate users.\n\n## manages user\n\nAnonymous requests are requests that are\n\n## authentication.\n\nnot rejected by other configured\nauthentication methods and are not tied to\nany individual user or Pod. In a server setup\nfor token authentication with anonymous requests enabled, a request without a token\npresent would be performed as an anonymous request. In Kubernetes 1.6 and newer,\nanonymous requests are enabled by default. When RBAC is enabled, anonymous\nrequests require explicit authorization of the system:anonymous user or\n```\nsystem:unauthenticated group. Anonymous requests should be disabled by\n\n```\npassing the --anonymous-auth=false option to the API server. Leaving anonymous\nrequests enabled could allow a cyber actor to access cluster resources without\nauthentication.\n\n\n##### Role-based access control \n\nRBAC, enabled by default, is one method to control access to cluster resources based\non the roles of individuals within an organization. RBAC can be used to restrict access\nfor user accounts and service accounts. To check if RBAC is enabled in a cluster using\n\nkubectl, execute kubectl api-version. The API version for\n```\n.rbac.authorization.k8s.io/v1 should be listed if RBAC is enabled. Cloud\n\n```\nKubernetes services may have a different way of checking whether RBAC is enabled for\n\nthe cluster. If RBAC is not enabled, start the API server with the --authorization```\nmode flag in the following command:\n       kube-apiserver --authorization-mode=RBAC\n\n```\n\n-----\n\nLeaving authorization-mode flags, such as AlwaysAllow, in place allows all\nauthorization requests, effectively disabling all authorization and limiting the ability to\nenforce least privilege for access.\n\nTwo types of permissions can be set:\n\n  - Roles – Set permissions for particular namespaces\n\n  - ClusterRoles – Set permissions across all cluster resources regardless of\n\nnamespace\n\nBoth Roles and ClusterRoles can only be used to add permissions. There are no deny\n\nrules. If a cluster is configured to use RBAC and anonymous access is disabled, the\nKubernetes API server will deny permissions not explicitly allowed. For an example\nRBAC Role, refer to Appendix K: Example pod-reader RBAC Role.\n\nA Role or ClusterRole defines a permission but does not tie the permission to a user. As\nillustrated in the following figure, RoleBindings and ClusterRoleBindings are used to tie\na Role or ClusterRole to a user, group, or service account. RoleBindings grant\npermissions in Roles or ClusterRoles to users, groups, or service accounts in a defined\nnamespace. ClusterRoles are created independent of namespaces and can be used\nmultiple times in conjunction with a RoleBinding to limit the namespace scope.\n\nThis is useful when users, groups, or service accounts require similar permissions in\nmultiple namespaces. One ClusterRole can be used several times with different\nRoleBindings to limit scope to different individual users, groups, or service accounts.\nClusterRoleBindings grant users, groups, or service accounts ClusterRoles across all\ncluster resources. For an example of RBAC RoleBinding and ClusterRoleBinding, refer\nto Appendix L: Example RBAC RoleBinding and ClusterRoleBinding.\n\n\n-----\n\n**_Figure 6: Possible Role, ClusterRole, RoleBinding, and ClusterRoleBinding combinations to assign access_**\n\nTo create or update Roles and ClusterRoles, a user must have the permissions\ncontained in the new role at the same scope or possess explicit permission to perform\nthe escalate verb on the Roles or ClusterRoles resources in the\n```\nrbac.authorization.k8s.io API group. After a binding is created, the Role or\n\n```\nClusterRole is immutable. The binding must be deleted to change a role.\n\nPrivileges assigned to users, groups, and service accounts should follow the principle of\n_least privilege, allowing only required permissions to complete tasks. User groups can_\nmake creating Roles easier to manage. Unique permissions are required for different\n\n\n-----\n\ngroups, such as users, administrators, developers, and the infrastructure team. Each\ngroup needs access to different resources and should not have permissions to edit or\nview other groups’ resources. Users, user groups, and service accounts should be\nlimited to interact and view specific namespaces where required resources reside.\nAccess to the Kubernetes API is limited by creating an RBAC Role or ClusterRole with\nthe appropriate API request verb and desired resource on which the action can be\napplied. Tools exist that can help audit RBAC policies by printing users, groups, and\nservice accounts with their associated assigned Roles and ClusterRoles.\n\n_▲Return to Contents_\n\n\n-----\n\n### Audit Logging and Threat Detection\n\nAudit logs capture attributed activity in the cluster. An effective logging solution and log\nreviewing are necessary, not only for ensuring that services are operating and\nconfigured as intended, but also for ensuring the security of the system. Systematic\nsecurity audit requirements mandate consistent and thorough checks of security\nsettings to help identify compromises. Kubernetes is capable of capturing audit logs for\ntracking attributed cluster actions, and monitoring basic CPU and memory usage\ninformation; however, it does not natively provide full featured monitoring or alerting\nservices.\n\n\n##### Key points\n####  Establish Pod baselines at creation to enable anomalous activity identification.\n\n  Perform logging at all levels of the environment.\n\n  Integrate existing network security tools for aggregate scans, monitoring, alerts,\n\nand analysis.\n\n####  Set up fault-tolerant policies to prevent log loss in case of a failure.\n\n\n##### Logging\n\nSystem administrators running applications within Kubernetes should establish an\neffective logging and monitoring system for their environment. Logging Kubernetes\nevents alone is not enough to provide a full picture of the actions occurring on the\n\nsystem. Logging should be performed at all levels of the environment, including on the\nhost, application, container, container engine, image registry, api-server, and the cloud,\nas applicable. Once captured, these logs should all be aggregated to a single service to\nprovide security auditors, network defenders, and incident responders a full view of the\nactions taken throughout the environment.\n\nWithin the Kubernetes environment, some events that administrators should monitor/log\ninclude the following:\n\n  - API request history\n\n  - Performance metrics\n\n  - Deployments\n\n  - Resource consumption\n\n\n-----\n\n  - Operating system calls\n\n  - Protocols, permission changes\n\n  - Network traffic\n\n  - Pod scaling\n\n  - Volume mount actions\n\n  - Image and container modification\n\n  - Privilege changes\n\n  - Scheduled job (cronjob) creations and modifications\n\nWhen administrators create or update a Pod, they should capture detailed logs of the\nnetwork communications, response times, requests, resource consumption, and any\nother relevant metrics to establish a baseline. RBAC policy configurations should also\nbe reviewed periodically and whenever personnel changes occur in the organization’s\nsystem administrators. Doing so ensures access controls remain in compliance with the\nRBAC policy-hardening guidance outlined in the role-based access control section of\nthis guide.\n\nRoutine system security audits should include comparisons of current logs to the\nbaseline measurements of normal activities to identify significant changes in any of the\nlogged metrics and events. System administrators should investigate significant\nchanges to determine the root cause. For example, a significant increase in resource\nconsumption could be indicative of a change in application usage or the installation of\nmalicious processes such as a cryptominer.\n\nAudits of internal and external traffic logs should be conducted to ensure all intended\nsecurity constraints on connections have been configured properly and are working as\nintended. Administrators can also use these audits as systems evolve to evaluate where\nexternal access may be restricted.\n\nStreaming logs to an external logging service will help to ensure availability to security\n\nprofessionals outside of the cluster, enabling them to identify abnormalities in as close\nto real time as possible. If using this method, logs should be encrypted during transit\nwith TLS 1.2 or 1.3 to ensure cyber actors cannot access the logs in transit and gain\nvaluable information about the environment.\n\n\n-----\n\nAnother precaution to take when utilizing an external log server is to configure the log\nforwarder within Kubernetes with append-only access to the external storage. This\nprotects the externally stored logs from being deleted or overwritten from within the\ncluster.\n\n**Kubernetes native audit logging configuration**\n\nThe kube-apiserver resides on the Kubernetes control plane and acts as the front\nend, handling internal and external requests for a cluster. Each request, whether\ngenerated by a user, an application, or the control plane,\n\n## Kubernetes audit\n\nproduces an audit event at each stage in its execution. When\n\n\nan audit event registers, the kube-apiserver checks for an\naudit policy file and applicable rule. If such a rule exists, the\nserver logs the event at the level defined by the first matched\nrule. Kubernetes’ built-in audit logging capabilities perform no\nlogging by default.\n\n\n## logging capabilities\n\n are disabled by\n\n default \n\n\nCluster administrators must write an audit policy YAML file to establish the rules and\nspecify the desired audit level at which to log each type of audit event. This audit policy\n\nfile is then passed to the kube-apiserver with the appropriate flags. For a rule to be\nconsidered valid, it must specify one of the four audit levels:\n\n  - `[None]`\n\n  - `[Metadata]`\n\n  - `[Request]`\n\n  - `[RequestResponse]`\n\nLogging all events at the RequestResponse level will give administrators the\nmaximum amount of information available for incident responders should a breach\noccur. However, this may result in capturing base64-encoded Secrets in the logs. NSA\nand CISA recommend reducing the logging level of requests involving Secrets to the\n```\nMetadata level to avoid capturing Secrets in logs. \n\n```\nAdditionally, logging all other events at the highest level will produce a large quantity of\nlogs, especially in a production cluster. If an organization’s constraints require it, the\naudit policy can be tailored to the environment, reducing the logging level of non-critical,\nroutine events. The specific rules necessary for such an audit policy will vary by\ndeployment. It is vital to log all security-critical events, paying close attention to the\n\n\n-----\n\norganization’s specific cluster configuration and threat model to indicate where to focus\nlogging. The goal of refining an audit policy should be to remove redundancy, while still\nproviding a clear picture and attribution of the events occurring in the cluster.\n\nFor some examples of general critical and non-critical audit event types and stages, as\n\nwell as an example of an audit policy file that logs Secrets at the metadata level, and\nall other events at the RequestResponse level, refer to Appendix M: Audit Policy\n\nFor an example where the kube-apiserver configuration file is located and an\n\nexample of the flags by which the audit policy file can be passed to the kube```\napiserver, refer to Appendix N: Example Flags to Enable Audit Logging. For\n\n```\ndirections on how to mount the volumes and configure the host path, if necessary, refer\nto Appendix N: Example Flags to Enable Audit Logging.\n\nThe kube-apiserver includes configurable logging and webhook backends for audit\nlogging. The logging backend writes the audit events specified to a log file, and the\nwebhook backend can be configured to send the file to an external HTTP API. The\n```\n--audit-log-path and --audit-log-maxage flags, set in the example in\n\n```\n**Appendix N: Example Flags to Enable Audit Logging, are two examples of the flags**\n\nthat can be used to configure the logging backend, which writes audit events to a file.\n\nThe log-path flag is the minimum configuration required to enable logging and the\nonly configuration necessary for the logging backend. The default format for these log\nfiles is Java Script Object Notation (JSON), though this can also be changed if\nnecessary. Additional configuration options for the logging backend can be found in the\n[Kubernetes documentation. Kubernetes also provides a webhook backend option that](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/)\n\nadministrators can manually configure via a YAML file submitted to the kube```\napiserver to push logs to an external backend. An exhaustive list of the configuration\n\n```\noptions, which can be set in the kube-apiserver for the webhook backend, can be\n[found in the Kubernetes documentation. Further details on how the webhook backend](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/)\n[works and how to set it up can also be found in the Kubernetes documentation. There](https://kubernetes.io/docs/tasks/debug-application-cluster/audit/)\n\nare also many external tools available to perform log aggregation, some of which are\ndiscussed briefly in the following sections.\n\n**Worker node and container logging**\n\nThere are many ways for logging capabilities to be configured within a Kubernetes\n\narchitecture. In the built-in method of log management, the kubelet on each node is\n\n\n-----\n\nresponsible for managing logs. It stores and rotates log files locally based on its policies\nfor individual file length, storage duration, and storage capacity. These logs are\n\ncontrolled by the kubelet and can be accessed from the command line. The following\ncommand prints the logs of a container within a Pod:\n```\n        kubectl logs [-f] [-p] POD [-c CONTAINER]\n\n```\nThe -f flag may be used if the logs are to be streamed, the -p flag may be used if logs\nfrom previous instances of a container exist and are desired, and the -c flag can be\nused to specify a container if there are more than one in the Pod. If an error occurs that\ncauses a container, Pod, or node to die, the native logging solution in Kubernetes does\n\nnot provide a method to preserve logs stored in the failed object. NSA and CISA\nrecommend configuring a remote logging solution to preserve logs should a node fail.\nOptions for remote logging include:\n\n**_Table III: Remote logging configuration_**\n\n|Remote logging option|Reason to use|Configuration implementation|\n|---|---|---|\n|Run a logging agent on every node to push logs to a backend|Gives the node the ability to expose logs or push logs to a backend, preserving them outside of the node in the case of a failure.|Configure an independent container in a Pod to run as a logging agent, giving it access to the node’s application log files and configuring it to forward logs to the organization’s SIEM.|\n|Use a sidecar container in each Pod to push logs to an output stream|Used to push logs to separate output streams. This can be a useful option when application containers write multiple log files of different formats.|Configure a sidecar container for each log type and use them to redirect these log files to their individual output streams, where the kubelet can manage them. The node-level logging agent can then forward these logs onto the SIEM or other backend.|\n|Use a logging agent sidecar in each Pod to push logs to a backend|When more flexibility is needed than the node-level logging agent can provide.|Configure for each Pod to push logs directly to the backend. This is a common method for attaching third- party logging agents and backends.|\n|Push logs directly to a backend from within an application|Allows logs to go directly to the aggregation platform. Can be useful if the organization has separate teams responsible for managing application security vs the Kubernetes platform security.|Kubernetes does not have built-in mechanisms for exposing or pushing logs to a backend directly. Organizations will need to either build this functionality into their application or attach a reputable third-party tool to enable this.|\n\n\n-----\n\nTo ensure continuity of logging agents across worker nodes, it is common to run them\nas a DaemonSet. Configuring a DaemonSet for this method ensures that there is a copy\nof the logging agent on every node at all times and that any changes made to the\nlogging agent are consistent across the cluster.\n\nLarge organizations with multiple teams running their own Kubernetes clusters should\nestablish logging requirements and a standard architecture to ensure that all teams\nhave an effective solution in place.\n\n**Seccomp: audit mode**\n\nIn addition to the node and container logging previously described, it can be highly\nbeneficial to log system calls. One method for auditing container system calls in\nKubernetes is to use the seccomp tool. This tool is disabled by default but can be used\nto limit a container’s system call abilities, thereby lowering the kernel’s attack surface.\nSeccomp can also log what calls are being made by using an audit profile.\n\nA custom seccomp profile defines which system calls are allowed, denied, or logged,\nand default actions for calls not specified. To enable a custom seccomp profile within a\nPod, Kubernetes admins can write their seccomp profile JSON file to the\n```\n/var/lib/kubelet/seccomp/ directory and add a seccompProfile to the Pod’s\nsecurityContext. \n\n```\nA custom seccompProfile should also include two fields: Type: Localhost and\n```\nlocalhostProfile: myseccomppolicy.json. Logging all system calls can help\n\n```\nadministrators know what system calls are needed for standard operations allowing\nthem to restrict the seccomp profile further without losing system functionality. It can\nalso help administrators establish a baseline for a Pod’s standard operation patterns,\nallowing them to identify any major deviances from this pattern that could be indicative\nof malicious activity.\n\n**Syslog**\n\nKubernetes, by default, writes kubelet logs and container runtime logs to journald if\nthe service is available. If organizations wish to utilize syslog utilities—or to collect logs\nfrom across the cluster and forward them to a syslog server or other log storage and\naggregation platform—they can configure that capability manually. Syslog protocol\ndefines a log message-formatting standard. Syslog messages include a header and a\n\n\n-----\n\nmessage written in plaintext. Syslog daemons such as syslog-ng[®] and rsyslog are\ncapable of collecting and aggregating logs from across a system in a unified format.\nMany Linux operating systems by default use rsyslog or journald—an event logging\ndaemon that optimizes log storage and output logs in syslog format via journalctl. The\nsyslog utility logs events, on nodes running certain Linux distributions by default at the\nhost level.\n\nContainers running these Linux distributions will, by default, collect logs using syslog as\nwell. Syslog utilities store logs in the local file system on each applicable node or\ncontainer unless a log aggregation platform is configured to collect them. The syslog\ndaemon or another such tool should be configured to aggregate both these and all other\nlogs being collected across the cluster and forward them to an external backend for\nstorage and monitoring.\n\n**SIEM platforms**\n\nSecurity information and event management (SIEM) software collects logs from across\nan organization’s network. It brings together firewall logs, application logs, and more,\nparsing them out to provide a centralized platform from which analysts can monitor\nsystem security. SIEM tools have variations in their capabilities. Generally, these\nplatforms provide log collection, aggregation, threat detection, and alerting capabilities.\nSome include machine-learning capabilities, which can better predict system behavior\nand help to reduce false alerts. Organizations using these platforms in their environment\nshould integrate them with Kubernetes to better monitor and secure clusters. Opensource platforms for managing logs from a Kubernetes environment exist as alternatives\nto SIEM platforms.\n\nContainerized environments have many interdependencies between nodes, Pods,\ncontainers, and services. In these environments, Pods and containers are constantly\nbeing deleted and redeployed on different nodes. This type of environment presents an\nextra challenge for traditional SIEMs, which typically use IP addresses to correlate logs.\n\nEven next-generation SIEM platforms may not always be suited to the complex\nKubernetes environment. However, as Kubernetes has emerged as the most widely\nused container orchestration platform, many of the organizations developing SIEM tools\nhave developed variations of their products specifically designed to work with the\nKubernetes environment, providing full monitoring solutions for these containerized\nenvironments. Administrators should be aware of their platform’s capabilities and\n\n\n-----\n\nensure that their logging sufficiently captures the environment to support future incident\nresponses.\n\n**Service meshes**\n\nService meshes are platforms that streamline micro-service communications within an\napplication by allowing for the logic of these communications to be coded into the\nservice mesh rather than within each micro-service. Coding this communication logic\ninto individual micro-services is difficult to scale, difficult to debug as failures occur, and\ndifficult to secure. Using a service mesh can simplify this coding for developers. Log\ncollection at this level can also give cluster administrators insight into the standard\n\nservice-to-service communication flow throughout the cluster. The mesh can:\n\n  - Redirect traffic when a service is down,\n\n  - Gather performance metrics for optimizing communications,\n\n  - Allow management of service-to-service communication encryption,\n\n  - Collect logs for service-to-service communication,\n\n  - Collect logs from each service,\n\n  - Help developers diagnose problems and failures of micro-services or\n\ncommunication mechanisms, and\n\n  - Help with migrating services to hybrid or multi-cloud environments.\n\nWhile service meshes are not necessary, they are an option that is highly suitable to the\nKubernetes environment. Their logging capabilities can also be useful in mapping the\nservice-to-service communications, helping administrators see what their standard\ncluster operation looks like and identify anomalies easier. Managed Kubernetes\nservices often include their own service mesh; however, several other platforms are\nalso available and, if desired, are highly customizable.\n\nAnother major benefit of modern service meshes is encryption of service-to-service\ncommunications. Many service meshes manage keys and generate and rotate\ncertificates, allowing for secure TLS authentication between services, without requiring\ndevelopers to set this up for each individual service and manage it themselves. Some\nservice meshes even perform this service-to-service encryption by default. If\nadministrators deploy a service mesh within a Kubernetes cluster, it is important to keep\nup with updates and security alerts for the service mesh as illustrated in the following\nfigure:\n\n\n-----\n\n**_Figure 7: Cluster leveraging service mesh to integrate logging with network security_**\n\n**Fault tolerance**\n\nOrganizations should put fault tolerance policies in place. These policies could differ\ndepending on the specific Kubernetes use case. One such policy is to allow new logs to\noverwrite the oldest log files, if absolutely necessary, in the event of storage capacity\nbeing exceeded. Another such policy that can be used if logs are being sent to an\nexternal service is to establish a place for logs to be stored locally if a communication\n\nloss or an external service failure occurs. Once communication to the external service is\nrestored, a policy should be in place for the locally stored logs to be pushed up to the\nexternal server.\n\n\n-----\n\n##### Threat Detection\n\nAn effective logging solution comprises two critical components: collecting all necessary\ndata and then actively monitoring the collected data for red flags in as close to real time\nas possible. The best logging solution in the world is useless if the data is never\nexamined. Much of the process of log examination can be automated; however, when\neither writing log parsing policies or manually examining logs, it is vital to know what to\nlook for. When attackers try to exploit the cluster, they will leave traces of their actions in\nthe logs.\n\nThe following table contains some of the ways attackers may try to exploit the cluster\nand how that may present in the logs. (Caveat: This table lists some known suspicious\n\nindicators. Administrators should also be aware of, and alert to, specific concerns and\nemerging threats in their environments. The most effective alerts are tailored to identify\n\nabnormal activity for a specific cluster.)\n\n**_Table IV: Detection recommendations_**\n\n|Table IV: Detection recommendations|Col2|\n|---|---|\n|Attacker Action|Log Detection|\n|Attackers may try to deploy a Pod or container to run their own malicious software or to use as a staging ground/pivot point for their attack. Attackers may try to masquerade their deployment as a legitimate image by copying names and naming conventions. They may also try to start a container with root privileges to escalate privileges.|Watch for atypical Pod and container deployments. Use image IDs and layer hashes for comparisons of suspected image deployments against the valid images. Watch for Pods or application containers being started with root permissions|\n|Attackers may try to import a malicious image into the victim organization’s registry, either to give themselves access to their image for deployment, or to trick legitimate parties into deploying their malicious image instead of the legitimate ones.|This may be detectable in the container engine or image repository logs. Network defenders should investigate any variations from the standard deployment process. Depending on the specific case this may also be detectible through changes in containers’ behavior after being redeployed using the new image version.|\n|If an attacker manages to exploit an application to the point of gaining command execution capabilities on the container, then depending on the configuration of the Pod, they may be able to make API requests from within the Pod, potentially|Unusual API requests (from the Kubernetes audit logs) or unusual system calls (from seccomp logs) originating from inside a Pod. This could also show as pod creation requests registering a Pod IP address as its source IP.|\n\n\n-----\n\n|Attacker Action|Log Detection|\n|---|---|\n|escalating privileges, moving laterally within the cluster, or breaking out onto the host.||\n|Attackers who have gained initial access to a Kubernetes cluster will likely start attempting to penetrate further into the cluster, which will require interacting with the kube-apiserver.|While they work to determine what initial permissions they have, they may end up making several failed requests to the API server. Repeated failed API requests and request patterns that are atypical for a given account would be red flags.|\n|Attackers may attempt to compromise a cluster in order to use the victim’s resources to run their own cryptominer (i.e., a cryptojacking attack).|If an attacker were to successfully start a cryptojacking attack it would likely show in the logs as a sudden spike in resource consumption.|\n|Attackers may attempt to use anonymous accounts to avoid attribution of their activities in the cluster.|Watch for any anonymous actions in the cluster.|\n|Attackers may try to add a volume mount to a container they have compromised or are creating, to gain access to the host|Volume mount actions should be closely monitored for abnormalities.|\n|Attackers with the ability to create scheduled jobs (aka Kubernetes CronJobs) may attempt to use this to get Kubernetes to automatically and repetitively run malware on the cluster [8].|Scheduled job creations and modifications should be closely monitored.|\n\n\nThe enormous quantity of logs generated in an environment such as this makes it\ninfeasible for administrators to review all of the logs manually and even more important\nfor administrators to know what indicators to look for. This knowledge can be used to\nconfigure automated responses and refine the criteria for triggering alerts.\n\n**Alerting**\n\nKubernetes does not natively support alerting; however, several monitoring tools with\nalerting capabilities are compatible with Kubernetes. If Kubernetes administrators\nchoose to configure an alerting tool to work within a Kubernetes environment,\nadministrators can use several metrics to monitor and configure alerts.\n\nExamples of actionable events that could trigger alerts include but are not limited to:\n\n  - Low disk space on any of the machines in the environment,\n\n\n-----\n\n  - Available storage space on a logging volume running low,\n\n  - External logging service going offline,\n\n  - A Pod or application running with root permissions,\n\n  - Requests being made by an account for resources they do not have permission\n\nfor,\n\n  - Anonymous requests being submitted to the API server,\n\n  - Pod or Worker Node IP addresses being listed as the source ID of a Pod creation\n\nrequest,\n\n  - Unusual system calls or failed API calls,\n\n  - User/admin behavior that is abnormal (i.e. at unusual times or from an unusual\n\nlocation), and\n\n  - Significant deviations from the standard operation metrics baseline.\n\nIn their 2021 Kubernetes blog post, contributors to the Kubernetes project made the\nfollowing three additions to this list [7]:\n\n  - Changes to a Pod’s securityContext,\n\n  - Updates to admission controller configs, and\n\n  - Accessing certain sensitive files/URLs.\n\nWhere possible, systems should be configured to take steps to mitigate compromises\nwhile administrators respond to alerts. In the case of a Pod IP being listed as the source\nID of a Pod creation request, automatically evicting the Pod is one mitigation that could\nbe implemented to keep the application available but temporarily stop any compromises\nof the cluster. Doing so would allow a clean version of the Pod to be rescheduled onto\none of the nodes. Investigators could examine the logs to determine if a breach\noccurred and, if so, how the malicious actors executed the compromise so that a patch\ncan be deployed. Automating such responses can help improve security professionals’\nresponse time to critical events.\n\n##### Tools\n\nKubernetes does not natively include extensive auditing capabilities. However, the\nsystem is built to be extensible, allowing users the freedom to develop their own custom\nsolution or to choose an existing add-on that suits their needs. Kubernetes cluster\nadministrators commonly connect additional backend services to their cluster to perform\nadditional functions for users, such as extended search parameters, data mapping\n\n\n-----\n\nfeatures, and alerting functionality. Organizations that already use SIEM platforms can\nintegrate Kubernetes with these existing capabilities. Open-source monitoring tools—\nsuch as the Cloud Native Computing Foundation’s Prometheus[®], Grafana Labs’\nGrafana[®], and Elasticsearch’s Elastic Stack (ELK)[®]—are also available. The tools can\nconduct event monitoring, run threat analytics, manage alerting, and collect resource\nisolation parameters, historical usage, and network statistics on running containers.\nScanning tools can be used when auditing the access control and permission\nconfigurations to identify risky permission configurations in RBAC.\n\nNSA and CISA encourage organizations utilizing Intrusion Detection Systems (IDSs) on\ntheir existing environment to consider integrating that service into their Kubernetes\nenvironment as well. This integration would allow an organization to monitor for—and\npotentially kill containers showing signs of—unusual behavior so the containers can be\nrestarted from the initial clean image. Many CSPs also provide container monitoring\nservices for those wanting more managed and scalable solutions.\n\n_▲Return to Contents_\n\n\n-----\n\n### Upgrading and application security practices\n\nFollowing the hardening guidance outlined in this document is a step toward ensuring\nthe security of applications running on Kubernetes orchestrated containers. However,\nsecurity is an ongoing process, and it is vital to keep up with patches, updates, and\nupgrades. The specific software components vary depending on the individual\nconfiguration, but each piece of the overall system must be kept as secure as possible.\nThis includes updating Kubernetes, hypervisors, virtualization software, plugins,\noperating systems on which the environment is running, applications running on the\nservers, all elements of the organization’s continuous integration/continuous delivery\n(CI/CD) pipeline and any other software hosted in the environment. Companies who\nneed to maintain 24/7 uptime for their services can consider using high-availability\nclusters, so that services can be off-loaded from physical machines one at a time,\nallowing for firmware, kernel, and operating system updates to be deployed in a timely\nmanner while still maintaining service availability.\n\nThe Center for Internet Security (CIS) publishes benchmarks for securing software.\nAdministrators should adhere to the CIS benchmarks for Kubernetes and any other\nrelevant system components. Administrators should periodically check to ensure their\nsystem's security is compliant with the current cybersecurity best practices. Periodic\n\nvulnerability scans and penetration tests should be performed on the various system\ncomponents to proactively look for insecure configurations and zero-day vulnerabilities.\nAny discoveries should be promptly remediated before potential cyber actors can\ndiscover and exploit them.\n\nAs administrators deploy updates, they should also keep up with uninstalling any old,\nunused components from the environment and deployment pipeline. This practice will\nhelp reduce the attack surface and the risk of unused tools remaining on the system\nand falling out of date. Using a managed Kubernetes service can help to automate\nupgrades and patches for Kubernetes, operating systems, and networking protocols.\nHowever, administrators must still ensure that their deployments are up to date and\ndevelopers properly tag new images to avoid accidental deployments of outdated\nimages.\n\n_▲Return to Contents_\n\n\n-----\n\n### Works cited\n\n[1] Center for Internet Security, \"CIS Benchmarks Securing Kubernetes,\" 2021. [Online].\n\nAvailable: https://cisecurity.org/benchmark/kubernetes/.\n\n[2] DISA, \"Kubernetes STIG,\" 2021. [Online]. Available:\n\nhttps://public.cyber.mil/stigs/downloads/.\n\n[3] The Linux Foundation, \"Kubernetes Documnetation,\" 2021. [Online]. Available:\n\nhttps://kubernetes.io/docs/ . [Accessed 02 2021].\n\n[4] The Linux Foundation, \"11 Ways (Not) to Get Hacked,\" 18 07 2018. [Online]. Available:\n\nhttps://kubernetes.io/blog/2018/07/18/11-ways-not-to-get-hacked/#10-scan-images-and-run\nids. [Accessed 03 2021].\n\n[5] MITRE, \"MITRE ATT&CK,\" 2021. [Online]. Available:\n\nhttps://attack.mitre.org/techniques/T1552/005/. [Accessed 7 May 2021].\n\n[6] CISA, \"Analysis Report (AR21-013A),\" 14 January 2021. [Online]. Available:\n\nhttps://www.cisa.gov/uscert/ncas/analysis-reports/ar21-013a. [Accessed 26 May 2021].\n\n[7] Kubernetes, \"A Closer Look at NSA/CISA Kubernetes Hardening Guidance,\" 5 October\n\n2021. [Online]. Available: https://www.kubernetes.io/blog/2021/10/05/nsa-cisa-kubernetes\nhardening-guidance/ 2021.\n\n[8] MITRE ATT&CK, \"Scheduled Task/Job: Container Orchestration Job,\" 27 7 2021. [Online].\n\nAvailable: https://attack.mitre.org/techniques/T1053/007/. [Accessed 9 11 2021].\n\n[9] The Kubernetes Authors, \"Pod Security Admission,\" [Online]. Available:\n\nhttps://kubernetes.io/docs/concepts/security/pod-security-admission/.\n\n\n-----\n\n### Appendix A: Example Dockerfile for non-root application\n\nThe following example is a Dockerfile that runs an application as a non-root user with\nnon-group membership. The lines highlighted in red below are the portion specific to\nusing non-root.\n```\n FROM ubuntu:latest\n #Update and install the make utility\n RUN apt update && apt install -y make\n #Copy the source from a folder called “code” and build the application with\n the make utility\n COPY . /code\n RUN make /code\n #Create a new user (user1) and new group (group1); then switch into that\n user’s context\n RUN useradd user1 && groupadd group1\n USER user1:group1\n #Set the default entrypoint for the container\n CMD /code/app\n\n```\n\n-----\n\n### Appendix B: Example deployment template for read-only file system\n\nThe following example is a Kubernetes deployment template that uses a read-only root\nfile system. The lines highlighted in red below are the portion specific to making the\ncontainer’s filesystem read-only. The lines highlighted in blue are the portion showing\nhow to create a writeable volume for applications requiring this capability.\n```\n apiVersion: apps/v1\n kind: Deployment\n metadata:\n  labels:\n   app: web\n  name: web\n spec: \n  selector:\n   matchLabels:\n    app: web\n  template:\n   metadata:\n    labels:\n     app: web\n    name: web\n   spec:\n    containers:\n    - command: [\"sleep\"]\n     args: [\"999\"]\n     image: ubuntu:latest\n     name: web\n     securityContext:\n      readOnlyRootFilesystem: true\n     volumeMounts:\n      - mountPath: /writeable/location/here\n       name: volName\n    volumes:\n    - emptyDir: {}\n     name: volName\n\n```\n\n-----\n\n### Appendix C: Pod Security Policies (deprecated)\n\nA Pod Security Policy (PSP) is a cluster-wide policy that specifies security\nrequirements/defaults for Pods to execute within the cluster. While security mechanisms\nare often specified within Pod/deployment configurations, PSPs establish a minimum\nsecurity threshold to which all Pods must adhere. Some PSP fields provide default\nvalues used when a Pod’s configuration omits a field. Other PSP fields are used to deny\nthe creation of non-conformant Pods. PSPs are enforced through a Kubernetes\nadmission controller, so PSPs can only enforce requirements during Pod creation.\nPSPs do not affect Pods already running in the cluster.\n\nPSPs are useful technical controls to enforce security measures in the cluster. PSPs\nare particularly effective for clusters managed by admins with tiered roles. In these\ncases, top-level admins can impose defaults to enforce requirements on lower-level\nadmins. NSA and CISA encourage organizations to adapt the Kubernetes hardened\nPSP template in Appendix D: **Example Pod Security Policy to their needs. The**\n\nfollowing table describes some widely applicable PSP components.\n\n**_Table V: Pod Security Policy components[3]_**\n\n**Field Name(s)** **Usage** **Recommendations**\n\nControls whether Pods can run\nprivileged Set to false.\nprivileged containers.\n\nControls whether containers can\nhostPID, hostIPC Set to false.\nshare host process namespaces.\n\nhostNetwork Controls whether containers can\nSet to false.\nuse the host network.\n\nallowedHostPaths Limits containers to specific paths Use a “dummy” path name (such\nof the host file system. as “/foo” marked as read-only).\n\nOmitting this field results in no\nadmission restrictions being placed\non containers.\n\nreadOnlyRootFilesystem Requires the use of a read only\nSet to true when possible.\nroot file system.\n\nrunAsUser, runAsGroup, Controls whether container - Set runAsUser to\nsupplementalGroups, applications can run with root MustRunAsNonRoot.\nfsGroup privileges or with root group - Set runAsGroup to non-zero (See\n\nmembership. the example in Appendix D:\n\nExample Pod Security Policy).\n\n                                 - Set supplementalGroups to nonzero (see example in appendix D).\n\n3 https://kubernetes.io/docs/concepts/policy/pod-security-policy\n\n|Field Name(s)|Usage|Recommendations|\n|---|---|---|\n|privileged|Controls whether Pods can run privileged containers.|Set to false.|\n|hostPID, hostIPC|Controls whether containers can share host process namespaces.|Set to false.|\n|hostNetwork|Controls whether containers can use the host network.|Set to false.|\n|allowedHostPaths|Limits containers to specific paths of the host file system.|Use a “dummy” path name (such as “/foo” marked as read-only). Omitting this field results in no admission restrictions being placed on containers.|\n|readOnlyRootFilesystem|Requires the use of a read only root file system.|Set to true when possible.|\n|runAsUser, runAsGroup, supplementalGroups, fsGroup|Controls whether container applications can run with root privileges or with root group membership.|- Set runAsUser to MustRunAsNonRoot. - Set runAsGroup to non-zero (See the example in Appendix D: Example Pod Security Policy). - Set supplementalGroups to non- zero (see example in appendix D).|\n\n\n-----\n\n|Field Name(s)|Usage|Recommendations|\n|---|---|---|\n|||- Set fsGroup to non-zero (See the example in Appendix D: Example Pod Security Policy).|\n|allowPrivilegeEscalation|Restricts escalation to root privileges.|Set to false. This measure is required to effectively enforce “runAsUser: MustRunAsNonRoot” settings.|\n|seLinux|Sets the SELinux context of the container.|If the environment supports SELinux, consider adding SELinux labeling to further harden the container.|\n|AppArmor annotations|Sets the AppArmor profile used by containers.|Where possible, harden containerized applications by employing AppArmor to constrain exploitation.|\n|seccomp annotations|Sets the seccomp profile used to sandbox containers.|Where possible, use a seccomp auditing profile to identify required syscalls for running applications; then enable a seccomp profile to block all other syscalls.|\n\n\n**Note: PSPs do not automatically apply to the entire cluster for the following reasons:**\n\n  - First, before PSPs can be applied, the PodSecurityPolicy plugin must be enabled\n\nfor the Kubernetes admission controller, part of kube-apiserver.\n\n  - Second, the policy must be authorized through RBAC. Administrators should\n\nverify the correct functionality of implemented PSPs from each role within their\ncluster’s organization.\n\nAdministrators should be cautious in environments with multiple PSPs as Pod creation\nadheres to the least restrictive authorized policy. The following command describes all\n\nPod Security Policies for the given namespace, which can help to identify problematic\noverlapping policies:\n```\n              kubectl get psp -n <namespace>\n\n```\n\n-----\n\n### Appendix D: Example Pod Security Policy\n\nThe following example is a Kubernetes Pod Security Policy that enforces strong security\nrequirements for containers running in the cluster. This example is based on official\n[Kubernetes documentation: https://kubernetes.io/docs/concepts/policy/pod-security-](https://kubernetes.io/docs/concepts/policy/pod-security-policy/)\n[policy/. Administrators are encouraged to tailor the policy to meet their organization’s](https://kubernetes.io/docs/concepts/policy/pod-security-policy/)\nrequirements.\n```\n apiVersion: policy/v1beta1\n kind: PodSecurityPolicy\n metadata:\n  name: restricted\n  annotations:\n   seccomp.security.alpha.kubernetes.io/allowedProfileNames:\n 'docker/default,runtime/default'\n   apparmor.security.beta.kubernetes.io/allowedProfileNames:\n 'runtime/default'\n   seccomp.security.alpha.kubernetes.io/defaultProfileName: \n 'runtime/default'\n   apparmor.security.beta.kubernetes.io/defaultProfileName: \n 'runtime/default'\n spec:\n  privileged: false # Required to prevent escalations to root.\n   allowPrivilegeEscalation: false \n  requiredDropCapabilities:\n   - ALL\n  volumes:\n   - 'configMap'\n   - 'emptyDir'\n   - 'projected'\n   - 'secret'\n   - 'downwardAPI'  \n   - 'persistentVolumeClaim' # Assume persistentVolumes set up by admin\n are safe\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  runAsUser:\n   rule: 'MustRunAsNonRoot' # Require the container to run without root \n  seLinux: \n   rule: 'RunAsAny' # This assumes nodes are using AppArmor rather than\n SELinux\n  supplementalGroups:\n   rule: 'MustRunAs'\n   ranges: # Forbid adding the root group.   \n    - min: 1\n     max: 65535\n  runAsGroup:\n   rule: 'MustRunAs'\n   ranges: # Forbid adding the root group.   \n    - min: 1\n     max: 65535\n  fsGroup:\n\n```\n\n-----\n\n```\n   rule: 'MustRunAs'\n   ranges: # Forbid adding the root group.   \n    - min: 1\n     max: 65535\n readOnlyRootFilesystem: true\n\n```\n\n-----\n\n### Appendix E: Example namespace\n\nThe following example is for each team or group of users, a Kubernetes namespace\n\ncan be created using either a kubectl command or YAML file. Any name with the\nprefix kube- should be avoided as it may conflict with Kubernetes system reserved\nnamespaces.\n```\nKubectl command to create a namespace:\n kubectl create namespace <insert-namespace-name-here>\n\n```\nTo create namespace using YAML file, create a new file called my-namespace.yaml\nwith the contents:\n```\n apiVersion: v1\n kind: Namespace\n metadata:\n  name: <insert-namespace-name-here>\n\n```\nApply the namespace using:\n```\n kubectl create –f ./my-namespace.yaml\n\n```\nTo create new Pods in an existing namespace, switch to the desired namespace using:\n```\n kubectl config use-context <insert-namespace-here>\n\n```\nApply new deployment using:\n```\n kubectl apply -f deployment.yaml\n\n```\nAlternatively, the namespace can be added to the kubectl command using:\n```\n kubectl apply -f deployment.yaml --namespace=<insert-namespace-here>\n\n```\nor specify namespace: <insert-namespace-here> under metadata in the YAML\ndeclaration.\n\nOnce created, resources cannot be moved between namespaces. The resource must\n\nbe deleted, then created in the new namespace.\n\n\n-----\n\n### Appendix F: Example network policy\n\nNetwork policies differ depending on the network plugin used. The following example is\na network policy to limit access to the nginx service to Pods with the label access using\n[the Kubernetes documentation: https://kubernetes.io/docs/tasks/administer-](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/)\n[cluster/declare-network-policy/](https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/)\n```\n apiVersion: networking.k8s.io/v1\n kind: NetworkPolicy\n metadata:\n  name: example-access-nginx\n  namespace: prod #this can any namespace or be left out if no\n namespace is used\n spec:\n   podSelector:\n     matchLabels:\n       app: nginx\n   ingress:\n   -from:\n     -podSelector:\n       matchLabels:\n         access: “true”\n\n```\nThe new NetworkPolicy can be applied using:\n```\n kubectl apply -f policy.yaml\n\n```\nA default deny all ingress policy:\n```\n apiVersion: networking.k8s.io/v1\n kind: NetworkPolicy\n metadata:\n   name: deny-all-ingress\n spec:\n   podSelector: {}\n   policyType:\n   - Ingress\n\n```\nA default deny all egress policy:\n```\n apiVersion: networking.k8s.io/v1\n kind: NetworkPolicy\n metadata:\n   name: deny-all-egress\n spec:\n   podSelector: {}\n   policyType:\n   - Egress\n\n```\n\n-----\n\n### Appendix G: Example LimitRange\n\nLimitRange support is enabled by default in Kubernetes 1.10 and newer. The following\nYAML file specifies a LimitRange with a default request and limit, as well as a min and\nmax request, for each container.\n```\n apiVersion: v1\n kind: LimitRange\n metadata:\n   name: cpu-min-max-demo-lr\n spec:\n   limits \n   - default:\n      cpu: 1\n    defaultRequest:\n      cpu: 0.5\n    max:  \n      cpu: 2\n    min:\n      cpu 0.5 \n    type: Container\n\n```\nA LimitRange can be applied to a namespace with:\n```\n kubectl apply -f <example-LimitRange>.yaml --namespace=<Enter-Namespace>\n\n```\nAfter this example LimitRange configuration is applied, all containers created in the\n\nnamespace are assigned the default CPU request and limit if not specified. All\ncontainers in the namespace must have a CPU request greater than or equal to the\nminimum value and less than or equal to the maximum CPU value or the container will\nnot be instantiated.\n\n\n-----\n\n### Appendix H: Example ResourceQuota\n\nResourceQuota objects to limit aggregate resource usage within a namespace are\ncreated by applying a YAML file to a namespace or specifying requirements in the\nconfiguration file of Pods. The following example is based on official Kubernetes\n[documentation: https://kubernetes.io/docs/tasks/administer-cluster/manage-](https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)\nresources/quota-memory-cpu-namespace/\n\nConfiguration file for a namespace:\n```\n apiVersion: v1\n kind: ResourceQuota\n metadata:\n   name: example-cpu-mem-resourcequota\n spec:\n   hard:\n     requests.cpu: “1”\n     requests.memory: 1Gi\n     limits.cpu: “2”\n     limits.memory: 2Gi\n\n```\nThis ResourceQuota can be applied with:\n```\n kubectl apply -f example-cpu-mem-resourcequota.yaml - namespace=<insert-namespace-here>\n\n```\nThis ResourceQuota places the following constraints on the chosen namespace:\n\n  - Every container must have a memory request, memory limit, CPU request, and\n\nCPU limit,\n\n  - Aggregate memory request for all containers should not exceed 1 GiB,\n\n  - Total memory limit for all containers should not exceed 2 GiB,\n\n  - Aggregate CPU request for all containers should not exceed 1 CPU, and\n\n  - Total CPU limit for all containers should not exceed 2 CPUs.\n\n\n-----\n\n### Appendix I: Example encryption\n\nTo encrypt Secret data at rest, the following encryption configuration file provides an\nexample to specify the type of encryption desired and the encryption key. Storing the\nencryption key in the encryption file only slightly improves security. The Secrets will be\n\nencrypted, but the key will be accessible in the EncryptionConfiguration file. This\nexample is based on official Kubernetes documentation:\n[https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/.](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/)\n```\n apiVersion: apiserver.config.k8s.io/v1\n kind: EncryptionConfiguration\n resources: \n   - resources:\n    - secrets\n    providers: \n    - aescbc:\n       keys:\n       - name: key1\n         secret: <base 64 encoded secret>\n    - identity: {} \n\n```\nTo enable encryption at rest with this encryption file, restart the API server with the `--`\n`encryption-provider-config` flag set with the location to the configuration file.\n\n\n-----\n\n### Appendix J: Example KMS configuration\n\nTo encrypt Secrets with a key management service (KMS) provider plugin, the following\nexample encryption configuration YAML file can be used to set the properties for the\nprovider. This example is based on official Kubernetes documentation:\n[https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/.](https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/)\n```\n apiVersion: apiserver.config.k8s.io/v1\n kind: EncryptionConfiguration\n resources:\n  - resources:\n     - secrets\n    providers:\n     - kms:\n        name: myKMSPlugin\n        endpoint: unix://tmp/socketfile.sock\n        cachesize: 100\n        timeout: 3s\n     - identity: {}\n\n```\nTo configure the API server to use the KMS provider, set the --encryption```\nprovider-config flag with the location of the configuration file and restart the API\n\n```\nserver.\n\nTo switch from a local encryption provider to KMS, add the KMS provider section of the\nEncryptionConfiguration file above the current encryption method, as shown below.\n```\n apiVersion: apiserver.config.k8s.io/v1\n kind: EncryptionConfiguration\n resources:\n  - resources:\n     - secrets\n    providers:\n     - kms:\n        name: myKMSPlugin\n        endpoint: unix://tmp/socketfile.sock\n        cachesize: 100\n        timeout: 3s\n     - aescbc:\n        keys:\n         - name: key1\n           secret: <base64 encoded secret>\n\n```\nRestart the API server and run the command below to re-encrypt all Secrets with the\nKMS provider.\n```\n kubectl get secrets --all-namespaces -o json | kubectl replace -f \n```\n\n-----\n\n### Appendix K: Example pod-reader RBAC Role\n\nTo create the example pod-reader Role, create a YAML file with the following contents:\n```\n apiVersion: rbac.authorization.k8s.io/v1\n kind: Role\n metadata:\n  namespace: your-namespace-name\n  name: pod-reader\n rules:\n - apiGroups: [“”]  # “” indicates the core API group\n   resources: [“pods”]\n   verbs: [“get”, “watch”, “list”]\n\n```\nApply the Role using:\n```\n kubectl apply --f role.yaml\n\n```\nTo create the example global-pod-reader ClusterRole:\n```\n apiVersion: rbac.authorization.k8s.io/v1\n kind: ClusterRole\n metadata: default\n  # “namespace” omitted since ClusterRoles are not bound to a\n namespace\n  name: global-pod-reader\n rules:\n - apiGroups: [“”] # “” indicates the core API group\n   resources: [“pods”]\n   verbs: [“get”, “watch”, “list”]\n\n```\nApply the Role using:\n```\n kubectl apply --f clusterrole.yaml\n\n```\n\n-----\n\n### Appendix L: Example RBAC RoleBinding and ClusterRoleBinding\n\nTo create a RoleBinding, create a YAML file with the following contents:\n```\n apiVersion: rbac.authorization.k8s.io/v1\n # This role binding allows “jane” to read Pods in the “your namespace-name” \n # namespace.\n # You need to already have a Role names “pod-reader” in that\n namespace.\n kind: RoleBinding\n metadata:\n   name: read-pods\n   namespace: your-namespace-name\n subjects:\n # You can specify more than one “subject”\n - kind: User\n   name: jane # “name” is case sensitive\n   apiGroup: rbac.authorization.k8s.io\n roleRef:\n   # “roleRef” specifies the binding to a Role/ClusterRole\n   # kind: Role # this must be a Role or ClusterRole\n   # this must match the name of the Role or ClusterRole you wish to\n   bind \n   # to\n   name: pod-reader \n   apiGroup: rbac.authorization.k8s.io\n\n```\nApply the RoleBinding using:\n```\n kubectl apply --f rolebinding.yaml\n\n```\nTo create a ClusterRoleBinding, create a YAML file with the following contents:\n```\n apiVersion: rbac.authorization.k8s.io/v1\n # This cluster role binging allows anyone in the “manager” group to\n read \n # Pod information in any namespace.\n kind: ClusterRoleBinding\n metadata:\n   name: global-pod-reader\n subjects:\n # You can specify more than one “subject”\n - kind: Group\n   name: manager # Name is case sensitive\n   apiGroup: rbac.authorization.k8s.io\n roleRef:\n   # “roleRef” specifies the binding to a Role/ClusterRole\n   kind: ClusterRole # this must be a Role or ClusterRole\n\n```\n\n-----\n\n```\n   name: global-pod-reader # this must match the name of the Role or\n ClusterRole you wish to bind to\n   apiGroup: rbac.authorization.k8s.io\n\n```\nApply the ClusterRoleBinding using:\n```\n kubectl apply --f clusterrolebinding.yaml\n\n```\n\n-----\n\n### Appendix M: Audit Policy\nThe following example is an Audit Policy that logs requests involving Kubernetes\n\nSecrets at the Metadata level, and all other audit events at the highest level:\n```\n apiVersion: audit.k8s.io/v1 \n kind: Policy\n rules:\n   - level: Metadata\n    resources:\n       - group:”” #this refers to the core API group\n         resources: [“secrets”]\n   - level: RequestResponse\n # This audit policy logs events involving secrets at the metadata\nlevel, and all other audit events at the RequestResponse level\n\n```\nIf an organization has the resources available to store, parse, and examine a large\nnumber of logs, then logging all events, other than those involving Secrets, at the\nhighest level is a good way of ensuring that, when a breach occurs, all necessary\ncontextual information is present in the logs. If resource consumption and availability\nare a concern, then more logging rules can be established to lower the logging level of\nnon-critical components and routine non-privileged actions, as long as audit logging\nrequirements for the system are being met. As Kubernetes API events consist of\nmultiple stages, logging rules can also specify stages of the request to omit from the\nlog. By default, Kubernetes captures audit events at all stages of the request. The four\npossible stages of Kubernetes API request are:\n\n  - `[RequestReceived]`\n\n  - `[ResponseStarted]`\n\n  - `[ResponseComplete]`\n\n  - `[Panic]`\n\nBecause clusters in organizations expand to meet growing needs, it is important to\nensure that the audit policy can still meet logging needs. To ensure that elements of the\n\nenvironment are not overlooked, the audit policy should end with a catch-all rule to log\nevents that the previous rules did not log. Kubernetes logs audit events based on the\nfirst rule in the audit policy that applies to the given event; therefore, it is important to be\naware of the order in which potentially overlapping rules are written. The rule regarding\nSecrets should be near the top of the policy file to ensure. This ensures that any\noverlapping rules do not inadvertently capture Secrets due to logging at a higher level\n\n\n-----\n\nthan the Metadata level. Similarly, the catch-all rule should be the last rule in the policy\nto ensure that all other rules are matched first.\n\nWhat follows are some examples of critical event types that should be logged at the\n```\nRequest or RequestResponse level. In addition are examples of less critical event\n\n```\ntypes and stages that can be logged at a lower level if necessary to reduce redundancy\nin the logs and increase the organization’s ability to effectively review the logs as close\nto real time as possible.\n\nCritical:\n\n  - Pod deployments and alterations\n\n  - Authentication requests\n\n  - Modifications to RBAC resources (clusterrolebindings, clusterroles, etc.)\n\n  - Scheduled job creations\n\n  - Edits to Pod Security Admissions or Pod Security Policies\n\nNoncritical:\n\n  - `[RequestRecieved] stage`\n\n  - Authenticated requests to non-critical, routinely accessed resources\n\nFor an example of how to establish these rules, refer to the official Kubernetes\ndocumentation: https://kubernetes.io/docs/tasks/debug-application-cluster/audit/.\n\n\n-----\n\n### Appendix N: Example Flags to Enable Audit Logging \nIn the control plane, open the kube-apiserver.yaml file in a text editor. Editing the\n```\nkube-apiserver configuration requires administrator privileges. \n sudo vi /etc/kubernetes/manifests/kube-apiserver.yaml  \n\n```\nAdd the following text to the kube-apiserver.yaml file:\n```\n    --audit-policy-file=/etc/kubernetes/policy/audit-policy.yaml\n    --audit-log-path=/var/log/audit.log\n    --audit-log-maxage=1825\n\n```\nThe audit-policy-file flag should be set with the path to the audit policy, and the\n```\naudit-log-path flag should be set with the desired secure location for the audit logs\n\n```\nto be written to. Other additional flags exist, such as the audit-log-maxage flag\nshown here, which stipulates the maximum number of days the logs should be kept,\nand flags for specifying the maximum number of audit log files to retain, max log file size\nin megabytes, etc. The only flags necessary to enable logging are the audit-policy```\nfile and audit-log-path flags. The other flags can be used to configure logging to\n\n```\nmatch the organization’s policies.\n\nIf a user’s kube-apiserver is run as a Pod, then it is necessary to mount the volume\n\nand configure hostPath of the policy and log file locations for audit records to be\n\nretained. This can be done by adding the following sections to the kube```\napiserver.yaml file as noted in the Kubernetes documentation:\n\n```\nhttps://kubernetes.io/docs/tasks/debug-application-cluster/audit/\n```\n   volumeMounts:\n   -mountPath: /etc/kubernetes/audit-policy.yaml\n    name: audit\n    readOnly: true\n   -mountPath: /var/log/audit.log\n    name: audit-log\n    readOnly: false\n   volumes:\n   - hostPath:\n    path: /etc/kubernetes/audit-policy.yaml\n    type: File\n   name: audit\n   - hostPath:\n    path: /var/log/audit.log\n    type: FileOrCreate\n   name: audit-log\n\n```\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "99fdc3ef-333d-48f5-a4a1-becd788c7b80",
            "created_at": "2022-10-25T15:28:29.802983Z",
            "updated_at": "2022-10-25T15:28:29.802983Z",
            "deleted_at": null,
            "name": "MITRE",
            "url": "https://github.com/mitre-attack/attack-stix-data",
            "description": "MITRE ATT&CK STIX Data",
            "reports": null
        }
    ],
    "references": [
        "https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF"
    ],
    "report_names": [
        "CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF"
    ],
    "threat_actors": [
        {
            "id": "d90307b6-14a9-4d0b-9156-89e453d6eb13",
            "created_at": "2022-10-25T16:07:23.773944Z",
            "updated_at": "2025-03-27T02:02:09.974695Z",
            "deleted_at": null,
            "main_name": "Lead",
            "aliases": [
                "Casper",
                "TG-3279"
            ],
            "source_name": "ETDA:Lead",
            "tools": [
                "Agentemis",
                "BleDoor",
                "Cobalt Strike",
                "CobaltStrike",
                "RbDoor",
                "RibDoor",
                "Winnti",
                "cobeacon"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "eb3f4e4d-2573-494d-9739-1be5141cf7b2",
            "created_at": "2022-10-25T16:07:24.471018Z",
            "updated_at": "2025-03-27T02:02:10.24394Z",
            "deleted_at": null,
            "main_name": "Cron",
            "aliases": [],
            "source_name": "ETDA:Cron",
            "tools": [
                "Catelites",
                "Catelites Bot",
                "CronBot",
                "TinyZBot"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "aa73cd6a-868c-4ae4-a5b2-7cb2c5ad1e9d",
            "created_at": "2022-10-25T16:07:24.139848Z",
            "updated_at": "2025-03-27T02:02:10.120505Z",
            "deleted_at": null,
            "main_name": "Safe",
            "aliases": [],
            "source_name": "ETDA:Safe",
            "tools": [
                "DebugView",
                "LZ77",
                "OpenDoc",
                "SafeDisk",
                "TypeConfig",
                "UPXShell",
                "UsbDoc",
                "UsbExe"
            ],
            "source_id": "ETDA",
            "reports": null
        }
    ],
    "ts_created_at": 1682474968,
    "ts_updated_at": 1743041369,
    "ts_creation_date": 1661759286,
    "ts_modification_date": 1661759286,
    "files": {
        "pdf": "https://archive.orkl.eu/9cfc6215544f2e337aa59b40318da4ab4d4fb6f3.pdf",
        "text": "https://archive.orkl.eu/9cfc6215544f2e337aa59b40318da4ab4d4fb6f3.txt",
        "img": "https://archive.orkl.eu/9cfc6215544f2e337aa59b40318da4ab4d4fb6f3.jpg"
    }
}