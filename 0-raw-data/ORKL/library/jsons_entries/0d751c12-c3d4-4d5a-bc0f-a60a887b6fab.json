{
    "id": "0d751c12-c3d4-4d5a-bc0f-a60a887b6fab",
    "created_at": "2022-10-25T16:48:16.919043Z",
    "updated_at": "2025-03-27T02:06:06.557416Z",
    "deleted_at": null,
    "sha1_hash": "bf0c5b619ccbbbe25349df0f6b7b14a4dafb3aec",
    "title": "ï€ ",
    "authors": "",
    "file_creation_date": "2017-04-29T08:13:58Z",
    "file_modification_date": "2017-04-29T08:14:02Z",
    "file_size": 292782,
    "plain_text": "### Proceedings of Student-Faculty Research Day, CSIS, Pace University, May 5[th], 2017\n\n\n## Detecting Algorithmically Generated Domains Using Data Visualization and N-Grams Methods \n\n#### Tianyu Wang and Li-Chiou Chen Seidenberg School of CSIS, Pace University, Pleasantville, New York {tianyu.wang, lchen}@pace.edu\n\n\n**_Abstractâ€” Recent Botnets such as Kraken, Torpig and Nugache_**\n**have used DNS based â€œdomain fluxingâ€ for command-and-**\n**control, where each bot queries for existence of a series of domain**\n**names and the owner has to register such domain name.**\n**Botmasters have begun employing domain generation algorithms**\n**(DGA) to dynamically produce a large number of random**\n**domains and select a small subset for actual use so that static**\n**domain lists ineffective. This article is to detect machine generated**\n**domain names; we tested common methods in classification on text**\n**strings of domain names has low accuracy. We introduced new**\n**features based on N-Grams in the classification methods and our**\n**experimental results show that the analysis of N-Gram methods**\n**can make a great progress in the accuracy of detection.**\n\n**_Index Termsâ€” Classification Algorithms, Domain Name_**\n**System, Network Security, Visualization**\n\nI. INTRODUCTION\n\nany botnet detection systems use a blacklist of\ncommand-and-control (C&C) domains to detect bots and\n\n# M\n\nblock their traffic. As a response, botmasters have begun\nemploying domain generation algorithms (DGA) to\ndynamically produce a large number of random domains and\nselect a small subset for actual use so that static domain lists\nineffective. DGA is to be deterministic, yet generate a huge\nnumber of random domains so that bot maintainer only has to\nregister one or few to enable the malware to work.\n\nThere is a trend that more recent botnets have used DNS\nbased â€œdomain fluxingâ€ for command-and-control, where each\nbot queries for existence of a series of domain names, such as\nConficker, Kraken and Torpig. This method is called DNS\nâ€œdomain fluxingâ€, which means each bot algorithmically\ngenerates a large set of domain names and queries each of them\nuntil one of them is resolved and then the bot contacts the\ncorresponding IP-address obtained that is typically used to host\nthe command-and-control (C&C) server [1] [2]. Besides, for\ncommand-and-control, spammers also routinely generate\nrandom domain names in order to avoid detection [3].\n\nThis paper use the data from Alexa ranking list and DataDrivenSecurity dga\ndataset [20, 21].\n\nTianyu Wang is now a PhD candidate with the Department of Computer\nScience, Pace University, 861 Bedford Rd, Pleasantville, NY 10570 (e-mail:\ntianyu.wang@pace.edu).\n\n\nDGA stands for Domain Generating Algorithm and these\nalgorithms are part of the evolution of malware\ncommunications. In the beginning, malware would be\nhardcoded with IP address or domain names and the botnet\ncould be disrupted by going after whatever was hardcoded. The\npurpose of the DGA is to be deterministic, of which the bot\nmaintainer only has to register one to enable the malware to\nphone home [4] [5]. If the domain or IP is taken down, the\nbotnet maintainer with a new IP address can use a new name\nfrom the algorithm and the botnet maintained. Another major\nuse case of detecting DGA is to protect non-authorized DNS\nservers, such as LDNS/ROOT-DNS.\n\nThe purpose of building a DGA classifier is not to take down\nbotnets, but to discover and detect the use on our network or\nservices. Furthermore, if we are able to have a list of domains\nresolved and accessed at oneâ€™s organization, it is possible to see\nwhich of those are potentially generated and used by malware.\n\nThis paper is organized as flows. In section 2, we discuss the\nbackground of domain names system and related security\nissues. We provide literature review in section 3. The DGA\ndetection is presented in Section 4. We conclude the paper with\nour further research plan in section 5.\n\nII. BACKGROUND\n\n_A._ _The Domain Name System_\n\nThe Domain Name System (DNS) is a core component of\nInternet operation. It ensures the finding of any resource on the\ninternet by just knowing the domain names of URL that is an\neasy way to remember.\n\n_B._ _Domain Name Space_\n\nThe naming system on which DNS is based is a hierarchical\nand logical tree structure called the domain namespace.\nOrganizations can also create private networks that are not\nvisible on the Internet, using their own domain namespaces.\n\nAs the following figure shows, the root of the domain name\nspace is the â€œ.â€ Node. The following figure shows a subtree of\nthe domain name space and the path to the root. Every node is\n\nLi-Chiou, Chen is the professor with the Department of Information System,\nSchool of Computer Science and Information Systems, Pace University, 861\nBedford Rd, Pleasantville, NY 10570 (e-mail: lchen@pace.edu).\n\n\n-----\n\ncalled a level domain. Node at the base of the tree is called first\nlevel domains or Top Level Domains (TLD), for example,\nâ€œeduâ€. Under the hierarchy, nodes are called second level\ndomains (2LD), for example â€œemailâ€, third level domains\n(3LD), etc.\n\nFigure 1. Domain Name Space Hierarchy.\n\n_C._ _DNS Related Security Issues_\n\nDNS is often used to hide other kind of network traffic\nthrough the Internet. More specifically, there are many different\nDNS based misuse and malicious activities and related solving\nmethods.\n\n_1)_ _DNS Fluxing_\n\nDNS fluxing is a series of activity that enhance the\navailability and resilience of malicious resources and contents\nby hiding the real location of a given resources within a\nnetwork. The hidden resource is a server that delivers malware,\nphishing website or command and control server of a botnet\n(C&C).\n\nFast flux is one of the most common used DNS fluxing\ntechnique. It is used by botnets to hide phishing and malware\ndelivery sites behind an ever-changing network of\ncompromised hosts acting as proxies. It can also refer to the\ncombination of peer-to-peer networking, distributed command\nand control, web-based load balancing and proxy redirection\nused to make malware networks more resistant to discovery and\ncounter-measures. The Storm Worm (2007) is one of the first\nmalware variants to make use of this technique [19].\n\nThe basic idea behind Fast flux is to have numerous IP\naddresses associated with a single fully qualified domain name,\nwhere the IP addresses are swapped in and out with extremely\nhigh frequency, through changing DNS records.\n\n_2)_ _Botnets_\n\nA botnet is a number of Internet-connected devices used by\na botnet owner to perform various tasks. These botnets are\ngroups of malware machines or bots that could be remotely\ncontrolled by botmasters. Botnets can be used to perform\nDistributed Denial of Service (DDoS) attack, steal data, send\nspam, and allow the attacker access to the device and its\nconnection. The owner can control the botnet using command\nand control (C&C) software.\n\nBotnets have become the main platform for cyber criminals\nto send spam, phishing and steal information, etc. Most of\nbotnets rely on a centralized server (C&C). Bot could query a\npredefined C&C domain names that resolves IP address of\nserver that malware commands will be received. Nowadays, in\norder to overcome the limitation that one single failure of C&C\nserver is taken down, the botmaster would lose control over the\nbotnet, C&C server have used P2P based structures in botnets,\nsuch as Storm, Zeus and Nugache [16, 17, 18]. To maintain a\ncentralized P2P-based structure, attacker have developed a\n\n\nnumber of botnet that locate their server through algorithms\ngenerated random domain names. The related algorithm is\ncalled domain generation algorithms (DGA).\n\n_3)_ _Domain Generation Algorithms (DGA)_\n\nDomain Generation Algorithms (DGA) is a series of\nalgorithm that automatically generated domains names by given\na random seed and then generate a list of candidate C&C\ndomains. The botnet attempts to resolve these domains by\nsending DNS queries until one of the domains resolves to the\nIP address of a C&C server. This method introduces a\nconvenient way to keep attacking resilience because if one\ndomain names are identified and taken down, the bot will\neventually get the valid IP address and using DNS queries to\nthe next DGA domains. For example, Kraken and Conficker are\nsome example of DGA-based botnets.\n\n_4)_ _DNS Monitoring_\n\nDNS service is widely used as a core service of the whole\nInternet. Monitoring the DNS traffic performs an important\nrole. Globally the technique to identify flux networks and\nbotnets using DNS analysis have been proved efficient.\nHowever, these techniques require previous know about fluxing\ndomain names, since it rely on classification algorithms that\nneed training on truth data. Another issue is these techniques\nrequire large amount of DNS replies from different locations so\nthat to compute relevant features to train classification\nalgorithms is not easy. The time taken by these methods to\nidentify flux networks is too long. Finally, DNS based\ntechniques for bot infected host detestation are involved with\nprivacy concerns.\n\nIII. RELATED WORK\n\nCharacteristics, such as IP addresses whose records and\nlexical features of phishing and non-phishing URLs have been\nanalyzed by McGrath and Gupta [10]. They observed that the\ndifferent URLs exhibited different alphabet distributions. Our\nwork builds on this earlier work and develops techniques for\nidentifying domains employing algorithmically generated\nnames, potentially for â€œdomain fluxingâ€. Ma, et al [9], employ\nstatistical learning techniques based on lexical features (length\nof domain names, host names, number of dots in the URL etc.)\nand other features of URLs to automatically determine if a URL\nis malicious, i.e., used for phishing or advertising spam.\n\nWhile they classify each URL independently, our work is\nfocused on classifying a group of URLs as algorithmically\ngenerated or not, solely by making use of the set of\nalphanumeric characters used. In addition, we experimentally\ncompare against their lexical features in Section V and show\nthat our alphanumeric distribution based features can detect\nalgorithmically generated domain names with lower false\npositives than lexical features. Overall, we consider our work\nas complimentary and synergistic to the approach in [8]. The\nauthors [13] develop a machine learning technique to classify\nindividual domain names based on their network features,\ndomain-name string composition style and presence in known\nreference lists. Their technique, however, relies on successful\nresolution of DNS domain name query. Our technique instead,\n\n\n-----\n\ncan analyze groups of domain names, based only on\nalphanumeric character features.\n\nWith reference to the practice of â€œIP fast fluxingâ€, e.g., where\nthe botnet owner constantly keeps changing the IP-addresses\nmapped to a C&C server, [12] implements a detection\nmechanism based on passive DNS traffic analysis. In our work,\nwe present a methodology to detect cases where botnet owners\nmay use a combination of both domain fluxing with IP fluxing,\nby having bots query a series of domain names and at the same\ntime map a few of those domain names to an evolving set of IPaddresses. In addition, earlier papers [11], [8] have analyzed the\ninner working of IP fast flux networks for hiding spam and\nfraud infrastructure. With regards to botnet detection, [6], [7]\nperform correlation of network activity in time and space at\ncampus network edges, and Xie et al in [14] focus on detecting\nspamming botnets by developing regular expression based\nsignatures for spam URLs. M. Antonakakis present a new\ntechnique to detect randomly generated domains that most of\nthe DGA-generated domains would result in Non-Existent\nDomain responses, and that bots from the same bot-net would\ngenerate similar NXDomain traffic [15].\n\nIV. DGA DETECTION\n\n_A._ _Detection System_\n\nClassification in machine learning would help in DGA\ndomains detection. The purpose of building a DGA classifier is\nnot to remove botnets, but to discover and detect the use on our\nnetwork or services. Furthermore, if we can have a list of\ndomains resolved and accessed at oneâ€™s organization, it is\npossible to see whether there are potentially generated and used\nby malware.\n\nDomain names are a series of text string, consisting of\nalphabet, numbers and dash sign. Therefore, it is common to\nuse several supervised approaches to identify domains. Thus,\nthe first step in any classifier is getting enough labeled training\ndata. All we need is a list of legitimate domains and a list of\ndomains generated by an algorithm.\n\n_B._ _Data Sets_\n\n_1)_ _Alexa Domains_\n\nFor legitimate domains, an obvious choice is the Alexa list\nof top web sites. The Alexa Top Sites web service provides\naccess to lists of web sites ordered by Alexa Traffic Rank.\nUsing the web service developers can understand traffic\nrankings from the largest to the smallest sites.\n\nAlexaâ€™s traffic estimates and ranks are based on the browsing\nbehavior of people in our global data panel, which is a sample\nof all internet users. Alexaâ€™s Traffic Ranks are based on the\ntraffic data provided by users in Alexaâ€™s global data panel over\na rolling 3-month period. Traffic Ranks are updated daily. A\nsiteâ€™s ranking is based on a combined measure of Unique\nVisitors and Page views. The number of unique Alexa users\nwho visit a site on a given day determines unique Visitors. Page\nviews are the total number of Alexa user URL requests for a\nsite. However, multiple requests for the same URL on the same\nday by the same user are counted as a single Page view. The site\nwith the highest combination of unique visitors and page views\nis ranked #1 [20].\n\n\nHowever, the raw data grab from 1 Million Alexa domains\nare not ready for use. After we grab the top 1 Million Alexa\ndomains (1,000,000 entries), we find that over 10 thousand are\nnot domains but full URLs, and there are thousands of domains\nwith subdomains that will not help. Therefore, after removing\nthe invalid URL and subdomain and duplicated domains, we\ncould have the clean Alexa data with 875,216 entries.\n\nIn this article, we only concentrate on the domains without\ntop level. For example, www.google.com, we only use google\nas domain.\n\nTable 1. First 5 Entries of Alexa data\n\ndomain\n0 google\n1 facebook\n2 youtube\n3 yahoo\n4 baidu\n\nIt is important to shuffle the data randomly for\ntraining/testing purpose and sample only 90% of total data. In\naddition, we put label for this Alexa dataset as â€˜legitâ€™. The\nnumber of Alexa domains: 787,694 out of the total Alexa\ndomains 875,216.\n\n_2)_ _DGA Domains_\n\nOn DataDrivenSecurity website, it provides file of domains\nand a high-level classification of â€œdgaâ€ or â€œlegitâ€ along with a\nsubclass of either â€œlegitâ€, â€œcryptolockerâ€, â€œgozâ€ or â€œnewgozâ€\n\n[21]. These dga data are from recent botnets: â€œCryptolockerâ€,\ntwo separate â€œGame-Over Zeusâ€ algorithms, and an anonymous\ncollection of algorithmically generated domains. Here we also\nresample 90% of the total data. Specifically, there are 47,398\nout of 52,665 entries of algorithmically generated domains in\nour experiment. Here we also use domain names that without\ntop-level parts.\n\nTable 2. First 5 entries of dga domain\ndomain class\n\n0 1002n0q11m17h017r1shexghfqf dga\n\n1 1002ra86698fjpgqke1cdvbk5 dga\n\n2 1008bnt1iekzdt1fqjb76pijxhr dga\n\n3 100f3a11ckgv438fpjz91idu2ag dga\n\n4 100fjpj1yk5l751n4g9p01bgkmaf dga\n\n_C._ _Basic Statistical Features_\n\nNow we need to implement some features to measure domain\nnames. The domain field here means second-level domain only.\nIn the following article, we use domains for abbreviation. The\nclass field is binary category, either dga or legit. DGA stands\nfor dynamic generated algorithms domain, and legit stands for\nlegitimate domains.\n\n_1)_ _Length_\n\nFirst, we calculate the length of each domain. In the\nmeantime, we drop those lengths that are less and equal to six,\nbecause for short domains, it is better use blacklist to filter out\ndga domains.\n\n|Col1|domain|\n|---|---|\n|0|google|\n|1|facebook|\n|2|youtube|\n|3|yahoo|\n|4|baidu|\n\n|Col1|domain|class|\n|---|---|---|\n|0|1002n0q11m17h017r1shexghfqf|dga|\n|1|1002ra86698fjpgqke1cdvbk5|dga|\n|2|1008bnt1iekzdt1fqjb76pijxhr|dga|\n|3|100f3a11ckgv438fpjz91idu2ag|dga|\n|4|100fjpj1yk5l751n4g9p01bgkmaf|dga|\n\n\n-----\n\n_2)_ _Entropy_\n\nAnother feature is entropy of domain. In information theory,\nsystems consist of a transmitter, channel, and receiver. The\ntransmitter produces messages that are sent through the\nchannel. The channel modifies the message in some way. The\nreceiver attempts to infer which message was sent. In this\ncontext, entropy (more specifically, Shannon entropy) is the\nexpected value (average) of the information contained in each\nmessage. This feature computes the entropy of character\ndistribution and measure the randomness of each domain\nnames.\n\nThe entropy can explicitly be written as\n\nğ’ğ’ ğ’ğ’\n\nğ‘¯(ğ‘¿ğ‘¯ ğ‘¿) = ï¿½ğ‘·ğ‘·(ğ’™ğ’™ğ’Šğ’Š)ğ‘°ğ‘°(ğ’™ğ’™ğ’Šğ’Š) = âˆ’ï¿½ğ‘·ğ‘·(ğ’™ğ’™ğ’Šğ’Š)ğ’ğ’ğ’ğ’ğ’ğ’ƒğ’ ğ’ƒğ‘·ğ‘·(ğ’™ğ’™ğ’Šğ’Š)\n\n\nğ’Šğ’Š=ğŸ\n\n\nğ’Š=ğŸğ’Š\n\n\nTable 3. Sampling first 5 entries with length and entropy\n\ndomain class length entropy\n0 uchoten-anime legit 13 3.392747\n1 photoprostudio legit 14 2.950212\n5 andhraboxoffice legit 15 3.506891\n6 kodama-tec legit 10 3.121928\n7 porntubster legit 11 3.095795\n\n_D._ _Data Visualization_\n\nBefore we begin our machine learning training, we plot\nscatter chart the check whether there is any correlation among\nthe features.\n\nFigure 2. Scatter Plot: Domain Entropy vs Domain Length\n\nIn this figure, we found that legit domain and DGA domain\nare overlapped together. When domain length is approximately\nequal to four, DGA has a trend that has a higher entropy than\nLegit.\n\n_E._ _Classification with Two Features_\n\nThe next step is to run several classification methods use\nthese two features (length, entropy). There are 787k legit and\n47k DGA domains, so we use 80/20 split techniques for our\ntraining set and testing set. We choose to use three common\nsupervised classification methods. Random Forest, Support\nVector Machines (SVM) and NaÃ¯ve Bayes.\n\nHypothesis:\n\n  - Positive: domain is dga\n\n  - Negative: domain is non-dga, in other words,\nlegitimate domain\n\n_1)_ _Using Random Forest Classifier_\n\nRandom forests or random decision forests are an ensemble\nlearning method for classification, regression and other tasks,\n\n\nthat operate by constructing a multitude of decision trees at\ntraining time and outputting the class that is the mode of the\nclasses (classification) or mean prediction (regression) of the\nindividual trees. Random decision forests correct for decision\ntrees' habit of overfitting to their training set\n\n_a)_ _Random Forest Algorithms_\nA forest is the average of the predictions of its trees:\n\nğ½ğ½\n\nğ¹ğ¹(ğ‘¥ğ‘¥) = [1]\nğ½ğ½ [ï¿½ğ‘“ğ‘“][ğ‘–ğ‘–][(ğ‘¥ğ‘¥)]\n\nğ‘—ğ‘—=1\n\nğ‘¤ğ‘¤â„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ğ½ğ½ ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¡ğ‘¡â„ğ‘’ğ‘’ ğ‘›ğ‘›ğ‘¢ğ‘¢ğ‘¢ğ‘¢ğ‘¢ğ‘¢ğ‘¢ğ‘¢ğ‘¢ğ‘¢ ğ‘œğ‘œğ‘œğ‘œ ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ ğ‘–ğ‘–ğ‘–ğ‘– ğ‘¡ğ‘¡â„ğ‘’ğ‘’ ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“\n\nFor a forest, the prediction is simply the average of the bias\nterms plus the average contribution of each feature:\n\nğ½ğ½ ğ¾ğ¾ ğ½ğ½\n\nğ¹ğ¹(ğ‘¥ğ‘¥) = [1] + ï¿½([1]\nğ½ğ½ [ï¿½ğ‘][ğ‘—ğ‘— ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“][ğ‘] ğ½ğ½ [ï¿½ğ‘][ğ‘][ğ‘][ğ‘][ğ‘][ğ‘][ğ‘] [ğ‘][ğ‘] [ğ‘][ğ‘] [ğ‘][ğ‘][ğ‘—ğ‘—][ğ‘] [(ğ‘¥][ğ‘¥][, ğ‘˜][ğ‘˜][))]\n\nğ‘—ğ‘—=1 ğ‘˜ğ‘˜=1 ğ‘—ğ‘—=1\n\n_b)_ _Classifier Paramteres_\n\nParameters Values\nThe number of features (N) 2\nThe number of trees in the forest (n) 100\nThe number of features for the best split\nâˆšğ‘ğ‘\nThe minimum number of samples to split 2\nThe minimum number of samples at a leaf node 1\n\n_c)_ _Classification Results_\n\nPredicted dga legit All\nTrue\n\ndga 2991 6379 9370\nlegit 427 127532 127959\nAll 3418 133911 137329\n\nTrue Positive Rate (TPR) = 31.92%\nFalse Negative Rate (FNR) = 68.08%\nFalse Positive Rate (FPR) = 0.33%\nTrue Negative Rate (TNR) = 99.67%\nFalse Acceptance Rate (FAR) = 4.76%\nFalse Rejection Rate (FRR) = 12.49%\n\nThe confusion matrix shows how our model predicts in\nclassification using random forest classifier. The row is the true\nlabel, either dga or legit. The column is what our model\npredicted. Both the row and column has a total field indicate\nour sample size. The model performs not well. It identified dga\ndomain as dga with only 31.92% accuracy (true positive rate).\nIt misclassified dga domain as legit domain with 68.08%\naccuracy (false negative rate). Even it has a good prediction on\ntrue positive rate, which is 99.67%, the overall results in a\nbiometric system is not good. False acceptance rate is 4.76%\nand false rejection rate is 12.48%. Therefore, the result of this\nmethod is not meet our requirement.\n\n_2)_ _Using SVM Classifier_\n\n_a)_ _SVM Algorithms_\nGiven a set of training examples, each marked as belonging\nto one or the other of two categories, an SVM training algorithm\n\n|Col1|domain|class|length|entropy|\n|---|---|---|---|---|\n|0|uchoten-anime|legit|13|3.392747|\n|1|photoprostudio|legit|14|2.950212|\n|5|andhraboxoffice|legit|15|3.506891|\n|6|kodama-tec|legit|10|3.121928|\n|7|porntubster|legit|11|3.095795|\n\n|b) Classifier Paramteres|Col2|\n|---|---|\n|Parameters|Values|\n|The number of features (N)|2|\n|The number of trees in the forest (n)|100|\n|The number of features for the best split|âˆšğ‘ğ‘|\n|The minimum number of samples to split|2|\n|The minimum number of samples at a leaf node|1|\n\n|Predicted|dga|legit|All|\n|---|---|---|---|\n|True||||\n|dga|2991|6379|9370|\n|legit|427|127532|127959|\n|All|3418|133911|137329|\n\n\n-----\n\nbuilds a model that assigns new examples to one category or the\nother, making it a non-probabilistic binary linear classifier.\n\n_b)_ _Classifier Parameters_\n\nParameters Value\nKernel Linear\nPenalty parameter C of the error term 1\n\n_c)_ _Classification Result_\n\nPredicted dga legit All\nTrue\n\ndga 1160 8210 9370\nlegit 105 127854 127959\nAll 1265 136064 137329\n\nTPR FNR FPR TNR FAR FRR\n12.38% 87.62% 0.08% 99.92% 6.03% 8.30%\n\nThe confusion matrix indicates how our model predicts in\nclassification using SVM classifier. The row is the true label,\neither dga or legit. The column is what our model predicted.\nBoth the row and column has a total field indicate our sample\nsize. The model performs not well. It identified dga domain as\ndga with only 12.38% accuracy (true positive rate). It\nmisclassified dga domain as legit domain with 87.62% accuracy\n(false negative rate). Even it has a good prediction on true\npositive rate, which is 99.67%, the overall results in a biometric\nsystem is not good. False acceptance rate is 6.03% and false\nrejection rate is 8.30%. Therefore, this method failed in\nclassification.\n\n_3)_ _Using NaÃ¯ve Bayes Classifier_\n\n_a)_ _NaÃ¯ve Bayes Algorithms_\n\nğ‘ƒğ‘ƒ(ğ‘ğ‘|ğ‘¥ğ‘¥) = [ğ‘ƒ][ğ‘ƒ][(ğ‘¥][ğ‘¥][|ğ‘][ğ‘][)ğ‘ƒ][ğ‘ƒ][(ğ‘][ğ‘][)]\nğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥)\n\nğ‘¤ğ‘¤â„ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ğ‘’ ğ‘ƒğ‘ƒ(ğ‘ğ‘|ğ‘‹ğ‘‹) = ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥1) Ã— ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥2) â€¦ ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥ğ‘›ğ‘›) Ã— ğ‘ƒğ‘ƒ(ğ‘ğ‘)\n\n    - ğ‘ƒğ‘ƒ(ğ‘ğ‘|ğ‘‹ğ‘‹) is the posterior probability of class (c,\ntarget) given predictor (x, metric features)\n\n    - ğ‘ƒğ‘ƒ(ğ‘ğ‘) is the prior probability of class\n\n    - ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥|ğ‘ğ‘) is the likelihood which is the probability\nof predictor given class\n\n    - ğ‘ƒğ‘ƒ(ğ‘¥ğ‘¥) is the prior probability of predictor\n\n    - NaÃ¯ve Bayes has no parameters to tune\n\n_b)_ _Classification Result_\n\nPredicted dga legit All\nTrue\n\ndga 3332 6038 9370\nlegit 5061 122898 127959\nAll 8393 128936 137329\n\nTPR FNR FPR TNR FAR FRR\n35.56% 64.44% 3.96% 96.04% 4.68% 60.30%\n\nThe confusion matrix indicates how our model predicts in\nclassification using NaÃ¯ve Bayes classifier. The row is the true\n\n\nlabel, either dga or legit. The column is what our model\npredicted. Both the row and column has a total field indicate\nour sample size. The model performs not well. It identified dga\ndomain as dga with only 35.56% accuracy (true positive rate).\nIt misclassified dga domain as legit domain with 64.44%\naccuracy (false negative rate). Even it has a good prediction on\ntrue positive rate, which is 96.04%, the overall results in a\nbiometric system is not good. False acceptance rate is 4.68%\nand false rejection rate is as high as 60.30%. Therefore, the\nclassifier predicts unsuccessful.\n\nSince these three models are not able to classify dga and legit\ndomains successfully, we need to add more features to improve\nour model.\n\n_F._ _Model Improvement_\n\nWe notice that dga domain either uses some random\ncharacters as text string or uses a dictionary to make up a new\ntext string. Therefore, we build up our own corpus for these\nfeatures.\n\n_1)_ _NGram Features_\n\nIf a domain is a legit domain, it more likely exists in the\nAlexa ranking list. Thus, it is necessary to find the similarity of\nlegit domains. We could use some text analysis techniques. The\nfirst step is to build up a legit text corpus. Given a subsequence\nof domains, we summarize the frequency distribution of Ngram among the Alexa domain name string with n = [3, 5]. We\ncalled it Alexa_grams matrix.\n\n_2)_ _Alexa Gram_\n\nWe calculate the similarity between every single domain and\nAlexa_grams matrix. In order to calculate the similarity, we use\nsome matrix transformation techniques to sum up the\nfrequency. Furthermore, we normalize the frequency by log10\nas a similarity score. (See Table 5.)\n\n_3)_ _Dictionary Gram_\n\nWe use a dictionary that contains 479,623 common used\nword terms [22]. The terms are combination of English\nvocabulary and common used words with mix of number and\nalphabet. We will use a words dictionary. After basic cleaning\nup work, the following is some basic discretions about the\ndictionary.\n\nSimilarly, we calculate the dictionary gram using N-gram, n\n= [3,5] and calculate the normalized similarity between words\ndictionary and every single domain. (See Table 5.) The reason\nwhy we choose n = 3, 4 and 5 is because we have tested n =\n\n[1,10] and found n = 3, 4, 5 have the best accuracy results.\n\nTable 4. First 5 entries of words dictionary\n\nword\n37 a\n48 aa\n51 aaa\n53 aaaa\n54 aaaaaa\n\nTable 5. Sample of domain with Alexa grams and dictionary\ngrams\ndomain Alexa match Dict match\ngoogle 23 14\nfacebook 42 27\n\n|Classifier Parameters|Col2|\n|---|---|\n|Parameters|Value|\n|Kernel|Linear|\n|Penalty parameter C of the error term|1|\n\n|Predicted|dga|legit|All|\n|---|---|---|---|\n|True||||\n|dga|1160|8210|9370|\n|legit|105|127854|127959|\n|All|1265|136064|137329|\n\n|TPR|FNR|FPR|TNR|FAR|FRR|\n|---|---|---|---|---|---|\n|12.38%|87.62%|0.08%|99.92%|6.03%|8.30%|\n\n|Predicted|dga|legit|All|\n|---|---|---|---|\n|True||||\n|dga|3332|6038|9370|\n|legit|5061|122898|127959|\n|All|8393|128936|137329|\n\n|Col1|word|\n|---|---|\n|37|a|\n|48|aa|\n|51|aaa|\n|53|aaaa|\n|54|aaaaaa|\n\n|TPR|FNR|FPR|TNR|FAR|FRR|\n|---|---|---|---|---|---|\n|35.56%|64.44%|3.96%|96.04%|4.68%|60.30%|\n\n|domain|Alexa match|Dict match|\n|---|---|---|\n|google|23|14|\n|facebook|42|27|\n\n\n-----\n\n|pterodactylfarts|53|76|\n|---|---|---|\n|ptes9dro- dwacty2lfa5rrts|30|28|\n\n\nNow, we compute N-Gram matches for all the domains and\nadd to our data frame.\n\nTable 6. Calculated N-Gram for legit domains\ndomain class alexa_grams word_grams\ninvestmentsonthebeach legit 144.721988 109.722683\ninfiniteskills legit 81.379156 72.785882\ndticash legit 26.557931 23.710317\nhealthyliving legit 76.710198 61.721689\nasset-cache legit 46.267887 31.690803\n\nTable 7. Calculated N-Gram for dga domains\ndomain class alexa_grams word_grams\nwdqdreklqnpp dga 11.242176 6.367475\nwdqjkpltirjhtho dga 14.303602 16.554439\nwdqxavemaedon dga 28.468264 28.699800\nwdraokbcnspexm dga 25.935386 19.784933\nwdsqfivqnqcbna dga 4.597991 3.629002\n\n_4)_ _Data Visualization_\n\nHere we plot scatter about whether our new 'alexa_grams'\nfeature can help us differentiate between DGA and Legit\ndomains.\n\nFigure 3. Scatter Plot: Alexa Gram vs Domain Length\n\nFigure 4. Scatter Plot: Alexa Gram vs Domain Entropy\n\nHere we want to see whether our new 'word_grams' feature\ncan help us differentiate between Legit/DGA.\n\nFigure 5. Scatter Plot: Dictionary Gram vs Domain Length\n\n|domain|class|alexa_grams|word_grams|\n|---|---|---|---|\n|investmentsonthebeach|legit|144.721988|109.722683|\n|infiniteskills|legit|81.379156|72.785882|\n|dticash|legit|26.557931|23.710317|\n|healthyliving|legit|76.710198|61.721689|\n|asset-cache|legit|46.267887|31.690803|\n\n|domain|class|alexa_grams|word_grams|\n|---|---|---|---|\n|wdqdreklqnpp|dga|11.242176|6.367475|\n|wdqjkpltirjhtho|dga|14.303602|16.554439|\n|wdqxavemaedon|dga|28.468264|28.699800|\n|wdraokbcnspexm|dga|25.935386|19.784933|\n|wdsqfivqnqcbna|dga|4.597991|3.629002|\n\n|Predicted|dga|legit|All|\n|---|---|---|---|\n|True||||\n|dga|9139|231|9370|\n|legit|254|127705|127959|\n|All|9393|127936|137329|\n\n|TPR|FNR|FPR|TNR|FAR|FRR|\n|---|---|---|---|---|---|\n|97.53%|2.47%|0.20%|99.80%|0.18%|2.70%|\n\n|Predicted|dga|legit|All|\n|---|---|---|---|\n|True||||\n|dga|8623|747|9370|\n|legit|534|127425|127959|\n|All|9157|128172|137329|\n\n|TPR|FNR|FPR|TNR|FAR|FRR|\n|---|---|---|---|---|---|\n|92.03%|7.97%|0.42%|99.58%|0.58%|5.83%|\n\n\nThe confusion matrix indicates how our model predicts in\nclassification using SVM classifier. The row is the true label,\neither dga or legit. The column is what our model predicted.\nBoth the row and column has a total field indicate our sample\nsize. The model performs pretty well. It identified dga domain\nas dga with 92.03% accuracy (true positive rate). It\nmisclassified dga domain as legit domain as low as 7.97% (false\nnegative rate). It has a good prediction on true positive rate,\n\n\nFigure 6. Scatter Plot: Dictionary Gram vs Entropy\n\nAfter we add two extra features, the overlapped issue\nimproved. We could have a clear view that legit, dga has their\nown clusters, and it is more reasonable to perform some\nclassification methods once again.\n\n_5)_ _Classification with Four Feature_\n\nNow we have four features in our model: Length, Entropy,\nAlexa_grams, and Dict_grams. We could use the same\nparameters tuning our classification model.\n\n_a)_ _Using Random Forest Classifier_\n\n\ndga 9139 231 9370\nlegit 254 127705 127959\nAll 9393 127936 137329\n\n\nThe confusion matrix indicates how our model predicts in\nclassification using random forest classifier. The row is the true\nlabel, either dga or legit. The column is what our model\npredicted. Both the row and column has a total field indicate\nour sample size. The model performs pretty well. It identified\ndga domain as dga with 97.53% accuracy (true positive rate). It\nmisclassified dga domain as legit domain as low as 2.47% (false\nnegative rate). It has a good prediction on true positive rate,\nwhich is 99.80%, It also has low false positive rate which is\n0.20%. The overall results in a biometric system is good as well.\nFalse acceptance rate is 0.18% and false rejection rate is 2.70%.\nTherefore, this method succeeds in classification.\n\n_b)_ _Using SVM Classifier_\n\n\ndga 8623 747 9370\nlegit 534 127425 127959\nAll 9157 128172 137329\n\n\n-----\n\nwhich is 99.80%, It also has low false positive rate which is\n0.42%. The overall results in a biometric system is good as well.\nFalse acceptance rate is 0.58% and false rejection rate is 5.83%.\nTherefore, this method succeeds in classification.\n\n_c)_ _Using NaÃ¯ve Bayes Classifier_\n\n|Col1|Length|Entropy|Alexa_grams|Dict_grams|\n|---|---|---|---|---|\n|Score|0.2925341|0.21776668|0.36576691|0.1239323|\n\n\ndga 7203 2167 9370\nlegit 354 127605 127959\nAll 7557 129772 137329\n\n|Predicted|dga|legit|All|\n|---|---|---|---|\n|True||||\n|dga|7203|2167|9370|\n|legit|354|127605|127959|\n|All|7557|129772|137329|\n\n|TPR|FNR|FPR|TNR|FAR|FRR|\n|---|---|---|---|---|---|\n|76.87%|23.13%|0.28%|99.72%|1.67%|4.68%|\n\n\nThe confusion matrix indicates how our model predicts in\nclassification using NaÃ¯ve Bayes classifier. The row is the true\nlabel, either dga or legit. The column is what our model\npredicted. Both the row and column has a total field indicate\nour sample size. The model performs pretty well. It identified\ndga domain as dga with only 76.87% accuracy (true positive\nrate). It misclassified dga domain as legit domain with 23.13%\n(false negative rate). It has a good prediction on true positive\nrate, which is 99.72%. It has low false positive rate, which is\n0.28%. The overall results in a biometric system is not good.\nFalse acceptance rate is 1.67% and false rejection rate is 4.68%.\nTherefore, this method failed in classification.\n\n_6)_ _Model Comparisons_\n\nTable 8. Model Comparisons\nPerformance Rate Random Forest SVM NaÃ¯ve Bayes\nTPR 97.53% 92.03% 76.87%\nFNR 2.47% 7.97% 23.13%\nFPR 0.20% 0.42% 0.28%\nTNR 99.80% 99.58% 99.72%\nFAR 0.18% 0.58% 1.67%\nFRR 2.70% 5.83% 4.68%\n\nFor true positive, true negative rate, the higher the better,\nbecause it means more accurate on our prediction. For false\npositive rate, true negative rate, false acceptance rate and false\nrejection rate, the lower the better, because it means the type I\nand type II error rates. Among all three models, Random Forest\nclassifier outperforms the best. The reason that random forest\nperforms the best is because random forest is a multi-layer\ndecision tree. It will subgroup every details of features in a tree\nstructure. The domain is a series of text string, and a tree\nstructure classifier very easily captures the specific features of\ntext string. However, linear SVM is trying to draw several\nstraight line between the features of data. The scatter plot shows\nthat we still have overlapped data among all the features so that\nthe accuracy of SVM is not as good as random forest. The NaÃ¯ve\nBayes is a combination of conditional probabilities, and a single\ngram is not effective among text string.\n\nWe used this classifier as our prediction model. We also\ncalculate the importance score on these four features. The\nimportance of a feature is computed as normalized total\nreduction of the criterion brought by that feature.\n\n\nTable 9. Importance Score on Random Forest\nLength Entropy Alexa_grams Dict_grams\nScore 0.2925341 0.21776668 0.36576691 0.1239323\n\nWe found that the most important feature in our model is\nAlexa_grams. It indicates that Alexa ranking maintains a good\ncontribution on dga classification. It proves our hypotheses that\nmost of botnet masters are using dictionary or random\ncharacters to generate malicious domains. The second ranking\nis length of domain names followed by entropy and\nDict_grams. It indicates that more and more botnet masters are\nusing some English words dictionary as their algorithms input.\nOur methods could also detect dga that using dictionary.\n\n_7)_ _Misclassification_\n\n_a)_ _Educational Institution Domains_\nFirst, look at a piece of our prediction sample. The following\ntable is an example of prediction using random forest as a\nclassifier. It performs and predicts well except some university\ndomain names. For example, tsinghua.edu.cn and sjtu.edu.cn\nare the domain names of university in China.\n\nTable 10. Prediction sample\n\ndomain prediction\ngoogle legit\nwebmagnat.ro legit\nbikemastertool.com legit\n1cb8a5f36f dga\npterodactylfarts legit\npybmvodrcmkwq.biz dga\nabuliyan.com legit\nbey666on4ce dga\nsjtu.edu.cn dga\ntsinghua.edu.cn dga\n\nTable 11. Misclassification sample\ndomain length entropy alexa_gram word_gram predict\nduurzaamthuis 13 3.18083 20.353 17.785 legit\nhutkuzwropgf 12 3.4183 14.240 10.431 legit\nxn-ecki4eoz0157d 28 4.28039 37.036 15.577 legit\nhv1bosfom5c\n\nnllcolooxrycoy 14 2.61058 31.160 26.914 dga\ndktazhqlzsnorer 15 3.64022 24.592 22.804 legit\neprqhtyhoplu 12 3.25163 24.762 19.213 dga\ndomowe-wypieki 14 3.23593 28.051 24.537 legit\ntaesdijrndsatw 14 3.23593 30.930 21.647 dga\nedarteprsytvhww 15 3.37356 36.684 29.358 dga\nukonehloneybmfb 15 3.37356 39.44 36.303 dga\nekgzkawofkxzlq 14 3.32486 7.0389 5.4897 legit\n\nFor those legit domains but our model treat them as dga,\nsome of legit domains come from foreigner countries. For\nexample, domowe-wypieki comes from www.domowewypieki.com, which is a homemade pastries food website in\npolish. These countries use very different word and character\nsystem than those in English. In order to use English words in\ndomain system, many of domains are adapted and made of\nsome initial letters of approximately pronunciation of foreigner\nlanguage. This is why some legit domain arise misclassification\nissue.\n\n|domain|prediction|\n|---|---|\n|google|legit|\n|webmagnat.ro|legit|\n|bikemastertool.com|legit|\n|1cb8a5f36f|dga|\n|pterodactylfarts|legit|\n|pybmvodrcmkwq.biz|dga|\n|abuliyan.com|legit|\n|bey666on4ce|dga|\n|sjtu.edu.cn|dga|\n|tsinghua.edu.cn|dga|\n\n|Performance Rate|Random Forest|SVM|NaÃ¯ve Bayes|\n|---|---|---|---|\n|TPR|97.53%|92.03%|76.87%|\n|FNR|2.47%|7.97%|23.13%|\n|FPR|0.20%|0.42%|0.28%|\n|TNR|99.80%|99.58%|99.72%|\n|FAR|0.18%|0.58%|1.67%|\n|FRR|2.70%|5.83%|4.68%|\n\n|domain|length|entropy|alexa_gram|word_gram|predict|\n|---|---|---|---|---|---|\n|duurzaamthuis|13|3.18083|20.353|17.785|legit|\n|hutkuzwropgf|12|3.4183|14.240|10.431|legit|\n|xn-- ecki4eoz0157d hv1bosfom5c|28|4.28039|37.036|15.577|legit|\n|nllcolooxrycoy|14|2.61058|31.160|26.914|dga|\n|dktazhqlzsnorer|15|3.64022|24.592|22.804|legit|\n|eprqhtyhoplu|12|3.25163|24.762|19.213|dga|\n|domowe-wypieki|14|3.23593|28.051|24.537|legit|\n|taesdijrndsatw|14|3.23593|30.930|21.647|dga|\n|edarteprsytvhww|15|3.37356|36.684|29.358|dga|\n|ukonehloneybmfb|15|3.37356|39.44|36.303|dga|\n|ekgzkawofkxzlq|14|3.32486|7.0389|5.4897|legit|\n\n\n-----\n\nFor those dga domains but our model regards them as legit,\nprobably because Alexa ranking only summarize the unique\nvisiting volume. Thus, there are still so many malicious and dga\ndomain are among Alexa dataset.\n\n_b)_ _Discussion_\nThere are some potential ways to address those issues above\nand improve our model. First, we could set up a filter to sort the\ntop-level domain (TLD) on those education and non-profit\ndomains. In addition, for those foreign websites, we would try\nto figure out how these domains works and find a better legit\ndataset, except for Alexa. We could also use other dictionary\nsuch as Wiki keywords as our classifier features. At last, we\nplan to build up a self-adapted machine learning architecture\nthat could learn from real-time DNS traffic, detect, and prevent\nthose anomaly activities in our future research.\n\nV. CONCLUSION AND DISCUSSION\n\nIn this paper, we introduce the necessary about detection of\nDGA domains. In addition, we tested three common machine\nlearning algorithms, random forest, SVM and NaÃ¯ve Bayes, to\nclassify legit and DGA domain names. We provide data\nvisualization techniques with two new features, Alexa gram and\nDictionary gram in classification experiment. At last, we found\nintroducing NGram features would increase the accuracy of\nclassification models and random forest classifier performs the\nbest among all. We also found some issue using our methods\nand come up some ideas to solve the problem. We plan to\nimprove our classification method and then setup our own DNS\nservers and build up two-engine network monitoring system.\nOne is for machine learning training and model updating. The\nother one is for real-time monitoring for prevention.\n\nREFERENCES\n\n[1] S. Yadav, A. K. K. Reddy, A. L. N. Reddy, and S. Ranjan, â€œDetecting\nalgorithmically generated malicious domain names,â€ presented at the the\n10th annual conference, New York, New York, USA, 2010, pp. 48â€“61.\n\n[2] S. Yadav, A. K. K. Reddy, A. L. N. Reddy, and S. Ranjan, â€œDetecting\nalgorithmically generated domain-flux attacks with DNS traffic analysis,â€\nIEEE/ACM Transactions on Networking (TON, vol. 20, no. 5, Oct. 2012.\n\n[3] A. Reddy, â€œDetecting Networks Employing Algorithmically Generated\nDomain Names,â€ 2010.\n\n[4] Z. Wei-wei and G. Qian, â€œDetecting Machine Generated Domain Names\nBased on Morpheme Features,â€ 2013.\n\n[5] P. Barthakur, M. Dahal, and M. K. Ghose, â€œAn Efficient Machine\nLearning Based Classification Scheme for Detecting Distributed\nCommand & Control Traffic of P2P Botnets,â€ International Journal of\nModern â€¦, 2013.\n\n[6] G. Gu, R. Perdisci, J. Zhang, and W. Lee. BotMiner: Clustering Analysis\nof Network Traffic for Protocol- and Structure-independent Botnet\nDetection. Proceedings of the 17th USENIX Security Symposium\n(Securityâ€™08), 2008.\n\n[7] G. Gu, J. Zhang, and W. Lee. BotSniffer: Detecting Botnet Command and\nControl Channels in Network Traffic. Proc. of the 15th Annual Network\nand Distributed System Security Symposium (NDSSâ€™08), Feb. 2008.\n\n[8] T. Holz, M. Steiner, F. Dahl, E. W. Biersack, and F. Freiling.\nMeasurements and Mitigation of Peer-to-peer-based Botnets: A Case\nStudy on Storm Worm. In First Usenix Workshop on Large-scale Exploits\nand Emergent Threats (LEET), April 2008.\n\n[9] S. S. J. Ma, L.K. Saul and G. Voelker. Beyond Blacklists: Learning to\nDetect Malicious Web Sites from Suspicious URLs. Proc. of ACM KDD,\nJuly 2009.\n\n\n\n[10] D.K.McGrathandM.Gupta.BehindPhishing:AnExaminationofPhisher\nModi Operandi. Proc. of USENIX workshop on Large-scale Exploits and\nEmergent Threats (LEET), Apr. 2008.\n\n[11] E. Passerini, R. Paleari, L. Martignoni, and D. Bruschi. Fluxor : Detecting\nand Monitoring Fast-flux Service Networks. Detection of Intrusions and\nMalware, and Vulnerability Assessment, 2008.\n\n[12] R. Perdisci, I. Corona, D. Dagon, and W. Lee. Detecting Malicious Flux\nService Networks Through Passive Analysis of Recursive DNS Traces.\nIn Annual Computer Society Security Applications Conference\n(ACSAC), dec 2009.\n\n[13] M. Antonakakis, R. Perdisci, D. Dagon,W. Lee, and N. Feamster.\nBuilding a Dynamic Reputation System for DNS. In USENIX Security\nSymposium,2010.\n\n[14] Y. Xie, F. Yu, K. Achan, R. Panigrahy, G. Hulten, and I. Osipkov.\nSpamming Botnets: Signatures and Characteristics. ACM SIGCOMM\nComputer.\n\n[15] Manos Antonakakis, Roberto Perdisci, Yacin Nadji, Nikolaos Vasiloglou,\n\nSaeed Abu-Nimeh, Wenke Lee, and David Dagon. 2012. From throwaway traffic to bots: detecting the rise of DGA-based malware.\nIn Proceedings _of_ _the_ _21st_ _USENIX_ _conference_ _on_ _Security_\n_symposium (Security'12). USENIX Association, Berkeley, CA, USA, 24-_\n24.\n\n[16] ZeuS Gets More Sophisticated Using P2P Techniques.\nhttp://www.abuse.ch/?p=3499, 2011\n\n[17] S. Stover, D. Dittrich, J. Hernandez, and S. Dietrich. Analysis of the storm\n\nand nugache trojans: P2P is here. In _USENIX; login:, vol. 32, no. 6,_\nDecember 2007.\n\n[18] Wikipedia. The storm botnet. http://en.wikipedia.org/wiki/Storm_botnet.\n\n[19] Prince, Brian (January 26, 2007). \"'Storm Worm' Continues to Spread\nAround Globe\". FOXNews.com. Retrieved 2007-01-27.\n\n[20] Alexa ranking, https://aws.amazon.com/alexa-top-sites/\n\n[21] Dataset collection, http://datadrivensecurity.info/blog/pages/dds-datasetcollection.html\n\n[22] Data hacking, http://clicksecurity.github.io/data_hacking/\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "99fdc3ef-333d-48f5-a4a1-becd788c7b80",
            "created_at": "2022-10-25T15:28:29.802983Z",
            "updated_at": "2022-10-25T15:28:29.802983Z",
            "deleted_at": null,
            "name": "MITRE",
            "url": "https://github.com/mitre-attack/attack-stix-data",
            "description": "MITRE ATT&CK STIX Data",
            "reports": null
        }
    ],
    "references": [
        "http://csis.pace.edu/~ctappert/srd2017/2017PDF/d4.pdf"
    ],
    "report_names": [
        "d4.pdf"
    ],
    "threat_actors": [],
    "ts_created_at": 1666716496,
    "ts_updated_at": 1743041166,
    "ts_creation_date": 1493453638,
    "ts_modification_date": 1493453642,
    "files": {
        "pdf": "https://archive.orkl.eu/bf0c5b619ccbbbe25349df0f6b7b14a4dafb3aec.pdf",
        "text": "https://archive.orkl.eu/bf0c5b619ccbbbe25349df0f6b7b14a4dafb3aec.txt",
        "img": "https://archive.orkl.eu/bf0c5b619ccbbbe25349df0f6b7b14a4dafb3aec.jpg"
    }
}