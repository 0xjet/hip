{
    "id": "ea951440-4a84-47a3-9ee5-68d4a691a92c",
    "created_at": "2023-01-12T15:02:33.39165Z",
    "updated_at": "2025-03-27T02:05:26.496512Z",
    "deleted_at": null,
    "sha1_hash": "ff99da42fafe6576aabc3af90fbc02ad54923add",
    "title": "2020-06-22 - Dynamic Correlation, ML and Hunting",
    "authors": "",
    "file_creation_date": "2022-05-27T23:00:42Z",
    "file_modification_date": "2022-05-27T23:00:42Z",
    "file_size": 124625,
    "plain_text": "# Dynamic Correlation, ML and Hunting\n\n**[findingbad.blogspot.com/2020/06/dynamic-correlation-ml-and-hunting.html](http://findingbad.blogspot.com/2020/06/dynamic-correlation-ml-and-hunting.html)**\n\nHunting has been my primary responsibility for the last several years. Over this time I've\ndone a lot of experimentation around different processes and methods of finding malicious\nactivity in log data. What has always stayed true though is the need for a solid understanding\nof the hypothesis you're using, familiarity with all the data you can take advantage of and a\nmethod to produce/analyze the results. For this post I'd like to share one of the ideas I've\nbeen working on lately.\n\nI've previously written a number of blog posts on beaconing. Over time I've refined much of\nhow I go about looking for these anomalous connections. My rule of thumb for hunting\nbeacons (or other types of malicious activity) is to ignore static IOC's as those are best suited\nfor detection. Instead, focus on behaviors or clusters of behaviors that will return higher\nconfidence output. Here's how I'm accomplishing this in a single Splunk search.\n\n1. Describe what you are looking for in numbers. This will allow you to have much more\n\ncontrol over your conditional statements which impacts the quality of your output.\n2. Define those attributes that you are interested in and assign number values to them.\n\nThese attributes will be your points of correlation.\n3. Reduce your output to those connections that exhibit any of what you are looking for.\n\nThis is the correlation piece where we can use the total score of all attributes identified\nwithin a src/dest pair. Higher sums translate to greater numbers of attributes identified.\n\nBelow is a screenshot of the search I came up with. This is again using the botsv3 data set\nfrom Splunk's Boss of the SOC competition. Thanks Splunk!\n\nThe following is a description of the fields in the output.\n\n-dest: Based on the data source, this field may include the ip address or domain name.\n\n\n-----\n\n-src_ip: Source of request\n-dest_ip: Destination of request\n-bytes_out: Data sent from src_ip.\n-distinct_event_count: The total number of connections per destination.\n-i_bytecount: The total count of bytes_out by src/dest/bytes_out. Large numbers may\nindicate beaconing.\n-t_bytecount: The total count of connections between src/dest.\n-avgcount: i_bytecount / t_bytecount. Beacon percentage. Values closer to 1 are more\nindicative of a beaocn.\n-distinct_byte_count: Total count of bytes_out (used in determining percentages for\nbeaconing).\n-incount: The count of unique bytes_in values. When compared with t_bytecount you may\nsee the responses to beacons.\n-time_count: The number of hours the src/dest have been communicating. Large numbers\nmay indicate persistent beaconing.\n-o_average: The average beacon count between all src/dests.\n-above: The percentage above o_average.\n-beaconmult: weight multiplier given to higher above averages.\n-evtmult: weight multiplier given to destinations with higher volume connections.\n-timemult: weight multiplier given to connections that last multiple hours.\n-addedweight: The sum of all multipliers.\n\nYou can see from the search results that we reduced 30k+ events down to 1700 that exhibit\nsome type of behavior that we're interested in. This is good, but still not feasible to analyze\nevery event individually. I have a couple of choices to reduce my output at this point. I can\nadjust my weighted condition to something like \"|where weighted > 100\" which would have\nthe effect of requiring multiple characteristics being correlated. My other choice is to use\nsome type of anomaly detection to surface those odd connections. You can probably tell\nfrom the \"ML\" portion of the title which direction I'm going to go. So from here we need a\nmethod to pick out the anomalies as the vast majority of this data is likely legitimate traffic.\nFor this I'll be inserting our results into a MySQL database. I don't necessarily need to for\nthis analysis, but it's a way for me to keep the metadata of the connections for greater\nperiods of time. This will allow me to do longer term analysis based on the data that is being\nstored.\n\nOnce it's in the database we can use python and various ML algorithms to surface\nanomalous traffic. For this I'll be using an Isolation Forest. I'll also be choosing fields that I\nthink best represents what a beacon looks like as I don't want to feed every field through this\nprocess.\n\n\n-----\n\ndistinct_event_count: Overall activity.\ntime_count: How persistent is the traffic?\nabove: How does the beacon frequency compare to all other traffic?\naddedweight: How many beacon characteristics does this traffic exhibit?\n\nThe following screenshot contains the code as well as the output.\n\nLooking at the top 3 tenths of 1 percent of the most anomalous src/dest pairs you can see\nthat there are 4 destination ip addresses that may need investigating. If you've read my last\n2 posts on beaconing the 45.77.53.176 ip should look familiar. This ip was definitely used for\nC2. The 172.16.0.178 ip is also interesting. Taking a quick look at the destination in the\nbotsv3 data, you can see memchached injection that appears to be successful. Additional\ninvestigation of the src ip's in this output would definitely be justified.\n\nI will say that this method is very good at identifying beacons, but beacons are not always\nmalicious. Greater work may be needed to surface those types malicious connections.\nSome additional ideas may be first seen ip's or incorporating proxy data where even more\ncharacteristics can be defined, scored and correlated.\n\nA large portion of hunting is experimentation so experiment with the data and see what you\ncan come up with!\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2020/2020-06-22 - Dynamic Correlation, ML and Hunting.pdf"
    ],
    "report_names": [
        "2020-06-22 - Dynamic Correlation, ML and Hunting.pdf"
    ],
    "threat_actors": [
        {
            "id": "610a7295-3139-4f34-8cec-b3da40add480",
            "created_at": "2023-01-06T13:46:38.608142Z",
            "updated_at": "2025-03-27T02:00:02.87217Z",
            "deleted_at": null,
            "main_name": "Cobalt",
            "aliases": [
                "Cobalt Gang",
                "GOLD KINGSWOOD",
                "COBALT SPIDER",
                "G0080",
                "Mule Libra",
                "Cobalt Group"
            ],
            "source_name": "MISPGALAXY:Cobalt",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        }
    ],
    "ts_created_at": 1673535753,
    "ts_updated_at": 1743041126,
    "ts_creation_date": 1653692442,
    "ts_modification_date": 1653692442,
    "files": {
        "pdf": "https://archive.orkl.eu/ff99da42fafe6576aabc3af90fbc02ad54923add.pdf",
        "text": "https://archive.orkl.eu/ff99da42fafe6576aabc3af90fbc02ad54923add.txt",
        "img": "https://archive.orkl.eu/ff99da42fafe6576aabc3af90fbc02ad54923add.jpg"
    }
}