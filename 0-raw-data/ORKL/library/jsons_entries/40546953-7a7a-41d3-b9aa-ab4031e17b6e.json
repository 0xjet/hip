{
    "id": "40546953-7a7a-41d3-b9aa-ab4031e17b6e",
    "created_at": "2022-10-25T16:48:19.538484Z",
    "updated_at": "2025-03-27T02:11:23.05434Z",
    "deleted_at": null,
    "sha1_hash": "37fa6061b337131904e4ab9bb14edd3281adf083",
    "title": "",
    "authors": "",
    "file_creation_date": "2020-04-21T16:36:32Z",
    "file_modification_date": "2020-04-21T16:36:38Z",
    "file_size": 491795,
    "plain_text": "# Detect and Prevent Web Shell Malware\n\n## Summary\n\nCyber actors have increased the use of web shell malware for computer network exploitation [1][2][3][4]. Web shell\nmalware is software deployed by a hacker, usually on a victim’s web server. It can be used to execute arbitrary system\ncommands, which are commonly sent over HTTP or HTTPS. Web shell attacks pose a serious risk to DoD components.\nAttackers often create web shells by adding or modifying a file in an existing web application. Web shells provide\nattackers with persistent access to a compromised network using communication channels disguised to blend in with\nlegitimate traffic. Web shell malware is a long-standing, pervasive threat that continues to evade many security tools.\n\nCyber actors deploy web shells by exploiting web application vulnerabilities or uploading to otherwise compromised\nsystems. Web shells can serve as persistent backdoors or as relay nodes to route attacker commands to other systems.\nAttackers frequently chain together web shells on multiple compromised systems to route traffic across networks, such as\nfrom internet-facing systems to internal networks [5].\n\nIt is a common misperception that only internet-facing systems are targeted for web shells. Attackers frequently deploy\nweb shells on non-internet facing web servers, such as internal content management systems or network device\nmanagement interfaces. Internal web applications are often more susceptible to compromise due to lagging patch\nmanagement or permissive security requirements.\n\nThough the term “web shells” is predominantly associated with malware, it can also refer to web-based system\nmanagement tools used legitimately by administrators. While not the focus of this guidance, these benign web shells may\npose a danger to organizations as weaknesses in these tools can result in system compromise. Administrators should use\nsystem management software leveraging enterprise authentication methods, secure communication channels, and\nsecurity hardening.\n\n## Mitigating Actions (DETECTION)\n\nWeb shells are difficult to detect as they are easily modified by attackers and often employ encryption, encoding, and\nobfuscation. A defense-in-depth approach using multiple detection capabilities is most likely to discover web shell\nmalware. Detection methods for web shells may falsely flag benign files. When a potential web shell is detected,\nadministrators should validate the file’s origin and authenticity. Detection techniques include:\n\n### “Known-Good” Comparison\n\nWeb shells primarily target existing web applications and rely on creating or modifying files. The best method of detecting\nthese web shells is to compare a verified benign version of the web application (i.e., a “known-good”) against the\nproduction version. Discrepancies should be manually reviewed for authenticity. Additional information and scripts to\nenable known-good comparison are available in Appendix A and are maintained on\nhttps://github.com/nsacyber/Mitigating-Web-Shells.\n\nWhen adjudicating discrepancies with a known-good image, administrators are cautioned against trusting timestamps on\nsuspicious systems. Some attackers use a technique known as “timestomping” [6] to alter created and modified times in\norder to add legitimacy to web shell files. Administrators should not assume that a modification is authentic simply\nbecause it appears to have occurred during a maintenance period. However, as an initial triage method, administrators\nmay choose to prioritize verification of files with unusual timestamps.\n\n### Web Traffic Anomaly Detection\n\nWhile attackers often design web shells to blend in with normal web traffic, some characteristics are difficult to imitate\nwithout advanced knowledge. These characteristics include user agent strings and client Internet Protocol (IP) address\nspace. Prior to having a presence on a network, attackers are unlikely to know which user agents or IP addresses are\n\n\n-----\n\ntypical for a web server, so web shell requests will appear anomalous. In addition, web shells routing attacker traffic will\ndefault to the web server’s user agent and IP address, which should be unusual in network traffic. Uniform Resource\nIdentifiers (URIs) exclusively accessed by anomalous user agents are potentially web shells. Finally, some attackers\nneglect to disguise web shell request “referer [sic] headers”[1] as normal traffic. Consequently, requests with missing or\nunusual referer headers could indicate web shell presence. Centralized log-querying capabilities, such as Security\nInformation and Event Management (SIEM) systems, provide a means to implement this analytic. If such a capability is\nnot available, administrators may use scripting to parse web server logs to identify possible web shell URIs. Example\nSplunk[®2] queries (Appendix B), scripts for analyzing log data (Appendix C), and additional information about detecting\nweb traffic anomalies are maintained at https://github.com/nsacyber/Mitigating-Web-Shells.\n\n### Signature-Based Detection\n\nFrom the host perspective, signature-based detection is unreliable because web shells may be obfuscated and are easy\nto modify. However, some cyber actors use popular web shells (e.g., China Chopper, WSO, C99, B374K, R57) with\nminimal modification. In these cases, fingerprint or expression-based detection may be possible. A collection of Snort[®3]\nrules to detect common web shell files, scanning instructions, and additional information about signature-based detection\nare maintained at https://github.com/nsacyber/Mitigating-Web-Shells.\n\nFrom the network perspective, signature-based detection of web shells is unreliable because web shell communications\nare frequently obfuscated or encrypted. Additionally, “hard-coded” values like variable names are easily modified to further\nevade detection. While unlikely to discover unknown web shells, signature-based network detection can help identify\nadditional infections of a known web shell. Appendix D provides a collection of signatures to detect network\ncommunication from common, unmodified or slightly modified web shells sometimes deployed by attackers. This list is\nalso maintained at https://github.com/nsacyber/Mitigating-Web-Shells.\n\n### Unexpected Network Flows\n\nIn some cases, attackers use web shells on systems other than web servers (e.g., workstations). These web shells\noperate on rogue web server applications and can evade file-based detection by running exclusively in memory (i.e.,\nfileless execution). While functionally similar to a traditional Remote Access Tool (RAT), these types of web shells allow\nattackers to easily chain malicious traffic through a uniform platform. These types of web shells can be detected on wellmanaged networks because they listen and respond on previously unused ports. Additionally, if an attacker is using a\nperimeter web server to tunnel traffic into a network, connections would be made from a perimeter device to an internal\nnode. If administrators know which nodes on their network are acting as web servers, then network analysis can reveal\nthese types of unexpected flows. A variety of tools including vulnerability scanners (e.g., Nessus[®4]), intrusion detection\nsystems (e.g., Snort[®]), and network security monitors (e.g., Zeek™[5] [formerly “Bro”]) can reveal the presence of\nunauthorized web servers in a network. Maintaining a thorough and accurate depiction of expected network activity can\nenhance defenses against many types of attack. The Snort[®] rule in Appendix E and maintained at\n[https://github.com/nsacyber/Mitigating-Web-Shells](https://github.com/nsacyber/Mitigating-Web-Shells) can be tailored for a specific network to identify unexpected network\nflows.\n\n### Endpoint Detection and Response (EDR) Capabilities\n\nSome EDR and enhanced host logging solutions may be able to detect web shells based on system call or process\nlineage abnormalities. These security products monitor each process on the endpoint including invoked system calls. Web\nshells usually cause the web server process to exhibit unusual behavior. For instance, it is uncommon for most benign\nweb servers to launch the ipconfig utility, but this is a common reconnaissance technique enabled by web shells. EDRs\nhave different automated capabilities and querying interfaces, so organizations are encouraged to review documentation\nor discuss web shell detection with the vendor. Appendix F illustrates how Sysmon’s enhanced process logging data can\n\n1 “Referer” is an HTTP header specified in Internet Engineering Task Force RFC 7231\n\n2 Splunk is a registered trademark of Splunk, Inc.\n\n3 Snort is a registered trademark of Cisco Technologies, Inc.\n\n4 Nessus is a registered trademark of Tenable Network Security, Inc.\n\n5 Zeek is a trademark of the Zeek Project\n\n\n-----\n\nbe used to identify process abnormalities in a Microsoft[®] Windows[®6] environment. Similarly, Appendix G illustrates how\nauditd can be used to identify process abnormalities in a Linux[®7] environment. Guidance for these identifying process\nabnormalities in these environments is also maintained at https://github.com/nsacyber/Mitigating-Web-Shells.\n\n### Other Anomalous Network Traffic Indicators\n\nWeb shell traffic may exhibit other detectable abnormal characteristics depending on the attacker. In particular, unusually\nlarge responses (possible data exfiltration), recurring off-peak access times (possible non-local work schedule), and\ngeographically disparate requests (possible foreign operator) could indicate URIs of potential web shells. However, these\ncharacteristics are highly subjective and likely to flag many benign URIs. Administrators may choose to implement these\ndetection analytics if the baseline characteristic is uniform for their environment.\n\n## Mitigating Actions (PREVENTION)\n\nPreventing web shells should be a priority for both internet-facing and internal web servers. Good cyber hygiene and a\ndefense-in-depth approach based on the mitigations below provide significant hardening against web shells. Prevention\ntechniques include:\n\n### Web Application Update Prioritization\n\nAttackers sometimes target vulnerabilities in internet-facing and internal web applications within 24 hours of a patch\nrelease. Update these applications as soon as patches are available. Whenever possible, enable automatic updating and\nconfigure frequent update cadence (at least daily). Deploy manual updates on a frequent basis when automatic updating\nis not possible. Appendix H lists some commonly exploited vulnerabilities.\n\n### Web Application Permissions\n\nWeb services should follow the least privilege security paradigm. In particular, web applications should not have\npermission to write directly to a web accessible directory or modify web accessible code. Attackers are unable to upload a\nweb shell to a vulnerable application if the web server blocks access to the web accessible directory. To preserve\nfunctionality, some web applications require configuration changes to save uploads to a non-web accessible area. Prior to\nimplementing this mitigation, consult documentation or discuss changes with the web application vendor.\n\n### File Integrity Monitoring\n\nIf administrators are unable to harden web application permissions as described above, file integrity monitoring can\nachieve a similar effect. File integrity software can block file changes to web accessible directories or alert when changes\noccur. Additionally, monitoring software has the benefit of allowing certain file changes but blocking others. For example, if\nan internal web application handles only Portable Document Format (PDF) files, integrity monitoring can block uploads\nwithout a “.pdf” extension. Appendix I provides a set of Host Intrusion Prevention System (HIPS) rules for use with\nMcAfee[®8] Host Based Security System (HBSS) to enforce file integrity on web accessible directories. These rules,\nimplementation instructions, and additional information about file integrity monitoring are maintained at\nhttps://github.com/nsacyber/Mitigating-Web-Shells.\n\n### Intrusion Prevention\n\nIntrusion Prevention Systems (IPS) and Web Application Firewalls (WAF) each add a layer of defense for web\napplications by blocking some known attacks. Organizations should implement these appliances to block known malicious\nuploads. If possible, administrators are encouraged to implement the OWASP™[9] Core Rule Set, which includes patterns\nfor blocking certain malicious uploads. As with any signature-based blocking, attackers will find ways to evade detection,\n\n6 Microsoft and Windows are registered trademarks of the Microsoft Corporation\n\n7 Linux is a registered trademark of the Linux Foundation\n\n8 McAfee is a registered trademark of McAfee, LLC\n\n9 OWASP is a trademark of the OWASP Foundation\n\n\n-----\n\nso this approach is only one part of a defense-in-depth strategy. Note that IPS and WAF appliances may block the initial\ncompromise but are unlikely to detect web shell traffic.\n\nTo maximize protection, security appliances should be tailored to individual web applications rather than using a single\nsolution across all web servers. For instance, a security appliance configured for an organization’s content management\nsystem can include application specific rules to harden targeted weaknesses that should not apply to other web\napplications. Additionally, security appliances should receive updates to enable real time mitigations for emerging threats.\n\n### Network Segregation\n\nNetwork segregation is a complex architectural challenge that can have significant benefits when done correctly. Network\nsegregation hinders web shell propagation by preventing connections between unrelated network segments. The simplest\nform of network segregation is isolating a demilitarized zone (DMZ) subnet to quarantine internet-facing servers.\nAdvanced forms of network segregation use software-defined networking (SDN) to enable a Zero Trust[10] architecture,\nwhich requires explicit authorization for communication between nodes. While web shells could still affect a targeted\nserver, network segmentation prevents attackers from chaining web shells to reach deeper into an organization’s network.\nFor additional information about network segregation, see Segregate Networks and Functions [7] on nsa.gov.\n\n### Harden Web Servers\n\nSecure configuration of web servers and web applications can prevent web shells and other compromises. Administrators\nshould block access to unused ports or services. Employed services should be restricted to expected clients if possible.\nAdditionally, routine vulnerability scans can help to identify unknown weaknesses in an environment. Some host-based\nsecurity systems provide advanced features, such as machine learning and file reputation, which provide some protection\nagainst web shells. Organizations should take advantage of these advanced security features when possible.\n\n## Mitigating Actions (RESPONSE and RECOVERY)\n\nWhile some web shells do not persist, running entirely from memory, and others exist only as binaries or scripts in a web\ndirectory, still others can be deeply rooted with sophisticated persistence mechanisms. Regardless, they may be part of a\nmuch larger intrusion campaign. A critical focus once a web shell is discovered should be on how far the attacker\npenetrated within the network. Packet capture (PCAP) and network flow data can help to determine if the web shell was\nbeing used to pivot within the network, and to where. If such a pivot is cleaned up without discovering the full extent of the\nintrusion and evicting the attacker, that access may be regained through other channels either immediately or at a later\ntime.\n\n10 Zero Trust is a model where both internal and external resources are treated as potentially malicious and thus each system verifies all access\n\n\n-----\n\n## Appendix A: Scripts to Compare a Production Website to a Known-Good Image\n\nThe scripts below can be used to compare the directory of an active website against a known-good image of that site.\nThis script requires file level access to both the production site and the known-good image, so it should be run on the web\nserver hosting the site or on a connected system that has a mapped drive to the web server. The script should be run with\nsufficient privileges to read the files in both directories. Alternatively, for Windows systems, Microsoft[®] developed the\n[WinDiff utility (available at https://support.microsoft.com/en-us/help/159214/how-to-use-the-windiff-exe-utility), which](https://support.microsoft.com/en-us/help/159214/how-to-use-the-windiff-exe-utility)\nallows directory comparison using a Graphical User Interface (GUI).\n\n**MICROSOFT[®] POWERSHELL[®11]**\n\nUSAGE _.\\dirChecker.ps1 -knownGood <known-good image path> -productionImage <production image path>_\nSCRIPT <#\n.DESCRIPTION\nThe script looks for files changes/additions between a production directory (target) and a known-good directory.\n\n.PARAMETER knownGood\nPath of the known-good directory.\n\n.PARAMETER productionImage\nPath of the production directory (target).\n\n-- Output -File analysis started.\nAny file listed below is a new or changed file.\n\nC:\\inetput\\wwwroot\\index2.aspx\n\nFile analysis completed.\n#>\nparam (\n[Parameter(Mandatory=$TRUE)][ValidateScript({Test-Path $_ -PathType 'Container'})][String] $knownGood,\n[Parameter(Mandatory=$TRUE)][ValidateScript({Test-Path $_ -PathType 'Container'})][String] $productionImage\n)\n\n# Recursevely get all files in both directories, for each file calculate hash.\n$good = Get-ChildItem -Force -Recurse -Path $knownGood | ForEach-Object { Get-FileHash -Path $_.FullName }\n$prod = Get-ChildItem -Force -Recurse -Path $productionImage | ForEach-Object { Get-FileHash -Path $_.FullName }\n\nWrite-Host \"File analysis started.\"\nWrite-Host \"Any file listed below is a new or changed file.`n\"\n\n# Compare files hashes, select new or changed files, and print the path+filename.\n(Compare-Object $good $prod -Property hash -PassThru | Where-Object{$_.SideIndicator -eq '=>'}).Path\n\nWrite-Host \"`nFile analysis completed.\"\n\n**LINUX[®] DIFF UTILITY**\nUSAGE _diff -r -q <known-good image path> <production image path>_\nCMD diff -r -q /path/to/good/image /path/to/production/site\n\n11 PowerShell is a registered trademark of Microsoft Corporation\n\n|USAGE SCRIPT|.\\dirChecker.ps1 -knownGood <known-good image path> -productionImage <production image path> <# .DESCRIPTION The script looks for files changes/additions between a production directory (target) and a known-good directory. .PARAMETER knownGood Path of the known-good directory. .PARAMETER productionImage Path of the production directory (target). -- Output -- File analysis started. Any file listed below is a new or changed file. C:\\inetput\\wwwroot\\index2.aspx File analysis completed. #> param ( [Parameter(Mandatory=$TRUE)][ValidateScript({Test-Path $_ -PathType 'Container'})][String] $knownGood, [Parameter(Mandatory=$TRUE)][ValidateScript({Test-Path $_ -PathType 'Container'})][String] $productionImage ) # Recursevely get all files in both directories, for each file calculate hash. $good = Get-ChildItem -Force -Recurse -Path $knownGood | ForEach-Object { Get-FileHash -Path $_.FullName } $prod = Get-ChildItem -Force -Recurse -Path $productionImage | ForEach-Object { Get-FileHash -Path $_.FullName } Write-Host \"File analysis started.\" Write-Host \"Any file listed below is a new or changed file.`n\" # Compare files hashes, select new or changed files, and print the path+filename. (Compare-Object $good $prod -Property hash -PassThru | Where-Object{$_.SideIndicator -eq '=>'}).Path Write-Host \"`nFile analysis completed.\"|\n|---|---|\n\n|USAGE CMD|diff -r -q <known-good image path> <production image path> diff -r -q /path/to/good/image /path/to/production/site|\n|---|---|\n\n\n-----\n\n## Appendix B: Splunk[®] Queries for Detecting Anomalous URIs in Web Traffic\n\nPrior to having a presence on the network, attackers are unlikely to be able to disguise web shell traffic as typical traffic for\na targeted web server. In these cases, requests to the web shell are likely to have an unusual user agent string. In some\nenvironments, the attacker’s IP address may also appear uncharacteristic for typical network traffic. The queries below\ncan highlight URIs requested by unusual user agents and client IP addresses. Administrators are encouraged to tailor\nthese queries to individual environments including targeting individual web applications or servers rather than running the\nquery for an entire network. In rare cases, certain web applications may generate unique URIs per request which would\nlimit the effectiveness of these queries.\n\n**SPLUNK[®] QUERY TO IDENTIFY URIS ACCESSED** **BY FEW USER AGENTS AND IP ADDRESSES**\n\nRATIONALE _Unlike benign URIs, web shell URIs are likely to have few user agents_\nQUERY sourcetype=\"access_combined”\n(APACHE[®12]) | fillnull value=- ‘comment(“Fill all empty fields with -”)’\n\n| search status>=\"200\" status <\"300\" uri!=- clientip!=- `comment(\"Only successful codes 200-299, eliminate blank\nURIs and client IPs\")`\n| stats min(_time) as start max(_time) as stop dc(useragent) as dc_user_agent values(useragent) as\nvalues_user_agent dc(clientip) as dc_src values(clientip) as values_src count by uri `comment(\"Find first and last\ntime the grouping was found, number of distinct User Agent strings and IP addresses used to access that URI\")`\n| convert ctime(start) ctime(stop) `comment(\"Convert the times to a readable format\")`\n| search dc_src<=5 OR dc_user_agent<=5 `comment(\"Only URIs with <=5 unique user agents or IP addresses\")`\n| table start stop uri dc_user_agent values_user_agent dc_src values_src\n\nQUERY sourcetype=\"iis\"\n(IIS™[13]) | fillnull value=- ‘comment(“Fill all empty fields with -”)’\n\n| search sc_status>=\"200\" sc_status <\"300\" cs_uri_stem!=- c_ip!=- `comment(\"Only successful codes 200-299,\neliminate blank URIs and client IPs\")`\n| stats min(_time) as start max(_time) as stop dc(cs_User_Agent) as dc_user_agent values(cs_User_Agent) as\nvalues_user_agent dc(c_ip) as dc_src values(c_ip) as values_src count by cs_uri_stem `comment(\"Find first and\nlast time the grouping was found, number of distinct User Agent strings and IP addresses used to access that\nURI\")`\n| convert ctime(start) ctime(stop) `comment(\"Convert the times to a readable format\")`\n| search dc_src<=5 OR dc_user_agent<=5 `comment(\"Only URIs with <=5 unique user agents or IP addresses\")`\n| table start stop cs_uri_stem dc_user_agent values_user_agent dc_src values_src\n\n**SPLUNK[®] QUERY TO IDENTIFY USER AGENTS UNCOMMON FOR A TARGET WEB SERVER**\nRATIONALE _Particularly for internal web applications, uncommon user agents can_ _indicate_ _web_ _shell activity_\nQUERY sourcetype=\"access_combined\"\n(APACHE[®]) | fillnull value=- ‘comment(“Fill all empty fields with -”)’\n\n| search status>=\"200\" status <\"300\" ` comment(\"Only successful codes 200-299”)`\n| stats count by useragent `comment(\"Group User Agent strings to determine frequency\")`\n| sort + count `comment(\"Sort count in ascending order\")`\n| head 10 `comment(\"Limit results to top 10. This can be changed to see more or fewer results\")`\n\nQUERY sourcetype=\"iis\" sc_status>=\"200\" AND sc_status<\"300\" ` comment(\"Only successful codes 200-299”)`\n\n(IIS™) | fillnull value=- ‘comment(“Fill all empty fields with -”)’\n\n| search sc_status>=\"200\" sc_status <\"300\" `comment(\"Only successful codes 200-299\")`\n| stats count by cs_User_Agent `comment(\"Group User Agent strings to determine frequency\")`\n| sort + count `comment(\"Sort count in ascending order\")`\n| head 10 `comment(\"Limit results to top 10. This can be changed to see more or fewer results\")`\n\n12 Apache is a registered trademark of the Apache Software Foundation\n\n13 Internet Information Services (IIS) is a trademark of the Microsoft Corporation\n\n|RATIONALE QUERY (APACHE®12) QUERY (IIS™13)|Unlike benign URIs, web shell URIs are likely to have few user agents sourcetype=\"access_combined” | fillnull value=- ‘comment(“Fill all empty fields with -”)’ | search status>=\"200\" status <\"300\" uri!=- clientip!=- `comment(\"Only successful codes 200-299, eliminate blank URIs and client IPs\")` | stats min(_time) as start max(_time) as stop dc(useragent) as dc_user_agent values(useragent) as values_user_agent dc(clientip) as dc_src values(clientip) as values_src count by uri `comment(\"Find first and last time the grouping was found, number of distinct User Agent strings and IP addresses used to access that URI\")` | convert ctime(start) ctime(stop) `comment(\"Convert the times to a readable format\")` | search dc_src<=5 OR dc_user_agent<=5 `comment(\"Only URIs with <=5 unique user agents or IP addresses\")` | table start stop uri dc_user_agent values_user_agent dc_src values_src sourcetype=\"iis\" | fillnull value=- ‘comment(“Fill all empty fields with -”)’ | search sc_status>=\"200\" sc_status <\"300\" cs_uri_stem!=- c_ip!=- `comment(\"Only successful codes 200-299, eliminate blank URIs and client IPs\")` | stats min(_time) as start max(_time) as stop dc(cs_User_Agent) as dc_user_agent values(cs_User_Agent) as values_user_agent dc(c_ip) as dc_src values(c_ip) as values_src count by cs_uri_stem `comment(\"Find first and last time the grouping was found, number of distinct User Agent strings and IP addresses used to access that URI\")` | convert ctime(start) ctime(stop) `comment(\"Convert the times to a readable format\")` | search dc_src<=5 OR dc_user_agent<=5 `comment(\"Only URIs with <=5 unique user agents or IP addresses\")` | table start stop cs_uri_stem dc_user_agent values_user_agent dc_src values_src|\n|---|---|\n\n|RATIONALE QUERY (APACHE®) QUERY (IIS™)|Particularly for internal web applications, uncommon user agents can indicate web shell activity sourcetype=\"access_combined\" | fillnull value=- ‘comment(“Fill all empty fields with -”)’ | search status>=\"200\" status <\"300\" ` comment(\"Only successful codes 200-299”)` | stats count by useragent `comment(\"Group User Agent strings to determine frequency\")` | sort + count `comment(\"Sort count in ascending order\")` | head 10 `comment(\"Limit results to top 10. This can be changed to see more or fewer results\")` sourcetype=\"iis\" sc_status>=\"200\" AND sc_status<\"300\" ` comment(\"Only successful codes 200-299”)` | fillnull value=- ‘comment(“Fill all empty fields with -”)’ | search sc_status>=\"200\" sc_status <\"300\" `comment(\"Only successful codes 200-299\")` | stats count by cs_User_Agent `comment(\"Group User Agent strings to determine frequency\")` | sort + count `comment(\"Sort count in ascending order\")` | head 10 `comment(\"Limit results to top 10. This can be changed to see more or fewer results\")`|\n|---|---|\n\n\n-----\n\n**SPLUNK[®] QUERY TO IDENTIFY URIS WITH AN UNCOMMON HTTP REFERER**\nRATIONALE _Web shell URIs are likely to have uncommon HTTP referers_\nQUERY sourcetype=\"access_combined\"\n(APACHE[®]) | fillnull value=- ‘comment(“Fill all empty fields with - (needed to make blank referer fields searchable)”)’\n\n| search status>=\"200\" status <\"300\" `comment(\"Only successful codes 200-299\")`\n| stats dc(uri) as dc_URIs values(uri) as All_URIs count by referer `comment(\"Counts number of times each URI\nrequest is associated with a unique referer\")`\n| table referer, All_URIs, dc_URIs\n| sort + dc_URIs `comment(\"Sort count in ascending order\")`\n| head 10 `comment(\"Limit results to top 10. This can be changed to see more or fewer results\")`\n\nQUERY sourcetype=\" iis\"\n\n(IIS™) | fillnull value=- ‘comment(“Fill all empty fields with - (needed to make blank referer fields searchable)”)’\n\n| search sc_status>=\"200\" sc_status<\"300\" `comment(\"Only successful codes 200-299\")`\n| stats dc(cs_uri_stem) as dc_URIs values(cs_uri_stem) as All_URIs count by cs_Referer `comment(\"Counts\nnumber of times each URI request is associated with a unique referer\")`\n| table cs_Referer, All_URIs, dc_URIs\n| sort + dc_URIs `comment(\"Sort count in ascending order\")`\n| head 10 `comment(\"Limit results to top 10. This can be changed to see more or fewer results\")`\n\n**SPLUNK[®] QUERY TO IDENTIFY URIS MISSING AN HTTP REFERER**\nRATIONALE _Web shell URIs are likely to have missing HTTP referrers_\nQUERY sourcetype=\"access_combined\"\n(APACHE[®]) | fillnull value=- ‘comment(“Fill all empty fields with - (needed to make blank referer fields searchable)”)’\n\n| search status>=”200” status<”300” referrer=- uri!=”/” `comment(\"Only successful codes 200-299 and blank referrer\nnot from root webpage\")\n| stats count by referer, uri `comment(\"Counts number of times each URI request is associated with a unique\nreferer\")`\n| table uri, count\n| sort - count `comment(\"Sort count in descending order\")`\n| head 10 `comment(\"Limit results to top 10. This can be changed to add more or fewer results\")`\n\nQUERY sourcetype=\"iis\"\n\n(IIS™) | fillnull value=- ‘comment(“Fill all empty fields with - (needed to make blank referer fields searchable)”)’\n\n| search sc_status>=\"200\" sc_status<\"300\" sc_Referer=- cs_uri_stem!=\"/\" `comment(\"Only looking for successful\nstatus codes 200-299 and blank referer not from the root webpage\")`\n| stats count by cs_Referer, cs_uri_stem `comment(\"Counts number of times each URI request is associated with a\nunique referer\")`\n| table cs_uri_stem, count\n| sort - count `comment(\"Sort count in descending order\")`\n| head 10 `comment(\"Limit results to top 10. This can be changed to add more or fewer results\")`\n\n|RATIONALE QUERY (APACHE®) QUERY (IIS™)|Web shell URIs are likely to have uncommon HTTP referers sourcetype=\"access_combined\" | fillnull value=- ‘comment(“Fill all empty fields with - (needed to make blank referer fields searchable)”)’ | search status>=\"200\" status <\"300\" `comment(\"Only successful codes 200-299\")` | stats dc(uri) as dc_URIs values(uri) as All_URIs count by referer `comment(\"Counts number of times each URI request is associated with a unique referer\")` | table referer, All_URIs, dc_URIs | sort + dc_URIs `comment(\"Sort count in ascending order\")` | head 10 `comment(\"Limit results to top 10. This can be changed to see more or fewer results\")` sourcetype=\" iis\" | fillnull value=- ‘comment(“Fill all empty fields with - (needed to make blank referer fields searchable)”)’ | search sc_status>=\"200\" sc_status<\"300\" `comment(\"Only successful codes 200-299\")` | stats dc(cs_uri_stem) as dc_URIs values(cs_uri_stem) as All_URIs count by cs_Referer `comment(\"Counts number of times each URI request is associated with a unique referer\")` | table cs_Referer, All_URIs, dc_URIs | sort + dc_URIs `comment(\"Sort count in ascending order\")` | head 10 `comment(\"Limit results to top 10. This can be changed to see more or fewer results\")`|\n|---|---|\n\n|RATIONALE QUERY (APACHE®) QUERY (IIS™)|Web shell URIs are likely to have missing HTTP referrers sourcetype=\"access_combined\" | fillnull value=- ‘comment(“Fill all empty fields with - (needed to make blank referer fields searchable)”)’ | search status>=”200” status<”300” referrer=- uri!=”/” `comment(\"Only successful codes 200-299 and blank referrer not from root webpage\") | stats count by referer, uri `comment(\"Counts number of times each URI request is associated with a unique referer\")` | table uri, count | sort - count `comment(\"Sort count in descending order\")` | head 10 `comment(\"Limit results to top 10. This can be changed to add more or fewer results\")` sourcetype=\"iis\" | fillnull value=- ‘comment(“Fill all empty fields with - (needed to make blank referer fields searchable)”)’ | search sc_status>=\"200\" sc_status<\"300\" sc_Referer=- cs_uri_stem!=\"/\" `comment(\"Only looking for successful status codes 200-299 and blank referer not from the root webpage\")` | stats count by cs_Referer, cs_uri_stem `comment(\"Counts number of times each URI request is associated with a unique referer\")` | table cs_uri_stem, count | sort - count `comment(\"Sort count in descending order\")` | head 10 `comment(\"Limit results to top 10. This can be changed to add more or fewer results\")`|\n|---|---|\n\n\n-----\n\n## Appendix C: Internet Information Services™ (IIS) Log Analysis Tool\n\nPrior to having a presence on the network, attackers are unlikely to be able to disguise web shell traffic as typical traffic for\na targeted web server. In these cases, requests to the web shell are likely to have an unusual user agent string. In some\nenvironments, the attacker’s IP address may also appear uncharacteristic for typical network traffic. The PowerShell and\nPython scripts below can highlight URIs requested by unusual user agents and client IP addresses. In rare cases, certain\nweb applications may generate unique URIs per request, which would limit the effectiveness of these queries.\n\n**MICROSOFT[®] POWERSHELL[®] SCRIPT TO ANALYZE IIS™ LOGS**\n\n|USAGE SCRIPT|.\\LogCheck.ps1 -logDir <path to IIS log directory> #Default parameters Param ( [ValidateScript({Test-Path $_ -PathType 'Container'})][string]$logDir = \"C:\\inetpub\\logs\\\", [ValidateRange(1,100)][int]$percentile = 5 ) If ($ExecutionContext.SessionState.LanguageMode -eq \"ConstrainedLanguage\") { Throw \"Use Full Language Mode (https://devblogs.microsoft.com/powershell/powershell-constrained-language- mode/)\" } function analyzeLogs ( $field ) { $URIs = @{} $files = Get-ChildItem -Path $logDir -File -Recurse If ($files.Length -eq 0) { \"No log files at the given location `n$($_)\"; Exit } #Parse each file for relevant data. If data not present, continue to next file $files | Foreach-Object { Try { $file = New-Object System.IO.StreamReader -Arg $_.FullName $Cols = @() While ($line = $file.ReadLine()) { If ($line -like \"#F*\") { $Cols = getHeaders($line) } ElseIf ($Cols.Length -gt 0 -and $line -notlike \"#*\" ) { $req = $line | ConvertFrom-Csv -Header $Cols -Delimiter ' ' If ( IrrelevantRequest $req ) { Continue; } #If target field seen for this URI, update our data; otherwise create data object for this URI/field If ($URIs.ContainsKey($req.uri) -and $URIs[ $req.uri ].ContainsKey($req.$field) ) { $URIs[ $req.uri ].Set_Item( $req.$field, $URIs[ $req.uri ][ $req.$field ] + 1 ) } ElseIf ($URIs.ContainsKey($req.uri)) { $URIs[ $req.uri ].Add( $req.$field, 1 ) } Else { $URIs.Add($req.uri, @{ $($req.$field) = 1 }) } } } $file.close() } Catch { Echo \"Unable to parse log file $($_.FullName)`n$($_)\" } } Echo \"These URIs are suspicious because they have the least number of $($field)s requesting them:\" $nth_index = [math]::ceiling( ($URIs.Count) * ([decimal]$percentile / 100)) #Count the unique fields for each URI ForEach ($key in $($uris.keys)) { $uris.Set_Item( $key, $uris.$key.Count) } $i = 0; $URIs.GetEnumerator() | sort Value | Foreach-Object { $i++|\n|---|---|\n\n\n-----\n\nIf($i -gt $nth_index) { Break; }\nEcho “  $($_.Name) is requested by $($_.Value) $($field)(s)\"\n}\n}\n\nFunction getHeaders ( $s ) {\n$s = (($s.TrimEnd()) -replace \"#Fields: \", \"\" -replace \"-\",\"\" -replace \"\\(\",\"\" -replace \"\\)\",\"\")\n$s = $s -replace “scstatus\",\"status\" -replace “csuristem\",\"uri\" -replace “csUserAgent\",\"agent\" -replace “cip\",\"ip\"\nReturn $s.Split(' ')\n}\n\nFunction IrrelevantRequest ( $req ) {\n#Skip requests missing required fields\nForEach ($val in @(“status\", “uri\",\"agent\",\"ip\"))\n{ If ($val -notin $req.PSobject.Properties.Name) { Return $True} }\n#We only care about requests where the server returned success (codes 200-299)\nIf ($req.status -lt 200 -or $req.scstatus -gt 299)\n{ Return $True }\nReturn $False\n}\n\nanalyzeLogs “agent”\nanalyzeLogs “ip”\n\n**PYTHON[®14] SCRIPT TO ANALYZE APACHE[®] LOGS**\nUSAGE _./LogCheck.py <path to Apache log file>_\nCMD import sys\nimport os.path\nimport csv\n# Script will generate a list of URLs from Apache web access log that have least unique IP address or unique user-agents\n# Written for Python 3\n\nurlpercentage = 0.05 # Bottom Percentile of URLs to display\nweblogfileName = None\napachelogsfields = ['ip', 'identd', 'frank', 'time_part0', 'time_part1', 'request', 'status', 'size', 'referer', 'user_agent']\ndef analyze_weblog(filename): # function output the url based on low unique ip address and low unique user-agents\nuniqueurlcount = 0          # count of unique URL in web log\nurls = []               # list of unique URL, also index into lists of lists of unique ip address and user-agents\nuniqueipcount = []          # list of unique ip address count for URL\nuniqueuseragentscount = []      # list of unique use agents for URL\niplist = []              # list of list of ip address per unique URL to keep track of unique URL\nuseragentlist = []          # list of list of user-agents per unique URL to keep track of unique user-agents\n\nprint(\"The weblog file to analyze is %s\" % filename)\nwith open(filename, mode='r') as csv_file:          # read in web log as csv file\ncsv_reader = csv.reader(csv_file, delimiter=' ')\nfor row in csv_reader:\nif (row[0][0] != '#'):           # handles simple case where file has comments start with “# “\nipaddress = row[apachelogsfields.index('ip')]    # ip address\nrequest = row[apachelogsfields.index('request')]   # request (URL part of request)\nstatus = row[apachelogsfields.index('status')]    # user-agent\nuser_agent = row[apachelogsfields.index('user_agent')]\nurl = (request.partition(' ')[2]).partition(' ')[0] # extract URL from request field\nif (status >= '200' and status <= '299'):      # only request with status of 200 - 299\nif (url not in urls):            # determine if URL is already been seen\n\n14 Python is a registered trademark of the Python Software Foundation\n\n|USAGE CMD|./LogCheck.py <path to Apache log file> import sys import os.path import csv # Script will generate a list of URLs from Apache web access log that have least unique IP address or unique user-agents # Written for Python 3 urlpercentage = 0.05 # Bottom Percentile of URLs to display weblogfileName = None apachelogsfields = ['ip', 'identd', 'frank', 'time_part0', 'time_part1', 'request', 'status', 'size', 'referer', 'user_agent'] def analyze_weblog(filename): # function output the url based on low unique ip address and low unique user-agents uniqueurlcount = 0 # count of unique URL in web log urls = [] # list of unique URL, also index into lists of lists of unique ip address and user-agents uniqueipcount = [] # list of unique ip address count for URL uniqueuseragentscount = [] # list of unique use agents for URL iplist = [] # list of list of ip address per unique URL to keep track of unique URL useragentlist = [] # list of list of user-agents per unique URL to keep track of unique user-agents print(\"The weblog file to analyze is %s\" % filename) with open(filename, mode='r') as csv_file: # read in web log as csv file csv_reader = csv.reader(csv_file, delimiter=' ') for row in csv_reader: if (row[0][0] != '#'): # handles simple case where file has comments start with “# “ ipaddress = row[apachelogsfields.index('ip')] # ip address request = row[apachelogsfields.index('request')] # request (URL part of request) status = row[apachelogsfields.index('status')] # user-agent user_agent = row[apachelogsfields.index('user_agent')] url = (request.partition(' ')[2]).partition(' ')[0] # extract URL from request field if (status >= '200' and status <= '299'): # only request with status of 200 - 299 if (url not in urls): # determine if URL is already been seen|\n|---|---|\n\n\n-----\n\nuniqueurlcount += 1           # if not increment unique URL count\nurls.append(url)             # append new URL to the unique URL list\nuniqueipcount.append(0)         # append an element of zero for the unique ip count list\nuniqueuseragentscount.append(0)     # append an element of zero for the unique user-agents count list\nnewiplist = []              # new empty element list for ip address tracking per URL\niplist.append(newiplist)         # append empty list to list of list of ip per URL\nnewuseragentlist = []          # new empty element list for user-agents tracking per URL\nuseragentlist.append(newuseragentlist)  # append empty list to list of user-agents per URL\nif (user_agent not in useragentlist[urls.index(url)]): # determine if user-agents is in the particular URL list\nuseragentlist[urls.index(url)].append(user_agent)  # if not append to user-agents list for the URL list\ntemp = uniqueuseragentscount[urls.index(url)] + 1  # also increment unique user-agents count\nuniqueuseragentscount[urls.index(url)] = temp\nif (ipaddress not in iplist[urls.index(url)]):       # determine if ip address is in the particular URL list\niplist[urls.index(url)].append(ipaddress)        # if not append ip address to list for the particular URL list\ntemp = uniqueipcount[urls.index(url)] + 1        # also increment unique ip address count for that URL\nuniqueipcount[urls.index(url)] = temp\n\nnumberofurltodisplay = urlpercentage * uniqueurlcount    # Determine line that represents percentile desired\nintnumberofurltodisplay = int(numberofurltodisplay)\nif (numberofurltodisplay > intnumberofurltodisplay):    # Round up\nintnumberofurltodisplay += 1\ntempuniqueuseragentscount = uniqueuseragentscount.copy()  # temp copy of unique user-agents count to sort\ntempuniqueuseragentscount.sort()\nuseragentcounttodisplay = tempuniqueuseragentscount[(intnumberofurltodisplay -1)] # determine count to display\ntempuniqueipcount = uniqueipcount.copy()          # Create a temporary copy unique ip address count to sort\ntempuniqueipcount.sort()\nipcounttodisplay = tempuniqueipcount[(intnumberofurltodisplay -1)]        # determine the count to display\n\nprint(--------------------'URL with least user agents-----------------------')\nfor count in range (0, (useragentcounttodisplay + 1)): # Increment unique user-agents\nindex = 0\nfor elementuseragentcount in uniqueuseragentscount:     # Increment thru unique user-agents count list\nif (elementuseragentcount == count):           #  List URL where user-agents is equal to count\nprint(urls[index])\nindex += 1\nprint(--------------------'URL with least user agents-----------------------')\nfor count in range (0, (ipcounttodisplay + 1)):  # Increment count to count of unique ip\nindex = 0\nfor elementipcount in uniqueipcount:            # Increment thru unique ip address count list\nif (elementipcount == count):              #  List URL where user-agents is equal to count\nprint(urls[index])\nindex += 1\n\nif __name__ == '__main__':\ntry:\nif len(sys.argv) == 2:                       # Simple check if an argument is passed (assume weblog file)\nweblogfileName=sys.argv[1]\nprint (\"Web log file to read is %s\" % weblogfileName)\nif(os.path.isfile(weblogfileName)):\nanalyze_weblog(weblogfileName)\nelse:\nprint ('Usage: python3 %s <weblogfile>' % sys.argv[0])     # Print usage statement\nexcept Exception as e:\nprint(\"You must provide a valid filename (path) of a web logfile\")\nraise\n\n\n-----\n\n## Appendix D: Network Signatures of Traffic for Common Web Shells\n\nWeb shell traffic is often obfuscated or encrypted. If organizations have inspection into Transport Layer Security (TLS)\nencrypted sessions for their network, such as via reverse proxy or Web Application Firewall (WAF), then the signatures in\nthe table below may be able to identify network traffic for some common web shells that have not been significantly\nmodified. These fingerprints are subject to change as attackers are likely to alter encoding techniques to evade these\nsignatures. This table is not comprehensive and should be used only as part of a defense-in-depth strategy.\n\n**SNORT[®] RULES TO DETECT COMMON UNMODIFIED WEB SHELL MALWARE**\nRATIONALE _Attackers sometimes use unmodified web shells which can be detected_ _by_ _network sensors_\nRULES # Be sure to put a valid SID in before implementing and test the signature for performance.\n\n# These signatures are targeted at the China Chopper web shell\n# Source: https://www.fireeye.com/blog/threat-research/2013/08/breaking-down-the-china-chopper-web-shell-part-ii.html\nalert tcp any any -> any any (msg: \"China Chopper with first Command Detected\"; flow:to_server,established; content:\n\"FromBase64String\"; content: \"z1\"; content:\"POST\"; nocase;http_method;\nreference:url,http://www.fireeye.com/blog/technical/botnet-activities-research/2013/08/breaking-down-the-china-chopperweb-shell-part-i.html; sid: 90000101;)\nalert tcp any any -> any any (msg: \"China Chopper with all Commands Detected\"; flow:to_server,established; content:\n\"FromBase64String\"; content: \"z\"; pcre: \"/Z\\d{1,3}/i\"; content:\"POST\"; nocase;http_method;\nreference:url,http://www.fireeye.com/blog/technical/botnet-activities-research/2013/08/breaking-down-the-china-chopperweb-shell-part-i.html; sid: 90000102;)\n\n# These signatures are targeted at the C99 web shell\n# Source: https://github.com/jpalanco/alienvault-ossim/blob/master/snort-rules-defaultopen/rules/2.9.2/emerging.rules/emerging-web_server.rules\nalert tcp any any -> any any (msg:\"ET WEB_SERVER c99 Shell Backdoor Var Override URI\"; flow:to_server,established;\ncontent:\"c99shcook[\"; nocase; http_uri; fast_pattern:only; pcre:\"/[&?]c99shcook\\[/Ui\";\nreference:url,thehackerblog.com/every-c99-php-shell-is-backdoored-aka-free-shells/; sid:2018601; rev:1;\nmetadata:created_at 2014_06_24, updated_at 2014_06_24;)\nalert tcp any any -> any any (msg:\"ET WEB_SERVER c99 Shell Backdoor Var Override Cookie\";\nflow:to_server,established; content:\"c99shcook\"; nocase; fast_pattern:only; pcre:\"/c99shcook/Ci\";\nreference:url,thehackerblog.com/every-c99-php-shell-is-backdoored-aka-free-shells/; sid:2018602; rev:1;\nmetadata:created_at 2014_06_24, updated_at 2014_06_24;)\nalert tcp any any -> any any (msg:\"ET WEB_SERVER c99 Shell Backdoor Var Override Client Body\";\nflow:to_server,established; content:\"c99shcook[\"; nocase; fast_pattern:only; http_client_body;\npcre:\"/(?:^|&)c99shcook\\[/Pi\"; reference:url,thehackerblog.com/every-c99-php-shell-is-backdoored-aka-free-shells/;\nsid:2018603; rev:1; metadata:created_at 2014_06_24, updated_at 2014_06_24;)\n\n#These signatures are targeted at the R57 web shell\n# Source: nsa.gov\nalert tcp any any -> any any (msg: \"R57 Web shell Detected\"; content: \"<title>r57 Shell Version \"; rev:1; sid: 90000201;)\n\n#These signatures are targeted at the B374k web shell\n# Source: nsa.gov\nalert tcp any any -> any any (msg: \"B374k Web shell Detected\"; content: \"<title>b374k \"; rev:1; sid: 90000301;)\n\n#These signatures are targeted at the WSO web shell\n# Source: nsa.gov\nalert tcp any any -> any any (msg: \"WSO Web shell Detected\"; content: \"onclick=\\\"g('SelfRemove',null,'','','')\\\">Self\nremove</a> ]\"; rev:1; sid: 90000401;)\n# Source: https://rules.emergingthreatspro.com/9598411999529178/suricata-2.0/rules/web_server.rules\nalert tcp any any -> any any (msg:\"ET WEB_SERVER WSO Web Shell Activity POST structure 2\";\nflow:established,to_server; content:\"POST\"; http_method; content:\" name=|22|c|22|\"; http_client_body;\ncontent:\"name=|22|p1|22|\"; http_client_body; fast_pattern;\npcre:\"/name=(?P<q>[\\x22\\x27])a(?P=q)[^\\r\\n]*\\r\\n[\\r\\n\\s]+(?:S(?:e(?:lfRemove|cInfo)|tringTools|afeMode|ql)|(?:Bruteforc|Co\nnsol)e|FilesMan|Network|Logout|Php)/Pi\"; sid:2016354; rev:2; metadata:created_at 2013_02_05, updated_at\n2013_02_05;)\n\n|RATIONALE RULES|Attackers sometimes use unmodified web shells which can be detected by network sensors # Be sure to put a valid SID in before implementing and test the signature for performance. # These signatures are targeted at the China Chopper web shell # Source: https://www.fireeye.com/blog/threat-research/2013/08/breaking-down-the-china-chopper-web-shell-part-ii.html alert tcp any any -> any any (msg: \"China Chopper with first Command Detected\"; flow:to_server,established; content: \"FromBase64String\"; content: \"z1\"; content:\"POST\"; nocase;http_method; reference:url,http://www.fireeye.com/blog/technical/botnet-activities-research/2013/08/breaking-down-the-china-chopper- web-shell-part-i.html; sid: 90000101;) alert tcp any any -> any any (msg: \"China Chopper with all Commands Detected\"; flow:to_server,established; content: \"FromBase64String\"; content: \"z\"; pcre: \"/Z\\d{1,3}/i\"; content:\"POST\"; nocase;http_method; reference:url,http://www.fireeye.com/blog/technical/botnet-activities-research/2013/08/breaking-down-the-china-chopper- web-shell-part-i.html; sid: 90000102;) # These signatures are targeted at the C99 web shell # Source: https://github.com/jpalanco/alienvault-ossim/blob/master/snort-rules-default- open/rules/2.9.2/emerging.rules/emerging-web_server.rules alert tcp any any -> any any (msg:\"ET WEB_SERVER c99 Shell Backdoor Var Override URI\"; flow:to_server,established; content:\"c99shcook[\"; nocase; http_uri; fast_pattern:only; pcre:\"/[&?]c99shcook\\[/Ui\"; reference:url,thehackerblog.com/every-c99-php-shell-is-backdoored-aka-free-shells/; sid:2018601; rev:1; metadata:created_at 2014_06_24, updated_at 2014_06_24;) alert tcp any any -> any any (msg:\"ET WEB_SERVER c99 Shell Backdoor Var Override Cookie\"; flow:to_server,established; content:\"c99shcook\"; nocase; fast_pattern:only; pcre:\"/c99shcook/Ci\"; reference:url,thehackerblog.com/every-c99-php-shell-is-backdoored-aka-free-shells/; sid:2018602; rev:1; metadata:created_at 2014_06_24, updated_at 2014_06_24;) alert tcp any any -> any any (msg:\"ET WEB_SERVER c99 Shell Backdoor Var Override Client Body\"; flow:to_server,established; content:\"c99shcook[\"; nocase; fast_pattern:only; http_client_body; pcre:\"/(?:^|&)c99shcook\\[/Pi\"; reference:url,thehackerblog.com/every-c99-php-shell-is-backdoored-aka-free-shells/; sid:2018603; rev:1; metadata:created_at 2014_06_24, updated_at 2014_06_24;) #These signatures are targeted at the R57 web shell # Source: nsa.gov alert tcp any any -> any any (msg: \"R57 Web shell Detected\"; content: \"<title>r57 Shell Version \"; rev:1; sid: 90000201;) #These signatures are targeted at the B374k web shell # Source: nsa.gov alert tcp any any -> any any (msg: \"B374k Web shell Detected\"; content: \"<title>b374k \"; rev:1; sid: 90000301;) #These signatures are targeted at the WSO web shell # Source: nsa.gov alert tcp any any -> any any (msg: \"WSO Web shell Detected\"; content: \"onclick=\\\"g('SelfRemove',null,'','','')\\\">Self remove</a> ]\"; rev:1; sid: 90000401;) # Source: https://rules.emergingthreatspro.com/9598411999529178/suricata-2.0/rules/web_server.rules alert tcp any any -> any any (msg:\"ET WEB_SERVER WSO Web Shell Activity POST structure 2\"; flow:established,to_server; content:\"POST\"; http_method; content:\" name=|22|c|22|\"; http_client_body; content:\"name=|22|p1|22|\"; http_client_body; fast_pattern; pcre:\"/name=(?P<q>[\\x22\\x27])a(?P=q)[^\\r\\n]*\\r\\n[\\r\\n\\s]+(?:S(?:e(?:lfRemove|cInfo)|tringTools|afeMode|ql)|(?:Bruteforc|Co nsol)e|FilesMan|Network|Logout|Php)/Pi\"; sid:2016354; rev:2; metadata:created_at 2013_02_05, updated_at 2013_02_05;)|\n|---|---|\n\n\n-----\n\n## Appendix E: Identifying Unexpected Network Flows\n\nThe following Snort[®] rule can aid administrators in identifying unexpected network flows. Identifying unexpected network\nflows requires that administrators maintain an accurate understanding of the expected network architecture. The rule\nbelow is unlikely to be effective without tailoring it for a specific network.\n\n**SNORT[®] RULE TO IDENTIFY UNEXPECTED WEB SERVERS**\n\nUSAGE _Replace “XXX.XXX.XXX.XXX/XX” with a target subnet (e.g., “192.168.1.0/24” ) and add the rule to Snort_\nSCRIPT alert tcp XXX.XXX.XXX.XXX/XX [443,80] -> any any (msg: \"potential unexpected web server\"; sid:4000921)\n\n|USAGE SCRIPT|Replace “XXX.XXX.XXX.XXX/XX” with a target subnet (e.g., “192.168.1.0/24” ) and add the rule to Snort alert tcp XXX.XXX.XXX.XXX/XX [443,80] -> any any (msg: \"potential unexpected web server\"; sid:4000921)|\n|---|---|\n\n\n-----\n\n## Appendix F: Identifying Abnormal Process Invocations in Sysmon Data\n\nMicrosoft[®] Sysmon is a logging tool that enhances logging performed on Windows[®] systems. Among other things,\nSysmon logs information about how each process is created. The information is valuable for identifying anomalous\nbehavior, such as in the case of malicious web shells. Sysmon can be obtained from Microsoft[®] at\n[https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon](https://docs.microsoft.com/en-us/sysinternals/downloads/sysmon) and must be installed on a system in order to begin\nlogging. Ideally, Sysmon and other Windows[®] logging should be mirrored to a central Security Information and Event\nManagement (SIEM) server where it can be aggregated and queried.\n\nThe query below will simply report which executables were launched by an IIS™ web server. In many cases, a web\napplication will cause IIS™ to launch a process for entirely benign functionality. However, there are several executables\ncommonly used by attackers for reconnaissance purposes which are unlikely to be used by a normal web application.\nSome of these executables are listed in the table below. Administrators are encouraged to review the results of the\nPowerShell[®] query below and verify that the web application in question is intended to use the identified executables.\n\n**POWERSHELL[®] SCRIPT TO IDENTIFY ANOMALOUS SYSMON ENTRIES FOR IIS™**\n\nUSAGE _Run the following command from a PowerShell[®] prompt with administrative access_\nSCRIPT Get-WinEvent -FilterHashtable @{logname=\"Microsoft-Windows-Sysmon/Operational\";id=1;} |\nWhere {$_.message -like \"*ParentImage: C:\\Windows\\System32\\inetsrv\\w3wp.exe*\"} |\n%{ $_.properties[4]} |\nSort-Object -Property value -Unique\n\n**Windows[®] environment executables frequently used by attackers and rarely launched by benign IIS™ apps**\n#### arp.exe hostname.exe ntdutil.exe schtasks.exe at.exe ipconfig.exe pathping.exe systeminfo.exe bitsadmin.exe nbtstat.exe ping.exe tasklist.exe certutil.exe net.exe powershell.exe tracert.exe cmd.exe net1.exe qprocess.exe ver.exe dsget.exe netdom.exe query.exe vssadmin.exe dsquery.exe netsh.exe qwinsta.exe wevtutil.exe find.exe netstat.exe reg.exe whoami.exe findstr.exe nltest.exe rundll32.exe wmic.exe fsutil.exe nslookup.exe sc.exe wusa.exe\n\n|USAGE SCRIPT|Run the following command from a PowerShell® prompt with administrative access Get-WinEvent -FilterHashtable @{logname=\"Microsoft-Windows-Sysmon/Operational\";id=1;} | Where {$_.message -like \"*ParentImage: C:\\Windows\\System32\\inetsrv\\w3wp.exe*\"} | %{ $_.properties[4]} | Sort-Object -Property value -Unique|\n|---|---|\n\n|Windows® environment executables frequently used by attackers and rarely launched by benign IIS™ apps|Col2|Col3|Col4|\n|---|---|---|---|\n|arp.exe|hostname.exe|ntdutil.exe|schtasks.exe|\n|at.exe|ipconfig.exe|pathping.exe|systeminfo.exe|\n|bitsadmin.exe|nbtstat.exe|ping.exe|tasklist.exe|\n|certutil.exe|net.exe|powershell.exe|tracert.exe|\n|cmd.exe|net1.exe|qprocess.exe|ver.exe|\n|dsget.exe|netdom.exe|query.exe|vssadmin.exe|\n|dsquery.exe|netsh.exe|qwinsta.exe|wevtutil.exe|\n|find.exe|netstat.exe|reg.exe|whoami.exe|\n|findstr.exe|nltest.exe|rundll32.exe|wmic.exe|\n|fsutil.exe|nslookup.exe|sc.exe|wusa.exe|\n\n\n-----\n\n## Appendix G: Identifying Abnormal Process Invocations with Auditd\n\nAuditd is the userspace component of the Linux[®] Auditing System. Auditd can provide users with insight into process\ncreation logs. The information is valuable for identifying anomalous behavior, such as in the case of malicious web shells.\nAuditd is available in default repositories for many Linux[®] distributions and must be installed and configured to log relevant\nweb server process data. Ideally, auditd and other Linux[®] logging should be mirrored to a central Security Information and\nEvent Management (SIEM) server where it can be aggregated and queried.\n\nThe query below will simply report which applications were launched by an Apache[®] web server. In many cases, a web\napplication will cause Apache[®] to launch a process for entirely benign functionality. However, there are several\napplications commonly used by attackers for reconnaissance purposes which are unlikely to be used by a normal web\napplication. Some of these executables are listed in the table below. Administrators are encouraged to review the results\nand verify that the web application in question is intended to use the identified applications.\n\n**Configuring Auditd**\n\n1. _Determine the web server uid:_\nAfter installing auditd (for example using “apt -y install auditd”), determine the uid of web server using:\napachectl -S\nThis will return apache details including the user id in a line such as:\nUser: name=\"www-data\" id=33\nHere the uid is “33”\n\n2. _Add the following auditd rules (/etc/audit/rules.d/audit.rules) replacing “XX” with the uid identified above:_\n-a always,exit -F arch=b32 -F uid=XX -S execve -k apacheexecve\n-a always,exit -F arch=b64 -F uid=XX -S execve -k apacheexecve\n\n3. _Restart auditd:_\nservice auditd restart\n\n**Review Auditd Log**\n\n1. _Applications launched by Apache[®] can be identified with:_\ncat /var/log/auditd/audit.* | grep \"apacheexecve\"\nThis will return the path to the launched application (see bolded path in the example output below)\n\ntype=SYSCALL msg=audit(1581519503.841:47): arch=c000003e syscall=59 success=yes exit=0\na0=563e412cbbd8 a1=563e412cbb60 a2=563e412cbb78 a3=7f065d5e5810 items=2 ppid=15483 pid=15484\nauid=4294967295 uid=33 gid=33 euid=33 suid=33 fsuid=33 egid=33 sgid=33 fsgid=33 tty=(none)\nses=4294967295 comm=\"cat\" exe=\"/bin/cat\" key=\"apacheexecve\"\n\n2. _Results can be analyzed to determine if unusual applications are launched (see table below)_\n\n3. _Detailed information, including call arguments, can be obtained using:_\ncat /var/log/auditd/audit.* | grep \"msg=audit(1581519503.841:47)\"\nReplace the value of “msg=audit” with the value returned in step 1 above\n\nLinux[®] environment applications frequently used by attackers and rarely launched by benign Apache[®] applications\n#### cat ifconfig ls route crontab ip netstat uname hostname iptables pwd whoami\n\n|Linux® environment applications frequently used by attackers and rarely launched by benign Apache® applications|Col2|Col3|Col4|\n|---|---|---|---|\n|cat|ifconfig|ls|route|\n|crontab|ip|netstat|uname|\n|hostname|iptables|pwd|whoami|\n\n\n-----\n\n## Appendix H: Commonly Exploited Web Application Vulnerabilities\n\nThe list below shows some web application vulnerabilities that are commonly exploited to install web shell malware. This\nlist is not intended to be exhaustive, but it provides insight on some frequently exploited cases. Organizations are\nencouraged to patch both internet-facing and internal web applications rapidly to counter the risks from “n-day”\nvulnerabilities.\n\n**Vulnerability Identifier** **Affected Application** **Reported**\n\n15 May 2019 [8]\nCVE-2019-0604 Microsoft[®] SharePoint[®15]\n\nCitrix[®16] Gateway, Citrix[®] Application Delivery Controller, and 22 Jan 2020 [9]\nCVE-2019-19781\nCitrix[®] SD-WAN WANOP appliance\n\n20 May 2019 [10]\nCVE-2019-3396 Atlassian[®] Confluence[®17] Server\n\nAtlassian[®] Confluence Server and Atlassian[®] Confluence Data 26 Nov 2019 [11]\nCVE-2019-3398\nCenter\n\n22 Apr 2019 [12]\nCVE-2019-9978 WordPress[®18] “Social Warfare” Plugin\n\nCVE-2019-18935\n\n7 Feb 2019 [13]\n\nCVE-2017-11317 Progress[®] Telerik[®19] UI\nCVE-2017-11357\n\n15 July 2019 [14]\nCVE-2019-11580 Atlassian[®] Crowd and Crowd Data Center\n\n6 Mar 2020 [15]\nCVE-2020-10189 Zoho[®] ManageEngine[®20] Desktop Central\n\n18 Feb 2019 [16]\nCVE-2019-8394 Zoho[®] ManageEngine[®] ServiceDesk Plus\n\n10 Mar 2020 [17]\nCVE-2020-0688 Microsoft[®] Exchange[®21] Server\n\n8 Nov 2018 [18]\nCVE-2018-15961 Adobe[®] ColdFusion[®22]\n\n15 SharePoint is a registered trademark of the Microsoft Corporation\n\n16 Citrix is a registered trademark of Citrix Systems, Inc.\n\n17 Atlassian and Confluence are registered trademarks of Atlassian Pty Ltd.\n\n18 WordPress is a registered trademark of the WordPress Foundation\n\n19 Progress and Telerik are registered trademarks of Progress Software EAD\n\n20 Zoho and ManageEngine are registered trademarks of ZOHO Corporation\n\n21 Exchange is a registered trademark of the Microsoft Corporation\n\n22 Adobe and ColdFusion are registered trademarks of Adobe Systems Incorporated\n\n|Vulnerability Identifier|Affected Application|Reported|\n|---|---|---|\n|CVE-2019-0604|Microsoft® SharePoint®15|15 May 2019 [8]|\n|CVE-2019-19781|Citrix®16 Gateway, Citrix® Application Delivery Controller, and Citrix® SD-WAN WANOP appliance|22 Jan 2020 [9]|\n|CVE-2019-3396|Atlassian® Confluence®17 Server|20 May 2019 [10]|\n|CVE-2019-3398|Atlassian® Confluence Server and Atlassian® Confluence Data Center|26 Nov 2019 [11]|\n|CVE-2019-9978|WordPress®18 “Social Warfare” Plugin|22 Apr 2019 [12]|\n|CVE-2019-18935 CVE-2017-11317 CVE-2017-11357|Progress® Telerik®19 UI|7 Feb 2019 [13]|\n|CVE-2019-11580|Atlassian® Crowd and Crowd Data Center|15 July 2019 [14]|\n|CVE-2020-10189|Zoho® ManageEngine®20 Desktop Central|6 Mar 2020 [15]|\n|CVE-2019-8394|Zoho® ManageEngine® ServiceDesk Plus|18 Feb 2019 [16]|\n|CVE-2020-0688|Microsoft® Exchange®21 Server|10 Mar 2020 [17]|\n|CVE-2018-15961|Adobe® ColdFusion®22|8 Nov 2018 [18]|\n\n\n-----\n\n## Appendix I: HIPS Rules for Blocking Changes to Web Accessible Directories\n\nMcAfee[®] HBSS allows specification of custom HIPS rules, which are then enforced by endpoint McAfee[®] agents. These\nrules can be used to block file creation and file changes to web accessible directories effectively neutering the primary\ninfection vector for web shell malware. If necessary, these rules can be temporarily disabled during site updates or web\napplication patches. As with any new HIPS rule, administrators should begin enforcement at Level 1 (informational) in\norder to identify potential conflicts with existing applications. Enforcement should be raised to Level 4 (high) once impact\nassessment is deemed acceptable.\n\n**WINDOWS[®] ENVIRONMENT**\n\nUSAGE _Replace “C:\\\\inetpub\\\\wwwroot\\\\*”_ _with the target directory path (i.e., the web directory)_\nRULE Rule {\n\nTag “Blocking Changes to Web Directory (Windows)”\nClass Files\nID -1 # this will select the next free ID number in the 4XXX series\nLevel 1\nfiles { Include “C:\\\\inetpub\\\\wwwroot\\\\*” }\ndirectives files:rename files:permissions files:create files:write\n}\n\n**LINUX[®] ENVIRONMENT**\n\nUSAGE _Replace “/var/www/html/*” with_ _the target directory path (i.e., the web directory)_\nRULE Rule {\n\nTag “Blocking Changes to Web Directory (Linux)”\nClass UNIX_file\nID -1 # this will select the next free ID number in the 4XXX series\nLevel 1\nfiles { Include “/var/www/html/*” }\ndirectives unixfile:symlink unixfile:create unixfile:mkdir unixfile:write\n}\n\n**Tuning Signatures:**\nSignatures should be tuned according to the operating environment. If there are any exemptions (e.g., web application\nuploads) an exception can be created by clicking on the HIPS custom signature under Policy Catalog, Host Intrusion\nPrevention IPS/IPS rules and selecting the name of the IPS signature usually under “My Default”.\n\nOnce on the signature page, the signature can be found by typing in the Search box key words including the name of the\nsignature. Once the signature is displayed, the check box to the left of it should be selected and the Exception Rule tab\ncan be clicked to add a Parameter for a specific file type (e.g., *.pdf) that should be allowed.\n\n|USAGE RULE|Replace “C:\\\\inetpub\\\\wwwroot\\\\*” with the target directory path (i.e., the web directory) Rule { Tag “Blocking Changes to Web Directory (Windows)” Class Files ID -1 # this will select the next free ID number in the 4XXX series Level 1 files { Include “C:\\\\inetpub\\\\wwwroot\\\\*” } directives files:rename files:permissions files:create files:write }|\n|---|---|\n\n|USAGE RULE|Replace “/var/www/html/*” with the target directory path (i.e., the web directory) Rule { Tag “Blocking Changes to Web Directory (Linux)” Class UNIX_file ID -1 # this will select the next free ID number in the 4XXX series Level 1 files { Include “/var/www/html/*” } directives unixfile:symlink unixfile:create unixfile:mkdir unixfile:write }|\n|---|---|\n\n\n-----\n\n## Works Cited\n\n[1] Microsoft Detection and Response Team (2020), Ghost in the shell: Investigating web shell attacks. [Online] Available at:\n[https://microsoft.com/security/blog/2020/02/04/ghost-in-the-shell-investigating-web-shell-attacks/](https://microsoft.com/security/blog/2020/02/04/ghost-in-the-shell-investigating-web-shell-attacks/) [Accessed Apr. 6, 2020]\n\n[2] Rascagneres, P. and Svajcer, V. (2019). China Chopper still active 9 years later. [Online] Available at:\n[https://blog.talosintelligence.com/2019/08/china-chopper-still-active-9-years-later.html](https://blog.talosintelligence.com/2019/08/china-chopper-still-active-9-years-later.html) [Accessed Apr. 6, 2020]\n\n[3] [CISA (2017), Alert TA15-314A. [Online] Available at: https://www.us-cert.gov/ncas/alerts/TA15-314A [Accessed Apr. 6, 2020]](https://www.us-cert.gov/ncas/alerts/TA15-314A)\n\n[4] [CISA (2018), Alert AA18-284A. [Online] Available at: https://www.us-cert.gov/ncas/alerts/AA18-284A](https://www.us-cert.gov/ncas/alerts/AA18-284A) [Accessed Apr. 6, 2020]\n\n[5] [ACSC (2015), Web Shells – Threat Awareness and Guidance. [Online] Available at https://cyber.gov.au/sites/default/files/2019-](https://cyber.gov.au/sites/default/files/2019-03/ACSC_Web_Shells.pdf)\n[03/ACSC_Web_Shells.pdf](https://cyber.gov.au/sites/default/files/2019-03/ACSC_Web_Shells.pdf) [Accessed Apr. 6, 2020]\n\n[6] [Dumont, R. (2017), MITRE ATT&CK Framework - Timestomp. [Online] Available at https://attack.mitre.org/techniques/T1099/](https://attack.mitre.org/techniques/T1099/) [Accessed Apr.\n6, 2020]\n\n[7] [NSA (2016), Segregate Networks and Functions. [Online] Available at https://apps.nsa.gov/iaarchive/library/ia-guidance/security-](https://apps.nsa.gov/iaarchive/library/ia-guidance/security-tips/segregate-networks-and-functions.cfm)\n[tips/segregate-networks-and-functions.cfm](https://apps.nsa.gov/iaarchive/library/ia-guidance/security-tips/segregate-networks-and-functions.cfm) [Accessed Apr. 6, 2020]\n\n[8] TrendMicro (2019), Security Alert: China Chopper Malware targeting vulnerable SharePoint servers. [Online] Available at\n[https://success.trendmicro.com/solution/000131747](https://success.trendmicro.com/solution/000131747) [Accessed Apr. 6, 2020]\n\n[9] Ballenthin et al. (2020), FireEye and Citrix Tool Scans for Indicators of Compromise Related to CVE-2019-19781. [Online] Available at\n[https://www.fireye.com/blog/products-and-services/2020/01/fireeye-and-citrix-tool-scans-for-iocs-related-to-vulnerability.html](https://www.fireye.com/blog/products-and-services/2020/01/fireeye-and-citrix-tool-scans-for-iocs-related-to-vulnerability.html) [Accessed Apr. 6,\n2020]\n\n[[10] Capuano, E. (2019), Analysis of Exploitation: CVE-2019-3396. [Online] Available at https://blog.reconinfosec.com/analysis-of-exploitation-of-](https://blog.reconinfosec.com/analysis-of-exploitation-of-cve-2019-3396/)\n\n[cve-2019-3396/](https://blog.reconinfosec.com/analysis-of-exploitation-of-cve-2019-3396/) [Accessed Apr. 6, 2020]\n\n[11] Joshi, A. (2019), CVE-2019-3398: Atlassian Confluence Download Attachments Remote Code Execution. [Online] Available at\n\n[https://blogs.juniper.net/en-us/threat-research/cve-2019-3398-atlassian-confluence-download-attachments-remote-code-execution](https://blogs.juniper.net/en-us/threat-research/cve-2019-3398-atlassian-confluence-download-attachments-remote-code-execution) [Accessed\nApr. 6, 2020]\n\n[12] Deng, Zhang, and Gao. (2019), Exploits in the Wild for WordPress Social Warfare Plugin CVE-2019-9978. [Online] Available at\n\n[https://unit42.paloaltonetworks.com/exploits-in-the-wild-for-wordpress-social-warfare-plugin-cve-2019-9978](https://unit42.paloaltonetworks.com/exploits-in-the-wild-for-wordpress-social-warfare-plugin-cve-2019-9978) [Accessed Apr. 6, 2020]\n\n[[13] Wulftange, M. (2019), Telerik Revisited. [Online] Available at https://codewhitesec.blogspot.com/2019/02/telerik-revisited.html](https://codewhitesec.blogspot.com/2019/02/telerik-revisited.html) [Accessed Apr.\n\n6, 2020]\n\n[14] Narang, S. (2019), CVE-2019-11580: Proof-of-Concept for Critical Atlassian Crowd Remote Code Execution Vulnerability Now Available.\n\n[[Online] Available at https://tenable.com/blog/cve-2019-11580-proof-of-concept-for-critical-atlassian-crowd-remote-code-execution](https://tenable.com/blog/cve-2019-11580-proof-of-concept-for-critical-atlassian-crowd-remote-code-execution) [Accessed\nApr. 6, 2020]\n\n[15] ManageEngine (2020), Identification and mitigation of remote code execution vulnerability CVE-2020-10189. [Online] Available at\n\n[https://manageengine.com/products/desktop-central/rce-vulnerability-cve-2020-10189.html](https://manageengine.com/products/desktop-central/rce-vulnerability-cve-2020-10189.html) [Accessed Apr. 6, 2020]\n\n[[16] CISA (2019), Bulletin SB19-056. [Online] Available at https://us-cert.gov/ncas/bulletins/SB19-056](https://us-cert.gov/ncas/bulletins/SB19-056) [Accessed Apr. 6, 2020]\n\n[[17] CISA (2020), Unpatched Microsoft Exchange Servers Vulnerable to CVE-2020-0688. [Online] Available at https://us-cert.gov/ncas/current-](https://us-cert.gov/ncas/current-activity/2020/03/10/unpatched-microsoft-exchange-servers-vulnerable-cve-2020-0688)\n\n[activity/2020/03/10/unpatched-microsoft-exchange-servers-vulnerable-cve-2020-0688](https://us-cert.gov/ncas/current-activity/2020/03/10/unpatched-microsoft-exchange-servers-vulnerable-cve-2020-0688) [Accessed Apr. 6, 2020]\n\n[18] Volexity Threat Research (2018), Active Exploitation of Newly Patched ColdFusion Vulnerability. [Online] Available at\n\n[https://volexity.com/blog/2018/11/08/active-exploitation-of-newly-patched-coldfusion-vulnerability-cve-2018-15961/](https://volexity.com/blog/2018/11/08/active-exploitation-of-newly-patched-coldfusion-vulnerability-cve-2018-15961/) [Accessed Apr. 6, 2020]\n\n### Disclaimer of Endorsement\n\nThe information and opinions contained in this document are provided \"as is\" and without any warranties or guarantees. Reference herein to any specific\ncommercial products, process, or service by trade name, trademark, manufacturer, or otherwise, does not constitute or imply its endorsement,\nrecommendation, or favoring by the United States Government, and this guidance shall not be used for advertising or product endorsement purposes.\n\n### Contact\n\n[NSA Client Requirements / General Cybersecurity Inquiries: Cybersecurity Requirements Center, 410-854-4200, Cybersecurity_Requests@nsa.gov](mailto:Cybersecurity_Requests@nsa.gov)\n[NSA Media Inquiries / Press Desk: 443-634-0721, MediaRelations@nsa.gov](mailto:MediaRelations@nsa.gov)\n[ASD Australian Cyber Security Centre / General Cybersecurity or Media Enquiries: asd.assist@defence.gov.au](mailto:asd.assist@defence.gov.au) or visit www.cyber.gov.au.\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "99fdc3ef-333d-48f5-a4a1-becd788c7b80",
            "created_at": "2022-10-25T15:28:29.802983Z",
            "updated_at": "2022-10-25T15:28:29.802983Z",
            "deleted_at": null,
            "name": "MITRE",
            "url": "https://github.com/mitre-attack/attack-stix-data",
            "description": "MITRE ATT&CK STIX Data",
            "reports": null
        },
        {
            "id": "5910e58f-4bc9-4e4e-9dbb-887804094a01",
            "created_at": "2022-10-25T16:32:58.457399Z",
            "updated_at": "2022-10-25T16:32:58.457399Z",
            "deleted_at": null,
            "name": "OTX",
            "url": "https://otx.alienvault.com",
            "description": "Alienvault Open Threat Exchange (OTX)",
            "reports": null
        }
    ],
    "references": [
        "https://media.defense.gov/2020/Jun/09/2002313081/-1/-1/0/CSI-DETECT-AND-PREVENT-WEB-SHELL-MALWARE-20200422.PDF"
    ],
    "report_names": [
        "CSI-DETECT-AND-PREVENT-WEB-SHELL-MALWARE-20200422.PDF"
    ],
    "threat_actors": [
        {
            "id": "dabb6779-f72e-40ca-90b7-1810ef08654d",
            "created_at": "2022-10-25T15:50:23.463113Z",
            "updated_at": "2025-03-27T02:00:55.47619Z",
            "deleted_at": null,
            "main_name": "APT1",
            "aliases": [
                "APT1",
                "Comment Crew",
                "Comment Group",
                "Comment Panda"
            ],
            "source_name": "MITRE:APT1",
            "tools": [
                "Seasalt",
                "ipconfig",
                "Cachedump",
                "PsExec",
                "GLOOXMAIL",
                "Lslsass",
                "PoisonIvy",
                "WEBC2",
                "Mimikatz",
                "gsecdump",
                "Pass-The-Hash Toolkit",
                "Tasklist",
                "xCmd",
                "pwdump"
            ],
            "source_id": "MITRE",
            "reports": null
        },
        {
            "id": "cf7fc640-acfe-41c4-9f3d-5515d53a3ffb",
            "created_at": "2023-01-06T13:46:38.228042Z",
            "updated_at": "2025-03-27T02:00:02.775905Z",
            "deleted_at": null,
            "main_name": "APT1",
            "aliases": [
                "GIF89a",
                "G0006",
                "PLA Unit 61398",
                "Group 3",
                "TG-8223",
                "Comment Group",
                "ShadyRAT",
                "COMMENT PANDA",
                "Comment Crew",
                "Byzantine Candor",
                "Brown Fox"
            ],
            "source_name": "MISPGALAXY:APT1",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        }
    ],
    "ts_created_at": 1666716499,
    "ts_updated_at": 1743041483,
    "ts_creation_date": 1587486992,
    "ts_modification_date": 1587486998,
    "files": {
        "pdf": "https://archive.orkl.eu/37fa6061b337131904e4ab9bb14edd3281adf083.pdf",
        "text": "https://archive.orkl.eu/37fa6061b337131904e4ab9bb14edd3281adf083.txt",
        "img": "https://archive.orkl.eu/37fa6061b337131904e4ab9bb14edd3281adf083.jpg"
    }
}