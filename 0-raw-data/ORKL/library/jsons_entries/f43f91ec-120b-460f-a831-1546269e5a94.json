{
    "id": "f43f91ec-120b-460f-a831-1546269e5a94",
    "created_at": "2022-10-25T16:48:19.376515Z",
    "updated_at": "2025-03-27T02:11:34.436215Z",
    "deleted_at": null,
    "sha1_hash": "fe2f8d32688a104ca4e6ba595f647dfa479ece44",
    "title": "",
    "authors": "",
    "file_creation_date": "2014-07-10T01:10:52Z",
    "file_modification_date": "0001-01-01T00:00:00Z",
    "file_size": 283863,
    "plain_text": "# A Look at Targeted Attacks Through the Lense of an NGO\n\n## Stevens Le Blond[1] Adina Uritesc[1] C´edric Gilbert[1]\n Zheng Leong Chua[2] Prateek Saxena[2] Engin Kirda[3]\n\n1MPI-SWS 2National Univ. of Singapore 3Northeastern Univ.\n\n\n## Abstract\n\nWe present an empirical analysis of targeted attacks\nagainst a human-rights Non-Governmental Organization\n(NGO) representing a minority living in China. In particular, we analyze the social engineering techniques, attack vectors, and malware employed in malicious emails\nreceived by two members of the NGO over a four-year\nperiod. We find that both the language and topic of\nthe emails were highly tailored to the victims, and that\nsender impersonation was commonly used to lure them\ninto opening malicious attachments. We also show that\nthe majority of attacks employed malicious documents\nwith recent but disclosed vulnerabilities that tend to\nevade common defenses. Finally, we find that the NGO\nreceived malware from different families and that over a\nquarter of the malware can be linked to entities that have\nbeen reported to engage in targeted attacks against political and industrial organizations, and Tibetan NGOs.\n\n## 1 Introduction\nIn the last few years, a new class of cyber attacks has\nemerged that is more targeted at individuals and organizations. Unlike their opportunistic, large-scale counterparts, targeted attacks aim to compromise a handful of\nspecific, high-value victims. These attacks have received\nsubstantial media attention, and have successfully compromised a wide range of targets including critical national infrastructures [19], Fortune 500 companies [23],\nnews agencies [20], and political dissidents [10,11,16].\nDespite the high stakes involved in these attacks, the\necosystem sustaining them remains poorly understood.\nThe main reason for this lack of understanding is that victims rarely share the details of a high-profile compromise\nwith the public, and they typically do not disclose what\nsensitive information has been lost to the attackers. According to folk wisdom, attackers carrying out targeted\nattacks are generally thought to be state-sponsored. Examples of national organizations that have been reported\nto be engaged in targeted attacks include the NSA’s of\n\nfice of Tailored Access Operations (TAO) [3] and the\nPeople’s Liberation Army’s Unit 61398 [15]. Recently,\nresearchers also attributed attacks in the Middle East to\nthe governments of Bahrain, Syria, and the United Arab\nEmirates [16].\nThere now exists public evidence that virtually every\ncomputer system connected to the internet is susceptible\nto targeted attacks. The Stuxnet attack even successfully\ncompromised air-gapped Iranian power plants [19] and\nwas able to damage the centrifuges in the facility. More\nrecently, Google, Facebook, the New York Times, and\nmany other global companies have been compromised\nby targeted attacks. Furthermore, political dissidents and\nNon-Governmental Organizations (NGOs) are also being\ntargeted [10,11,16].\nIn this paper, we analyze 1,493 suspicious emails collected over a four-year period by two members of the\nWorld Uyghur Congress (WUC), an NGO representing\nan ethnic group of over ten million individuals mainly\nliving in China. WUC volunteers who suspected that\nthey were being specifically targeted by malware shared\nthe suspicious emails that they received with us for analysis. We find that these emails contain 1,176 malicious\nattachments and target 724 unique email addresses belonging to individuals affiliated with 108 different organizations. This result indicates that, despite their targeted\ncontent, these attacks were sent to several related victims\n(e.g., via Cc). Although the majority of these targeted organizations were NGOs, they also comprised a few highprofile targets such as the New York Times and US embassies.\nWe leverage this dataset to perform an empirical analysis of targeted attacks in the wild. First, we analyze\nthe engineering techniques and find that the language\nand topic of the malicious emails were tailored to the\nmother tongue and level of specialization of the victims.\nWe also find that sender impersonation was common and\nthat some attacks in our dataset originated from compromised email accounts belonging to high-profile ac\n\n-----\n\ntivists. Second, whereas recent studies report that malicious archives and executables represented the majority\nof the targeted-attack threat [15, 22], we find that malicious documents were the most common attack vector in\nour dataset. Although we do not find evidence of zeroday vulnerabilities, we observe that most attacks used recent vulnerabilities, that exploits were quickly replaced\nto adapt to new defense mechanisms, and that they often bypassed common defenses. Third, we perform an\nanalysis of the first-stage malware delivered over these\nmalicious emails and find that WUC has been targeted\nwith different families of malware over the last year. We\nfind that over a quarter of these malware samples exhibited similarities with those used by entities reported to\nhave carried out targeted attacks.\nOur work complements existing reports on targeted attacks such as GhostNet, Mandiant, and Symantec Internet Security Threat (ISTR) 2013 [11, 15, 22]. Whereas\nthe GhostNet and Mandiant reports focus on the attack\nlifecycle after the initial compromise, this study provides\nan in-depth analysis of the reconnaissance performed be_fore the compromise. We note that both approaches have_\npros and cons and are complementary: While it is hard\nfor the authors of these reports to know how a system became compromised in retrospect, it is equally hard for us\nto know if the observed attacks will compromise the targeted system(s). Finally, whereas ISTR provides some\nnumbers about reconnaissance analysis for industrialespionage attacks [22], we present a thorough and rigorous analysis of the attacks in our dataset.\nFinally, to foster research in this area, we release our\ndataset of targeted malware to the community [4].\n**Scope. Measuring real-world targeted attacks is chal-**\nlenging and this paper has a number of important biases. First, our dataset contains mainly attacks against\nthe Uyghur and human-rights communities. While the\nspecifics of the social engineering techniques (e.g., use\nof Uyghur language) will vary from one targeted community to another, we argue that identifying commonly\nused techniques (e.g., topic, language, senders’ impersonation) and their purpose is a necessary step towards\ndesigning effective defenses. Another limitation of our\ndataset is that it captures only targeted attacks carried out\nover email channels and that were detected by our volunteers. Although malicious emails seem to constitute\nthe majority of targeted attacks, different attack vectors\nsuch as targeted drive-by downloads are equally important. Finally, we reiterate that the goal of this study is to\nunderstand the reconnaissance phase occurring before a\ncompromise. Analyzing second-stage malware, monitoring compromised systems, and determining the purpose\nof targeted attacks are all outside of the scope of this paper and are the topic of recent related work [10,16]. We\ndiscuss open research challenges in Section 6.\n\n\nFigure 1: Screenshot of a malicious email with an impersonated sender, and a malicious document exploiting Common Vulnerabilities and Exposures (CVE) number 2012-0158 and containing malware. The email re**plays an actual announcement about a conference in**\n**Geneva and was edited by the attacker to add that all**\n**fees would be covered.**\n\n## 2 Overview\n\n**Context. WUC, the NGO from which we have received**\nour dataset, represents the Uyghurs, an ethnic minority\nconcentrated in the Xinjiang region in China. Xinjiang\nis the largest Chinese administrative division, has abundant natural resources such as oil, and is China’s largest\nnatural gas-producing region. WUC frequently engages\nin advocacy and meeting with politicians and diplomats\nat the EU and UN, as well as collaborating with a variety\nof NGOs. Rebiya Kadeer, WUC’s current president, was\nthe fifth richest person in China before her imprisonment\nfor dissent in 1996, and is now in exile in the US. Finally, WUC is partly funded by the National Endowment\nfor Democracy (NED), a US NGO itself funded by the\nUS Congress to promote democracy. (We will see below\nthat NED has been targeted with the same malware as\nWUC.)\nWUC has been a regular target of Distributed Denial of Service (DDoS) attacks and telephone disruptions, as well as targeted attacks. For example, the\nWUC’s website became inaccessible from June 28 to\nJuly 10, 2011 due to such a DDoS attack. Concurrently\nto this attack, the professional and private phone lines of\nWUC employees were flooded with incoming calls, and\nthe WUC’s contact email address received 15,000 spam\nemails in one week.\n**Data acquisition.** In addition to these intermittent\nthreats, WUC employees constantly receive suspicious\nemails impersonating their colleagues and containing\n\n\n-----\n\nmalicious links and attachments. These emails consistently evade spam and malware defenses deployed by\nwebmail providers and are often relevant to WUC’s activities. In fact, our volunteers claim that the emails are\noften so targeted that they need to confirm their legitimacy with the impersonated sender in person. For example, Figure 1 shows the screenshot of such an email\nthat replays the actual announcement for a conference in\nGeneva organized by WUC. As a result, WUC members\nare wary of any emails containing links or attachments,\nand some of them save these emails for future inspection. We came in contact with two WUC employees who\nshared the suspicious emails that they had received (with\nconsent from WUC). The authors of this work were not\ninvolved in the data collection.\n**Characteristics of the dataset.** The two volunteers\nshared with us the headers and content of 1,493 suspicious emails that they received over a four-year period.\n1,178 (79%) of these emails were sent to the private\nemail addresses of the two NGO employees from whom\nwe obtained the data, 16 via the public email address of\nthe WUC, and the remaining 299 emails were forwarded\nto them (126 of these by colleagues at WUC). Overall,\n89% of these emails were received directly by our volunteers or their colleagues at WUC. As we will see below,\nthey also contain numerous email addresses in the To and\nCc fields belonging to individuals that are not affiliated\nwith WUC.\nThe emails contained 209 links and 1,649 attachments,\nincluding 1,176 with malware (247 RAR, 49 ZIP, 144\nPDF, and 655 Microsoft Office files, and 81 files in other\nformats). Our analysis revealed 1,116 malicious emails\ncontaining malware attachments. (We were not able to\nverify the maliciousness of the links as most of them\nwere invalid by the time we obtained the data.) In the following, we analyze malicious emails exclusively and we\nrefer to malicious archives or documents depending on\nwhether they contained RAR or ZIP, PDF or Microsoft\nOffice documents, respectively. Finally, the volunteers\nlabeled the data wherever necessary, enabling us, for example, to establish that the sender of the emails was impersonated for 84% of the emails. Table 1 summarizes\nthe main characteristics of these malicious emails.\n**Scope of the dataset. Analyzing the headers of the ma-**\nlicious emails revealed a surprisingly large number of recipients in the To or Cc fields. In particular, we observed\nthat malicious emails had been sent to 1,250 unique\nemail addresses and 157 organizations. A potential explanation for this behavior could be that the attacker tampered with the email headers (e.g., via a compromised\nSMTP server) as part of social engineering so these\nemails were only delivered to our volunteers, despite\nthe additional indicated recipients. To test this hypothesis, we considered only those emails received directly\n\n\nby our volunteers, originating from well-known webmail\ndomains (i.e., aol.com, gmx.de, gmx.com, gmail.com,\ngooglemail.com, hotmail.com, outlook.com, and yahoo.com), and verified via Sender Policy Framework\n(SPF) and DomainKeys Identified Mail (DKIM). SPF\nand DKIM are methods commonly used to authenticate\nthe sending server of an email message. By verifying\nthat these malicious emails originated from well-known\nwebmail servers, we obtain 568 malicious emails whose\nheaders are very unlikely to have been tampered with by\nthe attacker. By repeating our above analysis on these\nemails only, we obtain 724 unique email addresses and\n108 organizations. Other organizations besides WUC\ninclude NED (WUC’s main source of funding and itself funded by the US congress), the New York Times,\nand US embassies. In summary, while we obtained our\ndataset from two volunteers working for a single organization, it offers substantial coverage not only of one\nNGO, but also of those attacks against multiple NGOs in\nwhich attackers target more than one organization with\nthe same email. We show the full list of organizations\ntargeted in our dataset in Appendix A.\n\n**What are targeted attacks? There is no precise defini-**\ntion of targeted attacks. In this paper, we loosely define\nthese attacks as low-volume, socially engineered communication which entices specific victims into installing\nmalware. In the dataset we analyze here, the communication is by email, and the mechanism of exploitation is\nprimarily using malicious archives or documents. A targeted victim, in this work, refers to specific individuals,\nor an organization as a whole. When necessary, we also\nuse the term volunteer(s) to distinguish between our two\ncollaborators and other victims.\n\nThe terms targeted attacks and Advanced Persistent\nThreats (or APTs) are often used interchangeably. As\nthis paper focuses on the reconnaissance phase of targeted attacks (occurring before a compromise), we cannot measure how long attackers would have remained in\ncontrol of the targeted systems (i.e., their persistency).\nAs a result, we simply refer to these attacks as targeted\nattacks, and not APTs, throughout the rest of this paper. We discuss specific social engineering characteristics that make targeted attacks difficult to detect by unsuspecting average users in Section 3, the attack vectors\nused in our dataset in Section 4, and the malware families they install in Section 5. Finally, we will discuss\nopen research challenges in Section 6.\n\n**Ethics. The dataset was collected prior to our contact-**\ning WUC and for the purpose of future security analysis.\nFurthermore, WUC approved the disclosure of all the information contained in this paper and requested that the\norganization’s name not be anonymized.\n\n\n-----\n\nTable 1: Summary of our dataset originating from two volunteers. Malicious indicates the fraction of emails containing\nmalware, Impersonated the fraction of emails with an impersonated sender, # recipients and # orgs the number of\nunique email addresses that were listed in the To and Cc fields of the malicious emails and the corresponding number\nof organizations, respectively.\n\n_Beginning - end_ _Size_ _Malicious_ _Impersonated_ _# recipients_ _# orgs_\n_1st volunteer_ Sept 2012 - Sept 2013 98 MB 154/241 (64%) 141/154 (92%) 124 25\n_2nd volunteer_ Sept 2009 - Jul 2013 818 MB 962/1,252 (77%) 802/962 (83%) 666 102\n_Total_ Sept 2009 - Sept 2013 916 MB 1,116/1,493 (75%) 943/1,116 (84%) 724 108\n\n\n## 3 Analysis of social engineering\n\nThe GhostNet, Mandiant, ISTR, and other reports [11,\n15,22] mention the use of socially-engineered emails to\nlure their victims into installing malware, clicking on\nmalicious links, or opening malicious documents. For\nexample, the GhostNet report refers to one spoofed email\ncontaining a malicious DOC attachment, and the Mandiant report to one email sent from a webmail account\nbearing the name of the company’s CEO enticing several\nemployees to open malware contained in a ZIP archive.\nConcurrent work reports the use of careful social engineering against civilians and NGOs in the Middle East\n\n[16] and also Tibetan and human-rights NGOs [10]. Despite this anecdotal evidence, we are not aware of any\nrigorous and thorough analysis of the social engineering\ntechniques employed in targeted attacks. In this section,\nwe seek to answer the following questions in the context\nof our dataset:\n\n_• What social traits of victims are generally ex-_\n_ploited?_ Do attackers generally impersonate a\nsender known to the victim and if so who do they\nchoose to impersonate?\n\n_• Who are the victims?_ Are malicious emails sent\nonly to specific individuals, to entire organizations,\nor communities of users?\n\n_• When are users being targeted?_ When do users\nstart being targeted? Are the same users frequently\nbeing targeted and for how long? Are several\nusers from the same organization being targeted\nsimultaneously?\n\n## 3.1 Methodology\n\nThe analysis below focuses on 1,116 malicious emails\nreceived between 2009 and 2013.\n**Topics and language. To attempt to understand how**\nwell the attacker knows his victims, we manually categorized the emails (coded) by topic and language. (Unless\n\n\nindicated otherwise, the analysis below was performed\non emails that were coded by one of the author.) The\ntopic was determined by reading the emails’ titles and\nbodies and, in cases where emails were not written in English, we also used an online translation service. Emails\nwhose topic was still unclear after using the translator\nwere labeled as Unknown.\n**Targeted victims.** To determine the targeted victims\nof these attacks, we searched the email addresses and\nfull names of the senders and receivers for the malicious emails originating from trustworthy SMTP servers.\nWhen available, we used their public profiles available\non social media websites such as Google, Facebook, and\nSkype to determine their professional positions and organizations. We assume we have found the social profile\nof a victim if one of the three following rules applies (in\nthat order): First, if the social profile refers directly to\nthe email address seen in the malicious email; second,\nif the social profile refers to an organization whose domain matches the victims’ email address; or third, if we\nfind contextual evidence that the social profile is linked\nto WUC, Uyghurs, or the topic of the malicious email.\nOut of 724 victims’ email addresses, we found the profile of 32% (237), 4% (30), and 23% (167) using the first,\nsecond, and last rule, respectively.\n**Organizations and industries. In the following, WUC**\nrefers to victims directly affiliated with the organization (including our volunteers). Other Uyghur NGOs\ninclude Australia, Belgium, Canada, Finland, France,\nJapan, Netherlands, Norway, Sweden, and UK associations. Other NGOs include non-profit organizations such\nas Amnesty International, Reporters Without Borders,\nand Tibetan NGOs. Academia, Politics, and Business\ncontain victims working in these industries. Finally, Un_known corresponds to victims for which we were not able_\nto determine an affiliation.\n**Ranks.** We also translated the professional positions\nof the victims into one of the three categories: High,\n_Medium, and Low profile. We consider professional lead-_\nership positions such as chairpersons, presidents, and executives as high-profile, job positions such as assistants,\nand IT personnel as medium-profile, and unknown and\nshared email addresses (e.g., NGO’s contact information) as low-profile.\n\n\n-----\n\n1\n\n\n0.9 139 9 35 17\n\n0.8 32 17 28\n\n0.7 74 63 42\n\n326 10\n\n0.6 37 33 67\n\n0.5 156 76\n\n0.4\n\nUnknown\n\n0.3 575 171 Other\n\n0.2 8 57 31 76 Human rights\n\nUyghur\n\n0.1\n\nWUC\n\n0\nAll 2012 2013 2009 2010 2011 2012 2013\n\nYears\n\n|Topics of malicious emails|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1116 20 134 87 194 292 259 130 48 5 3 14 15 4|||||||||||||||||\n||139||2||32||3 17||9 28||35||17||8||\n||||||||||||||||||\n||||||||||||74||63||42||\n||||||||||||||||||\n||326||10||||||||||||||\n||||||37||33||67||||||||\n||||||||||||||156||76||\n||||||||||||||Un|kn|own||\n||575||8||57||||76||171||Oth Hu|er ma|n rig|hts|\n||||||||||||||||||\n||||||||31||||||Uy|gh|ur||\n||||||||||||||WU|C|||\n\n\nFigure 2: Distribution of the topics of the malicious\nemails for each year of the dataset shared by our two\nvolunteers. The left bar corresponds to the data shared\nby both volunteers, and the next two bar groups to each\nyear of the data shared by our first and second volunteer, respectively. The content of malicious emails is\n**targeted to the victims.**\n\n\n_known. Our assumption is that, because our volunteers_\nreceived most of the malicious emails directly, they were\nlikely to recognize cases where their contacts were being impersonated. We note that labeling is conservative:\nOur volunteers may sometimes label Spoofed or Typo addresses as Unknown because they do not know the person\nimpersonated in the attack. This may happen, for example, in cases where they were not the primary target of\nthe attack (e.g., they appeared in Cc).\n**Limitations. Our dataset originates from WUC and is**\nlimited to those victims that were targeted together with\nthat organization. We will see that these victims were often NGOs. As a result, the social engineering techniques\nobserved here may differ from attacks against different\nentities such as companies, political institutions, or even\nother NGOs. Despite these limitations, we argue that this\nanalysis is an important first step towards understanding\nthe human factors exploited by targeted attacks.\n\n## 3.2 Results\n\n\n1\n\n\n0.9 4 18\n\n0.8 9 216 112\n\n0.7 7\n\n89\n\n0.6 760 34 104 198\n\n0.5 Unknown\n\nOther\n\n0.4\n\nChinese\n\n0.3 11 Uyghur\n\n0.2 42 32 66 English\n\n0.1 267 68 35 13\n\n0\nAll 2012 2013 2009 2010 2011 2012 2013\n\nYears\n\n|Languages of malicious emails|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1116 20 134 87 194 292 259 130|||||||||||||||||\n||52||2||||8||18||18||||||\n||||||||4 9||||||||||\n||||7||||||||||216||112||\n||||||||||||||||||\n||||||89||||104||||||||\n||760||||||34||||198||||||\n|||||||||||||||U|nkn|own|\n|||||||||||||||O C|ther hine|se|\n||||11||||32||66|||||U E|ygh nglis|ur h|\n||||||||||||||||||\n||267||||42||||||68||||||\n||||||||||||||35||13||\n\n\nFigure 3: Distribution of languages for each year of our\ndataset. Malicious emails employ the language of their\n**victims.**\n\n**Impersonation. Finally, to understand the social con-**\ntext of the attack, each of our volunteers coded (based\non her experience within the organization) all the email\naddresses of the senders into one of five categories:\n_Spoofed, Typo, Name, Suspicious, or Unknown. (Coding_\nwas done based exclusively on the personal knowledge\nof the volunteers.) An email is marked as Spoofed if it\nbears the exact sender email address of a person known\nto our volunteers, as Typo if it resembles a sender email\naddress known to the receiver but is not identical, and as\n_Name if the attacker used the full name of a volunteer’s_\ncontact (with a different email address). Finally, email\naddresses that look as if they had been generated by\na computer program (e.g., uiow839djs93j@yahoo.com)\nare labeled as Suspicious and all remaining emails as Un\n\nIn this subsection, we discuss the results of our analysis of the social engineering techniques used in the malicious emails.\n**Topics and language. The topic of malicious emails in**\nour dataset can generally be classified into one of three\ncategories: WUC, Uyghur, and human-rights. In particular, we observed 51% (575) of malicious emails pertaining to WUC, 29% (326) to Uyghurs, 12% (139) to\nhuman-rights, and 3% (28) to other topics. In addition,\nthe native language of the victim is often used in the malicious emails. In fact, 69% (664) of the emails sent to the\nsecond volunteer were written in the Uyghur language,\nand 62% (96) for the first one. These results indicate that\nattackers invested significant effort to tailor the content\nof the malicious emails to their victims, as we see in Figure 2 and Figure 3.\n**Specialized events. In addition to being on topic, we**\nalso observed that emails often referred to specific events\nthat would only be of interest to the targeted victims.\nThroughout our dataset, we found 46% of events (491)\nrelated to organizational events (e.g., conferences). We\nnote that these references are generally much more specialized than those used in typical phishing and other\nprofit-motivated attacks. For example, Figure 1 shows a\nscreenshot of an attack that replayed the announcement\nof a conference on a very specialized topic. The malicious email was edited by the attacker to add that all fees\nwould be covered (probably to raise the target’s interest).\n**Impersonation. We find that attackers used carefully**\ncrafted email addresses to impersonate high-profile identities that the victims may directly know. That is, attackers used one of the following four techniques to add legitimacy to a malicious email: First, 41% (465) of the\n\n\n-----\n\n1\n\n\n1\n\n\n0.9 173 3 9 27 48 31 11\n\n0.8 134 30\n\n0.7 6 26\n\n0.6 89\n\n0.5 465 11 88 24 58 115 87\n\n0.4\n\nUnknown\n\n0.3 Suspicious 22\n\n0.2 Name 136\n\n337 Typo 61 76\n\n0.1 Spoofed 23 13 17\n\n0\nAll 2012 2013 2009 2010 2011 2012 2013\n\nYears\n\n\n0.9 173 5 48\n\n0.80.7 15071 8 25 27 21 2246 1214 13\n\n8\n\n0.6\n\n11 33 197 100\n\n0.5\n\n0.4\n\n0.3 722 11 91 190 Unknown\n\n0.2 41 92 Low\n\nMedium\n\n0.1\n\nHigh\n\n0\nAll 2012 2013 2009 2010 2011 2012 2013\n\nYears\n\n|Impersonation techniques|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1116 20 134 87 194 292 259 130||||||||||||||||||\n||173|||3||13 9||27||48||34 31||36||15 11||\n||134|||6||||||26||89||30||||\n||465|||||88||24||||||115||||\n|||||||||||||||||||\n|||||11||||||58||||||87||\n||||Unkn|own||||||||||||||\n||||Susp Nam|iciou e|s|||22||||136||||||\n|||||||||||||||||||\n||337||Typo|||||||61||||76||||\n||||Spoo|fed||23||13||||||||17||\n\n|Ranks of impersonated senders|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1116 20 134 87 194 292 259 130|||||||||||||||||\n||173 71||1||13 5||27||48||34 22||36 12||15 13||\n||||||||||||||||||\n||150||8||25||8||21||46||14||||\n||||||||11||33|||1|97||100||\n||||||||||||||||||\n||||||||||||||||||\n||722||||91||||||190||||||\n||||11||||41||92|||||U L|nkn ow|own|\n||||||||||||||||||\n|||||||||||||||M|ediu|m|\n|||||||||||||||H|igh||\n\n\nFigure 4: Distribution of senders’ impersonation techniques for each year of our dataset. Malicious emails\n**spoof the email address of a contact of the volunteers,**\n**use a very similar address controlled by the attacker,**\n**or a contact’s full name.**\n\nemail addresses have Typos (i.e., the email address resembles known sender addresses, but with minor, subtle differences). These email addresses are identical to\nlegitimate ones with the exception of a few characters\nbeing swapped, replaced, or added in the username. Second, 12% (134) of the senders’ full names corresponded\nto existing contacts of the volunteers. Third, we find\nthat most email addresses belonged to well-known email\nproviders — Google being the most prominent with 58%\nof all emails using the Gmail or GoogleMail domains,\nfollowed by Yahoo with 16%.\nFourth, we find that 30% (337) of the sender emails\nwere spoofed (i.e., the email was sent from the address of\na person that the volunteer knows). This observation suggests that the attacker had knowledge of the victim’s social context, and had either spoofed the email header, or\ncompromised the corresponding email account. To identify a subset of compromised email accounts, we consider spoofed emails authenticated by the senders’ domains using both SPF and DKIM. To reduce the chances\nof capturing compromised servers instead of compromised accounts, we also consider only well-known, trustworthy domains such as GMail. This procedure yields\nmalicious emails that were likely sent from the legitimate\naccount of the victims’ contacts. We found that three\nemail accounts belonging to prominent activists, including two out of 10 of the WUC leaders, were compromised and being used to send malicious emails. We have\nalerted these users and are currently working with them\nto deploy defenses and more comprehensive monitoring\ntechniques, as we will discuss in Section 6.\nWe show the distributions of malicious emails sent\nwith spoofed, typo, suspicious, or unknown email addresses in Figure 4, and the ranks of the impersonated\n\n\nFigure 5: Distribution of impersonated senders’ ranks for\neach year of our dataset. Malicious emails often imper**sonate high-profile individuals.**\n\n\n1\n\n\n0.9 19 2\n\n0.8\n\n16 6\n\n0.7 40\n\n0.6 760 183\n\n0.5 515\n\nUnknown\n\n0.4\n\n0.3 95 18 OtherChinese\n\n0.20.1 267 113 UyghurEnglish\n\n0 39 2\nAll WUC Uyghur Rights Others Unknown\n\nTopics\n\n|Correlation between topics and languages|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|1116 575 326 139 28 48||||||||||||||\n||52||||23||6||1 1|||4||\n||||||||19||2|||||\n||||||||16||6|||||\n||||||183|||||||40||\n|||||||||||||||\n||760||515|||||||||||\n|||||||||||||Un|known|\n||||||||95|||||Ot|her|\n||||||||||18|||Ch Uy|inese ghur|\n|||||||||||||||\n||267||||113|||||||En|glish|\n||||39|||||||||2||\n\n\nFigure 6: Distribution of languages employed to write\nabout the main topics of malicious emails. There is a\n**strong correlation between malicious emails’ topics**\n**and the language in which they are written.**\n\nsenders in Figure 5. (We do not show the corresponding ranks for receivers because NGOs generally function\nwith a handful of employees, all playing a key role in the\norganization.)\n**Targeted victims. For the analysis below, which lever-**\nages other recipients besides our two volunteers, we further filter emails to keep only those originating from\nwell-known domains (as described in Section 2). Doing\nthis leaves us with 568 malicious emails that are likely\nto have indeed been sent to all the email addresses in the\nheader. We find that the attacks target more organizations than WUC, including 38 Uyghur NGOs, 28 Other\n_NGOs, as well as 41 Journalistic, Academic, and Polit-_\n_ical organizations. (See Appendix A for the complete_\nlist of targeted organizations.) Interestingly, we find a\nstrong correlation between the topic of an email and the\nlanguage in which the email was written, as we show in\nFigure 6. Our results show that English was more and\n\n\n-----\n\nFigure 7: Timeline of attacks, in number of malicious\nemails per month, against the 60 most targeted victims\n(our two volunteers’ rows are shaded and the vertical\nline corresponds to one of our volunteer joining WUC).\nThe Y axis represents victims grouped by organization.\n_ETUE corresponds to the East Turkestan Union in Eu-_\nrope NGO and Others to different organizations. Each of\n**the top 60 victims has been frequently attacked over**\n**the last four years and several victims from the same**\n**NGOs were attacked simultaneously.**\n\nmore common as the topic became less and less specialized. We hypothesize that attackers may have sent the\nsame email messages to several recipients with similar\ninterests to reduce the costs involved in manually crafting these emails.\n**Timing. Our dataset shows that the same victims were**\nfrequently targeted and that several members of the\nsame organization were routinely targeted simultaneously. This suggests that attackers were using a “spray”\nstrategy, trying to find the weakest links in the targeted\norganization, and hence, optimizing their chance of success. Spraying is clearly visible in Figure 7 where we\nsee that the top 60 most targeted victims in our dataset\nreceived malicious emails often over the last four years.\n(We note that the dataset shared by one of our volunteers starts on August 2012, explaining why we observe\nmore malicious emails after that date.) We also see that,\n31 email accounts from individuals without affiliation to\nWUC were often targeted simultaneously to the WUC\naccounts.\n\n\n**Summary of Findings. We now revisit the initial ques-**\ntions posed at the beginning of this section. First, we\nsaw that most emails in our dataset pertained to WUC,\nUyghurs, or human-rights, were written in the recipient’s mother tongue, and often referred to very specialized events. We also found that sender impersonation\nwas common and that some email accounts belonging to\nWUC’s leadership were compromised and used to spread\ntargeted attacks. (We note that many more accounts may\nbe compromised but remain dormant or do not appear\nas compromised in our dataset.) Second, we showed\nthat numerous NGOs were being targeted simultaneously\nwith WUC and that the specialization of emails varied depending on the recipient(s). Finally, we observed\nthat the most targeted victims received several malicious\nemails every month and that attacks were sprayed over\nseveral organizations’ employees.\n\n## 4 Analysis of attack vectors\n\nWe now analyze the techniques used to execute arbitrary\ncode on the victim’s computer. The related work reports the use of malicious links, email attachments, and\nIP tracking services [10, 16]. Whereas ISTR 2013 reports that EXE are largely used in targeted attacks, and\nthe Mandiant report that ZIP is the predominant format\nthat they have observed in the last several years, we find\nthat these formats represent 0% and 4% (49) of malicious\nattachments in our dataset, respectively. Instead, we find\nRAR archives and malicious documents to be the most\ncommon attack vectors. Hypotheses that may explain\nthese discrepancies with the Mandiant report include the\ntuning of attack vectors to adapt to the defenses mechanisms used by different populations of email users (e.g.,\nNGOs vs. corporations); Mandiant’s attacker (APT1),\nmainly using primitive attack vectors such as archives;\nand/or Mandiant having excluded more advanced attack\nvectors, such as documents, from its report. However, in\nthe absence of empirical data on APT1’s attack vectors,\nwe cannot test these hypotheses. In this section, we perform a quantitative study of the attack vectors employed\nin our dataset, and also analyze their dynamics. We seek\nto answer the following questions:\n\n_• What attack vectors are being employed against_\n_WUC? Do they generally rely only on human fail-_\nures or also on software vulnerabilities? Do they\nevolve in time and if so, how quickly do they adapt\nto new defense mechanisms?\n\n_• What is the efficacy of existing countermeasures?_\nAs all malicious documents in our dataset used\nwell-known vulnerabilities, would commercial,\nstate-of-the-art defenses have detected all of them?\n\n\n## 4\n\n\n-----\n\nTargeted vulnerabilities\n\n\nTargeted applications\n\n\n60\n\n50\n\n\nUnknown\n\n\n60\n\n50\n\n\nOffice\nAcrobat\n\n\n40\n\n30\n\n\n40\n\n30\n\n\n20\n\n10\n\n\n20\n\n10\n\n|Col1|Unknown|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||Others 2006−2492 2009−3129|||||||||||||\n|||||||||||||||\n||2010−3333|||||||||||||\n|2012−0158||||||||||||||\n\n|Col1|Office Acrobat|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n||||||||||||||||\n||||||||||||||||\n||||||||||||||||\n||||||||||||||||\n\n\n0\n2009−09 2010−09 2011−09 2012−09 2013−09\n\nTime\n\n\n0\n2009−09 2010−09 2011−09 2012−09 2013−09\n\nTime\n\n\nFigure 8: Number of malicious documents containing a given vulnerability (CVE) (left) and target application (right)\nfor each month of our dataset. We represent the top four CVEs in number of attacks over the whole trace individually and others are represented in aggregate. The vertical line in November 2010 corresponds to the deployment of\nsandboxing in Acrobat Reader. Although Acrobat Reader was the most targeted application until 2010, recent\n**attacks mainly target the Office suite.**\n\n\n## 4.1 Methodology\n\n**Malicious archives. To analyze the archives’ contents,**\nwe extracted them in a disconnected VM environment\nand manually inspected their contents to determine the\ntype of files they contain, independently of their extensions. In the case of EXE files, we also examined them\nmanually to determine whether their Microsoft Windows\nicons were similar to those used for other file formats\n(e.g., JPEG) in order to persuade average users that they\nwere not executable.\n**Malicious documents. We used two methodologies to**\ndetermine the characteristics of the vulnerabilities being\nexploited by malicious documents. First, we submitted\nthe documents to VirusTotal [1] for analysis. Each of the\n45 Antivirus (AVs) on VirusTotal classified the checked\nsample as benign or malicious, and attached a “tag” describing the auxiliary information relating to the sample.\nOften the tag is a Common Vulnerabilities and Exposures\n(CVE) number, presumably corresponding to the signature that matched, but in some cases, the tag field is not\na CVE; it is either tagged as “unknown” or contains a\nsymptomatic description such as the inclusion of a suspicious OLE object. We refer to these three tags as CVE,\n_Unknown, and Heuristic, respectively. Often all AVs re-_\nported a Single CVE but sometimes, they reported Mul_tiple, conflicting CVEs. Once we collected all CVE tags,_\nwe then scraped the National Vulnerability Database [18]\nto obtain the release date and vulnerable applications for\neach of the CVEs that we found.\nSecond, we inspected the documents manually to confirm that they contain malware, and also used taintassisted analysis both to verify the accuracy of the CVEs\nreported in AV reports and to investigate the presence of\nzero-day vulnerabilities.[1] The methodological details of\n\n\n1Hereafter, zero-day vulnerabilities refer to vulnerabilities that were\n\n\nour taint-assisted manual analysis are described in Appendix B.\n**Defenses. We performed a retrospective analysis of the**\nprotection offered by common defenses such as AV and\nwebmail providers in the context of our malicious documents. For AV, we used VirusTotal to determine whether\na malicious document is detected by the scanning engine\nof each AV, as described above. For webmail channels,\nwe created an email account on GMail, Hotmail, and Yahoo, and used a dedicated SMTP server to send emails\nto that account with malicious documents attached. We\nconsidered malicious documents delivered without modifications as undetected by the webmail defenses. Otherwise, if an email or its attachment is dropped, or if the\nattachment’s payload is modified, we considered it as detected. The analyses based on webmails and VirusTotal\nwere performed in November 2013 and July 2014, respectively.\n**Limitations. As with social engineering, our analysis**\nof attack vectors is biased towards NGOs. In addition,\nthe above methodology is limited to the attack vectors\ncaptured in our dataset. For example, we miss attacks\nagainst the NGOs’ web servers unless the corresponding\nmalicious link appears in the suspicious emails.\nSecond, our taint-assisted analysis of vulnerabilities is\nlimited to those documents for which we were able to\nanalyze the logs manually. For example, we found that\nopening PDF files in our environment generated log files\nthat were far too large (around 15GB in the median case)\nfor manual analysis. As a result, we were able to manually confirm vulnerabilities only against Microsoft Office. However, despite this limitation, we were also able\nto determine which PDF documents contained malware\nthrough manual inspection.\n\nnot publicly disclosed at the time of the attack.\n\n\n-----\n\nTable 2: List of well-known vulnerabilities exploited by\nmalicious documents. Release corresponds to the release\ndate of the vulnerability and First to its first exploitation\nin our data set (in number of days relative to the release\ndate). Resolved corresponds to the number of Microsoft\nOffice vulnerabilities that were mistagged in AV reports\nbut that we were able to resolve using taint-assisted manual analysis.\n\n_CVE_ _Release_ _First_ _Apps_ _# emails Resolved_\n2006-0022 06/13/06 1,191 Office 2 0\n2006-2389 07/11/06 1,166 Office 18 16\n2006-2492 05/20/06 1,125 Office 59 47\n2007-5659 02/12/08 588 Acrobat 3 0\n2008-0081 01/16/08 651 Office 1 0\n2008-0118 03/11/08 1,010 Office 1 0\n2008-4841 12/10/08 824 Office 1 0\n2009-0557 06/10/09 405 Office 2 0\n2009-0563 06/10/09 880 Office 31 0\n2009-0927 03/19/09 180 Acrobat 11 0\n2009-1862 07/23/09 68 Acrobat 3 0\n2009-3129 11/11/09 188 Office 58 4\n2009-4324 12/15/09 4 Acrobat 15 0\n2010-0188 02/22/10 28 Acrobat 15 0\n2010-1297 06/08/10 0 Acrobat 9 0\n2010-2883 09/09/10 7 Acrobat 7 0\n2010-3333 11/10/10 49 Office 220 0\n2010-3654 10/29/10 0 Office 7 0\n2011-0611 04/13/11 0 Acrobat 19 0\n2011-0097 04/13/11 224 Office 3 0\n2011-2462 12/07/11 2 Acrobat 5 0\n2012-0158 04/10/12 37 Office 278 12\n2013-0640 02/14/13 68 Acrobat 1 0\n\nFinally, our defense analysis was performed in bulk,\nafter the time of the attacks. As a result of the difference\nbetween the times of attack and analysis (up to four years\nfor the first malicious documents), the detection rates reported hereafter should be treated as upper bounds. This\nis because the AV signatures at the time of the analysis\nwere more up-to-date than they would have been at the\ntime of the attack.\n\n## 4.2 Results: Attack vectors\n\n**4.2.1** **Malicious archives**\n\nWe observed numerous targeted attacks leveraging social\nengineering and human failure to install malware on the\nvictim’s computer. In particular, we found 247 RAR and\n49 ZIP containing malicious EXE. In 10 cases, the malicious archives were password protected with the password included in the email’s body. We hypothesize that\narchiving was used as a rudimentary form of packer for\nthe malware to evade detection by the distribution channels. Finally, we found that 20% of all EXEs contained\nin the archives used an icon that resembled a non-EXE,\ni.e., a DOC, JPEG, or PDF icon, in 20%, 19%, and 7%\nof the cases.\n\n\nFigure 9: Timeline of the target vulnerabilities. The Y\naxis corresponds to CVEs and each circle to the number\nof CVEs seen each month after the public disclosure of\nthe vulnerability (day 0). All vulnerabilities were first\n**targeted after their public disclosure.**\n\n**4.2.2** **Malicious documents**\n\nWe used taint-assisted analysis to resolve the conflicts\ndue to AV mistagging and summarize the CVE information in Table 2. The number of conflicts resolved using\ntaint-assisted manual analysis is reported in the last column Resolved. Additional taint-analysis results are reported in Appendix B.\n**Zero-day versus unpatched vulnerabilities. We find**\nno evidence of the use of zero-day vulnerabilities against\nour dataset, but several uses of disclosed vulnerabilities\nwithin the same week as their public release date. In\naddition, we see in Figure 9 that vulnerabilities continued\nto be exploited for years after their disclosure, and this\nconfirms that unpatched vulnerabilities represent a large\nfraction of attacks in our dataset. To ascertain the CVE\nbeing exploited in each sample, we used a combination\nof the telemetry data available in CVE tags generated by\nAVs, and a manual analysis to resolve cases where the tag\nwas ambiguous. For each sample, we then recover the\npublic disclosure date for the vulnerability manually, and\ntreat it as the corresponding day-zero. By comparing the\ntime of use in our email dataset, we are able to ascertain\nthe lifetime of vulnerability exploits.\nWe find several instances of exploits that were used\nin publicly-reported targeted attacks in our dataset. For\n\n\n-----\n\n1\n\n\n0.9\n\n0.8\n\n0.7\n\n0.6\n\n0.5\n\n0.4\n\n0.3\n\n0.2\n\n0.1\n\n0\n\n\nFigure 10: Detection rates of popular webmails for the\nmalicious documents. The efficacy of webmails to de**tect malicious documents varies widely.**\n\ninstance, vulnerabilities such as CVE-2009-4324, CVE2010-3654, and CVE-2010-2883 have been reported to\nbe zero-day vulnerabilities [6]. However, in our dataset,\nthese vulnerabilities were used after their disclosure.\n**Evolution of target applications.** Our data shows a\nsudden switch from Adobe Reader to Microsoft Office\nsuite as the primary targeted application as of November 2010, as seen in Figure 8. We find a correlation between the time of this switch and two events: (a) the deployment of sandboxing defenses in Adobe Reader and\n(b) the disclosure of vulnerabilities in the Office suite.\nThe first version of Acrobat Reader to support sandboxing for Windows (version 10.0) was released on November 15, 2010. Within the same month, a stack buffer\noverflow against Microsoft Office was released publicly\n(November 2010), reported as CVE-2010-3333. We see\nthis CVE being massively exploited in our dataset as of\nJanuary 2011, which is a time lag of two months. We\nobserve the use of CVE-2010-3333 being replaced with\nCVE-2012-0158 in January 2013. This evidence suggests that attackers adapted their targeted vectors to use\nnewly disclosed vulnerabilities within a few days to a few\nmonths of disclosure, and that updates to the security design of software reduces its exploitability in the wild (as\none would expect).\n\n\n## 4.3 Results: Bypassing common defenses\n\nWe now investigate the efficacy of existing defenses\nagainst malicious documents.\n**Email / Webmail Filtering. Despite the retrospective**\nanalysis of the malicious documents, we find that the\ndetection rates of malicious documents for GMail, Hotmail, and Yahoo were still relatively low (see Figure 10).\nWe also find that GMail failed to detect most malicious\ndocuments sent after March 2012. In particular, while\nthe detection of documents sent before March 2012 was\n\n\nFigure 11: Detection rates of malicious documents for\neach of the top 30 AVs as reported by VirusTotal. No sin**gle AV detected all malicious documents despite their**\n**use of well-known vulnerabilities.**\n\n73%, it is 28% after that date. Interestingly, 71% of the\ntrue positives for GMail after March 2012 corresponded\nto RTF files with all \\r\\n character sequences substituted\nwith the \\n character. While this substitution did not deactivate the malware, we observed that it broke the shellcodes embedded into these documents as they require the\ndocument size to remain unchanged to function properly.\nAs a result, the malware was never executed. Although\nwe cannot verify the purpose of this substitution, we note\nthat its appearance coincided with that of the malicious\nRTF files. We conclude this discussion by pointing out\nthat Yahoo’s low detection rate is interesting as it claims\nto be using Symantec AV for its webmail service [12] —\nwhich, as we will see below, has a much higher detection\nrate.\n**Signature-based AV Scanning. In Figure 11, we show**\nthe detection rates for the top 30 VirusTotal AVs, sorted\nby decreasing detection rate of the malicious documents.\nThere are two main takeaways from this graph. First,\nno single vendor detected all original malicious documents, even though we have seen that they used wellknown vulnerabilities. For example, Qihoo, the vendor\nwith the overall best efficacy, was unable to detect 3% of\nthe malicious documents based on scanning. Second, we\nobserve large variations among the efficacy of different\nAV vendors. That is, the detection rate dropped by 30%\nfrom the first to the twentieth AV (CAT QuickHeal) and\nthe 15 AVs with the lowest detection rate (not shown) all\nhad a detection rate of less than 35%.\n**Summary of Findings. We found that malicious docu-**\nments are the most popular attack vectors in our dataset\nfollowed by malicious archives. Malicious documents\ntended to use newly released vulnerabilities, often within\n\n\n-----\n\nTable 3: Summary of the malware clusters. For each cluster, we show the malware family (or an ID if we could\nnot determine it), the number of malicious emails containing the malware, the number of Command and Control\n(C2) servers, the similarities in terms of communication protocols and C2 with malware attributed to known entities\n(entity(Com,C2)). Our dataset contains several families of first-stage malware previously seen in targeted attacks\n**carried out in the wild.**\n\n_Clusters_ _1_ _2_ _Surtr_ _4_ _5_ _6_ _7_ _8_ _9_ _TravNet_\n_# samples_ 67 (9%) 58 (8%) 51 (7%) 37 (5%) 30 (4%) 22 (3%) 19 (2%) 19 (2%) 18 (2%) 13 (2%)\n_# C2_ 6 2 6 18 13 3 8 13 9 4\n_Similarity_ DTL(C2) — DTL(C2) — — — — — — TravNet(Com,C2)\n\n\na week, continued to utilize them for several years, and\nmost of them used well-known instead of zero-day vulnerabilities. In particular, our taint-assisted manual analysis of Office documents did not reveal a single zeroday vulnerability in our dataset. This raises the question of whether defense mechanisms deployed in webmails and state-of-the-art commercial defenses are effective in blocking these well-known attacks. Furthermore,\nwe found that malicious archives often contained EXE\nfiles that masquerade as pictures or documents.\n\n## 5 Malware analysis\n\nWe now analyze the first-stage malware found in malicious documents. Unlike the Mandiant report, which\nprovides an analysis for malware that targets different\norganizations and that (they claim) originates from the\nsame group, our analysis focuses on all malware (in our\ndataset) that has targeted a single organization. By looking at targeted attacks from the perspective of the target\nrather than the attacker, our analysis enables us to determine whether WUC has been targeted with the same\nor different malware over the years. We also take a different approach from the authors of the GhostNet report\nwho performed malware analysis on a few compromised\nsystems belonging to different but related organizations.\nWe instead analyze over six hundred malware samples\nused to establish a foothold on the targeted systems of a\nsingle organization. Our analysis differs from the related\nwork in its scale and context [16] or focus [10]. This\nsection aims to answer the following question:\n\n_• Is WUC targeted with the same or different mal-_\n_ware? In the latter case, are there similarities be-_\ntween this first-stage malware and others found in\ntargeted attacks in the wild?\n\n## 5.1 Methodology\n\nOur analysis below was done on 689 malware samples\nthat we extracted from malicious documents.\n**Clustering.** To make our analysis tractable for 689\nmalware samples, we started by clustering the malware\n\n\nbased on its behavior. To do so, we ran the malicious\nEXE and DLL files in a disconnected sandboxed environment and hooked the function calls to resolve domain\nnames and establish network communications. In addition, to obtain the TCP port number on which communication is done, we intercepted function calls to\ngethostbyname and returned a dummy routable IP address. As a result, the malware subsequently reveals the\nport number when it initiates a connection with the returned IP. (See Appendix C for the complete list of Command and Control (C2) domains.) Finally, we generated\nbehavioral profiles for 586 samples, clustered them using\nan approach similar to [5,14], and manually verified the\naccuracy of the resulting clusters.\n**Malware family and similarities. Similarly to Bailey**\net al. [5], we found that determining the malware family using AV signature scanning was unproductive. To\ndetermine whether our malware shares similarities with\nother known targeted malware, we relied on several reports on targeted attacks [9,13]. We extracted the C2 domains and, when available, additional information about\nthe malware (e.g., hashes and behavior) from these reports. Finally, we correlated the domains, IP addresses,\nhashes, and behavioral profiles with those from the reports in order to find similarities between the different\nsets of malware. We performed this analysis in February\n2014.\n**Limitations. Our behavioral analysis was performed in a**\ndisconnected environment and as a result, it is limited to\nthe first stage of the malware behavior. Studying the behavior of additional payload that would be downloaded\nafter the compromise is beyond the scope of this paper\nand will be the subject of future work.\n\n## 5.2 Results\n\nWe now analyze the malware clusters and their similarities with other targeted malware found in the wild.\n**Cluster sizes.** We find that 57% of our malware belonged to the ten largest clusters (we show additional\ninformation about these clusters in Table 3). In total,\nfive clusters (two in the top ten) used at least one of\ndtl6.mooo.com, dtl.dnsd.me, or dtl.eatuo.com as their\n\n\n-----\n\nC2 domains, indicating some operational link between\nthem. In fact, at the time of analysis, these three domains resolved into the same IP address and the malware in each cluster connected to different ports of the\nsame server. Despite these apparent similarities, however, manual analysis of the behavioral logs revealed that\ntheir logic differed from one another, explaining their assignment to different clusters. Combined, these five clusters represented 24% of the malware that we analyzed.\n\n**Malware family and similarities. We found various de-**\ngrees of similarities between our clusters and targeted attacks reported in the wild. First, the five clusters above\nhad the same C2 as the DTL group reported by FireEye\nin November 2013 and that the malware was of the same\nfamily as one of these clusters’ (APT.9002, not shown)\n\n[9]. In particular, we found that one of our samples in\nthat cluster had the same MD5 hash as those described\nin the FireEye report and that eight had identical manifest resources. FireEye claims that this malware has been\nused in targeted attacks against various governmental and\nindustrial organizations.\n\nSecond, malware in the Surtr cluster had the same behavioral profile as samples used against the Tibetan community in March 2012 [7]. Although the two sets of samples had different MD5 hashes, they both connected to\nthe same C2 server (shared with APT.9002) on the same\nport number, and exhibited the same behavior to establish persistency on the victim’s machine.\n\nThird, our 13 TravNet samples exhibited similar behavior as those used against Indian targets in 2013 [2]. To\ndo so, we obtained the samples used in India, generated\ntheir behavioral profiles, and compared them manually\nwith the malware in our TravNet cluster. Although both\nsets connected to different C2 servers and exhibited variations in the way they searched the victims’ file system,\nwe found that they both used the same communication\nprotocol with the C2.\n\nFourth, samples in another cluster communicated with\nthe same C2 server and exhibited the same behavior as a\nVidgrab sample found in a malicious document sent to a\nvictim in Hong Kong in August 2013 [8].\n\n**Summary of findings. We found that WUC has been**\ntargeted with several malware families in the last year.\nWe also showed that the Surtr and APT.9002 clusters\ncorresponded to malware that Citizenlab and FireEye\nidentified as having targeted the Tibetan community,\nas well as other political and industrial organizations\n\n[7, 9]. Furthermore, 24% of our malware (including\n_Surtr, APT.9002 and three other clusters) had at least one_\nC2 domain in common, which was identical to those of\nthe Citizenlab and FireEye reports.\n\n\n## 6 Future Work\n\nSeveral directions for future work arise from this work.\nWe briefly discuss them below.\n**Attack vectors and generalization.** Our analysis is\nlimited to attack vectors used against WUC. Similar studies on a wider range of targets would benefit understanding this emerging threat better. Further, our attack vectors distributed over email channels and have two main\nlimitations. First, it is possible that our volunteers have\nbeen attacked via other channels besides email. Second, although we have seen various organizations targeted with the same malware as WUC, it is generally\nhard to determine with certainty which victims were the\nprimary target of these attacks. Therefore, it is possible that other victims have been targeted with additional\nattack vectors when the attacks did not involve WUC.\nFurther research is needed to overcome these limitations\nExploring different channels that attackers use for distributing malicious payloads is important. As a step towards this goal, we are currently collaborating with the\nSafebrowsing team at Google to investigate the emergent threat of watering-hole attacks. These attacks are\nconceptually very similar to drive-by download attacks\nwith one key difference: They compromise very specific\nwebsites commonly visited by the targeted community\n(e.g., a company’s website) and wait for victims to visit\nthe website. As compared to spear phishing, wateringhole attacks offer the advantage of potentially targeting\na fairly large number of victims (e.g., all employees of a\nlarge company) before raising suspicion. We conjecture\nthat the small number of suspicious links in our dataset\nmay be due to the small size of the targeted organizations\nand the public availability of their employees’ email addresses.\nOther attack vectors include but are not limited to\npackets injection to redirect victims to malicious servers\n(similar to those used in watering-hole attacks) and physical attacks on the victims’ devices [3]. Detecting these\nattacks would require completely different methodologies than the one we used in this paper.\n**Monitoring. We have seen that a few high-profile mem-**\nbers of the Uyghur community were compromised and\nthat their email accounts were being used as stepping\nstones to carry out targeted attacks. Although it is possible that these email accounts were compromised via targeted attacks, we have not yet confirmed this hypothesis.\nMore generally, we do not know yet what is the specific\naim of these targeted attacks. Monitoring the full lifecycle of targeted attacks would require novel measurement\nsystems, deployed at the end users, that can identify compromises without being detected.\nPinpointing the geolocation of attackers carrying out\ntargeted attacks, or attack attribution, is another open\n\n\n-----\n\nmonitoring challenge. Marczak et al. were able to attribute targeted attacks to governments in the Middle\nEast by analyzing relationships of cause and effect between compromises and real-world consequences [16].\nIn contrast to monitoring and attack attribution, this paper has presented a extensive, complementary analysis of\nthe life cycle of targeted attacks before the compromise.\n**Large-scale malware analysis and clustering.** We\nfound it challenging to (a) cluster targeted malware and\n(b) locate similar samples. First, this malware sometimes\nexhibits significant similarity in its logic and different\nmalware may also use the same Command and Control\n(C2) infrastructure. As a result, traditional clustering algorithms tend not to work very well. Second, we located\nsimilar samples based on a limited set of indicators such\nas C2, cryptographic hash, or YARA signatures, however, we feel that our current capability in that respect has\na lot of room for improvement. We foresee that a search\nengine that can, for example, locate malware matching\ncertain indicators out of an arbitrarily large corpus would\nbe a useful instrument for researchers working on targeted attacks.\nOur analysis of CVEs highlights that telemetry data\nfrom commercial AVs is not always reliable. Our analysis complemented with taint-analysis was largely manual\nand time-intensive. Analysis techniques to quickly diagnose known CVEs directly from given exploits is an open\nproblem and perhaps one of independent interest.\n**Defenses.** Our findings confirm that AVs may miss\nknown CVEs, even years after their release dates.\nClearly, known CVEs contribute a large part of the\nemerging threat of targeted attacks. Understanding why\ncommercial AVs miss known attacks conclusively, for\nexample to tradeoff false positives or performance for\nsecurity, is an important research direction. Designing\neffective defenses against targeted attacks is a major research challenge which depends on our ability to understand the threat at hand. As part of future work, one\ncould evaluate the effectiveness of novel defenses based\non the findings from this paper. As a small step towards\nthat goal, we plan to soon deploy a webmail plugin that\ncombines metadata and stylometry analysis [17] to detect\ncontact impersonation.\n\n## 7 Conclusion\n\nWe have presented an empirical analysis of a dataset\ncapturing four years of targeted attacks against a humanrights NGO. First, we showed that social engineering\nwas an important component of targeted attacks with\nsignificant effort paid in crafting emails that look legitimate in terms of topics, languages, and senders. We also\nfound that victims were targeted often, over the course\nof several years, and simultaneously with colleagues\n\n\nfrom the same organization. Second, we found that\nmalicious documents with well-known vulnerabilities\nwere the most common attack vectors in our dataset and\nthat they tended to bypass common defenses deployed in\nwebmails or users’ computers. Finally, we provided an\nanalysis of the targeted malware and showed that over\na quarter of samples exhibited similarities with entities\nknown to be involved in targeted attacks against a variety\nof industries. We hope that this paper, together with\nthe public release of our malware dataset, will facilitate\nfuture research on targeted attacks and, ultimately, guide\nthe deployment of effective defenses against this threat.\n\n**Acknowledgements. The authors would like to thank**\nthe anonymous reviewers, our shepherd Stuart Schechter,\nand Peter Druschel for their useful feedback. We would\nalso like to acknowledge Karmina Aquino (F-Secure),\nEmiliano Martinez (VirusTotal), Mila Parkour (Contagio), and Nart Villeuneuve (FireEye) for their help locating similar malicious documents. Finally, we thank the\nMax Planck Society, the Ministry of Education of Singapore under Grant No. R-252-000-495-133, and the NSF\nunder Grant No. CNS-1116777 for partially supporting\nthis work.\n\n## References\n\n[1] http://www.virustotal.com.\n\n[2] Inside report APT attacks on indian cyber space. Tech. rep. http://g0s.org/wpcontent/uploads/2013/downloads/Inside Report by Infosec\n\nConsortium.pdf.\n\n[3] Inside TAO: Documents reveal top nsa hacking unit.\nhttp://www.spiegel.de/international/world/the-nsa-usespowerful-toolbox-in-effort-to-spy-on-global-networks-a940969.html.\n\n[4] The Slingshot Project. http://slingshot.mpi-sws.org.\n\n[5] BAILEY, M., ANDERSEN, J., MORLEYMAO, Z., AND\nJAHANIAN, F. Automated classification and analysis of\ninternet malware. In Proceedings of Recent Advances in\n_Intrusion Detection (RAID 2007)._\n\n[6] BILGE, L., AND DUMITRAS, T. Before we knew it: An\nempirical study of zero-day attacks in the real world. In\n_Proceedings of the 2012 ACM conference on Computer_\n_and communications security (CCS 2012) (2012)._\n\n[7] CITIZEN LAB. Surtr: Malware family targeting the tibetan community. Tech. rep.\nhttps://citizenlab.org/2013/08/surtr-malware-familytargeting-the-tibetan-community/.\n\n[8] CONTAGIO. http://contagiodump.blogspot.de/2013/09/\nsandbox-miming-cve-2012-0158-in-mhtml.html.\n\n[9] FIREEYE. Supply chain analysis: From quartermaster to\nsunshop. Tech. rep.\n\n\n-----\n\n[10] HARDY, S., CRETE-NISHIHATA, M., KLEEMOLA, K.,\nSENFT, A., SONNE, B., WISEMAN, G., AND GILL,\nP. Targeted threat index: Characterizing and quantifying\npolitically-motivated targeted malware. In Proceedings of\n_the 23rd USENIX Security Symposium (San Diego, CA)._\n\n[11] INFORMATION WARFARE MONITOR. Tracking ghostnet: Investigating a cyber espionage network. Tech. rep.,\n2009.\n\n[12] JANA, S., AND SHMATIKOV, V. Abusing file processing\nin malware detectors for fun and profit. In Proceedings\n_of the 33rd IEEE Symposium on Security & Privacy (San_\nFrancisco, CA).\n\n[13] KASPERSKY. The nettraveler (aka TravNeT). Tech. rep.\nhttps://www.securelist.com/en/downloads/vlpdfs/kasperskythe-net-traveler-part1-final.pdf.\n\n[14] KRUEGEL, C., KIRDA, E., COMPARETTI, P. M.,\nBAYER, U., AND HLAUSCHEK, C. Scalable, behaviorbased malware clustering. In Proceedings of the 16th\n_Annual Network and Distributed System Security Sympo-_\n_sium (NDSS 2009) (2009)._\n\n[15] MANDIANT. APT1 exposing one of chinas cyber espionage units. Tech. rep., 2013.\nhttp://intelreport.mandiant.com/.\n\n[16] MARCZAK, W. R., SCOTT-RAILTON, J., MARQUISBOIRE, M., AND PAXSON, V. When governments hack\nopponents: A look at actors and technology. In Proceed_ings of the 23rd USENIX Security Symposium (San Diego,_\nCA).\n\n[17] NARAYANAN, A., PASKOV, H., GONG, N. Z., BETHEN\nCOURT, J., CHUL, E., SHIN, R., AND SONG, D. On the\nfeasibility of internet-scale author identification. In Pro_ceedings of the 33rd conference on IEEE Symposium on_\n_Security and Privacy. IEEE (San Francisco, CA, 2012)._\n\n[18] NATIONAL VULNERABILITY DATABASE.\nhttps://nvd.nist.gov/.\n\n[19] RALPH LANGNER. To kill a centrifuge: A technical analysis of what stuxnets creators tried to achieve. Tech. rep.,\n2013.\n\n[20] REUTERS. Journalists, media under attack from hackers: Google researchers.\nwww.reuters.com/article/2014/03/28/us-mediacybercrime-idUSBREA2R0EU20140328.\n\n[21] SONG, D., BRUMLEY, D., YIN, H., CABALLERO, J.,\nJAGER, I., KANG, M. G., LIANG, Z., NEWSOME, J.,\nPOOSANKAM, P., AND SAXENA, P. Bitblaze: A new approach to computer security via binary analysis. In Pro_ceedings of the 4th International Conference on Informa-_\n_tion Systems Security (Hyderabad, India, 2008)._\n\n[22] SYMANTEC. 2013 internet security\nthreat report, volume 18. Tech. rep.\nhttp://www.symantec.com/security response/publications/\nthreatreport.jsp.\n\n[23] WIRED. Google hackers targeted\nsource code of more than 30 companies.\nhttp://www.wired.com/2010/01/google-hack-attack/.\n\n\n## A Targeted organizations\n\n_Organization_ _# Recipients_ _# Emails_ _First-Last_\nWorld Uyghur Congress (WUC) 53 2,366 2009-2013\nEast Turkestan Union in Europe (ETUE) 7 153 2010-2013\nAustralian Uyghur Association 3 129 2009-2013\nEuro-Asia Foundation in Turkey 2 101 2010-2013\nUyghur Canadian Association 6 98 2009-2013\nGermany Uyghur Women Committee 2 82 2009-2013\nRadio Free Asia (RFA) 12 80 2010-2013\nFrance Uyghur Association 5 80 2009-2013\nEastern Turkestan Australian Association (ETAA) 6 77 2009-2013\nUyghur American Association (UAA) 10 72 2010-2013\nEastern Turkestan Uyghur Association in Netherlands 4 69 2010-2013\nNetherland Uyghur Union 1 60 2012-2013\nUnited Nations for a Free Tibet (UK) 1 57 2011-2013\nEastern Turkestan Culture and Solidarity Association 13 53 2009-2013\nViktoria Uyghur Association 1 48 2010-2013\nJapan Uyghur Association 2 43 2012-2013\nSwitzerland East Turkestan Association 2 43 2010-2013\nHacettepe University Turkey 3 41 2010-2013\nKazakhstan Academy of Poetry 2 36 2009-2013\nBelgium Uyghur Association 1 35 2009-2013\nKyrgyzstan Uyghur Association 2 33 2011-2013\nUyghur Canadian Society 1 31 2009-2013\nUyghur Academy 5 25 2009-2013\nMunich Uyghur Elders Meshrep 1 22 2012-2013\nRepublican Uyghur Cultural Center of Kazakhstan 1 22 2009-2013\nSweden Uyghur Association 4 12 2010-2013\nVirginia Department of Social Services 1 11 2009-2012\nUnrepresented Nations and Peoples Organization (UNPO) 5 8 2010-2013\nSociale Verzekeringsbank (SVB) NGO Netherland 1 8 2012-2013\nChina Democratic Party (CDP) 5 5 2009-2011\nFinland Uyghur Association 1 5 2013\nJet Propulsion Laboratory, founded by NASA 1 5 2012\nPennsylvania State University US 1 5 2010-2013\nUyghur Support Group Nederland 2 5 2010-2013\nNorway Uyghur Committee 1 5 2010-2013\nAmnesty International 4 4 2010-2012\nAssociation of European Border Regions (AEBR) 1 4 2010-2011\nHoward University US 1 4 2012-2013\nInitiatives for China 3 4 2009-2010\nLSE Asia Research Center and Silk Road Dialogue 2 4 2012-2013\nThe Government-in-Exile of the Republic of East Turkistan 2 4 2010-2011\nUyghur Human Rights Project (UHRP) 2 4 2010\nAustralian Migration Options Pty Ltd 3 4 2010\nAgence France-Presse 1 3 2013\nNational Endowment for Democracy (NED) 2 3 2010-2012\nPEN International 3 3 2009-2013\nSyracuse University US 1 3 2013\nWorldwide Protest in Honor and Support of Uyghurs Dying for Freedom 1 3 2013\nAustralian Government - Department of Foreign Affairs and Trade 2 3 2010\nNew Tang Dynasty Television China 2 3 2010\nThe Epoch Times 2 3 2010\nMinistry of Foreign Affairs Norway 1 2 2013\nInternational University of Kagoshima Japan 1 2 2013\nAssociation of Islam Religion 1 2 2013\nBilkent University Turkey 2 2 2011-2012\nEmbassy of Azerbaijan in Beijing 2 2 2010\nIndiana University School of Law-Indianapolis LL.M. 1 2 2012\nKYOCERA Document Solutions Development America 1 2 2013\nNew York Times 2 2 2009\nPfizer Government Research Laboratory - Clinical Pharmacology 1 2 2011-2012\nSaudi Arabia - Luggage Bags and Cases Company 1 2 2013\nStudents for a Free Tibet 2 2 2010\nSweden Uyghur Education Union 1 2 2010-2013\nUyghur International Culture Center 1 2 2012\nThe Protestant Church Amsterdam 1 2 2010\nSwiss Agency for Development and Cooperation (SDC) Kargyzstan 1 1 2013\nAmerican Bar Association for Attorneys in US 1 1 2010\nAssistance for Work Germany Frankfurt 1 1 2010\nBishkek Human Rights Committee 1 1 2012\nCentral Tibetan Administration (CTA) 1 1 2010\nChinese Translation Commercial Business 1 1 2009\nCircassian Cultural Center (CHKTS) 1 1 2012\nColombian National Radio 1 1 2010\nEmbassy of the United States in Australia 1 1 2010\nEuropa Haber Newspaper Turkey 1 1 2010\nEurope-China Cultural Communication (ECCC) 1 1 2011\nFreelance Reporter and writer Turkey 1 1 2012\nGoethe University Frankfurt am Main Germany 1 1 2012\nHuman Rights Campaign in China 1 1 2010\nInternational Enterprise (IE) - Singapore Government 1 1 2010\nInternational Tibet Independence Movement 1 1 2010\nJasmine Revolution China (Pro-Democracy Protests) 1 1 2009\nSocialist Party (Netherlands) 1 1 2011\nLos Angeles Times 1 1 2010\nMilli Gazete (National Newspaper Turkey) 1 1 2010\nNorwegian Tibet Committee 1 1 2010\nPhotographer Turkey 1 1 2012\nCNN International Hong Kong 1 1 2012\nReporters Without Borders 1 1 2012\nRepublican National Lawyers Association Maryland 1 1 2010\nSave Tibet - International Campaign for Tibet 1 1 2010\nSociety for Threatened People (STPI) 1 1 2012\nSouthern Mongolian Human Rights 1 1 2012\nStucco Manufacturers Association US 1 1 2013\nSuperior School of Arts France 1 1 2012\nThe George Washington University 1 1 2013\nTurkishNews Newspaper 1 1 2010\nUS Bureau of Transportation Statistics 1 1 2009\nUmit Uighur Language School 1 1 2010\nUnion of Turkish-Islamic Cultural Associations in Europe 1 1 2012\nUniversity of Adelaide Melbourne 1 1 2010\nUniversity of Khartoum Sudan 1 1 2012\nUS Embassy and Consulate in Munich Germany 1 1 2011\nWei Jingsheng Foundation 1 1 2009\nXinjiang Arts Institute China 1 1 2010\nYenicag Gazetti (Newspaper Turkey) 1 1 2010\nAmerican University 1 1 2012\nIslamic Jihad Union 1 1 2012\n\n\n-----\n\n## B Dynamic taint-assisted analysis of mali- cious documents B.1 Methodology\nWe use BitBlaze [21] to perform dynamic taint-tracking analysis of the targeted applications under the malicious documents\nas input and configure it to report four kinds of reports: (a)\nwhen a tainted Extended Instruction Pointer (EIP) is executed,\n(b) when a memory fault is triggered in the target program, (c)\nwhen a new process is spawned from the target program, or\n(d) when the analysis “times out” (i.e., runs without interruption for over 15 minutes). To mark malicious documents as a\nsource of taint, we tainted the network inputs and routed the\nmalicious input file using netcat. Additionally, we set BitBlaze to exit tracing at the detection of null pointer exceptions,\nuser exceptions, tainted EIPs, and process exits before the start\nof the trace. A tag was generated from the trace by obtaining the last instruction with tainted operands, and matching it\nwith the list of loaded modules generated by TEMU. Our guest\n(analysis) system configuration used in the image consists of\nclean installations of Windows XP SP2 with TEMU drivers and\nMicrosoft Office 2003.\n## B.2 Results\nAnti-virus software typically uses static signature-matching or\nwhitelisting techniques to analyze malware. To validate the\nanalysis results available from commercial AV, we ran a separate semi-automated dynamic analysis of the targeted application under our malicious documents.\nOut of 817 unique input documents (725 malicious and 92\nlegitimate), 295 timed out with our BitBlaze analysis without\nreporting a tainted EIP, a memory fault, or a newly spawned\nprocess.[2] Another 13 of them were incompatible with our analysis infrastructure (using a more recent DOCX format). We\ncould not compare these cases directly to the results obtained\nfrom VirusTotal. Therefore, we focus on the remaining 509\nmalicious documents in the evaluation.\n**Efficacy of Taint EIP Detection.** Taint-tracking detected\ntainted EIP execution in 477 out of the 509 documents. In\n19 cases of the undetected 32 cases, however, a new process\nwas spawned without it being detected by taint-tracking. We\ntreat these as false negatives in taint-tracking. We speculate\nthat this is likely to be due to missed direct flows, untracked\nindirect flows (via control dependencies, or table-lookups), or\nattacks using non-control-flow hijacking attacks (such as argument corruption). 13 documents did not lead to a tainted EIP\nexecution, but instead caused a memory fault. This could be\ndue to a difference in our test infrastructure and the victim’s,\nor an attempt to evade analysis. In 33 of the 477 cases where\ntainted EIP was detected, no new spawned process was created,\nand the tainted EIP instruction did not correspond to any shellcode. All these cases correspond to a particular instruction triggering the tainted EIP detection in MSO.DLL, a dynamic-link\nlibrary found in Microsoft Office installations. To understand\nthis case better, we manually created blank benign documents\nand fed them to Microsoft Office — they too triggered tainted\n\n2We believe 150 of these are due to user-interaction which we could\nnot presently automate, and the remaining could potentially be analyzed with a faster test platform; we plan to investigate this in the future.\n\n\n1\n\n\n0.9 56 60 3\n\n0.8\n\n0.7 140 70 8\n\n0.6\n\n0.5 23\n\n12\n\n0.4 257 16\n\n0.3\n\nUnknown 142\n\n0.2 Heuristic 5\n\n0.1 Multiple\n\nSingle\n\n0\nTainted Timeout Spawned Fault DOCX\n\n|477 295 19 13 13|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||24|||||||1||||\n|||56|||60||3||||||\n||||||||||||||\n||||||||||||||\n|||140|||70||||||8||\n||||||||||||||\n||||||23||||||||\n|||257|||||16||12||||\n||||||||||||||\n||Un|know|n||142||||||||\n||He Mu|uristic ltiple|||||||||5||\n||||||||||||||\n|Sin||gle|||||||||||\n\n\nFigure 12: Breakdown of dynamic taint-assisted analysis, and comparison to VirusTotal AV results. Single,\n_Multiple, Heuristics, and Unknown correspond to the dif-_\nferent AV tags assigned to documents. The main bars\nshow the detection result from BitBlaze: (a) Detected\nby Tainted EIP execution, (b) Timeout, (c) Spawned process without tainted EIP execution, (d) Memory Fault\nwithout tainted EIP execution, and (e) DOCX unable to\nrun in our analysis environment. Within each main bar,\neach stacked bar represents the corresponding tag given\nby VirusTotal.\n\nEIP detections. We treat these cases as false positives in taint\ndetection, possibly because of benign dynamic generation of\ncode. All the remaining cases (i.e., 444 out of 477) are legitimate exploits that we could confirm to execute shellcode.\n**Dynamic Taint versus VirusTotal. Figure 12 shows the de-**\ntailed comparison of taint-assisted classification of vulnerabilities versus the results from VirusTotal. Out of a total of\n477 documents on which tainted EIP was detected, VirusTotal\ntagged 397 documents with one or more CVEs. Of the remaining 80 cases that are detected by tainted EIP execution, 24 are\nundetected by VirusTotal, and 56 are detected, but marked Un_known (i.e., no CVE assigned) by VirusTotal. Dynamic taint_\nanalysis to determine the tainted EIP was helpful to further\nrefine the results of AV detection for a majority of these 56\ntagged-Unknown cases. Specifically, for 55 out of the 56 documents, taint-assisted manual analysis was able to resolve it to\nthe exploited CVE.\nOut of a total of 477 documents on which tainted EIP was\ndetected, VirusTotal tagged 397 documents with one or more\nCVEs. Our taint-assisted manual analysis agrees with the\nVirusTotal CVE tag results on 372 of these 397. That is, 372\ndocuments were detected to execute a tainted EIP for which we\ncould manually correlate to a single CVE that was the same as\nthe one reported by a majority of the AVs in VirusTotal.[3] Thus,\nfor a large majority of the cases, taint-assisted analysis agrees\nwith the AV results. Of the remaining 25 cases, 17 could be\nidentified as misclassifications because the CVE reported by\nmost of the AVs in VirusTotal was not the one that affected\nthe program. The 8 remaining documents were tagged by taint\nanalysis as being false positives even though a CVE was obtained from VirusTotal.\n\n3Note that different AVs often tag the same vulnerability with different tags in VirusTotal. We took the tag given by a majority of the\nreported tags, as the representative of the sample.\n\n\n-----\n\n## C Command and Control (C2) servers\n\nC2 # Emails C2 # Emails C2 # Emails C2 # Emails\n61.178.77.169 74 mzyzy.vicp.net 3 www.info-microsoft.com 2 googlehk.dynamicdns.co.uk 1\ndtl.dnsd.me 66 mygoodbug.dnsd.info 3 www.uyhanur.nna.cc 2 113.10.201.254 1\nns.dns3-domain.com 55 www.uyghuri.mrface.com 3 www.micosofts.com 2 152.101.38.177 1\ndtl.eatuo.com 44 6.test.3322.org.cn 3 100.4.43.2 2 blog.sina.com.cn 1\n202.85.136.181 32 218.82.206.229 3 61.234.4.214 1 uyghur.epac.to 1\nupdate.googmail.org 31 uyghur.sov.tw 3 a.yahoohello.com 1 xinxin20080628.gicp.net 1\ndtl6.mooo.com 29 3.test.3322.org 3 bc1516.7766.org 1 yah00mail.gicp.net 1\nwww.discoverypeace.org 26 newwhitehouse.org 3 202.68.226.250 1 hbnjx.6600.org 1\n58.64.172.177 22 goodnewspaper.f3322.org 3 msdn.homelinux.org 1 humanbeing2009.gicp.net 1\nemail.googmail.org 22 nskupdate.com 3 207.204.245.192 1 webhelp01.freetcp.com 1\nnews.googmail.org 22 webmonder.gicp.net 3 216.131.66.96 1 mobile.yourtrap.com 1\n61.128.122.147 17 61.132.74.68 3 www.avasters.com 1 125.141.149.23 1\nsoftmy.jkub.com 15 61.178.77.108 3 202.130.112.231 1 222.73.27.223 1\n61.234.4.213 13 betterpeony.com 3 nbsstt.3322.org 1 www.jiapin.org 1\ndnsmm.bpa.nu 11 4.test.3322.org 3 goodnewspaper.3322.org 1 ibmcorp.slyip.com 1\n121.170.178.221 10 61.234.4.210 3 webposter.gicp.net 1 182.16.11.187 1\nzeropan007.3322.org 10 9.test.3322.org.cn 3 uyghur1.webhop.net 1 star2.ksksz.com 1\nwwzzsh.3322.org 9 8.test.3322.org.cn 3 webwx.3322.orgxiexie.8866.org 1 69.197.132.130 1\n222.77.70.237 9 1.test.3322.org 3 125.141.149.49 1 www.yahooprotect.com 1\n3.test.3322.org.cn 8 radio.googmail.org 3 guanshan.3322.org 1 xiexie.8866.org 1\n1.test.3322.org.cn 8 7.test.3322.org.cn 3 leelee.dnset.com 1 img.mic-road.com 1\n2.test.3322.org.cn 8 tokyo.collegememory.com 2 uygur.eicp.net 1 photo.googmail.org 1\neemete.freetcp.com 8 201.22.184.42 2 kxwss.8800.org 1 tonylee38.gicp.net 1\napple12.crabdance.com 8 61.178.77.96 2 173.208.157.186 1 suggest.dns1.us 1\nwolf001.us109.eoidc.net 7 webproxy.serveuser.com 2 rc.arkinixik.com 1 worldview.instanthq.com 1\n4.test.3322.org.cn 7 www.bbcnewes.net 2 www.uusuanru.nna.cc 1 goodnewspaper.gicp.net 1\netdt.cable.nu 6 done.youtubesitegroup.com 2 uxz.fo.mooo.com 1 112.121.182.150 1\n205.209.159.162 6 alma.apple.cloudns.org 2 uygur.51vip.biz 1 abc69696969.vicp.net 1\nbr.stat-dns.com 6 webmailsvr.com 2 peopleunion.gicp.net 1 put.adultdns.net 1\n66.79.188.23 6 polat.googmail.org 2 free1000.gnway.net 1 loadbook.strangled.net 1\nwww.southstock.net 6 religion.xicp.net 2 uxz.fo.dnsd.info 1 internet.3-a.net 1\nns1.3322.net 5 connectsexy.dns-dns.com 2 wodebeizi119.jkub.com 1 news.scvhosts.com 1\n121.254.173.57 5 dns3.westcowboy.com 2 itsec.eicp.net 1 98.126.20.221 1\nwww.uyghur.25u.com 5 61.220.138.100 2 stormgo.oicp.net 1 mydeyuming.cable.nu 1\n202.96.128.166 5 27.254.41.7 2 boy303.2288.org 1 gshjl.3322.org 1\nns1.oray.net 5 116.92.6.197 2 webjz.9966.org 1 forever001.dtdns.net 1\njhska.cable.nu 5 apple12.co.cc 2 zbing.strangled.net 1 grt1.25u.com 1\ntest195.3322.org 5 58.64.129.149 2 tommark5454.xxxy.info 1 66.197.202.242 1\n61.234.4.218 5 worldmaprsh.com 2 oyghur1.webhop.net 1 kaba.wikaba.com 1\n61.128.110.37 5 phinex127.gicp.net 2 addi.apple.cloudns.org 1 221.239.96.180 1\nns1.china.com 5 wxjz.6600.org 2 60.170.255.85 1 174.139.133.58 1\na2010226.gicp.net 5 gecko.jkub.com 2 toolsbar.dns0755.net 1 125.141.149.46 1\nlogonin.uyghuri.com 4 smtp.126.com 2 61.132.74.113 1 frank.3feet.com 1\nmacaonews.8800.org 4 errorslog.com 2 113.10.201.250 1 115.126.3.214 1\nbook.websurprisemail.com 4 uyghurie.51vip.biz 2 home.graffiti.net 1 liveservices.dyndns.tv 1\ndesk.websurprisemail.com 4 tanmii.gicp.net 2 statistics.netrobots.org 1 inc.3feet.com 1\ntest.3322.org.cn 4 211.115.207.7 2 freesky365.gnway.net 1 1nsmm.bpa.nu 1\n221.239.82.21 4 59.188.5.19 2 greta.ikwb.com 1 www.yahooprotect.net 1\nliveservices.dyndns.info 4 206.196.106.85 2 englishclub.2288.org 1 222.82.220.118 1\n180.169.28.58 4 religion.8866.org 2 mm.utf888.com 1 webwxjz.3322.org 1\nportright.org 4 68.89.135.192 2 annchan.mrface.com 1 61.234.4.220 1\nvideo.googmail.org 4 blogging.blogsite.org 2 www.shine.4pu.com 1 thankyou09.gicp.net 1\nwww.guzhijiaozihaha.net 4 softjohn.ddns.us 2 copy.apple.cloudns.org 1 218.28.72.138 1\n207.46.11.22 4 report.dns-dns.com 2 220.171.107.138 1 soft.epac.to 1\nwww.googmail.org 4 115.160.188.245 2 uyghuri.mrface.com 1 www.yahooip.net 1\n2.test.3322.org 3 newyorkonlin.com 2 218.108.42.59 1 msejake.7766.org 1\ndcp.googmail.org 3 tw252.gicp.net 2 58.64.193.228 1 202.67.215.143 1\ntest.3322.org 3 61.222.31.54 2 tt9c.2288.org 1 www.yahoohello.com 1\nnp6.dnsrd.com 3 tomsonmartin.ikwb.com 2 forum.universityexp.com 1 202.109.121.138 1\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "5d2b9e7f-cf43-4b54-ba18-065aa3003611",
            "created_at": "2022-10-25T16:06:24.199525Z",
            "updated_at": "2022-10-25T16:06:24.199525Z",
            "deleted_at": null,
            "name": "CyberMonitor",
            "url": "https://github.com/CyberMonitor/APT_CyberCriminal_Campagin_Collections",
            "description": "APT & Cybercriminals Campaign Collection",
            "reports": null
        }
    ],
    "references": [
        "https://github.com/CyberMonitor/APT_CyberCriminal_Campagin_Collections/raw/master/2014/2014.08.13.TargetAttack.NGO/Targeted_Attacks_Lense_NGO.pdf"
    ],
    "report_names": [
        "Targeted_Attacks_Lense_NGO"
    ],
    "threat_actors": [
        {
            "id": "3cc6c262-df23-4075-a93f-b496e8908eb2",
            "created_at": "2022-10-25T16:07:23.682239Z",
            "updated_at": "2025-03-27T02:02:09.920109Z",
            "deleted_at": null,
            "main_name": "GhostNet",
            "aliases": [
                "GhostNet",
                "Snooping Dragon"
            ],
            "source_name": "ETDA:GhostNet",
            "tools": [
                "AngryRebel",
                "Farfli",
                "Gh0st RAT",
                "Gh0stnet",
                "Ghost RAT",
                "Ghostnet",
                "Moudour",
                "Mydoor",
                "PCRat",
                "Remosh",
                "TOM-Skype"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "d90307b6-14a9-4d0b-9156-89e453d6eb13",
            "created_at": "2022-10-25T16:07:23.773944Z",
            "updated_at": "2025-03-27T02:02:09.974695Z",
            "deleted_at": null,
            "main_name": "Lead",
            "aliases": [
                "Casper",
                "TG-3279"
            ],
            "source_name": "ETDA:Lead",
            "tools": [
                "Agentemis",
                "BleDoor",
                "Cobalt Strike",
                "CobaltStrike",
                "RbDoor",
                "RibDoor",
                "Winnti",
                "cobeacon"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "808d8d52-ca06-4a5f-a2c1-e7b1ce986680",
            "created_at": "2022-10-25T16:07:23.899157Z",
            "updated_at": "2025-03-27T02:02:10.019417Z",
            "deleted_at": null,
            "main_name": "NetTraveler",
            "aliases": [
                "APT 21",
                "Hammer Panda",
                "NetTraveler",
                "TEMP.Zhenbao"
            ],
            "source_name": "ETDA:NetTraveler",
            "tools": [
                "Agent.dhwf",
                "Destroy RAT",
                "DestroyRAT",
                "Kaba",
                "Korplug",
                "NetTraveler",
                "Netfile",
                "PlugX",
                "RedDelta",
                "Sogu",
                "TIGERPLUG",
                "TVT",
                "Thoper",
                "TravNet",
                "Xamtrav"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "254f2fab-5834-4d90-9205-d80e63d6d867",
            "created_at": "2023-01-06T13:46:38.31544Z",
            "updated_at": "2025-03-27T02:00:02.802896Z",
            "deleted_at": null,
            "main_name": "APT21",
            "aliases": [
                "HAMMER PANDA",
                "TEMP.Zhenbao",
                "NetTraveler"
            ],
            "source_name": "MISPGALAXY:APT21",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        },
        {
            "id": "e91dae30-a513-4fb1-aace-4457466313b3",
            "created_at": "2023-01-06T13:46:38.974913Z",
            "updated_at": "2025-03-27T02:00:02.967085Z",
            "deleted_at": null,
            "main_name": "GhostNet",
            "aliases": [
                "Snooping Dragon"
            ],
            "source_name": "MISPGALAXY:GhostNet",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        },
        {
            "id": "72aaa00d-4dcb-4f50-934c-326c84ca46e3",
            "created_at": "2023-01-06T13:46:38.995743Z",
            "updated_at": "2025-03-27T02:00:02.972623Z",
            "deleted_at": null,
            "main_name": "Slingshot",
            "aliases": [],
            "source_name": "MISPGALAXY:Slingshot",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        },
        {
            "id": "dabb6779-f72e-40ca-90b7-1810ef08654d",
            "created_at": "2022-10-25T15:50:23.463113Z",
            "updated_at": "2025-03-27T02:00:55.47619Z",
            "deleted_at": null,
            "main_name": "APT1",
            "aliases": [
                "APT1",
                "Comment Crew",
                "Comment Group",
                "Comment Panda"
            ],
            "source_name": "MITRE:APT1",
            "tools": [
                "Seasalt",
                "ipconfig",
                "Cachedump",
                "PsExec",
                "GLOOXMAIL",
                "Lslsass",
                "PoisonIvy",
                "WEBC2",
                "Mimikatz",
                "gsecdump",
                "Pass-The-Hash Toolkit",
                "Tasklist",
                "xCmd",
                "pwdump"
            ],
            "source_id": "MITRE",
            "reports": null
        },
        {
            "id": "cf7fc640-acfe-41c4-9f3d-5515d53a3ffb",
            "created_at": "2023-01-06T13:46:38.228042Z",
            "updated_at": "2025-03-27T02:00:02.775905Z",
            "deleted_at": null,
            "main_name": "APT1",
            "aliases": [
                "GIF89a",
                "G0006",
                "PLA Unit 61398",
                "Group 3",
                "TG-8223",
                "Comment Group",
                "ShadyRAT",
                "COMMENT PANDA",
                "Comment Crew",
                "Byzantine Candor",
                "Brown Fox"
            ],
            "source_name": "MISPGALAXY:APT1",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        },
        {
            "id": "f55c7778-a41c-4fc6-a2e7-fa970c5295f2",
            "created_at": "2022-10-25T16:07:24.198891Z",
            "updated_at": "2025-03-27T02:02:10.138587Z",
            "deleted_at": null,
            "main_name": "Slingshot",
            "aliases": [],
            "source_name": "ETDA:Slingshot",
            "tools": [
                "Cahnadr",
                "GollumApp",
                "NDriver"
            ],
            "source_id": "ETDA",
            "reports": null
        }
    ],
    "ts_created_at": 1666716499,
    "ts_updated_at": 1743041494,
    "ts_creation_date": 1404954652,
    "ts_modification_date": 0,
    "files": {
        "pdf": "https://archive.orkl.eu/fe2f8d32688a104ca4e6ba595f647dfa479ece44.pdf",
        "text": "https://archive.orkl.eu/fe2f8d32688a104ca4e6ba595f647dfa479ece44.txt",
        "img": "https://archive.orkl.eu/fe2f8d32688a104ca4e6ba595f647dfa479ece44.jpg"
    }
}