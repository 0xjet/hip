{
    "id": "2bc39ef2-af0c-4b74-bae1-4ebd9a0cbb81",
    "created_at": "2023-01-12T15:04:36.982805Z",
    "updated_at": "2025-03-27T02:05:20.144692Z",
    "deleted_at": null,
    "sha1_hash": "fc13f65f27828e1c025904cfe4ea6156737ad57c",
    "title": "2019-10-25 - The Deep Dive Malware Analysis Approach",
    "authors": "",
    "file_creation_date": "2022-05-28T19:25:01Z",
    "file_modification_date": "2022-05-28T19:25:01Z",
    "file_size": 102065,
    "plain_text": "# The Deep Dive Malware Analysis Approach\n\n**[agdcservices.com/blog/the-deep-dive-malware-analysis-approach/](https://agdcservices.com/blog/the-deep-dive-malware-analysis-approach/)**\n\nBy AGDC Services\n\nToday, we will review the primary approaches to malware analysis. Each approach is\ndiscussed and compared to one another to try and understand when you should use each\nmethod and why. We will show why the deep dive analysis approach is generally the most\noptimal and spend most of our time discussing this methodology. You should come away\nwith the knowledge of how to appropriately apply the deep dive analysis strategy to any file\nand tips on how to learn this skill even if you have limited training time.\n\n## Analysis Goals\n\nBefore deciding on an analysis approach, we must first understand the goals of malware\nanalysis. There are numerous goals possible. On some projects you will want to accomplish\nall of the goals, and others you will only focus on only a subset. The general goals are as\nfollows:\n\nIdentify Indicators of Compromise (IOC)\nDevelop advanced network (SNORT) signatures\nUnderstand the malware infection impact\nDevelop Yara signatures\nObtain complete and accurate analysis results\n\nThese goals can be accomplished in any order, but they are listed in the most efficient order.\nIOC’s are necessary for almost every analysis project and typically are the highest priority.\nDeveloping high fidelity network signature comes next because it can help with protecting the\nnetworks from the malware even if the malware network infrastructure changes. It can also\nhelp in providing attribution of the malware if you find new network infrastructure being used\nwhich is already attributed. Next you often want to understand the malware infection impact.\nThis isn’t identifying an exhaustive understanding of all the malware capabilities, but more of\nan overview of what the malware could have done within the network. For example, does\nthe malware contain spreading capabilities, can it exfil files from the system back to the\nCommand and Control (C2) server, etc. The next goal of an analyst is often to develop Yara\nsignatures. This comes after understanding the infection impact so that you can design the\nbest signatures. Yara signatures should ideally be built from code blocks which are difficult\nfor the actor to change. If you perform this step before reviewing the majority of the binary,\nyou won’t identify the most unique code blocks to use in your signature. By waiting until after\nyou skimmed over the entire code base of the malware, you can choose the spots in code\nwhich will be the most painful for the actor to change, leading to the best Yara signatures.\nFinally, your last goal is to obtain complete and accurate analysis results. This goal is not\n\n\n-----\n\nperformed all of the time. Very often, an 80% solution in half of the time is better than a 100%\nsolution which takes twice as long. This goal involves filling in any gaps that you came\nacross previously. When you were reviewing the infection impact, you might have made\nsome educated guesses that you weren’t 100% confident about. This step involves reviewing\nthose guesses to make an absolute determination. This goals is typically only performed by\nanalysts who are writing in-depth analysis reports on the malware.\n\n## Alternative Approaches\n\nThere are three primary analysis approaches:\n\nTriage analysis\nFocused Debugging\nDeep Dive Analysis\n\nTriage analysis generally doesn’t review any actual code, or if it does, it is done minimally.\nThis analysis approach usually performs environmental monitoring. You will set up a known\nenvironment, run the malware, and observe any changes to the system. It is a combination\nof performing an environment baseline comparison and physically watching for indicators\nsuch as network call outs in real time.\n\nThe Focused Debugging approach is centered around debugging. You may perform some\nsimple analysis on the malware characteristics to identify expected behaviors, e.g. view the\nthe strings and imports, but then you will run the malware inside a debugger to discover the\ndetails. The method revolves around knowing which library functions, or API’s, are typically\nused for different purposes and putting breakpoints on those functions. When the\nbreakpoints are hit, you can observe the arguments and memory dumps to capture the\nIOC’s. You generally are only doing minimal reading of the assembly code. You will read the\ncode around the functions of interest, but usually not outside of that. The focus is on\ndebugging, identifying where in the code you want the malware to get to, and how to make it\nget there. This approach is much more advanced than triage. You can work around most\nanti-debugging tricks and also identify additional indicators that may be in memory, but only\nused as a backup, etc. These additional indicators will often be missed by triage analysis.\n\nDeep Dive Analysis is the last approach. This approach centers around static analysis, i.e.\nreading the assembly code. You also use debugging, but only in a targeted manner. Your\ngoal is to treat the assembly as source code, no different than if you were reading C or\nPython code. Your general understanding of what the malware is attempting to do will come\nfrom static analysis. The actual details may come from targeted debugging. For instance,\nreading the assembly may indicate a function is decrypting a URL to communicate to. You\nmay not spend the time to statically reverse engineer the decryption routine, but instead you\ncould use the debugger to simply decrypt the string of interest to uncover the URL. What the\nmalware was doing came from static analysis, the details came from debugging.\n\n\n-----\n\n## Typical Usages\n\nNow that we understand the three main approaches, when is each approach typically\nused?\n\nTriage analysis has two main use cases, for incident responders where an infection is\nongoing and IOC’s are needed quickly to stem the bleeding, and for newer analysts who are\nstill learning and don’t know enough yet to dive into a debugger. Triage analysis often will\nonly get the surface indicators and may miss things like backup C2 domains, be defeated by\nanti-sandbox techniques, etc. This approach really isn’t appropriate when higher confidence\nresults are needed, but it does have its place.\n\nFocused debugging is probably the approach used by the majority of experienced analysts.\nYou are able to somewhat quickly find most of the major indicators and you have the\nopportunity to identify and circumvent anti-sandbox techniques. It takes a bit more practice\nand education to learn what API’s are important, how to read some assembly code, and to\nunderstand common programming patterns malware uses so that you know where to focus\nyour debugging on. If you were to compare this to typing, you can think of this as the “hunt\nand peck” type of typing where you use two fingers. With some practice, you can type at a\ndecent speed. You will never approach the speed of someone who uses all ten fingers, but\nyou can get up to a decent speed much quicker with minimal practice.\n\nDeep Dive analysis is the last approach. This is used by a smaller number of experienced\nanalysts, most often only those who need to perform exhaustive reviews of the malware to\nput out reports. You can identify all IOC’s with this methodology and will easily identify and\nwork around any anti-analysis techniques used. For our typing analogy, this approach is\ncompared to a typist who uses all ten fingers and has put the time in to practice and reach\nthe top speeds.\n\n## The Dilemma\n\nNow we come to a dilemma. We noted that focused debugging is used by the highest\nnumber of experienced malware reverse engineers, but we also contend that the deep dive\napproach is the most efficient. The question is “does common usage translate to the most\noptimal choice”? Our answer is a resounding no. There are reasons focused debugging is\ncommon, which we will get into, but we will also demonstrate why deep dive analysis is a\nmuch more efficient approach to use.\n\n## Focused Debugging Advantages\n\nFocused debugging is so prevalent for two main reasons.\n\n\n-----\n\nThe first reason is debugging is relatively easy. Your focus isn t on fully understanding\nwhat’s going on, but only trying to find the target areas where you might find IOC’s. Once\nyou’ve learned the main, high-valued API’s, focused debugging is almost like a find feature.\nYou simply put breakpoints on all the high-value API’s, run the debugger, and wait until a\nbreakpoint it hit. Once you’re there, you examine the surrounding code / memory and extract\nIOC’s. You don’t need a complex strategy or full understanding of what happened. You just\nneed to know the main breakpoints to set. There are only so many API’s that malware\ncommonly use, so even if you are just guessing, you can often get to the answer.\n\nThe second reason is focused debugging gives you answers which are “good enough”.\nThey may not be 100% complete, but what you have probably isn’t wrong. You can think of\nyour answers with this strategy as the 80% answer. Your results are going to be useful and\nyou won’t be embarrassed with your output, so many analysts don’t feel the need to go\nfurther than that. If you can block 80% of the malware with relatively minimal training, that\nbecomes a pretty popular strategy.\n\n## Deep Dive Advantages\n\nNow that we know why focused debugging is so common, what makes deep dive analysis so\noptimal? Why should you choose this method?\n\nThe deep dive approach is generally the most optimal methodology you can use to find all of\nthe IOC’s in the shortest time frame. Deep dive analysis relies heavily on reading the code\nthrough static analysis. Static analysis is going to be far quicker and get more accurate\nanswers than just debugging. Additionally, you can read and understand functions statically\nthat you either can’t reach with a debugger, or are very difficult to get to. That’s a good\nreason to choose the deep dive approach, but what’s backing up that statement?\n\nUltimately, malware analysis is just source code review. Only instead of reading C code,\nyou’re reading assembly. So let’s ask our original question in a slightly different context. If\nsomeone gave you source code to analyze and determine its functionality, would you use\ntriage, focused debugging, or the deep dive approach? In this situation, the answer is\nobvious. You would read the code statically, and parts you didn’t quite understand, you might\nrun in a debugger to examine the memory contents. That is the deep dive approach. If you\ntried to debug the entire program to understand its functionality, it would take forever and\nyou’d probably be out of a job.\n\nAnother reason that supports the deep dive methodology is really a point against focused\ndebugging. Debugging by itself provides an answer without context. Without the context of\nwhere you are in a program and how you got there, it’s very easy to misinterpret the results.\nYou can also easily miss indicators. Because you are focused just around the high-value\nAPI’s, you may not have realized that there was an alternative path just prior to that highvalue API which led to a backup indicator. Or maybe you’re in the backup back and\ncompletely missed the primary indicator.\n\n\n-----\n\nThe biggest reason deep dive analysis is so efficient is that you can use tools optimally.\nYou’re not limited to static analysis or to debugging, you have a mastery of each and use\neither of the tools where it makes the most sense. You will spend the majority of your time\nreviewing the code statically to understand what is going on. Understanding a capability is\ngenerally independent of the arguments. For example, it is very easy to statically review a\nfunction to see that it is searching a directory to delete a file. You may not know what file is\nbeing searched for, but that doesn’t impact your ability to recognize a file deletion capability\nin any way. When you need the actual indicator of what file is being searched for, you can\nswitch to the debugger to view the memory contents to see the file path being searched.\nThat is the power of the deep dive approach. You are never blindly searching the debuggers\nmemory. You are always using it in a targeted manner to identify an item of interest. The flip\nside of using the debugger in a targeted manner is there isn’t a need to step through the\nentire program in a debugger. You can jump around to just where you want to debug without\nneeding to have the debugger follow the full normal path to get you there. That will save an\nenormous amount of time.\n\nThe last benefit is that you don’t need to worry about VM snapshots, save points, etc.\nBecause most of your time is spent reviewing the code statically, you don’t have to worry\nabout the state of the memory. All you are doing is marking up the code with your comments\nto make it easier to understand. If you are stuck on one file, you can just open up another to\nreview, taking a break from the first. There is no need to revert your VM’s to another\nsnapshot, worry about accidentally reverting a machine and losing your work, etc. It sounds\nlike a small thing, but it’s surprising how much the wasted time of reverting snapshots and\ntrying to remember which snapshot was in what state adds up to. In addition to the wasted\ntime, it also takes you out of the analysis mindset and you lose a little extra time trying to\nremember exactly what you were doing prior to reverting, etc.\n\nTo summarize, the deep dive approach is the really the approach you use to review source\ncode. Our goal is to treat assembly as just another programming language, no different than\nC or Python. You will want to apply the same techniques you use to review source code to\nreviewing malware.\n\n## The Disconnect\n\nIf the deep dive approach is so much more efficient than focused debugging, why do the\nmajority of experienced analysts use the focused debugging methodology? Where’s the\ndisconnect?\n\nThe answer is pretty simple. The deep dive analysis approach has a much steeper learning\ncurve before it becomes more effective. Essentially, it’s the static analysis learning curve.\nReading assembly is no different than learning to read any other foreign language. You have\nto spend hours learning the alphabet, then learn to sound out words, and eventually you will\nbe able to read. Once you can read, you still need to build your vocabulary. If you never put\n\n\n-----\n\nin the practice, static analysis will be error prone and inefficient. But if you put the time in to\npractice, you will learn to read the language without mistakes, and finally you will learn to\nskim. Once you can skim assembly code, static analysis becomes really effective.\nConversely, identifying high-value API’s for focused debugging can be done very quickly with\nminimal practice.\n\nTo further compound the problem, most jobs don’t provide the time for you to spend\npracticing static analysis. Answers are demanded immediately and as long as you get an\nadequate answer, it doesn’t matter if there was a better answer that you could have provided\nif you only had the training. Given the time pressures most jobs put on analysts, they are\noften forced into the focused debugging approach whether they want to use it or not.\n\nThe last disconnect is that many think static analysis is only needed if you are required to\nunderstand every aspect of the malware, e.g. provide a full in-depth report. That’s an\nunfortunate misconception. It misses the fact that static analysis can be tailored. Just like you\ncan skim a book to find key elements, you can skim assembly code to find the indicators, to\nprioritize what to review, etc. The misconception comes from the steep learning curve\nrequired. Analysts believe that because it is so slow in the beginning, it must be that slow all\nof the time and only appropriate when you have to really dig deep into the malware.\n\n## What Deep Dive Analysis Isn’t\n\nLet’s dispel a few of the common myths about static analysis. Many think static analysis is\ndigging down deep into the assembly to understand what every instruction is doing,\nunderstanding every possible path through the binary. Static analysis is not “viewing the\nmatrix”. You aren’t concerned with reviewing every instruction. You aren’t even concerned\nwith reviewing every function. You don’t need to understand every line of code to understand\nwhat’s going on, you only need to understand the key elements.\n\n## What Deep Dive Analysis Is\n\nNow we now what deep dive analysis isn’t, what is it then? Ultimately, deep dive analysis is\nsource code review. It’s a mix of static and dynamic analysis, but generally more focused on\nstatic. Static provides the what, dynamic provides the details. Deep dive analysis is looking\nat the binary holistically to understand the file at a targeted level. The key here is targeted\nlevel. You can target a full in-depth level to understand every function, or you can target a\ntriage level to just understand the basic program flow. You also want to always keep in mind\nthat you should be viewing the file holistically so you have the full context. You should never\ncarve out specific parts of the file to look at individually. You lose so much context by not\nunderstanding how the target component fits within the rest of the file. You’re likely to either\nmake mistakes, or spend much more time than is needed if you would have stepped back\n\n\n-----\n\nand looked at the target component as just one part of the whole file. Deep dive analysis\nshould always be looking at the file holistically so that you have the largest amount of\ninformation possible to help with your analysis.\n\n## The Deep Dive Analysis Algorithm\n\nFinally onto the deep dive algorithm itself. We’ve talked around it, discussing some benefits,\nmisconceptions, what it is and isn’t, but we haven’t defined the actual steps yet. The\napproach can be visualized by the graphic below.\n\nThe first step is to identify the main capabilities function. This may not be the actual main\nfunction the actor wrote, but could be a sub function within the main. First, we have to\nremember that the address of entry point in a file isn’t the main function the actor wrote. The\ncompiler adds code prior to the actor main which allocates some memory, initializes global\nvariables, parses the command line, calls the actor main, and finally exits. Sometimes your\ndisassembler will recognize the actual actor main and bypass the compiler generated code,\nbut sometimes it won’t. So first, we have to parse the compiler generated startup code to find\nthe actor main. Next, we evaluate the actor main to determine if it contains the main\ncapabilities loop of the file. Often times it will, but other times the actor main will perform\nsome trivial capability or call a sub function or thread, and that sub function will contain the\nbulk of the malicious code. We are interested in analyzing the malicious capability and aren’t\nconcerned with a trivial function that only spawns a thread, etc.\n\nNow that we have identified the main capabilities function, we want to build an outline of the\nmain program flow. Remember that we are trying to understand the main program flow at an\noutline level, not an in-depth view. At this stage, we simply want a map of how the program\nworks so that we know what components make up the file. Is a configuration file read,\npersistence set, networking activity performed, C2 commands executed, etc. You want to\nspend a minimal amount of time in this step. Just long enough to understand enough about\neach primary step of the file so that you can decide what you want to dig into during the next\nstep.\n\nWith an outline of our main program flow, you now have a map of the file. The next step is to\ndecide if further reverse engineering is needed. If you’re just doing triage, more reverse\nengineering may not be needed. An outline of the program gives you enough information to\nidentify approximately where the IOC’s will be performed so that you can view the code and\nextract the IOC’s.\n\nIf we want a more in-depth understanding, we move onto the next step and choose an area\nfrom our outline to dig into. You can choose any area to RE further, but a few primary areas\nof interest follow:\n\nDetermine persistence methodology\nIdentify network protocol\n\n\n-----\n\nSkim C2 commands to get general idea of impact of infection\nPerform in-depth analysis of C2 commands\nIdentify optimal areas to build Yara signatures\n\nAlso note that while any area can be reverse engineered in any order, different goals can\nmake certain orders of analysis make the most sense. For example, if you have limited time\nbut want higher fidelity IOC’s, it would make the most sense to skim the C2 commands prior\nto trying to generate Yara signatures. Because of the limited time, you probably won’t want\nto perform an in-depth review of the C2 commands, but if you don’t spend a little time to\nreview the C2 capabilities, your Yara rules will be much more limited. Yara signatures are\nthe most effective when they are built off of code blocks which are difficult for the actor to\nchange. By skimming the C2 commands, you can quickly look for unique code blocks in\neach command function that would lead to high fidelity Yara signatures and be very difficult\nfor the actor to circumvent.\n\nOnce you’ve identified the area to reverse engineer further, the next step is obviously to\nreverse engineer the target area. Once you’ve finished the analysis, you go back to your\nchoice of deciding what area you want to reverse engineer next.\n\nAfter you’ve analyzed all the areas of interest, the next step is to actually collect the IOC’s.\nIn the previous steps, you’ve identified all of the locations where IOC’s would be generated.\nYou know where persistence is set, you’ve worked through the network protocol, identified\noptimal areas for Yara signatures, etc. Now you actually turn that information into IOC’s. You\nconvert the network protocol into SNORT signatures, build the actual Yara signatures, etc.\n\nThe last step in the deep dive methodology is to report your findings. If you analyze a file\nand don’t report the findings, what’s the purpose of the analysis? Like every other step of\nthe deep dive process, the important part of this step is to tailor it. You don’t always need to\nproduce a full, publishable, report on your findings. If you are performing triage, combining\nthe indicators in a readable format along with the associated file details may be enough. If\nyou performed an in-depth review, you may need a multi-page report. The goal of this step is\nto combine whatever findings you have come up with into a releasable format that can\nprevent another analyst from needing to duplicate your work.\n\n## Warnings\n\nLet’s discuss two caveats that are key to effectively performing deep dive analysis.\n\nThe first is that your understanding of what is happening should come from static analysis. If\nyou are using a debugger to understand the what, you aren’t using the approach correctly.\nWhat the file is doing, whether it’s searching a directory, performing network communication,\nsetting persistence, etc. doesn’t depend on the inputs. You can identify network\ncommunication without knowing what the final address is. You can identify a directory is\n\n\n-----\n\nbeing searched to delete a file without knowing which file or directory is being targeted. The\ndebugger is orders of magnitude slower than static analysis. If you are spending time in the\ndebugger trying to identify the what, you are wasting huge amounts of time for no reason.\n\nIn some ways, deep dive analysis is fairly structured. You need to take a step back and\nidentify your goals instead of trying to figure out everything at once. Most often analysis is\ndone iteratively. You will make a first pass statically to identify an outline of what’s going on.\nThen you will make another pass on the areas of interest to dive in deeper. Now you will\nhave a solid understanding of what’s going on. Then you use the debugger, if needed, to\nview the memory and identify actual values. If you try to do all those steps at once, you wind\nup going too deep into the weeds and it makes understanding much more difficult, and as a\nresult, much more time consuming.\n\nThe second caveat is that dynamic analysis should always be targeted and be used only for\nverification and to identify memory components. This doesn’t mean you may not have to rely\non the debugger in the beginning while you are learning. But your goal is to increase your\nskills so that the debugger is only used for its intended purpose; to step through a program\nviewing and manipulating memory. Debugging should be used in any situation where you\nwant to know a specific value, but are having a hard time identifying statically. There may be\nan API call that is dynamically resolved, but the API name is encrypted. That’s a perfect\nsituation to rely on the debugger. You may need to see a list of URLs which are being\nrandomly chosen. The debugger is a great tool to examine the memory and view the entire\nlist without having to wait for the program to randomly select every possible URL.\n\n## Insights\n\nHere are a number of insights to help you better perform deep dive analysis.\n\nThe first insight is perhaps the most important. You always want to strive to analyze the file\njust as the programmer wrote it without jumping around. Malware is a computer program,\nand computer programs have a logical flow. Malware is no different. By following the logical\nflow of the code, you have the most context to help you understand what’s going on. If I\nwere to give you the number 32 and ask what the next number in the sequence is, you\ncouldn’t answer. But if I gave you the sequence 1, 2, 4, 8, 16, 32 and asked you what’s next,\nyou’ll likely pick 64. You want to use that same context when analyzing malware. If you jump\naround the code, you lose all of the context and it becomes harder to understand what’s\ngoing on, harder to understand the importance of variables, etc. By reading the code in the\nlogical order, you can make educated guesses about what should come next. It’s always\neasier to confirm your suspicion rather than blindly determine what a random piece of code is\ndoing.\n\nAnother insight is the goal is to understand a function’s purpose, and that should always be\nkept in mind. If you’re reading a book and you don’t understand a specific word, can you still\nunderstand the sentence, the paragraph? Most often yes. And the same is true with\n\n\n-----\n\nunderstanding what a function is attempting to do. You don t need to focus on every\ninstruction within the function. Look for key junctions and control flow to understand what’s\ngoing on. You may see a loop that is comparing two string values with an eventual exit\ncondition. Often, all you need to understand is that you’re attempting to find a target string. It\nwon’t matter what the string is, or possibly even what the string represents. The important\nnote is that you are searching for a target string. There is a high likelihood that some other\npart of the function will provide context as to what the string represents.\n\nThat brings us to our next insight. A functions purpose is primarily identified through library\ncalls. In windows files, API calls perform all of the heavy lifting. Probably 95% of all\nfunctionality is performed through a call to a library function. By understanding what API\ncalls are used, in what order, and with what control logic, you basically reverse engineered\nthe pseudocode of the function. Most of the assembly instructions are used to support the\nAPI calls They move around the variables in memory, build appropriate strings, setup the API\narguments, but they rarely do meaningful work. The primary use of the assembly\ninstructions are to get the environment setup for the API calls.\n\nThere are some exceptions to this. Library calls can be statically compiled into the binary and\nbe unrecognized as an API call. The API call can be inlined with the actual assembly code.\nAnd a select number of functions can be manually performed straight through assembly\ninstructions. These are all the exceptions though. 90% of recognizing a functions purpose is\nby matching the order the API calls are performed to a known algorithm. If you aren’t able to\nrecognize the 10% of the exceptions listed, it means you will be missing some of the library\ncalls, which will make identifying the algorithm more difficult. Not impossible, but definitely\nharder.\n\nAn important insight is everyone starts off thinking malware code is very sneaky and will hide\nwhat it’s trying to do, send you down false paths, etc., but that’s not the case. Malware rarely\ntries to hide its purpose. If you see a function that looks like it’s performing a specific\ncapability, it probably is. If you don’t quite understand how the program flow gets to that\nfunction, just know it probably does at some point. Malware authors don’t spend a lot of time\ntrying to trick the reverse engineer. There are exceptions to this, namely packed code and\nobfuscated code, but that’s more of an exception. Packed code is more geared towards\nhiding from AV scanners. Once you unpack the code, the functions are straightforward like\nall other malware.\n\nThe one true exception is that some malware will actually be put through an obfuscation\nprogram. This could add a lot of garbage code that doesn’t do anything, add calls that aren’t\ngermaine to the program’s flow, and even add whole functions that never get used. This is a\nsolid exception to the rule, but luckily only a very small percentage of x86-64 files uses this\ntechnique. Managed code, like .NET files, where you can decompile the code directly back\nto the original source code will be put through obfuscation programs more as a rule than the\nexception. But x86-64 code where you are looking at assembly instructions rarely implement\nthe obfuscation techniques discussed here\n\n\n-----\n\nA critical insight, somewhat obvious on the surface but which has a deeper meaning, is the\nmore functions you recognize, the less guesswork is needed. Malware analysis is a mixture\nof art and science. You often won’t recognize every line of code, every function. There will be\ngaps in your understanding, but you still need to make a determination of the intended\npurpose. A good analogy is solving a jigsaw puzzle. Science lets you turn over the puzzle\npieces. The more pieces you have turned over, the easier it is to recognize the picture. What\norder you turn the pieces over, how you identify the puzzle when you have a pixelated\npicture with blank spots is the art. This insight is specifically geared towards learning to\nrecognize common library functions which are optimized by the compiler.\n\nThere are three main ways a function can be included in a file, through an external API call,\nstatically compiled, and inlined. An external API call is easy, you will have the API name and\nthere is no guesswork. A statically compiled function will have the entire library function\nplaced into your file. These will often be recognized by your disassembler tool through a\nsignature database, though not always. The third way is to be inline optimized by the\ncompiler. In this case, instead of having a separate function which is called, the compiler\ntakes the bulk of the library code and places it directly inline with all of the other assembly\ninstructions. That means instead of having a call to a function which might be recognized\nand labeled as a library call, you simply have the assembly instructions. On top of that, some\nlibrary functions will be optimized and different assembly instructions from what’s in the\noriginal API function will be used. The replacement instructions perform the same capability,\nbut in a more optimized manner. In both of these cases, because there is no actual function\ncalled, there will be no signature database to identify the function. From the disassemblers\nperspective, there is no difference between an inline function and regular assembly\ninstructions. That means it is up to you to recognize the assembly instructions as a library\nfunction as you are reviewing the code. If you don’t recognize the inline function, you will not\nhave the complete information about what is being done inside the function you are\nreviewing.\n\nAs we previously stated, the bulk of the work inside a program is performed through API\ncalls. So if you miss API calls because they are inlined and not recognized, you will be\nmissing some portion of the work being performed. For our puzzle analogy, this means you\nwill have more blank spaces in your puzzle. You may still be able to recognize the picture,\nbut it could be significantly more difficult depending on what the inline functions are.\n\nOur next insight is that static comprehension is primarily pattern recognition. Luckily for us,\npattern recognition doesn’t depend on matching a pattern line for line. You only need to\nmatch the key points. The key points for programming algorithms are the library calls and the\ncontrol flow, i.e. the conditional logic such as if statements, for and while loops. The library\ncalls are the most important. Often times, you can identify a functions purpose simply by the\norder of API’s called. If you see the API’s, OpenProcess VirtualAllocEx,\nWriteProcessMemory, and CreateRemoteThread without knowing anything else, you can\nalmost certainly say the function is performing DLL Injection. To be more confident, you\n\n\n-----\n\nwould check the control flow to make sure the API s are called in the appropriate order and\nall are used in the same control path. If one or two of the API’s are used only in error\nconditions, then it may change the functions purpose. Outside of a few niche categories like\ncryptography and unpacking, the assembly instructions are there mostly to set up the\nenvironment for the API calls and don’t add significant information for determining a functions\npurpose. The end result is 80% of learning to statically analyze a function comes down to\nlearning a large set of programming algorithms and then comparing the API’s you see in a\nfunction against your known algorithms.\n\nOur last insight is about your tools. Disassembler signatures of library functions will fail at\nsome point. If you pay for premium disassemblers and always stay current with the latest\nrelease, your tool will recognize all of the statically compiled library functions that are\npossible. But even then, it will not recognize all of them. Disassemblers recognize statically\ncompiled library functions based on signatures. Unfortunately, there are just so many ways\nlibrary functions can be changed by the compiler based on optimization settings and updates\nto the library code themselves. The disassembler will never recognize them all. What\nhappens very often is that the tool signature will be updated for a new version of a library\nfunction. But then that library function will be updated more frequently then your tool and it\nwill cease to be recognized.\n\nFortunately there is a silver lining to this. While tool signatures have a hard time recognizing\nslightly modified library calls, people do not. The small changes that disrupt a disassembler\nwill probably go unnoticed to you. That means you want to use your disassembler signatures\nto manually learn to recognize statically compiled library functions so that you can recognize\nthem yourself when the tool fails you.\n\nThis is very simple and doesn’t need to take a lot of time. All you need to do is to take a\nquick scan of statically compiled library functions when they are recognized and labeled by\nthe disassembler. Look primarily at the graph view, and then look to see if there are any\nidentifiable constants or strange instructions that you don’t often see. These aspects rarely\nchange much between versions. Don’t spend a lot of time trying to memorize these\nfunctions, just take a quick look. The more times you see the library call when you know what\nit is, the better the chance is you will remember it when you see it and the tool doesn’t\nrecognize it.\n\nIn this case, volume is key. If you only see the statically compiled rand function once, you\nprobably won’t identify it on your own later on. But if you see it 20 times over the course of 6\nmonths, there is a very good chance your brain will make that needed connection and on the\n21st time, you will be able to recognize it with or without a label. As we’ve mentioned,\nidentifying the API calls is key to understanding malware capability. A lot of time and money\ngoes into building disassemblers. Use that to your advantage to learn from them so that you\ncan identify all of the possible API calls to make your job as a reverse engineer much easier.\n\n\n-----\n\n## Tips For Learning With Limited Time\n\nUp until this point, we’ve discussed the different analysis approaches, identified why deep\ndive analysis is most optimal but why it’s not the most common, and explained the approach\nalong with warning and insights to make the best use of it. While the approach may sound\ngreat on paper, you may still be in the position where your job just doesn’t provide the time to\nbuild up the static analysis skills necessary to make deep dive analysis optimal. To that point,\nwe will discuss a few methods to build the deep dive skills with limited time to practice.\n\nOur first tip is to get your answers first the way you know how, then review statically. People\nlearn in funny ways. In the beginning, code will most often be pretty confusing when you look\nat it statically without already knowing what it’s doing. If you spend time using any method\nyou know of to understand what’s going on, and then look back at it statically, it’s surprising\nhow much clearer the assembly will look. It’s basically the same as reviewing a solution\nguide at that point.\n\nYou won’t learn it as quickly compared to if you spent the time to really struggle through\nstatically, but you can do this in much smaller chunks of time. If you just try to read unknown\ncode statically, it could take you hours to understand a target function. If you don’t have the\nhours to spend to get to the answer, you never really know if you were reading it correctly\nand it’s not conducive to learning. If you can spend the hours to get a definitive answer, your\nbrain will make the strongest connections and you will learn it in the shortest amount of total\ntime. But that’s only if you have the hours to spend.\n\nWhen you don’t have that long, it’s much better to understand the capability dynamically,\nthen review statically. It will be much clearer then. You will have to do this a number of times\nbefore you will make a permanent connection in your brain so that you can understand the\ncode purely by static analysis. But when your job demands quick answers, it’s often more\nfeasible to spend 10, thirty minute chunks of time over a longer period reviewing code where\nyou know the answer rather than spending 3 hours straight. The 3 hours will be less total\ntime, but the 30 minute chunks of time are much easier to fit into a busy schedule.\n\nThis tip is critical. You have minimal time to practice. You have a small break and are\nmotivated to dive into a piece of malware to practice your skills, but all you have available\nare simplistic samples that won’t challenge you. Now your small training window has passed\nand you didn’t get a chance to hone your skills. That’s why you should always keep a training\nindex. Each file you work, you should try to learn something new, dig a little bit deeper than\nthe last time. But there will be many, many files where you don’t understand everything. The\ncrypto is a little too advanced, the networking protocol is too complicated. All of those files\nshould be recorded in your training index along with a note of what interesting aspect they\ncontain. Now you have another 30 minute window where you are free and want to get better\nat reversing cryptography routines. You pull out your training index and search on crypto and\nfind half a dozen files that have crypto routines in them that you weren’t quite able to\n\n\n-----\n\nunderstand. You choose the first one and spend your time digging into the routine. Your 30\nminutes weren’t wasted this time and you improved your tradecraft in the precise area you\nwere interested in.\n\nIf you don’t keep a training index, you will be at the mercy of whatever you have available\nwhen you have the time. That’s not a recipe for success. You should always keep notes of\nwhat you worked on with the interesting capabilities. In the beginning, they can be used to\npractice. Later, once your skills have progressed, they can be used as examples to help\nteach others the skills. There is no greater way to learn a skill in-depth than to teach it to\nsomeone else.\n\nThe next tip is that it’s quicker to learn programming versus learning to reverse engineer\ncode. This is somewhat counter-intuitive at first. It’s common to think that you must practice\nreverse engineering assembly to become a better malware analyst, but that’s not 100%\naccurate. One skill, and the easiest, is reading the assembly. Once you’re comfortable\nreading and skimming assembly, the more important skill is recognizing what programming\ncapability the assembly is attempting to implement. And that comes down to knowing how to\nprogram a wide range of capabilities.\n\nWhat that means is if you have limited time to practice, you will build your static analysis\nskills by learning a multitude of ways to program common malware capabilities. This can be\ndone in much shorter blocks of time because there are far more resources dedicated to\nprogramming, so you can easily find good source code to study. The larger your knowledge\nof programming capabilities is, the easier it will be to recognize what capability the assembly\nis attempting to perform in malicious files. Often times, the difference between an\nintermediate reverse engineer and an advanced analyst is simply the breadth of capabilities\nthey are aware of and are able to recognize.\n\nThe last piece of advice is an age old concept that applies to learning any new skill. Simply\npractice every chance you get. Don’t be concerned with how much time you have or if you\nwill make a breakthrough. Learning reverse engineering, particularly learning to read\nassembly, is very much like learning to read a foreign language. You will have to sound out a\nword X number of times before your mind will make the connection and you begin to\nrecognize it on sight. You will sound out the same word dozens of times, making slow\nprogress and the same mistakes each time. Finally, it will just click and you will recognize it.\nThis makes practice frustrating because you aren’t making constant progress. Your progress\ncomes in steps. You will feel like you aren’t making any progress for a while, then you will\nmake a big jump forward. Then no progress for a while, then a big jump forward. Those\nsmall windows where you can take 15 or 30 minutes to practice may not seem like you are\nmaking progress, but it is instrumental to your brain forming the new connections needed to\nmake that big leap forward. The bottom line is it will take many dozens of hours to learn this\nskill. If you don’t take the time to practice, you will be forced to rely heavily on debugging\nand guessing at what’s going on instead of simply being able to know what is going on.\n\n\n-----\n\n## Summary\n\nThat brings us to the end of learning the deep dive malware analysis approach. Let’s leave\noff with a summary of a few key points we went over.\n\nThe deep dive approach is a mix of static and dynamic analysis, but more focused on static.\nThe what should come from static, the details can come from dynamic. It’s important to use\nthe right method in the right circumstance to get optimal results. Don’t fall into the trap of\ntrying to debug everything because it’s easier to understand when you see real values and\ncan watch the exact code progression. Take a step back and remember the details don’t\nchange what’s going on and the “what” really is 90% of what you need to understand. Once\nyou know how persistence is being set, how the network protocol is set up, determining the\ndetails is often trivial. When you don’t know those whats, you’re searching for a needle in a\nhaystack. You can find it with enough time and effort, but it won’t be efficient.\n\nOn the flip side, if you’re trying to identify a specific value that’s being built in memory, static\nanalysis may not be the best approach. If the value is being decrypted, concatenated with\nother values, copied around into different variables, then debugging is the right approach.\nThe key is targeted debugging. Don’t step through the entire program just to find one value.\nUse your understanding of how the program is set up gained from static analysis and jump\nright up to the point you need to observe the targeted value. When debugging, you shouldn’t\nbe single stepping too much. You should either be changing the EIP to get right to the\nlocation you need, or setting key breakpoints at a target location because you know the\nbinary will reach that point. Combining static and dynamic analysis is where you will get the\nmost efficiency in your approach.\n\nStatic analysis will provide you more accurate answers far quicker than debugging, but only\nafter you get good at it. There is a far steeper learning curve to static analysis before it will\nbe more efficient than focused debugging. If you want to reach the expert levels, you really\nhave to get good at static analysis. It’s easy in the beginning to think debugging is superior.\nStatic analysis seems so slow and error prone because you’re not good at it yet. It’s hard to\nimagine it would ever be best. Think back to grade school when you were learning algebra.\nIt is so hard in the beginning when you just have symbols and no real numbers to look at.\nYou often would have to put in fake numbers just to understand what was going on. But if\nyou really took the time to work on it and get good at algebra, you eventually got to the level\nwhere the actual numbers just slowed you down. You could solve more problems quicker\nwith algebra than if you had to always fill in the numbers. Algebra is like static analysis and\nusing discrete numbers is like debugging.\n\nDon’t get discouraged, progress is not linear. Take the time to practice and you won’t be\ndisappointed. It really comes down to belief. If you know the end result is you will attain a\nmastery that’s not possible using focused debugging, it’s easier to stick with the practice in\nthe beginning. It will be discouraging. You’ll feel like you aren’t making any progress, but I\ncan assure you that you are. Tell yourself everyday that progress is not linear. If you are\n\n\n-----\n\nputting in the time, you will be getting better. You may just not realize it. Keep a history of\nfiles you worked on and look back at them after 6 months and you will see how far you have\ncome. What was incredibly complex 6 months ago will be much more understandable today.\n\nThe last thought I want to leave you with is to come back to comparing this deep dive\napproach to learning to read a foreign language. Focused debugging is similar to buying a\ntravel phrase book. You will be able to ask where the bathroom is and get around a city in a\nrelatively short order. It can allow you to visit the main tourist sites on your own. Deep dive\nanalysis is like becoming fluent in the language. Now you can visit the tourist locations, but\nyou can also read the signs to get more background information. You can talk to the locals\nand get tips on hidden gems that aren’t in the guide books. You can travel the country\noutside the main attractions to see everything and not be limited just to the tourist traps. All\nyou have to do is spend time to master this approach and you can understand everything\nabout a binary, or you can quickly skim a file to find the IOC’s and move on. The choice will\nbe yours, but you will gain the knowledge to actually have that choice.\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2019/2019-10-25 - The Deep Dive Malware Analysis Approach.pdf"
    ],
    "report_names": [
        "2019-10-25 - The Deep Dive Malware Analysis Approach.pdf"
    ],
    "threat_actors": [
        {
            "id": "faa4a29b-254a-45bd-b412-9a1cbddbd5e3",
            "created_at": "2022-10-25T16:07:23.80111Z",
            "updated_at": "2025-03-27T02:02:09.985067Z",
            "deleted_at": null,
            "main_name": "LookBack",
            "aliases": [
                "FlowingFrog",
                "LookBack",
                "LookingFrog",
                "TA410",
                "Witchetty"
            ],
            "source_name": "ETDA:LookBack",
            "tools": [
                "FlowCloud",
                "GUP Proxy Tool",
                "SodomMain",
                "SodomMain RAT",
                "SodomNormal"
            ],
            "source_id": "ETDA",
            "reports": null
        }
    ],
    "ts_created_at": 1673535876,
    "ts_updated_at": 1743041120,
    "ts_creation_date": 1653765901,
    "ts_modification_date": 1653765901,
    "files": {
        "pdf": "https://archive.orkl.eu/fc13f65f27828e1c025904cfe4ea6156737ad57c.pdf",
        "text": "https://archive.orkl.eu/fc13f65f27828e1c025904cfe4ea6156737ad57c.txt",
        "img": "https://archive.orkl.eu/fc13f65f27828e1c025904cfe4ea6156737ad57c.jpg"
    }
}