{
    "id": "bef5aee6-6802-4a4f-80d6-47351cb15b7d",
    "created_at": "2023-01-12T15:04:45.908106Z",
    "updated_at": "2025-03-27T02:16:25.640437Z",
    "deleted_at": null,
    "sha1_hash": "006a378f90b3535c99c64ff7c4a7ad4fecf4886c",
    "title": "2022-03-11 - Part 2- LockBit 2.0 ransomware bugs and database recovery attempts",
    "authors": "",
    "file_creation_date": "2022-05-27T23:12:42Z",
    "file_modification_date": "2022-05-27T23:12:42Z",
    "file_size": 1702967,
    "plain_text": "# Part 2: LockBit 2.0 ransomware bugs and database recovery attempts\n\n**techcommunity.microsoft.com/t5/security-compliance-and-identity/part-2-lockbit-2-0-ransomware-bugs-and-database-**\nrecovery/ba-p/3254421\n\nMarch 11, 2022\n\n[In Part 1 of this series (which you can find here), we provided background about our analysis](https://techcommunity.microsoft.com/t5/security-compliance-and-identity/part-1-lockbit-2-0-ransomware-bugs-and-database-recovery/ba-p/3254354)\nof the LockBit 2.0 ransomware and described our suspicions that \"faulty crypto\" was at play.\nIn this post, we will outline the issues that the decryptor poses and how we simply cannot\ntrust it and must remove it from any equation we intend on using to successfully decrypt\nthese database files.\n\n**_Disclaimer: The technical information contained in this article is provided for general_**\n_informational and educational purposes only and is not a substitute for professional advice._\n_Accordingly, before taking any action based upon such information, we encourage you to_\n_consult with the appropriate professionals. We do not provide any kind of guarantee of a_\n_certain outcome or result based on the information provided. Therefore, the use or reliance_\n_of any information contained in this article is solely at your own risk._\n\n### If only it were so easy…\n\nOur [earlier Procmon observations identified the encryptor randomly encrypting 65k bytes](https://techcommunity.microsoft.com/t5/security-compliance-and-identity/part-1-lockbit-2-0-ransomware-bugs-and-database-recovery/ba-p/3254354)\n_after it was only supposed to encrypt the first 4k. So, while we do successfully decrypt the_\n_intended encrypted region of the encrypted file, which is the first 0x1000 bytes, we fail to_\nidentify and decrypt the unintended regions which are splattered throughout the nowdecrypted file due to the bug we’ve outlined in the encryptor.\n\n\n-----\n\nAnd as this is a customer-provided file, we don t have the luxury of a Procmon or TTD trace\nto quickly identify the corruption. To tackle this problem, we instead crafted an algorithm that\nwill be outlined shortly, that can scan the encrypted file and identify all regions of unintended\nencryption.\n\nFigure 12. Example of our first\n\nimplementation of the algorithm that identifies regions of unintended encryption\n\n\n-----\n\nFigure 13. Valid data and corrupted data\n\nIn case it’s not clear by now, patience, the willingness to remain calm and wait, seems to be\na virtue that is prioritized in blocking I/O. Due to the LockBit 2.0 developers not giving this\nvirtue its due diligence, it gets worse for us in the regard that the decryptor itself suffers from\n_the exact same bugs as the encryptor. It fails to handle STATUS_PENDING states; it falsely_\nassumes all NTSTATUS errors/non-successes values are signed. To put it much more\nsuccinctly, we cannot trust the decryptor.\n\n\n-----\n\n_Figure 14. Part of the decryptor code that illustrates the trust issues that we have with it_\n\nBecause of suffering from the identical misconceptions as the encryptor, when decrypting the\ndatabase file that ended up having the appearance of being correctly decrypted, it in actuality\n_further corrupted the file trying to decrypt regions that were never encrypted to begin with!_\n\nThese random regions further complicate the situation for us and now force us to deal with\nthem. Or do they? Fixing the encrypted unintended regions that were a result of the\nencryptor is a logical step; fixing the newly encrypted regions from software that is solely\nresponsible for decrypting is not. So, to make our lives easier we took the logical high road\nand decided to make our own decryptor.\n\n\n-----\n\n### Encryption overspill and rebuilding database files\n\nBefore outlining our decryptor and the details of the algorithm alluded to earlier, we must\npoint out yet another subtlety that must be addressed. Due to the unpredictable behavior the\nencryptor is capable of we are facing further issues, outside of the encryption procedure\nitself, of potentially irrecoverable corruption. The best way to see this is to have some\nsemblance of the underlying structures involved for a .ndf file, which is the format of the\ndatabase files that we had to work with. The understanding of this structure, at least the\nessential parts relevant to us, serves as the basis for our recovery algorithm.\n\nFor our purposes, it suffices to understand that for every 0x2000 bytes, we have what are\ncalled pages. Each page begins with a header that is 0x60 bytes in size. Pages can also be\nclassified as empty; 0x2000 bytes full of 0’s.\n\nThe header contains valuable metadata that we can leverage to identify areas of corruption.\nUpon careful examination and side-by-side comparison of all the .ndf files that we had to\nwork with, we were able to uncover three relevant properties in the header that would serve\nas the cornerstone of our recovery algorithm.\n\n\n-----\n\nFigure 15. Illustration of all three properties as shown in a hex editor\n\nThe PageType field identifies the type of that individual page, which from our understanding\ncan either be a 1 (an occupied page) or a 0 (unoccupied/empty page). The PageIndex\nproperty identifies the current page and its location within the database file.\n\nSo, ”Page 0” would be at “index 0”; “Page 1” would be at “index 1”, and so on. It is a way to\nget to individual pages inside the .ndf file. And speaking of the database file itself, what\nfollows the PageIndex is yet another unique value that serves to identify the entire .ndf file as\na whole. In the above case this is indicated by the value “4”, but other database files had “3”\nas a value here instead. What we care about, which is being able to identify the integrity of\neach individual page that we come across, is that this is a value that we know must be\nconstant throughout each page for each database file we are processing.\n\nFrom having a sufficient understanding of the page header, we can construct an algorithm to\nverify the integrity of each individual page, which in turn allows us to also identify any\npotential corruption to any of the pages. We can iterate from the start of the file at 0x2000\n\n\n-----\n\n(page sized) increments and inspect the validity of each header. Wherever we don t have a\nvalid header, we at least know that something is going on at that location which we can\ninvestigate further as needed.\n\nFor example, if we wanted to verify that a specific page is valid, we ascertain that the first\nbyte is either a 1 or a 0, and if it is a 1, we go to the 0x20 offset from the start of the header,\npull out the 4-byte value there, and calculate whether the PageIndex value matches the\noffset to the start of the page header. We also further validate that the database identifier is\nconsistent throughout.\n\nFigure 16. Calculating the PageIndex value\n\nIf none of the above conditions are satisfied, then we are looking at corrupted data and we\ncan begin to programmatically identify all these areas. In our case, these were almost\nexclusively the unintended encrypted regions we outlined earlier.\n\nIf the above conditions are indeed satisfied, we know that we have a valid page at its correct\nlocation, so we can note that as well.\n\nWhere the conditions are half satisfied is where it gets interesting i.e., we pass the PageType\nand Database Identifier check but the PageIndex value doesn’t match the offset to the start\nof the page header. We classify these as a misaligned header because the PageIndex\nvalue is pointing to the location of where this header is supposed to be:\n\n\n-----\n\nFigure 17. Identifying a misaligned header (off by 0x1000 bytes)\n\nAlso, whenever we hit a zero page (PageType == 0), we can safely ignore and continue.\n\n### Moving closer to the ultimate goal: successfully restoring all encrypted database files\n\nIn the sections described above, we discussed the commonality that all of these database\nfiles share: their file format. We outlined the characteristics of an algorithm that can validate\nthe integrity of these database files and categorized four types of classifications by\nleveraging our understanding of how a .ndf database file is supposed to be structured. This,\nin theory, should be able to deal with all intended and unintended corruption the encryptor\nand decryptor are known to impose.\n\nNow it's time to put this theory and understanding into practice and build upon this algorithm\nto achieve our ultimate goal: the successful restoration of all encrypted database files.\n\n### Recovering encrypted and corrupted database files\n\nWith the stage now set given that all known underlying issues have been exposed, we\napproached the problem in the following manner.\n\n1. Identify and decrypt (fully) any encrypted database files with our homemade decryptor\n2. Process the output of step 1 and account for any misaligned page headers accordingly\n3. Process the output of step 2 and “clean up” the final remnants of leftover data from the\n\nmisaligned headers\n\n\n-----\n\n**Step 1. Identify and decrypt (fully) any encrypted database files with our homemade**\n**decryptor**\n\nWe come back to our homemade decryptor now. The details of how our homemade\ndecryptor works under the hood are not as relevant as understanding how we’re going to\nleverage it. More important is being able to identify all the encrypted regions throughout the\nfile and not letting the modified LockBit 2.0 decryptor loose on it to further destroy it.\n\nBut the primary structure of our decryptor is to identify, decrypt, and extract the necessary\ninitialization vector and AES key for the encrypted file, and then utilize this information to\n[carry out the AES decryption through the mbedtls library, which is exactly the same 3 partyrd](https://github.com/ARMmbed/mbedtls)\nlibrary that the Lockbit 2.0 developers are using.\n\nFigure 18. AES decryption through\n\nthe mbedtls library\n\nThe approach we took to finding the encrypted regions was outlined earlier and revolves\naround what a valid, misaligned, or null page is expected to look like. We further build on this\n[with our decryptor by adding Shannon entropy checks on buffers of 0x1000 bytes in size.](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication)\nAny buffer that has a very high level (>= 7.8), we will decrypt and then further validate the\ndecrypted data based on what a page header constitutes.\n\nTaking advantage of the fact that a .ndf file is so “well” structured i.e., every 0x2000 bytes will\nalways follow a guaranteed format, we can run this algorithm to identify every encrypted\nregion within the file and successfully decrypt it. Further validation after the decryption is also\nrequired because .ndf file formats unsurprisingly house compressed data which can flag on\nour entropy scan. We need to ignore all these cases and leave them as is.\n\nBelow is the successful output for step one. Also note, for one file the distance between\nencrypted regions is at 0xA000000 intervals, whereas the other is at 0x17570000 intervals.\nAgain, the effects of the unpredictable nature of malformed asynchronous I/O, which do not\npose a threat anymore.\n\n\n-----\n\n-----\n\n**Step 2. Process the output of Step 1 and account for any misaligned page headers**\n**accordingly**\n\nNow that we can successfully decrypt the files, we need to account for any headers that are\nmisaligned. We saw how to do this earlier by comparing the PageIndex field inside the page\nheader. This is the index value that identifies where this particular page needs to be inside\nthe .ndf file. Refer to the misaligned header on Figure 17.\n\nSimilar to how we found all the encrypted regions, we will proceed in the same manner\n(excluding the Shannon entropy check this time) of validating each expected page header,\nand in any instances where there is a case of misalignment, we will create a new file where\nwe correctly insert it at its expected location. We will, of course, copy over all the already\nvalid and existing data into the new file as well. This new file will then be fully decrypted and\nmore importantly, correctly aligned.\n\n\n-----\n\nFigure 19. Validating misaligned headers\n\n\n-----\n\nFigure 20. Before and after header alignment\n\n**Step 3. Process the output of step 2 and “clean up” the final remnants of leftover data**\n**from the misaligned headers**\n\nThis is great and all and certainly brings us very close to fully realizing our ultimate goal,\nhowever, the data still present at the misalignment location is just that: still present. We need\nto do something about this leftover data.\n\nOne approach is to place “dummy” headers at these locations, in the hopes they satisfy the\nloading of the database file. But playing the dummy roles ourselves, we opted to just null\nthese locations. Again, we follow the same pattern of validating the headers, but this time we\nknow there cannot be any more misalignment, so for any leftover data that we encounter we\nsimply null that entire page, making it in effect a null/empty page. Naturally, this loses the\ndata there but is a willing compromise to make since these entries at this state should be far\nand few in between compared to the enormity of the entire database file.\n\nCombining all three steps outlined above led to the full restoration of the MSSQL database\nfiles to the extent that was possible, even reverting their functionality back to normal in the\nmajority of cases. Throughout our analysis, we also had an internal MSSQL subject matter\nexpert (SME) continuously verify our undertakings and found around 7 million\ninconsistencies with the initial, corrupted files, give or take, down to just single thousand\ndigits after the entirety of our restoration process was completed. Conjuring up SQL queries\nbecame possible once again, and although at DART [we prefer our KQL, we still carry a](https://techcommunity.microsoft.com/t5/security-compliance-and-identity/leveraging-the-power-of-kql-in-incident-response/ba-p/3044795)\nfondness for our SQL predecessors.\n\n## Conclusion\n\nLockBit 2.0 is one of the leading ransomware strains currently active and has been over the\nlast six months. DART became engaged with a particular customer where we were exposed\nto our first instance of a Lockbit 2.0 afflicted customer, curiously interested in the plausibility\nof recovering their corrupted database files. Through the combined efforts of this customer\nand DART, we were able to successfully satisfy the customer’s curiosity and in doing so,\noutlined the implications “buggy code” can have, and given the right set of circumstances,\n\n\n-----\n\ncan paradoxically become a catalyst to make recovery of destroyed, critical database files a\nreality, even though it was the original culprit responsible for corrupting them in the first\nplace.\n\nIt is typical in [incident response engagements for incident responders to identify the full](https://www.microsoft.com/security/blog/2020/03/09/real-life-cybercrime-stories-dart-microsoft-detection-and-response-team/)\nfunctionality of any collected samples, extract all relevant forensic evidence that can further\nfacilitate the ongoing investigation, all while having proper detections in place. However, we\nsimply cannot overlook our ultimate goal as cybersecurity consultants: that of satisfying the\nneeds of our customers, who as any organization victim to a devastating cyber attack, is\nseeking the right guidance and support. If those needs are within our means, we have a\nresponsibility to act on them.\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2022/2022-03-11 - Part 2- LockBit 2.0 ransomware bugs and database recovery attempts.pdf"
    ],
    "report_names": [
        "2022-03-11 - Part 2- LockBit 2.0 ransomware bugs and database recovery attempts.pdf"
    ],
    "threat_actors": [
        {
            "id": "b740943a-da51-4133-855b-df29822531ea",
            "created_at": "2022-10-25T15:50:23.604126Z",
            "updated_at": "2025-03-27T02:00:55.505366Z",
            "deleted_at": null,
            "main_name": "Equation",
            "aliases": [
                "Equation"
            ],
            "source_name": "MITRE:Equation",
            "tools": null,
            "source_id": "MITRE",
            "reports": null
        },
        {
            "id": "75108fc1-7f6a-450e-b024-10284f3f62bb",
            "created_at": "2024-11-01T02:00:52.756877Z",
            "updated_at": "2025-03-27T02:00:55.544216Z",
            "deleted_at": null,
            "main_name": "Play",
            "aliases": null,
            "source_name": "MITRE:Play",
            "tools": [
                "Nltest",
                "AdFind",
                "PsExec",
                "Wevtutil",
                "Cobalt Strike",
                "Playcrypt",
                "Mimikatz"
            ],
            "source_id": "MITRE",
            "reports": null
        }
    ],
    "ts_created_at": 1673535885,
    "ts_updated_at": 1743041785,
    "ts_creation_date": 1653693162,
    "ts_modification_date": 1653693162,
    "files": {
        "pdf": "https://archive.orkl.eu/006a378f90b3535c99c64ff7c4a7ad4fecf4886c.pdf",
        "text": "https://archive.orkl.eu/006a378f90b3535c99c64ff7c4a7ad4fecf4886c.txt",
        "img": "https://archive.orkl.eu/006a378f90b3535c99c64ff7c4a7ad4fecf4886c.jpg"
    }
}