{
    "id": "91280f56-bda1-4737-aeba-654b65bd637d",
    "created_at": "2023-01-12T15:09:34.366134Z",
    "updated_at": "2025-03-27T02:05:47.697175Z",
    "deleted_at": null,
    "sha1_hash": "3333f7eca8f995a3fa688501978c6a53fbb02d3a",
    "title": "2022-04-01 - BERT Embeddings- A Modern Machine-learning Approach for Detecting Malware from Command Lines (Part 2 of 2)",
    "authors": "",
    "file_creation_date": "2022-05-28T17:42:07Z",
    "file_modification_date": "2022-05-28T17:42:07Z",
    "file_size": 525438,
    "plain_text": "# BERT Embeddings Part 2: A Modern ML Approach For Detecting Malware\n\n**[crowdstrike.com/blog/bert-embeddings-new-approach-for-command-line-anomaly-detection-part-2/](https://www.crowdstrike.com/blog/bert-embeddings-new-approach-for-command-line-anomaly-detection-part-2/)**\n\nCristian Popa April 1, 2022\n\nA novel methodology, BERT embedding, enables large-scale machine learning model\ntraining for detecting malware\nIt reduces dependency on human threat analyst involvement in training machine\nlearning models\nBidirectional Encoder Representation from Transformers (BERT) embeddings enable\nperformant results in model training\nCrowdStrike researchers constantly explore novel approaches to improve the\nautomated detection and protection capabilities of machine learning for Falcon\ncustomers\n\n[CrowdStrike data science researchers recently explored and experimented with the use of](https://www.crowdstrike.com/blog/bert-embeddings-new-approach-for-command-line-anomaly-detection/)\nBidirectional Encoder Representation from Transformers (BERT) for embedding command\nlines, focusing on anomaly detection, but without detailing the model itself. Diving deeper into\nthat research, CrowdStrike researchers explain the reasons for using BERT for command\nline representation and how to train the model and assess its performance.\n\n\n-----\n\nThe purpose of this experimental research was to leverage self-supervised deep learning\nmethods to obtain better representations of the string fields that show up in CrowdStrike\nFalcon® telemetry. CrowdStrike constantly tests the latest advancements in the field to see\nwhether they fit the existing machine learning toolkit. This research ultimately demonstrates\nthat a deep learning approach for embedding strings of this nature (command line) is feasible\nand can produce satisfactory results.\n\n## Defining Objectives\n\nAn embedding is a representation of an input, usually into a lower-dimensional space.\nAlthough embeddings are in scope for various input types, string embeddings are a common\nchoice for representing textual inputs in numeric format for machine learning models. An\nessential trait of embedding models is that inputs similar to each other tend to have closer\nlatent space representations. This similarity is a consequence of the training objective, but\nresearchers are often interested in lexical similarity when strings are involved.\n\nThis experiment aimed to better represent string fields encountered in a large stream of\nevent data. The focus was on two such fields: “command line” and “image file name.” The\n[first field was discussed extensively in the previous blog — it consists of an instruction that](https://www.crowdstrike.com/blog/bert-embeddings-new-approach-for-command-line-anomaly-detection/)\nstarts a process that is then recorded as an event and sent to the CrowdStrike Security\nCloud. The second field is the name for the executable starting the process, which is a\nsubstring of the corresponding command line. Two main factors dictated the pursuit of such\nan embedding model: First, we aimed to improve the string processing in our models, and\nsecond, we wanted to benefit from the significant developments achieved in the area of\nnatural language processing (NLP) in the last few years.\n\n## Building the Model\n\n### Data\n\nThe experiment started by collecting data for training the model from events related to\nprocess creation. To ensure variety, it was collected from all of the supported platforms\n(Windows, Mac, Linux) and sampled from long spans of time to ensure that the processes\n[are not biased by temporary events (e.g., the Log4j vulnerability).](https://www.crowdstrike.com/blog/crowdstrike-services-launches-log4j-quick-reference-guide/)\n\n### Model Architecture\n\n[For the model architecture, BERT was the primary candidate. The end-to-end model consists](https://arxiv.org/pdf/1810.04805.pdf)\nof two main components that will be discussed separately: the tokenizer and the neural\nnetwork (which is what people generally refer to when talking about BERT).\n\nA tokenizer is a simple mapping from strings, called tokens, to numbers. This numeric\nrepresentation of the inputs is necessary because the BERT neural network, like any other\nmachine learning algorithm, cannot use textual data directly in its computations. The\n\n\n-----\n\ntokenizer s job is to find the optimal tokens given a vocabulary size, which, in this case, was\n30,000 tokens. Another advantage of using a tokenizer is that unknown strings from the\ntraining set can still be composed out of tokens learned from other strings. This is important\nbecause English has a well-defined set of words, while command lines can theoretically\nfeature any character combination. In Figure 1, there is an example of a tokenized command\nline in the data set.\n\nFigure 1. Example of a tokenized command line\n\nThe BERT model is an extensive neural network that relies on an NLP concept called “self[attention mechanism.” The concept, introduced by Vaswani et al., 2017, has gained](https://arxiv.org/pdf/1706.03762.pdf)\nsignificant traction in the research community, to the point where almost all modern NLP\nsolutions use it. To briefly explain how it works, Figure 2 shows how tokens passed through\nthe network pay attention to other relevant tokens in the input. BERT can compute the\nattention of tokens and use it to build an understanding of the language. Another important\nconcept used by BERT is masked language modeling (MLM). This is the training objective\nused by the network. Tokens in the input are randomly masked, and the model has to predict\nthe initially masked tokens.\n\n\n-----\n\nFigure 2. The “it” token in the sentence “The animal didn’t cross the street because it was too tired” comes\n\ninto focus. The attention is supposed to make sense from a syntactic point of view and is used in machine\n\nlearning models. Source: https://jalammar.github.io/illustrated-transformer/\n\nThe MLM objective allows for self-supervised learning, meaning that researchers do not\nneed to explicitly label data, but instead they only need the input strings themselves. This\ntechnique presents an advantage for cybersecurity as it removes the need to involve threat\nanalysts in this initial training step.\n\nDiving deeper, there are some appropriate training steps for BERT: pre-training and fine**tuning. First, the model is pre-trained using MLM on a large dataset consisting of command**\nlines and image file names. The model is then fine-tuned with a malware classification\nobjective, on a different dataset, for example. With this approach for pre-training, it becomes\neasy to collect a large dataset from which the model will learn the representation of the data.\nHence, the second phase requires significantly fewer labeled samples in the dataset.\nLearning from smaller amounts of labeled data constitutes an obvious advantage that\nbecomes applicable in this case. Additionally, the first step is task-agnostic, meaning that the\npre-trained model can be fine-tuned later for any task needed — malware classification,\nmalware family identification, anomaly detection, etc.\n\n## Experiments\n\nAfter getting the model ready for training, one of the first steps is sifting through the data for a\ndiverse subset of samples because the tens of billions of events collected from Falcon\ntelemetry were too many for training BERT from scratch in a reasonable amount of time\n\n\n-----\n\nSmall embedding models were repeatedly trained briefly to identify the samples they were\nperforming worse on, excluding them from the subset.\n\nAfterward, modeling efforts mainly revolved around finding the right hyper-parameters. Focus\nwas placed on the performance for malware classification using a holdout set from the finetuning data, as this was a good measurement of the added benefit of using the embeddings\nover previous features.\n\nThe hyper-parameter that brought the most significant improvement was the change to the\nmaximum number of tokens that can get into the BERT model. This is relevant because\nwhile image file name strings, which are shorter, would often fit fine into the default token\nlimit, command lines would not. As a result, a huge chunk of information is lost due to the\ntruncation of the tokens, which was intended to bring all inputs to the same size. Increasing\nthis limit to a calibrated value was crucial in the modeling process. The other experiments\nfocused on the embeddings’ size and the number of hidden layers in the neural network.\nOptimal values were found for them according to the evolution of the classification metrics.\nFigures 3 and 4 show the fine-tuning process and its rationale.\n\nFigure 3. Classification performance while varying the hidden (embedding) size\n\n\n-----\n\nFigure 4. Classification performance while varying the number of hidden layers\n\nIn the end, our experiment resulted in an embedding model whose embeddings seem on par\nwith the original features for two of CrowdStrike’s existing machine learning models that used\nstrings. These models use not only command lines or image file names for classification, but\nalso other fields coming from the event data, such as execution flags and start times. Results\ncan be observed in Figures 5 and 6.\n\n\n-----\n\nFigure 5. Weighted* true positive rate (TPR) and false positive rate (FPR) for different versions of one of\nour classification models. “Finetuned_v5” is the version using the embeddings from our latest (and best)\n\nfine-tuned BERT model, while “v3” is an earlier version of the model.\n\n- Note: “Weighted” means that the frequency of samples in the dataset is accounted for when computing\n\nthe metrics, as the data used is guaranteed to contain duplicates\n\n\n-----\n\nFigure 6. TPR vs. FPR for another one of our classification models\n\nOne observation was that the fine-tuned model shows better performance, which makes\nsense because it was specifically trained to separate between clean and dirty samples.\n\n## Future Research Opportunities\n\nFuture areas of research interest involve training an end-to-end neural network for\nclassification that incorporates BERT for the string-type features. Currently, the embeddings\nare computed and used in our gradient boosted tree models. Putting everything together into\na single neural network would probably improve the efficiency of the BERT embeddings, as\nthey are trained along with the rest of the features. There is significant potential that the\n\n\n-----\n\nembeddings would be better used by a deep learning algorithm than a tree-based one, since\nthe latter makes predictions based on higher or lower features than a trained value, while the\ndeep learning algorithm can process the input more freely.\n\nCrowdStrike researchers constantly explore novel approaches to improve the efficacy and\nthe automated detection and protection capabilities of machine learning for customers.\n\n**Additional Resources**\n\n_[Learn more about the CrowdStrike Falcon® platform by visiting the product webpage.](https://www.crowdstrike.com/endpoint-security-products/falcon-platform/)_\n_Learn more about CrowdStrike endpoint detection and response on the Falcon Insight_\n_webpage._\n_[Test CrowdStrike next-gen AV for yourself. Start your free trial of Falcon Prevent™](https://go.crowdstrike.com/try-falcon-prevent.html)_\n_today._\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2022/2022-04-01 - BERT Embeddings- A Modern Machine-learning Approach for Detecting Malware from Command Lines (Part 2 of 2).pdf"
    ],
    "report_names": [
        "2022-04-01 - BERT Embeddings- A Modern Machine-learning Approach for Detecting Malware from Command Lines (Part 2 of 2).pdf"
    ],
    "threat_actors": [],
    "ts_created_at": 1673536174,
    "ts_updated_at": 1743041147,
    "ts_creation_date": 1653759727,
    "ts_modification_date": 1653759727,
    "files": {
        "pdf": "https://archive.orkl.eu/3333f7eca8f995a3fa688501978c6a53fbb02d3a.pdf",
        "text": "https://archive.orkl.eu/3333f7eca8f995a3fa688501978c6a53fbb02d3a.txt",
        "img": "https://archive.orkl.eu/3333f7eca8f995a3fa688501978c6a53fbb02d3a.jpg"
    }
}