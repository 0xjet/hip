{
    "id": "73a2a787-7354-41ea-8d65-0d91c7cdf7eb",
    "created_at": "2023-01-12T15:06:10.850323Z",
    "updated_at": "2025-03-27T02:05:29.175437Z",
    "deleted_at": null,
    "sha1_hash": "eed8938a335897b01911f778bec91e781b5981ab",
    "title": "2021-08-13 - When Malware Changes Its Mind - A Study of Variable Program Behaviors",
    "authors": "",
    "file_creation_date": "0001-01-01T00:00:00Z",
    "file_modification_date": "2021-08-01T18:48:09Z",
    "file_size": 1392041,
    "plain_text": "# When Malware Changed Its Mind: An Empirical Study of Variable Program Behaviors in the Real World\n\n#### Erin Avllazagaj, University of Maryland, College Park; Ziyun Zhu, Facebook; Leyla Bilge, NortonLifeLock Research Group; Davide Balzarotti, EURECOM; Tudor Dumitras, University of Maryland, College Park\n\n##### https://www.usenix.org/conference/usenixsecurity21/presentation/avllazagaj\n\n## This paper is included in the Proceedings of the 30th USENIX Security Symposium.\n\n#### August 11–13, 2021\n\n##### 978-1-939133-24-3\n\n\n#### Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.\n\n\n-----\n\n### When Malware Changed Its Mind\n\n#### An Empirical Study of Variable Program Behaviors in the Real World\n\n##### Erin Avllazagaj, Ziyun Zhu[+], Leyla Bilge[*], Davide Balzarotti[†], Tudor Dumitras, University of Maryland, College Park\n_+Facebook_\n_*NortonLifeLock Research Group_\n_†EURECOM_\n\n\n##### Abstract\nBehavioral program analysis is widely used for understanding\nmalware behavior, for creating rule-based detectors, and for\nclustering samples into malware families. However, this approach is ineffective when the behavior of individual samples\nchanges across different executions, owing to environment\nsensitivity, evasive techniques or time variability. While the\ninability to observe the complete behavior of a program is a\nwell-known limitation of dynamic analysis, the prevalence of\nthis behavior variability in the wild, and the behavior com_ponents that are most affected by it, are still unknown. As_\nthe behavioral traces are typically collected by executing the\nsamples in a controlled environment, the models created and\ntested using such traces do not account for the broad range\nof behaviors observed in the wild, and may result in a false\nsense of security.\nIn this paper we conduct the first quantitative analysis\nof behavioral variability in Windows malware, PUP and benign samples, using a novel dataset of 7.6M execution traces,\nrecorded in 5.4M real hosts from 113 countries. We analyze\nprogram behaviors at multiple granularities, and we show how\nthey change across hosts and across time. We then analyze\nthe invariant parts of the malware behaviors, and we show\nhow this affects the effectiveness of malware detection using\na common class of behavioral rules. Our findings have actionable implications for malware clustering and detection, and\nthey emphasize that program behavior in the wild depends\non a subtle interplay of factors that may only be observed at\nscale, by monitoring malware on real hosts.\n\n##### 1 Introduction\n\nThe ability to understand and model malware behavior plays\na key role in many security applications. This typically involves executing samples inside an instrumented environment, designed to collect system and API call traces that\ncan be further analyzed to reconstruct the runtime behavior.\nSuch behavioral analysis methods have been applied to detecting [10,12,16,23,24,32] new or polymorphic malware for\n\n\nwhich static analysis fails [37,48], and to clustering samples\ninto malware families [5, 7, 40, 41], in order to identify the\nmalicious behaviors that characterize each family. However,\nthe effectiveness of all these methods depends on their ability\nto identify invariant parts of the behavioral traces. In consequence, variations in the observed malware behavior, which\nmay arise from adversarial intent [6,20] or biases in the data\ncollection [43], can result in models that overfit the analysis\nenvironment and fail to generalize to the behavior observed\nin the wild. This problem, which is a consequence of the\nlimitations of dynamic analysis, is widely accepted among\nresearchers and practitioners as a fundamental challenge for\nbehavioral analysis.\nUnfortunately, just how much the behavior of malware\n_varies in the wild is a largely open question, outside of a_\nfew prominent and well studied malware families. A common\napproach to accounting for behavior variability is to acquire\nmultiple samples of the same family and to analyze their executions together, in order to extract the common behavior\npatterns of the malware family. However, if the behavior of in_dividual samples varies, across different hosts and across time,_\nthe common patterns extracted will not be representative of\nthe malware’s behavior on real hosts. Additionally, the behavioral traces are typically collected by executing the malware\nin a controlled environment [1,2,17,52], in order to prevent\nit from harming other hosts. If the behavioral models are created and tested with traces collected in the same environment\nand during the same time period, artifacts that only manifest\nunder those conditions will inflate the apparent effectiveness\nof those models and give a false sense of security.\nIt has been challenging to measure per-sample variability\nsystematically, despite the fact that researchers and practitioners have known about it for over a decade. For example, Lindorfer et al. reported that one sample’s behavior may change\nacross execution environments because of different OS versions and libraries [31]. Other researchers studied the evasive\ntechniques implemented by malware authors to ensure that\ntraces collected in a sandbox environment are not representative of its behavior in the real world [6,20]. Rossow et al.\n\n\n-----\n\nreported how downloader behaviors change over time, owing\nto time bombs or new instructions received from the command and control (C&C) channel [42]. These prior studies\nhave confirmed the existence of per-sample behavior variability and showed its potential impact. However, because they\nwere conducted in experimental infrastructures, they did not\nreveal the prevalence of this variability in the wild, or which\ncomponents of the sample’s behavior are most likely to vary.\nHow much, and in what ways, the behavior of benign programs varies in the wild are also open questions. The prior\nresearch has also showed that the effectiveness of malwaredetection models degrades over time, as new samples exhibit\npreviously unseen behaviors [19,38,47]. Previously unseen\nbehaviors of the samples already covered by the model may\nsimilarly degrade the detection performance, but this effect\nhas not been quantified before.\nIn this paper we conduct the first study to understand and\nmeasure the variability in the behavior of malware and potentially unwanted programs (PUP) at scale. We focus on APIand system-call based behavioral profiles, and we conduct\na quantitative analysis of per-sample behavioral differences\non end hosts. To this end, we use a unique dataset of 7.6M\nexecution traces, recorded in 5.4M real Windows hosts from\n113 countries. At the time when the data was collected, it was\nnot known whether the samples were benign or malicious.\nThe samples were executed by the users, who interacted with\nthem naturally, and the behavioral monitoring and analysis\nwas employed as a last line of defense against unwanted behaviors.\nWe measure the variability in the behavior of samples later\ndetermined to be malware and PUPs, and we compare it to\na baseline we draw from the benign samples. Across executions recorded on different hosts we found that the number\nof actions performed (e.g. the creation of a new file or the\nmodification of a registry key) varies 6× more for malware\nthan for benign samples, and this difference increases to 15×\nwhen looking at the number of created files. In contrast, different executions recorded weeks apart on the same host do\nnot show such a high range of action variability. When considering action parameters, (e.g. file names), we observe little\nto no variability across time for benign samples (the action\nparameters tend to remain constant on the same machine), and\na very large variability for malicious samples (the intersection\nof the common values is almost empty).\nWe further assess the challenges for identifying the invari_ant parts of per-sample behaviors, which have implications_\nfor building behavioral rule-based detection signatures, and\nfor clustering samples into malware families. We show that,\nwhen building rules that use actions and tokenized parameters,\nthe information collected from a single execution is inconclusive, but it is possible to observe most of the behaviors from\na few traces. For instance, file names extracted from three\ndifferent hosts cover, on average, 90% of the executions and\nusing more than four traces provides diminishing returns. We\n\n\nalso show that, when performing a malware clustering experiment, one third of the samples exhibit sufficient variability in\nbehavior that their traces appear in multiple clusters. As this\nwould not be observed when using a single trace per sample,\nour result suggests that the accuracy of mapping samples to\nthe correct family, through clustering, is lower than previously\nbelieved.\nThese findings emphasizes that real malware behavior depends on a subtle interplay of factors, such as environments,\ntime, and user interactions, which cannot be observed by executing the sample once in a sandbox environment. We discuss\nthe actionable implications of our results and the alternatives\nto account for behavioral variability. More importantly, these\nresults emphasize the unique insights that we can gain by monitoring malware behavior at scale, on real hosts. Importantly,\nsuch monitoring can be performed ethically by anti-virus systems. This radical shift from the way behavioral analysis is\nconducted today may bring a degree of external validity that\nsandboxes cannot provide.\nIn summary, we make three contributions:\n\n  - We analyze program behavior at scale, using 7.6M call\ntraces recorded in 5.4M real hosts. These traces include\nnatural user interactions with the programs and have high\nexternal validity compared to the prior work.\n\n  - We study how the behavior of individual samples\nchanges across hosts and time, and we compare the variability of Windows malware, PUP, and benign programs\nat multiple granularities.\n\n  - We analyze the invariant parts of the malware behaviors, and we show how this impacts a common class of\nbehavioral rules for malware detection.\n\n##### 2 Problem and Methodology\n\nThe main goal of malware analysis is to identify and characterize the behavior of unknown samples such that behavioral\nindicators that are specific to a malware family could be used\nfor malware detection or classification. Because the behavior\nof executables could vary depending on when, where and at\nwhat setting it is executed, part of the behavior for any given\nprogram is transient in nature.\nIn our dataset, we observed that some executions of the\nRamnit worm [39], result in the creation of a large number\nof mutexes. The reason is that the worm uses a privilege escalation exploit, which creates a lot of mutexes, only if it is\nexecuted in user-mode on a vulnerable version of Windows 7.\nIf instead Ramnit is executed with admin privileges or within\na different Windows version, the malware would not perform\nthe exploit. If an analyst, or an automated system, created a\nsignature by looking at the behavior collected on Windows\n7 (a popular choice by many malware analysis sandboxes),\n\n\n-----\n\nthose mutex creations could be used for constructing the signature. However, these actions would only appear in a fraction\nof end user machines, thus resulting in a poor detection coverage.\nTo mitigate this problem and identify truly invariant parts\nof malware behavior, it is important to collect malware executions across multiple machines, as suggested by Rossow et\nal. [43] and over time, as suggested by Pendlebury et al. [38].\nHowever, prior works does not make concrete recommendations for the most optimal set up (e.g, the optimal re-execution\ninterval, the number of different machines, the number of\ndifferent OSes, etc.) that allows those invariant parts to be\nidentified accurately. Our goal is to fill this gap in the stateof-the-art.\nDespite these very time-consuming therefore costly suggestions, the industry practitioners often choose to aggregate\nbehavior of different samples of a family for signature generation [11,24]. However, the majority of malicious samples\ncannot be mapped to a known family (malware with generic\nlabels are 1.3 times more common than those that belong to\na well-defined family) [27], making it impossible to perform\nsuch an aggregation.\nTo shed light on the magnitude of this problem, we analyze 7.6M executions out of which 3.1M belong to malicious\nand unwanted programs and the rest to benign. In total, the\nexecutions of each sample span at least 10 machines, while\n45% appear at least 1 week from the sample’s first appearance.\nThis measurement, the first of its kind, allows us to assess the\namount of behavior variability in the wild, and to study the\nminimum number of experiments required to rule out transient behaviors and derive signatures that achieve the highest\ncoverage on end hosts, filling a crucial gap in the state-ofknowledge about the most optimal execution configurations\nfor signature generation.\n\n##### 2.1 Measuring Variability\n\nWe describe the behavior of a sample through its interactions\nwith the host Operating System. Because a semantic interaction, such creating a new file or spawning an OS process,\nmay be accomplished with various system or API calls, and\nthe calls differ across OS versions, we abstract these interactions as actions. Our actions model high-level operations,\nsuch as process injection, file creation, or the modification\nof a registry key; we report all the action types analyzed in\nSection 3. An action may have one or several parameters\nto specify the target that the action is operating on (e.g. the\nregistry key being modified), as well as the actual value it\nwrites or modifies (e.g. the value written in the registry). An\n_execution trace for a sample consists of a sequence of actions_\nand the corresponding parameters. The traces captured by\nmalware detectors based on both system calls [10,34] and native API calls [6,7,11,18,21] can be mapped to action-based\nexecution traces.\n\n\nWe measure variability at two levels of granularity. First,\nwe count the actions in an execution trace and compute the\n_action variability. We maintain separate counts for each ac-_\ntion type, as well as for all the actions taken together. We\nthen compare these counts across all the execution traces of\na sample, using several measures of variability as described\nbelow. This provides a conservative assessment of variability,\nindicating for example when a sample creates one file on a\nhost and two on another. We report how much action variability we observe, which action types account for most of the\nvariability, and how these the variability changes across space\n(a sample executing on different hosts in the same week) and\n_time (a sample executing on the same host in different weeks)._\nWe also compare the action variability in malware, PUP, and\nbenign samples.\nSecond, we compare the action parameters coming from\ndifferent execution traces of the same sample, using measures\nof set similarity. This parameter variability allows us to identify differences among executions when the number of actions\nremain the same, for instance when a sample creates a file\nwith different names on each host. This comparison provides\nfurther insight into the semantics of the variable actions; for\ninstance, we identify which parameter parts (e.g., the filename\nvs the directory path) differ among different executions.\n\n**Measuring action variability. The action counts coming**\nfrom different execution traces of a sample form an empirical distribution. We can characterize this distribution using\nvarious measures of location (e.g. mean, median, mode) and\nspread (e.g. variance, standard deviation, median absolute\ndeviation, interquartile range); we are interested in the latter when assessing action variability. The main challenge in\nselecting a statistical measure of spread is to avoid drawing\nincorrect conclusions because of outliers in the distribution.\nWe illustrate this challenge by showing how different variability measures perform over the executions of one sample\nof AutoPico, a Windows piracy software. Usually the sample\ncreates four files when executed: two log files, one dll and\none .sys file. However, in six traces out of 62, AutoPico only\ndropped the two log files (because the samples was unable to\nexecute correctly), and in four traces it created more than 15\ntimes the same log files in the same location (possibly due\nto the fact that the sample modified more registry keys, each\ntime recreating the log file from scratch). The ordered list of\nthe number of file creation events for all executions in our\ndataset looks like the following:\n\n[2, 2, 2, 2, 2, 2, 4, 4, 4,..., 4, 17, 19, 19, 20]\n\nThe Interquartile Range (IQR) and the median absolute\ndeviation (MAD) are measures of spread that are robust to\noutliers. Unlike the classic standard deviation, these measures\nare not affected by measurement values that are either too\nlow or too high. For this reason, IQR and MAD are widely\nused in other experimental fields [26,30]. In our study, a trace\n\n\n-----\n\nmay exhibit an atypical number of actions owing to the malfunctioning of the sample, because of the lack of a required\ncomponent or because the host was shut down mid-execution.\nHigh action counts may also occur when the malware was\ndesigned to infect all of the files in a directory and it encounters a few machines with an unusually large number of files,[1]\n\nwhich results in outliers for the number of file actions. The\nIQR is the difference between the 75[th] and 25[th] percentile\nvalues of the action-count distribution. In the AutoPico example, the IQR is 0, as it is the difference of the value in the\n47[th] position (a 4 in our example) and that in the 16[th] position\n(again a 4) in the ordered list of 62 values. The MAD is the\nmedian of the absolute values of each count’s deviation from\nthe median. In the example the MAD is 0 as well, because,\nafter subtracting 4 (the median) from each count, we get a\nvector where 0 is repeated 52 times and there are only 10\nnon-zero values, and the median of this vector is 0. In contrast, the standard deviation for the AutoPico traces is 3.67,\nwhich inflates the action variability that would be reported.\nMoreover, the variability would be heavily influenced by the\nfour large outliers (17, 19, 19, 20): without them, the standard\ndeviation of the action counts would drop 0.61, while the IQR\nand MAD would remain 0. This suggests that robust measures\nof spread, such as the IQR and MAD, are not likely to lead to\nconclusions biased by artifacts in the data.\n\nAt the same time, the tail of the distribution may also provide meaningful insights, e.g. when it reflects the behavior\nof targeted malware. We therefore select two additional measures, the 90-10 and 99-1 percentile ranges, because they are\nanalogous to the IQR but are gradually less conservative in\ndiscarding the distribution tails. In the AutoPico example,\nthe 90-10 and 99-1 percentile ranges are 0 and 17 (19-2) respectively. In our analysis, we compute the MAD, and the\n75–25 (IQR), 90–10 and 99– 1 percentile ranges. We report\none representative measure when the results are similar, and\nwe discuss when we observe differences among the four measures.\n\n**Measuring parameter variability. We measure variability**\non parameters for each action type separately. We then compute the Jaccard index, which is a popular choice to measure\nthe distance between the parameters observed in two malware\nexecutions [22,31], on the parameters observed in different\nexecutions of each sample. This way it is possible to identify whether similar parameters are chosen (e.g., create a file\nwith the same filename) or on contrary, the parameters are\nrandomized or very different among executions, therefore,\nmalware detection signatures should not incorporate them.\nWe also perform IQR measurements on the count of unique\nparameter values,to get a precise picture of what, and how,\nchanges across multiple executions.\n\n1An example is the authors’ fileserver, which stored so many files at one\ntime that it crashed our backup service.\n\n\nlogsource:\n\ncategory: process_creation\n\nproduct: windows\n\ndetection:\n\nselection:\n\nCommandLine: '*‐noni ‐ep bypass $*'\n\ncondition: selection\n\nFigure 1: A sample signature from SIGMA.\n\n##### 2.2 Finding Behavioral Invariants\n\nAs we introduced in the previous section, actions are a common abstraction to represent units of behavior. On top of that,\nresearchers have proposed many different models to build signatures by expressing patterns over sets of actions. While the\nliterature of models is very rich, ranging from simple ngrams\nor ordered bags [10] to complex graph-based structures [24],\nthe industry still lacks a common framework for expressing\nand sharing behavioral models (the role that Yara [3] plays\nfor static signatures).\nTo the best of our knowledge, the only available resource\nthat contains a sufficiently-large set of signatures of this kind\nis provided by SIGMA [44], a project that proposes a language to express patterns for log analysis. As OS audit logs\ncontain information about the interaction of each process with\nthe environment (something equivalent in nature to system\ncalls or the abstract actions in our model) the language used\nto express SIGMA rules allows analysts to write Yara-like\npattern over the runtime events of a sample.\nBy reviewing previous papers and the SIGMA ruleset, we\nfound that a common building block of all these signatures is\nthe ability to check for the presence of an action and match\na portion of the its parameters (typically through a regular\nexpression). For instance, Canali et al. [10] use the action\ntype and the full parameter value to create complex signatures.\nSimilarly Trinius et al. construct a representation of malware\nbehavior that uses the action type and parts of the parameters\nto create a behavior profile for their malware and Trinius\net al. [51] use an exact match on their proposed features.\nThis is also the case for SIGMA rules, as the one reported in\nFigure 1, which matches a process creation action in which\nthe command line parameter matches the specified pattern.\nOur goal is not to create signatures nor to evaluate different\nsignature models. Instead, we want to measure which constant\nelements exist across multiple executions, with the assumption that any good signature would need to build upon these\nelements and avoid using information from other transient\nbehaviors.\nFor this reason, we break down each parameter value in a\nset of tokens according to classic windows delimiters [49]—\nsuch as backslashes for directories and spaces for commandline arguments—and study the evolution of each token both\n\n\n-----\n\n**Mal** **PUP** **Ben**\n\nNum. samples 2424 1621 22443\nNum. machines 0.5M 0.9M 4M\nNum. executions 1.1M 2M 4.5M\n\nTable 1: Dataset summary.\n\nindividually and aggregated with other tokens extracted from\nthe execution traces. As an example, we observed that around\n70% of the SIGMA rules contain at least one of such token,\nconfirming their role of building blocks for more complex\nsignatures.\n\n##### 3 Dataset\n\nThe dataset we are using is a collection of 7.6M execution\ntraces that the AV vendor has collected across 5.4M real users\nduring the year of 2018. The data is collected by a component\nof the AV engine that is responsible for behavior-based detection. This component records high-level behavioral data about\nthe executed programs until they terminate or until the system\nis able to classify them as either benign or malicious and\nkill. For the sake of validity, our data only includes programs\nthat terminate normally. Therefore, unlike data collected from\nsandboxes, our data is not limited to few minutes of execution and because the traces are collected from real users, they\ndo not suffer from the limitations introduced by synthetic\nanalysis environment. Our data does not consist of malware\nsamples that were executed intentionally for data collection,\nbut samples that at the time of collection were not yet known\nto be malicious or pup and were able to evade the static malware detection solution installed on the computers. Our data\nis a reflection of the set of threats with which the behavioral\ndetection components need to combat.\n\n**Dataset coverage statistics.** Each item in our data consists\nof a sequence of behavioral actions performed by a sample\ntogether with SHA-256 hash of the sample, an anonymous\nmachine identifier and a timestamp. Thanks to the unique\nSHA-256 hashes of the samples, we were able to query VirusTotal (VT) in the following year (2019) of the data collection\nand identify the corresponding labels assigned to those samples by various AV engines. While we labeled the samples\nthat were consistently labeled as benign by all AV products,\nwe label samples as malicious or PUP using AVClass [46], a\nstate-of-the-art technique for massive malware labeling. From\nthe VT reports obtained in August 2019, AVClass identified\n22, 443 benign, 2, 424 malware and 1, 621 PUP samples, as\nlisted in Table 1. We perform our variability analysis on execution flows we were able to label with high confidence and\nthose that were observed in at least 10 machines. This experimentally chosen threshold made it possible to accurately\n\n\n**Ratio**\n\nWindows 7 56%\nWindows 10 35%\nWindows 8.1 3.1%\nWindows Server 2.6%\nWindows XP 2%\nOther Windows Versions 1.3%\n\nTable 2: OS version distribution.\n\nmeasure variability of the sample sample across different\nmachines. 85% of the samples were executed between 10\nand 100 machines, rest were observed in more than 100. The\ndata was collected from computers from across 133 countries:\nUSA(48%) and China (14%) have the largest fraction of our\ndata points.\nIn Table 2 we show the distribution of the Operating Systems for the machines in our dataset. The vast majority of\nmachines run the Windows 7 build 7601 and the rest run a\nflavor of Windows 8.1 or 10. 55% of the executions happen\nless than one week apart, while respectively 12%, 6%, 4%\nand 3% are executions that were collected after the second,\nthird, fourth and fifth week from the initial recorded execution.\nOn the 11% of the samples’ re-execution happens 9 weeks\nafter the first appearance of the malware. As a matter of fact,\nwe measure the time variability for the executions happening\nduring the first 4 weeks after the first appearance, covering\nover 80% of the executions. For instance, a crypto miner sample first appeared on April 5th and within 7 days we had 47\nexecutions from 35 machines. During the next 7 days we\ncaptured 18 executions, 4 of which on new machines. In the\n3rd week we record the last 7 executions, none of which from\nany new machines.\n\n**Execution statistics.** An execution trace is composed of\nmultiple actions. The actions are heuristically-defined behavior units such as file creation/modification, registry key\ncreation/modification, mutex creation etc. In addition to those\ncommon behaviors analyzed largely by the literature, we also\nhave some behavioral actions that were defined by the security\nvendor such as disable Windows defender, disable updates,\nchange firewall options, keylogging, change IE settings etc.\nIn table 3 we show the top 8 action types in our dataset\nwhich corresponds to 87% of the whole data. On average,\nper execution trace we identified 150 actions out of which\n39 are file creations. In our study, due to space constraints\nwe present the action level variability analysis for a subset of\nthese actions. To this end, we set the following criteria:\n\n1. Action occurs in any execution in more than 25% of\n**the machines. To measure a non-zero machine variabil-**\nity of a certain action with IQR, it is necessary to observe\nit at least 25% of the machine.\n\n\n-----\n\n**Ratio of dataset**\n\n**File Create** 26%\n**Mutex Create** 20%\n**Registry Set** 14%\n**ProcessLoad** 8%\n**PECreation** 6%\n**RegistryKeyCreated** 6%\n**DirectoryCreated** 4%\nServiceCreation 2%\nOthers 14%\n\nTable 3: Ratio of action types over all the dataset.\n\n2. More than 1 action appears in the executions. If the\naction happens only once in executions, it is not possible\nto measure its variability (the only possible result could\nbe 0 or 1.\n\n7 of the actions in Table 3 meets this criteria. More details\nfor other actions in our dataset are provided in Tables 8 and 7\nin Appendix.\n\n**Ethical Considerations.** As mentioned earlier, we did not\ndistribute or launch any samples on the user machines; instead,\nall the executions in our data set were triggered by the users,\nand the malware and PUPs we report reflect real-world attacks\nagains those machines. The anti-virus detects and blocks\nall the malicious samples known at the time and collects\nexecution traces for samples that remain suspicious, in a lastresort effort to discover unknown malware. In consequence,\nwe do not cause any harm to the machines in our study that\nwould not have occurred without the anti-virus product and\nour data collection. Moreover, future updates to the anti-virus\nwill clean the infected hosts once the malware is discovered,\nowing to the data collection. The behavioral analysis data\nwas collected from users who opted in sharing their data.\nNecessary anonymization actions are taken to preserve the\nprivacy of the customers. None of the data fields in the dataset\ncontains any identifiable information.\n\n##### 4 Behavior Variability in the Wild\n\nIn this section, we analyze the variability we observed in\nthe behavior of malware, PUP, and benign programs when\nexecuted on different end-user machines. We measure both the\naction variability and the parameter variability, as discussed\nin Section 2.1. We first conduct these measurements across\nspace (the differences among a sample’s execution traces on\ndifferent machines) and time (the differences among its traces\nin different weeks).\nIn our data, some executions contain duplicates (i.e., the\nsame action type with the same parameter value). This could\n\n\nhappen for example when a sample opens the same file multiple times. As these operations are idempotent with respect\nto the our behavioral specifications, defined in Section 2.1,\nwe perform deduplication on our data before we apply the\nvariability analysis.\n\n##### 4.1 Machine Variability\n\nWe start by measuring the variability of executions of the\nsame sample across different machines. Our goal here is to\nunderstand whether this phenomenon exists and if it does, on\nwhich type of executables it is more prevalent. We only look\nat executions of a sample that happen max one week apart\nto identify variability that happen only due to being run on\ndifferent machines not due to time.\n\n**4.1.1** **Action Variability**\n\nWe analyze action variability through IQR and MAD. In order to understand the impact of the outliers on the results,\nwe also look at the difference among 90-10 and 99-1 percentiles. Figure 2 illustrates the distribution of IQR variability, across all actions (Figure 2a) and only for the two most\ncommon actions we observed in our dataset: file creation (Figure 2b) and registry modification (Figure 2c). The numbers\nbetween parentheses for each category is the number of samples that we use for our variability analysis. Because not all\nsamples had file creation or registry key modification actions\nin their executions, these numbers are lower than the total\nnumber of samples (Table 6 in the appendix provides a detailed breakdown of action variability). The separate boxplots\nfor malware, PUP and benign samples allow us to compare the\naction-variability distributions within these categories and to\nassess the extent of these differences. To confirm these visual\nobservations we compare these empirical distributions using\npairwise U-tests [33], a non-parametric method for inferring\nwhether the samples are likely drawn from distinct distributions. In the paper, we report differences that are statistically\nsignificant at p < 0.001 level.\n\n**Malware exhibits higher variability across machines. We**\nexpect to see a higher behavior variability in malware samples, owing to a host targeting, evasion and obfuscation attempts, or the tendency to attempt operations that may fail\non some hosts (e.g. privilege escalation). Figure 2a confirms\nthis: comparing the three boxes, which represent the bulk of\nthe measurements from each empirical distribution, suggests\nthat the action-variability of malware is typically higher than\nthat of PUPs, which is typically higher than that of benign\nprograms. The median IQR for malware is 59 actions, which\nmeans that the top 25% of a sample’s execution traces are\n\n_> 59 actions longer that its bottom 25% traces, for half of the_\nmalware samples in our dataset. In contrast, the median IQRs\nare 19.25 and 8 actions for PUP and benign, respectively. We\nobserve similar trends with the MAD measurements. While\n\n\n-----\n\n**200**\n\n**175**\n\n**150**\n\n**125**\n\n**100**\n\n**75**\n\n**50**\n\n**25**\n\n**0**\n\n\n**8.0**\n\n**Malware(2424)** **PUP(1621)** **Benign(22443)**\n**Category**\n\n\n**140**\n\n**120**\n\n**100**\n\n**80**\n\n**60**\n\n**40**\n\n**20**\n\n**0**\n\n|IQR for total number of actions|Col2|Col3|Col4|Col5|Col6|Col7|\n|---|---|---|---|---|---|---|\n||||||||\n||||||||\n||||||||\n||||||||\n||||||||\n||59.0||||||\n|||||19.25||8.0|\n\n|Col1|Col2|IQR for File Creation actions|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|||||\n|||||\n|||||\n||33.875|||\n\n|IQR for Registry Modification actions|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n|7.5||3.0||1.0|\n\n\n**Malware(2060)** **PUP(1363)** **Benign(18624)**\n**Category**\n\n(b)\n\n\n**2.75** **1.75**\n\n\n**30**\n\n**25**\n\n**20**\n\n**15**\n\n**10**\n\n**5**\n\n**0**\n\n\n**1.0**\n\n\n**Malware(2361)** **PUP(1311)** **Benign(15415)**\n**Category**\n\n(c)\n\n\n(a)\n\n\nFigure 2: IQR action variability for all actions and the two most common actions in our dataset.\n\n\nthe average difference from the median for malware is 35.5\nactions, for PUP and benign, it is 10.3 and 2.9 respectively.\nThis leads to the question where does this variability come\n_from? When breaking down the variability according to action_\ntypes, we observe a striking difference for file creation actions\n(Figure 2b). The median IQR for malware is 15× larger than\nfor PUP and benign samples, and the bulk of the distribution\nincludes much larger values. In contrast, the variability distributions for PUP and benign samples do not appear to be\ndifferent for file-creation actions, while PUPs exhibit more\nvariability for registry-modification actions (Figure 2c). This\nsuggests that malware classification solutions based on file\ncreation actions could lead to inaccurate results, as the high\nvariability among the execution traces of a sample may place\nsome of these traces in different clusters; we investigate this\nin more depth in Section 6. Conversely, a malware detector\nable to observe executions on multiple hosts could utilize\nfile-creation variability as an indicator of malicious behavior.\n**Case study. We refer back to the Ramnit sample in Section 2**\nAt least 25% of the executions occur on Windows 7 machines\nwhere the malware is running with user privileges. Therefore,\nthe malware runs a privilege escalation exploit causing a large\nnumber of mutex creations. The rest of the executions happen\nin different OS version or with admin privileges, thus the\nexecutions are shorter. The action variability is affected by\nthe longer executions showing an IQR of 34, which is the\nnumber of mutex creations.\nThere is a significant variability on the behavior of\nmalware among different machines. When malware\ndetection solutions rely on data collected only from\none sample up to 200 (med. 59) behaviors could be\nunderrepresented in the detection model.\n\nEven though malware still shows a significantly higher\nvariability when expressing the variability in terms of the\n90–10 and 99–1 percentile ranges instead of the IQR, the\naction-variability distribution of malware becomes harder to\ndistinguish from the PUP and benign distributions This is\nnot surprising as these measures are not as robust to outliers\n\n\n**Median** 75[th] **percentile**\n**Mal** **PUP** **Ben** **Mal** **PUP** **Ben**\n\nPath 4 1     - 10 3 2\nName 25 2 1 49 8 8\nExt. 3 1     - 5 2 1\n\nPath 1     -     - 1 1     Name 1    -    - 3 2 1\nExt. 1     -     - 1 1     \nKey Path 2 1     - 3 3 1\nKey Name 5 2    - 9 6 2\nValue 5 2 1 10 6 3\n**D** Path 1 - - 1 1 **RC** Path 2 1 - 3 3 1\n**MC** Name 6 3 1 9 7 4\n**P** CMD line 4 - - 6 1 1\n\nTable 4: Parameter(s) IQR variability for malware, PUP and\nbenign.\n\n[PE] PE File Creation actions, [D] Directory Creation,\n\n[RM] Registry Key Modification, [RC] Registry Key\nCreation, [M] Mutex Creation, [P] Process Creation actions.\n\n|Col1|Col2|Median|75th percentile|\n|---|---|---|---|\n|||Mal PUP Ben|Mal PUP Ben|\n|File|Path Name Ext.|4 1 - 25 2 1 3 1 -|10 3 2 49 8 8 5 2 1|\n|PE|Path Name Ext.|1 - - 1 - - 1 - -|1 1 - 3 2 1 1 1 -|\n|RM|Key Path Key Name Value|2 1 - 5 2 - 5 2 1|3 3 1 9 6 2 10 6 3|\n|D|Path|1 - -|1 1 -|\n|RC|Path|2 1 -|3 3 1|\n|MC|Name|6 3 1|9 7 4|\n|P|CMD line|4 - -|6 1 1|\n\n\nWe now take a closer look at the variability in the parameter\nvalues. We calculate the Jaccard index among the parameters\nof the same actions types in the execution traces. In this\nexperiment use the full value of the parameters types listed\nin Table 4. Our observation was that the parameters values\n\n\nas the IQR and MAD. The PUP and malware distributions\nremain distinguishable for file-creation actions, but overlap\nsignificantly for registry-modification and mutex-creation actions. In fact, a few PUP samples seem to have more extreme\noutliers causing the 99–1 range to be larger than that of malware.\nWhile malware and PUP both vary more than benign in all the action types, they show variability in\ndifferent action types.\n\n**4.1.2** **Parameter Variability**\n\n\n-----\n\nacross different machines have a high variability for both\nmalware, PUP and benign programs. For all of the parameter\ntypes, we obtain a Jaccard index is 0, indicating that there\nis no full value shared across all executions. Note that for\nthis step, we do not normalize the data to remove computer\nspecific artifacts and also do not extract substrings from the\nparameters. However in Section 5, when exploring invariants\nparts among executions of samples, we will perform deeper\ninvestigation on the substrings as well.\nNo parameter value is shared among all machines\nexcept for file extension. An analyst has to rely on\nsubstrings/tokens of the parameter values to create\nsignatures.\nWe also perform IQR measurements, to obtain more insights about the distribution of the variability among different\nparameter types. In Table 4 we report the median and 75th percentile IQRs of different parameter types. We can clearly see\nthat benign programs rarely change the directory they work\non, the file names created or the executables they launch. On\nthe other hand, malicious samples tend to create a significant\namount of new files (25 new files for 50% of the malware),\nto work on several directories and even to create different\nexecutables over different executions. This finding indicates\nthat the same sample’s behavior can vary to an extent that it\ncould be hard to identify a behavioral indicator that is common among all. We will explore this aspect in more detail in\nthe following section.\n**Case study. In the executions of a Glupteba malware sam-**\nple we observed that the Jaccard index across multiple\nmachines is 0 for file names and 0.2 for mutex names,\nwhile the IQR for file and mutex creation is 0 and 2 respectively. This means that the malware changes significantly the name of files but not their absolute number,\nwhile mutex names are more similar but with a larger variability in terms of number. In this particular case, mutexes were a better candidate for building signatures, as we\nfound that h48yorbq6rm87zot appeared in all the machines,\nwhich is also confirmed by a report from TrendMicro [50].\nOn the contrary, the mutex ZonesCacheCounterMutex and\nZoneAttributeCacheCounterMutex only appeared in half\nof the machines, which explains the IQR of 2.\n\n##### 4.2 Time Variability\n\nIn this section we look at how variability is impacted by the\ntime in which a sample is executed. We start again by looking\nat the volume of actions and then zoom into those actions by\nincluding their parameters into the analysis.\n\n**4.2.1** **Total Action Variability**\n\nWe measure the action variability by comparing executions of\nthe samples in different weeks. Since 80% of the samples in\nour data were executed at most four weeks after the first week\n\n\nof their appearance, we perform the per-week time analysis\non those next 4 weeks. To simulate what an analyst would\ndeal with, we consider the first week’s executions as the base\nand compare it with each of the 4 consecutive weeks. Here,\nwe cannot use IQR as for each sample we only have 4 data\npoints. We simply count the number of missing and new\nactions observed in each sample’s executions compared to the\nprevious week. The results are reported in Figure 3.\n\n**Malware have the highest number of missing and addi-**\n**tional actions. The general takeaway for coarse-grained time**\nvariability analysis is that there is a significantly larger time\nvariability in malware compared to PUP and benign samples.\n\nAccording to the Figure 3, on average across all machines\nwe see 6 missing actions 1 week after the first execution. Even\nthough this number might seem low, depending on what those\nactions types are variability over time might have an impact\non the malware detection solutions. Note that there are also\nsome malware samples that show a tremendous variability\n(average of max being 17, max of max being 219). One possible explanation for this significant number of missing actions\nis that when malware is re-executed on the same machines,\nmight not need to repeat some of the behavior such as creating\nparticular files. At time of the data collection, the malware\nsamples were not yet known and therefore, the machines were\nnot cleaned up before the re-execution. However, we also observe variability over time when looking on the new actions\nthat appear on the following weeks. Similarly, malware samples have on average 1 new action appearing every week,\nwhich is larger than PUP and benign. We also highlight that\nthe machine with the maximum number of additional actions\nseems to have a maximum of 63 new actions and more than 3\nnew actions for 50% of the malware samples. For malware\nexecution longer than 1 week from their first appearance less\nnew actions appear, indicating a more stable behavior by time.\n\n**Case studies. A TOR-connected coinminer was dropping**\n_miners.ini, miners.ini.* and minergate.log and launching_\n_minergate-cli.exe before January 18th in 2018. In some ma-_\nchines it was also dropping up to 14 *.tmp files. After 18th this\nbehavior completely stopped, resulting a number of missing\nactions in the following weeks. On the other side of the scale,\nwe also identified a Remote Administration Tool, which initially dropped 5 dlls files and an executable (setacl.exe) before\nMarch 16th 2018. On April 3rd, it started dropping 7 more dll\nfiles and 3 new executables. We also have examples for malware that misses and adds new actions at the same. In it’s first\nweek of appearance the software was dropping various files\non different machines such as microsoft office, foxit pdf edi_tor, autocad 2015 qqlivedownloader.exe. This behavior could_\nbe due to the user’s interaction with the malware or simply\nthe malware hiding its purpose. The next week’s executions\nno longer drop any of these files, but zny_znykb030.exe or\n_kuaizip_setup_2523474329_rytx2_001.exe appears to down-_\nload consistently in almost all the machines. We believe that\n\n\n-----\n\n**100**\n\n**80**\n\n\n**100**\n\n**80**\n\n\n**100**\n\n**80**\n\n\n**60**\n\n**40**\n\n\n**60**\n\n**40**\n\n\n**60**\n\n**40**\n\n\n**20**\n\n**0**\n\n|Col1|Malware missing total actions per week|Col3|Col4|Col5|\n|---|---|---|---|---|\n||||||\n||||||\n||||||\n||||||\n\n|Benign missing total actions per week|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|||||\n|||||\n|||||\n\n\n**1(607)** **2(275)** **3(158)** **4(102)**\n**Week**\n\n\n**PUP missing total actions per week**\n\n**1(871)** **2(439)** **3(295)** **4(197)**\n**Week**\n\n\n**20**\n\n**0**\n\n\n**1(2527)** **2(925)** **3(464)** **4(278)**\n**Week**\n\n\n**20**\n\n**0**\n\n\nFigure 3: Missing actions compared to the previous week.\n\n\nin this case the user had no control of the malware and the\ndownloads happened silently. This malware was still running\n4 weeks later performing the same actions. A final example is a sample of Adware.Chinad, which dropped various\nfiles (microsoft office, foxit pdf editor, autocad 2015). On the\nfollowing week, a new executable (zny_znykb030.exe) followed by potentially pirated other software are downloaded.\nIn malware, time variability is the largest. While\nthe variability is mainly due to the missing actions,\nthere are also new events that appear on the following weeks.\n\n\nsoftware eventually detected the most had a higher variability\nacross time. This shows that even if in general time variability\nis low across the board, for easier to classify samples it seems\nlike time has a more significant effect on the variability.\nTo get a better understanding of this phenomenon we conducted two case-studies. In the 75[th] percentile we found a\nversion of kuaizip and analyzed its behavioral data manually.\nThis malware seemed to stop working sometime around the\nsecond week of April 2018, after which it still performed\nhost-related actions but failed to download the PE files it\nwas retrieving before. At the other side of the spectrum, we\nchose a malicious sample that exhibits low variability. We\nfound an open source DLL injection tool classified as malware which performs exactly the same actions every time it\nruns. This likely-to-be-malicious sample injects into roblox_playerbeta.exe, creates settings.xml, and sets some registry_\nkeys. Upon further analysis we found that this is being intentionally used for cheating in games and this exact behavior\nis observed over and over again. While preliminary, these\nexperiments seem to confirm that time variability affects the\nmost those samples that rely on an external infrastructure.\n\n**4.2.2** **Parameter Variability**\n\n\n**0.05**\n\n**0.04**\n\n\n**0.03**\n\n**0.02**\n\n\n**0.01**\n\n**0.00**\n\n\n**Malware detections for variability**\n\n**low var.**\n**high var.**\n\n**0** **10** **20** **30** **40** **50**\n**Detection**\n\n|Col1|Col2|Col3|Col4|low var.|\n|---|---|---|---|---|\n||||high var.|high var.|\n||||||\n||||||\n||||||\n||||||\n\n\nFigure 4: Relation between detection and the time\n**variability. This result shows that malware that have high**\ntime variability have higher VT detections.\n\n**Malware with highest detection rates vary more over time.**\nOne interesting observation on the malware that exhibit more\nvariability over time was that in general more of the AV engines would label them as malicious. In Figure 4 we show\nthe distributions of the malicious samples with low time variability (lower than 25[th] percentile) against the ones with high\nvariability (on the top of the 75[th] percentile). For each of samples, we check the total number of detections in VirusTotal in\nNovember 2019 (i.e., one year after the first time we observed\nthe samples). As it can be seen seen, the samples that are AV\n\n\nWhen switching to the fine-grained analysis of each parameter,\nwe now observe a very different picture from the results we\nobtained by looking at the variability among hosts. In fact,\nthe Jaccard Indexes show that for goodware and PUP there\nare a large number of perfect matches over time when the\nsample is re-executed. For the full results we refer the reader\nto Table 5 in the Appendix.\n\n**Over time, malware actions parameters vary a lot, while**\n**PUP’ and benign’s ones do not vary at all. The difference**\nis remarkable. Even at the median, the parameters of actions\nperformed by benign software change very little, and the 75th\npercentile they change almost nothing at all. If we consider\nthat the same indexes were zero when considering executions\nacross different hosts, this result emphasize a very important\ndistinction. Goodware creates different files, mutexes and\nregistry keys in different machines. But when we consider two\n\n\n-----\n\nexecutions on the same machine, those values remain constant.\nThe same phenomenon does not happens for malware, where\nthe Jaccard index is zero across the board, both in case of\ndifferent machines and in case of different executions on the\nsame host. The only few exceptions to this rule are regarding\nfile paths and file extensions, which still have a low similarity.\nIf we look at the 75th percentile things get more stable and\nboth malware and benign files show a high similarity. This\nmeans that for at least 25% of the samples in our dataset we\nobserve a stable set of parameters at different points in time.\nAt least 50% of the malware do not reuse same file\nnames, registry keys values and paths, directories\nin their reexecutions, and 25% execute at least 1\nnew command.\n\n##### 5 Invariant Analysis\n\n\n**1.0**\n\n**0.9**\n\n**0.8**\n\n**0.7**\n\n**0.6**\n\n**0.5**\n\n**0.4**\n\n**0.3**\n\n**0.2**\n\n\n**Number of machines needed to capture all tokens**\n\n**File name**\n**File Path**\n**CMD**\n\n**10[0]** **10[1]** **10[2]**\n**Number of machines(log scaled)**\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|||File name File Path||||\n|||CMD||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n|||||||\n\n\nOur variability analysis confirms that malware behavior\nchanges over time and on different machines. This indicates\nthat if a behavioral malware detection system is designed\nwith data collected at a fixed time or from a single computer\nwith a particular configuration setting, the real behavior that\nis common to all possible executions might not be identified\ncorrectly. However, as we showed in Section 4.2.1, the fact\nthat malware samples carry large variability across different\nexecutions does not rule out the possibility of building accurate detection models from behavior that remains stable.\nTherefore, in this section we focus on measuring the invariant\npart of malware behavior, to better understand how effective\nbehavioral-based detection systems can be if their models are\nbuilt upon the right set of events.\nRoughly 80% of the SIGMA rules are created from values\nextracted from file and process creation events, and these\ntwo are also in top 7 most popular actions in our dataset.\nTherefore, due to space limitations, in this section we focus on\nthose actions and their parameters. We identify the invariant\nbehaviors only from the malicious samples as our goal is to\nevaluate behavioral malware detection techniques. We only\nuse benign samples when simulating a signature generation\nprocess, by extracting the invariant parts that are not observed\nin the benign execution traces.\n\n**Beyond full parameter value.** In Sections 4.1.2 and 4.2.2\nwe utilize the full value of the parameters to measure the\njaccard index. In this section we follow a simple approach to\nsplit those values into smaller tokens, explained in Section 2.2,\nwith the aim of finding a shared value across machines.\n\n\n##### 5.1 How Many Hosts Are Enough?\n\nOne of the main consequences of the findings discussed in\nSection 4 is that for building more effective and accurate signatures it is necessary to collect multiple data points rather\n\n\nFigure 5: CDF of number of machines and the amount of\n**malware values. It takes more machines to capture all the**\nCMD tokens than other parameters’ tokens.\n\nthan looking at a single trace collected from one environment.\nWhile this sounds intuitive, to our knowledge, there is no\nexisting study that attempted to measure how many executions from different machines are needed to identify tokens\nthat maximize the coverage of the generated signatures. To\nthis end, we measure the number of executions of the same\nmalware in the wild that can be detected by using the tokens\nextracted from a small set of executions, as well as the number\nof those executions that are needed to obtain all the malicious\ntokens. Based on that, we estimate how many machines are\nneeded to achieve a high coverage of observed behaviors.\nFigure 5 shows the empirical cumulative density function\n(CDF) of the fraction of malware samples for which we capture all tokens, for an increasing number of machines (x-axis).\nWe only consider tokens that never appear on benign traces\nand we also exclude the unique tokens, i.e., those that only appear in one machine in the wild (as they could be the result of\nrandom values). Finally, we construct the CDF by adding first\nthe traces that contain the highest number of tokens, and thus\nrepresent a conservative estimate. For 20% of the malware,\nwe need only 1 machine to capture all the malicious command\nline tokens. If we increase the threshold to 10, we can identify\nall the command line tokens for 85% of the malware and all\nthe file path tokens for 98% of the malware.\nNaturally, the more machines we use the more tokens we\ncan extract, but adding more machines provides diminishing returns. As we can see, for 21% of the malware we can\ncapture all filenames in a single execution, and for 29% of\nthem one execution is sufficient to discover all file path tokens. However, we need to collect 39 traces to observe all the\nmalicious filename tokens that appear in 90% of the malware,\nwhile this decreases to only 11 and 17 machines for capturing\nfile path and command line tokens respectively. Since the\nfile path seems to converge faster, in Figure 6 we check the\n\n\n-----\n\nFigure 6: CDF of number of machines and the amount of\n**malware values. After 4-5 machines we start to get**\ndiminishing returns in the number of new malware file path\ntokens discovered.\n\nimpact of the first 8 machines in the amount of tokens we\nare extracting. We observe that after 7 machines the return of\ninvestment becomes small, as for the top 50% of the malware\nsamples we already extracted 68% of the tokens. When also\ncheck the other parameters and we noticed similar correlation\nbetween the amount of tokens extracted and the detection rate\nin Figure 7, which makes sense since having all the malware\ntokens means we have 100% detection.\nWhile counting the new tokens can give an idea of how\nmany traces we need to compensate for the diversity of behaviors, it does not tell us whether those tokens are sufficient\nor not to detect malware in the wild. Therefore, we conducted\na second experiment. Here we use the tokens extracted from\none execution to match the malware traces collected in other\nmachines. If the combination of the tokens can cover all the\nother executions of the same sample, then we conclude that\none execution is sufficient (in theory) to extract a perfect signature. If instead the extracted tokens cannot achieve 100%\ncoverage, we add a second trace collected on a different (randomly chosen) machine in the same week (as for the moment\nwe want to study the machine impacts and not the time impact)\nand we re-iterate the process. Since the result is dependent on\nthe select machines, we repeat the experiments ten times and\nreport the average.\nFrom the boxplot in Figure 7a we can see that while for\nsome malware one execution might be enough, in average\nthe filenames extracted from one trace cover 82% of the executions and the value decreases to 77% if we use path information. However, the execution traces collected on three\ndifferent machines are sufficient to achieve the highest coverage when using file name as the parameter. Similarly we\nfind that it takes four machines to saturate the coverage for\nthe command line and seven for the file path. The respective\n\n\nresults can be found in Figures 7b and 7c.\nOur results suggest that an analyst should analyze\nthe malware in 3 random virtual machines to capture most of the file names, 4 for CMD line and 7\nfor file path. A possible way to generate such random machines, instead using the same machines\nfor all malware, may be to use a random vm generator like SecGen [45] with the features proposed\nby Miramirkhani et al. [35].\n\n##### 5.2 How Soon Should We Re-Execute?\n\nWe now investigate the re-execution interval needed to\nachieve the best coverage in the wild. This is more difficult to measure, as it represents a trade-off. If you re-execute\nthe sample too early, you may learn little and your signature\nmay not catch the behavior that the malware will exhibit in\nthe future. But if you re-execute the sample too far in the\nfuture, than your initial model might get outdated before you\nre-analyze the sample.\nFor this analysis, we take a first execution trace during the\nfirst week of appearance of the malware. Then we collect a\nsecond trace on the same machine, varying the time between\none and four weeks in the future. We then use the tokens\nextracted from the first execution to match all malware executions until we re-execute the sample. From that time on,\nwe incorporate the information of the second execution and\nupdate the signature to be used for future executions.\nFigure 8 shows the results for the filename tokens. While\nthe median detection does not change much, re-executing after\nthree weeks provide more benefits (the minimum detection\nand the 25 percentile are much higher, which suggests that\nfor some malware this makes a big difference).\nFor the file path the difference in the re-execution interval\nis smaller, which means that we need more machines to get\nbetter detection. However, even in this case we still notice\na slightly smaller range when re-executing on the 4[th] week,\nwhich means some malware show a different behavior around\nthat time. The results are the same for command line arguments, where in week 4 we have a more impactful increase\nin detection, suggesting that malware will be spawning new\nprocesses or using different parameters one month from their\nfirst appearance.\nAn analyst should re-execute a sample between 3–4\nweeks apart. However, having multiple executions\nin different days provides less useful information\nabout the malware behavior than having different\nexecutions on different machines.\n\n##### 5.3 Hunting for the Most Invariant Artifacts\n\nAs we showed in previous sections the number of file creations is not a good metric to profile a malware sample due\nto variability. Similarly, the same file name doesn’t appear in\n\n\n-----\n\n(a)\n\n\n(b)\n\n\n(c)\n\n\nFigure 7: Detection coverage of tokens obtained by combining multiple execution traces The detection rate/coverage of file\nnames or extensions reaches the maximum after 3 to 4 machines while for file path we need about 7 machines to capture all the\nmalware tokens.\n\n\nFigure 8: Total filename signature coverage for\n**re-execution intervals. A periodic execution of every 3**\nweeks yields the highest coverage across all malware.\n\nall machines. Using more than 1 file name to profile malware\nseems like the right choice. While using both variant and\ninvariant features is not going to affect the performance of\nthe detector, we need intuition to be sure we have an invariant in our signature. In this section, we measure the covered\nmachines that individual tokens can detect the malware on.\nFor this we extract the malware tokens in the first week and\ncompare their performance to executions happening in the\nfollowing weeks, to simulate the scenario where an analyst\ncreates a signature with 1 token and deploys it. We show the\naverage effectiveness of each token. We don’t remove the\nrandom tokens to show the amount of randomness that an\nanalyst has to deal with for each parameter.\nWe measure the file name token coverage for file writes.\nThe results show that most of the tokens are random and having more than 1 machine allows the analyst to remove them.\nWe noticed that the tokens with the highest coverage were the\nextensions, therefore we encourage the analysts to split the\n\n\nfile name using the dot(.) delimiter and remove the known\nbenign extensions to obtain highly performing malware file\nextension signatures. Random tokens happen more often in\nfile names than any other parameter, which means that an analyst should have more than 1 execution to remove the tokens\nthat appear only once.\nWe noticed that malware tends to write to non-random and\nnon-benign paths. However, there is no clear trend to which\nsubdirectories and on which level are invariant to the malware,\ntherefore, an analyst will need multiple values to construct a\nsignature based on the file path. While we couldn’t identify\na heuristic to pick the better path tokens we noticed that on\naverage, for all malware, 25% of non-benign subdirectory\nnames (tokens) appear in all the machines. This means that\nan analyst will achieve a better detection using a subdirectory\nname to detect malicious file write than the file name, extension or even command line of process creation. Our study\nreveals a source for constructing high-performance detection\nrules using file extension tokens, which future generations of\nmalware may no longer posess. We also reveal the success of\nfile path tokens in constructing a malware detection signature.\n\n##### 6 Discussion and Limitations\n\n**Impact on State-of-the-Art Solutions. In our paper, we per-**\nformed an extensive analysis about behavioral variability on\nmalware, concluding that to observe the complete behavior\nof malware it is necessary to run the malware on several\nmachines repeatedly over time. We conduct two further experiments to illustrate the impact of our results on state-of-the-art\nsolutions.\nFirst we reproduce the experiments conducted in one of\nthe most cited behavioral malware clustering techniques [5].\nSuch clustering techniques commonly rely on only one execution trace per sample. Note that our goal here is not to call\ninto question the results of the prior work, but to demonstrate\n\n\n-----\n\nthe effects of variability in a typical malware-clustering experiment. When evaluating this technique on our data, which\nconsists of several executions of the same malware samples,\nwe observe that one third of the samples exhibit sufficient\nvariability in behavior that their traces were scattered among\nmultiple clusters, thus decreasing the accuracy of mapping\nsamples to the correct family. This suggests that we must be\ncautious when drawing conclusions from clustering experiments, as the results may be inaccurate if the experiment\nutilizes a single trace per sample. The details of this experiment can be found in Appendix A.1.\nIn a second experiment, we assess the impact of behavioral\nvariability on the accuracy of anomaly-detection approaches.\nIn this case, we selected AccessMiner [29], a popular solution\nfor building models based on benign execution alone. It is\ninteresting to note that although variability was not explicitly\ndiscussed by the authors, Accessminer correctly accounted\nfor it by including multiple execution of benign software\ncollected from different real-world machine.\nAgain, we repeated the experiments by following the technique explained in the paper (the details can be found in\nAppendix A.2), training the AccessMiner model with a progressively increasing number of traces from benign files in\nour dataset. Our results suggest that behavioral variability of\nbenign programs also impact the detection rate and that only\nfew executions are insufficient to build and accurate model.\nMoreover, our experiment shows that to improve the accuracy\nof the models and reduce false alarms, it is necessary to also\ninclude lower-reputation and low-prevalence benign files to\nthe dataset. In the original AccessMiner paper, only traces\nfrom popular benign files behavior were incorporated into the\nanomaly detector.\n\n**Alternative Solutions to Account for Behavioral Variabil-**\n**ity. Our findings suggest that the more accurate way to collect**\ninformation about malware behavior is to record program\nexecutions on real end-user machines. However, the main\ndrawback of this solution is that known malware needs to\nbe blocked to guarantee the user security, and thus the data\ncollection is limited to files that other methods cannot classify\none way or another.\nOther options exist for researchers to account for the behavioral variability of malware. For instance, Multipath Explo_ration, proposed by Moser et al. [36], allows to automatically_\nexplore multiple execution paths of the malware binary in the\nsame system. As this method is capable of triggering hidden\nfunctionalities, it can replace the need to observe the behavior over different machines and at different points in time.\nHowever, this solution is complex and has a very high performance overhead, which makes it unsuitable for large-scale\nexperiments.\nSimilarly, Symbolic execution could be used to trigger unobserved behaviors during malware analysis, such as in the case\nof time-triggered malware [15]. While this can also help an\n\n\nanalyst to tackle the issue of behavior variability, similarly to\nthe multipath exploration solution, symbolic execution is difficult to scale due to the large overhead and the state explosion\nproblems [53].\nA more practical solution consists in running the samples\non different VMs, with different configurations. While still resource intensive, this method has comparably lower overhead\nthan the previous approaches, making it is easier to apply to\na large number of samples. As we showed in section 5, we\nsuggest running the malware in at least three (and for better\ncoverage even seven) different VMs to capture significant\nmachine-induced variability. We also suggest the analyst to\nre-execute the samples at least every three weeks to capture\nany time-induced variability.\n\n**Threats to validity and limitations. Our study carries some**\nlimitations due to the nature of the data that was provided\nby the security vendor. The data was collected from users\nwho have installed the AV product, who might be in general\nmore careful with the security of their computers and, therefore, might be less exposed to attacks. Although we cannot\nrule out the possibility of selection bias, the large size of the\npopulation in our study, the large fraction of malware (9.15%\nof the unknown samples that could not be classified with\nother means), and the large spectrum of variability that we\nobserved in the experiments, suggest that our results have a\nbroad applicability.\nOur data consists of executions of Windows PE files and\ntherefore, our findings might not apply to the behavior of programs that run on other platforms (Linux [14], Android [55],\nIoT, etc.). Another unfortunate limitation is that the data does\nnot contain network events. Previous work [43], however, has\nalready shown the existence of a high variability in the network events and discussed its impact on the overall behavior\nof malware. Since our goal is not to establish a root cause for\nthe behavior variability, the lack of network data does not impact our main findings. We expect to actually observe higher\nvariability on network events.\nAll samples in our dataset were not flagged neither as benign nor malicious at the time of their collection. Therefore,\nthe data does not include popular benign programs and malware that can be detected by traditional means (i.e., AV Engines). While this might be seen as a limitation because easier\nto label programs might not show similar variability on their\nbehavior, the set of samples we analyzed also make our study\nmore unique in its nature. We only analyze those programs\nthat need to be detected by looking at the behavior. In reality,\nsamples that can be identified simply by other means, such\nas static signatures, do not require a behavioral analysis in\nthe first place. Even if our measurement does not capture the\nvariability of those samples, the impact on behavioral detection would have been irrelevant. Moreover, since our goal\nis to study variations in the runtime behavior, the analysis\ncan only be performed if a sample is executed multiple times,\n\n\n-----\n\nboth in the same environment and across a different set of\nmachines. Therefore, polymorphic samples (in which each\nSHA-256 hash is only observed once) cannot be included in\nour analysis.\nA recent work [46] has shown that for the vast majority\nof malware samples its impossible to identify a family name,\nowing to the use of generic signatures and to inconsistencies\namong the AV labels. Our clustering experiment provides\nfurther insight into this challenge. As we were unable to find\na family name for most of the samples in our dataset, we did\nnot study the behavior variability across malware families.\nWhen measuring the time variability, some actions may not\nre-occur. For example, the malware might not recreate files\nalready created in the previous runs, resulting in a significant\nnumber of missing events in following runs. However, our\nresults show that during the re-executions of the same sample\nwe often observe new events, thus confirming the existence\nof variability over time.\nFinally, our study might have missed malware that can\ncompromise the kernel of the operating system to evade the\nAV data collection component. This is common to all studies\nperformed on AV telemetry, and since we do not have control\nover the execution environment we cannot verify the extent\nof this problem.\n\n##### 7 Related Work\n\nMany prior works explore malware behavior and evolution\nover time [4,8,28] as well as their effects on the accuracy of\nmalware detectors [19,38]. Our work is also inspired by prior\nwork that establish differences in malware behavior across\ndifferent sandbox [6,20,22] or on behavior that remains dormant [9, 13, 25, 47]. Prudent practices have been proposed\nwhen dealing with behavioral data, such as reporting on the\nexact OS version used for the analysis, which is assumed to\naffect the observed malicious behavior [43]. Some effort has\nbeen made by Lindorfer et al. to detect the existence of one\nof the factors that affect the behavior of malware: the environmental bias [31]. Our work does not aim at detecting malware\nthat show such biases, but instead focus on measuring which\nparts of the behavior are more prone to environment sensitivity and to which extent. We also differ from this paper, since\nwe are not trying to establish a causal relationship for our\nresults. Pendlebury et al. show that time is another factor affecting the behavior of malware, which they observe through\nthe deteriorating performance of a behavioral classifier [38].\nWe also measure the effect of time, but look at changes in the\nbehavior instead of at the precision of a classifier.\nFinally, while a large body of research has been dedicated\nto the construction of complex detection models (such as ML\nclassifiers [5,11,24,54]), in but our work we focus on simple token-based rules like those used by SIEM systems [44]\nand other rule-based detection models [10,51], because these\ntokens are the building blocks for more elaborate signatures.\n\n\n##### 8 Conclusions\n\nIt has been known, for over a decade, that malware samples\ncan change their behavior on different hosts and at different\npoints in time, but no study has yet measured this variability\nin the real world. In this paper, we report the first analysis of\nmalware, PUP and benign-sample behavior in the wild, using\nexecution traces collected from 5.4M real hosts from around\nthe world. We show that malware exhibits more variability\nthan benign samples, In particular, we find that, for at least\n50% of the malware, 30% of the actions observed in an execution will not appear in other machines. While there is a lower\nvariability in benign actions, the parameters of these actions\nare often different. In fact most of the parameters (except\nfor file extension) for all the 3 classes of programs have few\nvalues in common across machines. We further show that, for\nmalware that can still execute 2 or more weeks from their\nfirst appearance, the variability is lower and so is their detection rate. We then assess the prevalence of invariant parameter\ntokens that are commonly used to derive behavior based signatures for malware. Even though action parameters that appear\nin every machine execution are uncommon in malware—50%\nof the malware samples have only 8% of parameters in common across all executions—we show that we can use 3 to 7\nmachines to collect parameter tokens that appear in more than\n90% of the executions. Our results also suggest that analysts\nshould re-execute the malware samples 3 weeks after first\nreceiving them to update their behavior models. The findings\nhave important implications for malware analysts and sandbox operators, and they emphasize the unique insights that\nwe can gain by monitoring malware behavior at scale, on real\nhosts.\n\n##### Acknowledgements\n\nWe thank Sandeep Bhatkar and Omer Yampel for helping\nus understand the behavior data, the anonymous reviewers\nand SangHyun Hong for their constructive feedback. This\nresearch was partially supported by A. James & Alice B.\nClark Foundation, the US Department of Defense, and by the\nEuropean Research Council (ERC) under the Horizon 2020\nresearch and innovation program (grant agreement No 771844\n– BitCrumbs).\n\n##### References\n\n[1] Anubis. http://anubis.cs.ucsb.edu.\n\n[2] Norman sandbox. http://www.norman.com/.\n\n[3] Yara. https://virustotal.github.io/yara/.\n\n[4] ABU RAJAB, M., ZARFOSS, J., MONROSE, F., AND TERZIS,\nA. A multifaceted approach to understanding the botnet phenomenon. In Proceedings of the ACM SIGCOMM Internet\n_Measurement Conference, IMC (New York, New York, USA,_\n2006), ACM Press, pp. 41–52.\n\n\n-----\n\n[5] BAILEY, M., OBERHEIDE, J., ANDERSEN, J., MAO, Z. M.,\nJAHANIAN, F., AND NAZARIO, J. Automated Classification\nand Analysis of Internet Malware. Recent Advances in Intru_sion Detection (2007), 178–197._\n\n[6] BALZAROTTI, D., COVA, M., KARLBERGER, C., KRUEGEL,\nC., KIRDA, E., AND VIGNA, G. Efficient Detection of Split\nPersonalities in Malware. NDSS (apr 2010).\n\n[7] BAYER, U., COMPARETTI, P., HLAUSCHEK, C., KRUEGEL,\nC., AND KIRDA, E. Scalable, behavior-based malware clustering. In Network and Distributed System Security Symposium\n_(NDSS) (2009)._\n\n[8] BAYER, U., HABIBI, I., BALZAROTTI, D., KIRDA, E., AND\nKRUEGEL, C. A View on Current Malware Behaviors. In\n_LEET (2009)._\n\n[9] BRUMLEY, D., HARTWIG, C., LIANG, Z., NEWSOME, J.,\nSONG, D., AND YIN, H. Automatically Identifying Trigger_based Behavior in Malware. Springer US, Boston, MA, 2008,_\npp. 65–88.\n\n[10] CANALI, D., LANZI, A., BALZAROTTI, D., KRUEGEL, C.,\nCHRISTODORESCU, M., AND KIRDA, E. A quantitative study\nof accuracy in system call-based malware detection. In Pro_ceedings of the 2012 International Symposium on Software_\n_Testing and Analysis - ISSTA 2012 (New York, New York,_\nUSA, 2012), ACM Press, p. 122.\n\n[11] CHRISTODORESCU, M., JHA, S., AND KRUEGEL, C. Mining\nspecifications of malicious behavior. In Proceedings of the the\n_6th joint meeting of the European software engineering confer-_\n_ence and the ACM SIGSOFT symposium on The foundations of_\n_software engineering - ESEC-FSE ’07 (New York, New York,_\nUSA, 2007), ACM Press, p. 5.\n\n[12] CHRISTODORESCU, M., JHA, S., SESHIA, S. A., SONG, D.,\n\nAND BRYANT, R. E. Semantics-aware malware detection.\nIn Proceedings - IEEE Symposium on Security and Privacy\n(2005), IEEE, pp. 32–46.\n\n[13] COMPARETTI, P. M., SALVANESCHI, G., KIRDA, E., KOL\nBITSCH, C., KRUEGEL, C., AND ZANERO, S. Identifying\nDormant Functionality in Malware Programs. In 2010 IEEE\n_Symposium on Security and Privacy (2010), IEEE, pp. 61–76._\n\n[14] COZZI, E., GRAZIANO, M., FRATANTONIO, Y., AND\nBALZAROTTI, D. Understanding linux malware. In 2018\n_IEEE symposium on security and privacy (SP) (2018), IEEE,_\npp. 161–175.\n\n[15] CRANDALL, J. R., WASSERMANN, G., DE OLIVEIRA, D. A.,\nSU, Z., WU, S. F., AND CHONG, F. T. Temporal search:\nDetecting hidden malware timebombs with virtual machines.\n_ACM SIGOPS Operating Systems Review 40, 5 (2006), 25–36._\n\n[16] DAVID, O. E., AND NETANYAHU, N. S. DeepSign: Deep\nlearning for automatic malware signature generation and classification. In Proceedings of the International Joint Conference\n_on Neural Networks (jul 2015), vol. 2015-Septe, IEEE, pp. 1–8._\n\n[17] DINABURG, A., ROYAL, P., SHARI, M., AND LEE, W. Ether:\nMalware analysis via hardware virtualization extensions. In\n_Proceedings of the ACM Conference on Computer and Commu-_\n_nications Security (New York, New York, USA, 2008), ACM_\nPress, pp. 51–62.\n\n\n\n[18] FREDRIKSON, M., JHA, S., CHRISTODORESCU, M., SAILER,\nR., AND YAN, X. Synthesizing Near-Optimal Malware Specifications from Suspicious Behaviors. In 2010 IEEE Symposium\n_on Security and Privacy (2010), IEEE, pp. 45–60._\n\n[19] JORDANEY, R., HOLLOWAY, R., SHARAD, K., LABORATO\nRIES, N. E. C., DASH, S. K., WANG, Z., PAPINI, D., ELET\nTRONICA, S. A., NOURETDINOV, I., AND CAVALLARO, L.\nTranscend : Detecting Concept Drift in Malware Classification\nModels. In USENIX Security Symposium (2017), USENIX\nAssociation, pp. 625–642.\n\n[20] KIRAT, D., AND VIGNA, G. Malgene: Automatic extraction of\nmalware analysis evasion signature. In Proceedings of the 22nd\n_ACM SIGSAC Conference on Computer and Communications_\n_Security (New York, NY, USA, 2015), CCS ’15, Association_\nfor Computing Machinery, pp. 769–780.\n\n[21] KIRAT, D., AND VIGNA, G. MalGene: Automatic extraction\nof malware analysis evasion signature. In Proceedings of the\n_ACM Conference on Computer and Communications Security_\n(New York, New York, USA, 2015), vol. 2015-Octob, ACM\nPress, pp. 769–780.\n\n[22] KIRAT, D., VIGNA, G., AND KRUEGEL, C. BareCloud: Baremetal Analysis-based Evasive Malware Detection. In 23rd\n_USENIX Security Symposium (USENIX Security 14) (2014)._\n\n[23] KIRDA, E., KRUEGEL, C., BANKS, G., VIGNA, G., AND\nKEMMERER, R. Behavior-based spyware detection. In Usenix\n_Security Symposium (2006), p. 694._\n\n[24] KOLBITSCH, C., COMPARETTI, P. M., KRUEGEL, C., KIRDA,\nE., ZHOU, X., AND WANG, X. Effective and Efficient Malware Detection at the End Host. In Presented as part of the 18th\n_USENIX Security Symposium (USENIX Security 09) (Montreal,_\nCanada, 2009), USENIX.\n\n[25] KOLBITSCH, C., KIRDA, E., AND KRUEGEL, C. The power of\nprocrastination: Detection and mitigation of execution-stalling\nmalicious code. In Proceedings of the ACM Conference on\n_Computer and Communications Security (New York, New_\nYork, USA, 2011), ACM Press, pp. 285–296.\n\n[26] KONNO, H., SHIRAKAWA, H., AND YAMAZAKI, H. A meanabsolute deviation-skewness portfolio optimization model. An_nals of Operations Research 45, 1 (1993), 205–220._\n\n[27] KOTZIAS, P., BILGE, L., VERVIER, P.-A., AND CABALLERO,\nJ. Mind Your Own Business: A Longitudinal Study of Threats\nand Vulnerabilities in Enterprises. In Network and Distributed\n_System Security Symposium (NDSS) (2019)._\n\n[28] KWON, B. J., MONDAL, J., JANG, J., BILGE, L., AND DUMI\nTRA ¸S, T. The dropper effect: Insights into malware distribution\nwith downloader graph analytics. In Proceedings of the 22nd\n_ACM SIGSAC Conference on Computer and Communications_\n_Security (2015), pp. 1118–1129._\n\n[29] LANZI, A., BALZAROTTI, D., KRUEGEL, C., CHRISTODOR\nESCU, M., AND KIRDA, E. Accessminer: Using systemcentric models for malware protection. In Proceedings of\n_the 17th ACM Conference on Computer and Communications_\n_Security (New York, NY, USA, 2010), CCS ’10, Association_\nfor Computing Machinery, pp. 399–412.\n\n\n-----\n\n[30] LEYS, C., LEY, C., KLEIN, O., BERNARD, P., AND LICATA,\nL. Detecting outliers: Do not use standard deviation around\nthe mean, use absolute deviation around the median. Journal\n_of Experimental Social Psychology 49, 4 (2013), 764–766._\n\n[31] LINDORFER, M., KOLBITSCH, C., AND MILANI COM\nPARETTI, P. Detecting environment-sensitive malware. In Lec_ture Notes in Computer Science (including subseries Lecture_\n_Notes in Artificial Intelligence and Lecture Notes in Bioinfor-_\n_matics) (2011), vol. 6961 LNCS, Springer, Berlin, Heidelberg,_\npp. 338–357.\n\n[32] LIU, L., CHEN, S., YAN, G., AND ZHANG, Z. Bottracer:\nExecution-based bot-like malware detection. In International\n_Conference on Information Security (2008), Springer, pp. 97–_\n113.\n\n[33] MANN, H. B., AND WHITNEY, D. R. On a test of whether\none of two random variables is stochastically larger than the\nother. The annals of mathematical statistics (1947), 50–60.\n\n[34] MARTIGNONI, L., STINSON, E., FREDRIKSON, M., JHA, S.,\n\nAND MITCHELL, J. C. A layered architecture for detecting\nmalicious behaviors. In International Workshop on Recent\n_Advances in Intrusion Detection (2008), Springer, pp. 78–97._\n\n[35] MIRAMIRKHANI, N., APPINI, M. P., NIKIFORAKIS, N., AND\nPOLYCHRONAKIS, M. Spotless sandboxes: Evading malware analysis systems using wear-and-tear artifacts. In 2017\n_IEEE Symposium on Security and Privacy (SP) (2017), IEEE,_\npp. 1009–1024.\n\n[36] MOSER, A., KRUEGEL, C., AND KIRDA, E. Exploring Multiple Execution Paths for Malware Analysis. In 2007 IEEE\n_Symposium on Security and Privacy (SP ’07) (may 2007),_\nIEEE, pp. 231–245.\n\n[37] MOSER, A., KRUEGEL, C., AND KIRDA, E. Limits of Static\nAnalysis for Malware Detection. In Twenty-Third Annual\n_Computer Security Applications Conference (ACSAC 2007)_\n(dec 2007), IEEE, pp. 421–430.\n\n[38] PENDLEBURY, F., PIERAZZI, F., JORDANEY, R., KINDER, J.,\n\nAND CAVALLARO, L. {TESSERACT}: Eliminating experimental bias in malware classification across space and time. In\n_28th USENIX Security Symposium 2019) (2019), pp. 729–746._\n\n[39] PRASZMO, M. Ramnit – in-depth analysis.\nhttps://www.cert.pl/en/news/single/ramnit-in-depth-analysis/.\n\n[40] RIECK, K., HOLZ, T., WILLEMS, C., DÜSSEL, P., AND\nLASKOV, P. Learning and classification of malware behavior. In International Conference on Detection of Intrusions\n_and Malware, and Vulnerability Assessment (2008), Springer,_\npp. 108–125.\n\n[41] RIECK, K., TRINIUS, P., WILLEMS, C., AND HOLZ, T. Automatic analysis of malware behavior using machine learning.\n_Journal of Computer Security 19, 4 (jun 2011), 639–668._\n\n[42] ROSSOW, C., DIETRICH, C., AND BOS, H. Large-Scale Analysis of Malware Downloaders. In DIMVA (2012), Springer,\nBerlin, Heidelberg, pp. 42–61.\n\n[43] ROSSOW, C., DIETRICH, C. J., GRIER, C., KREIBICH, C.,\nPAXSON, V., POHLMANN, N., BOS, H., AND VAN STEEN,\nM. Prudent practices for designing malware experiments:\nStatus quo and outlook. In Proceedings - IEEE Symposium on\n_Security and Privacy (may 2012), IEEE, pp. 65–79._\n\n\n\n[44] ROTH, F. Generic Signature Format for SIEM Systems.\nhttps://github.com/Neo23x0/sigma.\n\n[45] SCHREUDERS, Z. C., SHAW, T., SHAN-A-KHUDA, M.,\nRAVICHANDRAN, G., KEIGHLEY, J., AND ORDEAN, M. Security scenario generator (secgen): A framework for generating\nrandomly vulnerable rich-scenario vms for learning computer\nsecurity and hosting {CTF} events. In 2017 {USENIX} Work_shop on Advances in Security Education ({ASE} 17) (2017)._\n\n[46] SEBASTIAN, M., RIVERA, R., KOTZIAS, P., AND CA\nBALLERO, J. Avclass: A tool for massive malware labeling.\nIn Research in Attacks, Intrusions, and Defenses (2016).\n\n[47] SHARIF, M. I., LANZI, A., GIFFIN, J. T., AND LEE, W. Impeding malware analysis using conditional code obfuscation.\nIn NDSS (2008).\n\n[48] SONG, Y., LOCASTO, M. E., STAVROU, A., KEROMYTIS,\nA. D., AND STOLFO, S. J. On the infeasibility of modeling polymorphic shellcode. In Proceedings of the 14th ACM\n_conference on Computer and communications security (2007),_\npp. 541–551.\n\n[49] SS64. Quotes, Escape Characters, Delimiters - Windows CMD\n\n   - SS64.com. https://ss64.com/nt/syntax-esc.html.\n\n[50] TRENDMICRO. TrojanSpy.Win32.GLUPTEBA.A\n   - Threat Encyclopedia    - Trend Micro USA.\nhttps://www.trendmicro.com/vinfo/us/threatencyclopedia/malware/trojanspy.win32.glupteba.a.\n\n[51] TRINIUS, P., WILLEMS, C., HOLZ, T., AND RIECK, K. A\nMalware Instruction Set for Behavior-Based Analysis. Sicher_heit Schutz und Zuverlässigkeit SICHERHEIT (2011)._\n\n[52] WILLEMS, C., HOLZ, T., AND FREILING, F. Toward automated dynamic malware analysis using cwsandbox. IEEE\n_Security & Privacy 5, 2 (2007), 32–39._\n\n[53] YADEGARI, B., AND DEBRAY, S. Symbolic execution of\nobfuscated code. In Proceedings of the 22nd ACM SIGSAC\n_Conference on Computer and Communications Security (2015),_\npp. 732–744.\n\n[54] YIN, H., SONG, D., EGELE, M., KRUEGEL, C., AND KIRDA,\nE. Panorama: Capturing system-wide information flow for\nmalware detection and analysis. In Proceedings of the ACM\n_Conference on Computer and Communications Security (New_\nYork, New York, USA, 2007), ACM Press, pp. 116–127.\n\n[55] ZHOU, Y., AND JIANG, X. Dissecting android malware: Characterization and evolution. In 2012 IEEE symposium on secu_rity and privacy (2012), IEEE, pp. 95–109._\n\n##### A Appendix\n\n A.1 Implications of Variability on Malware Clustering\n\nDynamic malware clustering [5, 7, 40, 41] aims to identify\nmalware families (or variations withing the same family) by\ngrouping together samples with similar behaviors. These approaches commonly rely on only one execution trace per\nsample. Therefore, we investigate how the large variability\n\n\n-----\n\namong the traces of each sample could influence the results\nreported from clustering experiments.\nThis can be performed by clustering execution traces, and\nthen verifying whether the traces of the same sample are clustered together or they are scattered among multiple clusters.\nFor this experiment, we implemented the clustering technique\ndescribed by Bailey et al. [5], which also uses similar features to our dataset. As suggested in the paper, we apply\ntheir normalized compression distance to our samples, and\nwe utilize the same hierarchical clustering algorithm and the\nsame method to determine the number of clusters. We clustered execution traces from 2,424 malware samples. For each\nsample we randomly select 4 traces collected in the same\nweek but on different machines; we repeat this step 10 times.\nWe cluster the resulting 9,696 traces, and we obtain 88–105\nclusters, of which we pick the median with 93 clusters. To\ninterpret these clusters as families of malware samples with\nsimilar behaviors, it is necessary that all executions of a sample fall within its family cluster. In average we found that for\n67% of malware samples all 4 executions appeared indeed in\nthe same cluster. However, one third of the samples exhibit\nsufficient variability in behavior that their traces appear in\nmultiple clusters: 27% fall into 2 clusters, 5% in 3 clusters,\nand 1% in 4 different clusters. This calls into question the\nconclusion that the behavior clusters reflect malware families.\nBecause some samples exhibit too much behavior variability\nto be clustered correctly into families, we must be cautious\nwhen drawing conclusions from clustering experiments. Importantly, this threat to validity comes to light when we cluster\nmultiple traces per sample, but remains hidden when using\nonly a single trace per sample.\n\n##### A.2 Impact on Anomaly Detection\n\n\nrate. As AccessMiner found file write events to be the most\nsuccessful in identifying malware, we first build the graph\nusing the file write actions in our dataset. We measure the\nsuccess of an anomaly based model that relies on only one\nexecution per benign sample (Figure 9a), then the success\nwhen all of the executions available to us included (9b). As\nseen from the figures, a single random benign execution is\nnot sufficient to train an anomaly detector, because it treats\nmost of the executions as anomalies.\nThe detection rates we obtained from this experiment are\nlower than the ones reported in the original paper. Concerning\nthat the nature of our data is very different to the benign\ndataset of AccessMiner this is actually expected. Note that\nour data consists of unpopular benign applications, whose\nbehavior might be more similar to malicious and unwanted\nprograms. To obtain a better behavioral coverage for benign\nprograms, not only popular benign files such as those used\nin AccessMiner should be consider but also lower reputation,\nlower prevalence benign files.\n\n\n**0.7**\n\n**0.6**\n\n**0.5**\n\n**0.4**\n\n**0.3**\n\n**0.2**\n\n**0.1**\n\n**0.0**\n\n\n**Ratio of samples for each detection rate**\n\n**Category**\n\n**Malware**\n**PUP**\n**Benign**\n\n**0.0** **0.1** **0.2** **0.3** **0.4** **0.5** **0.6** **0.7** **0.8** **0.9** **1.0**\n**Detection rate**\n\n|Category|Col2|\n|---|---|\n|Malware PUP||\n|Benign||\n|||\n|||\n|||\n|||\n|||\n\n\nOne way of detecting malware regardless of their variability\nis to detect deviations from benign behavior. In this category, Lanzi et al. proposed AccessMiner [29], as system-level\nanomaly detector based on behavioral traces of benign programs. It is interesting to note that the authors already adopted\na technique that accounted for behavioral variability over time\nand different machine profiles. Similarly to our data, their\ndataset was also collected from real users but their goal was\nnot to study changes in the application behavior but to obtain\na complete picture about how benign files interact with the\nunderlying operating system.\nSince in the AccessMiner paper the authors did not discuss\nhow many executions of benign programs are needed to train\nthe anomaly detector, we decided to leverage our data to find\nan answer to this question such that security companies that\nopt for anomaly detection rather than malware detection could\nbenefit from our results.\nFollowing the AccessMiner approach, we construct the\nbenign profile by using 90% of the benign executions in our\ndataset. Remaining 10% is used to measure the false-positive\n\n\n(a) Using 1 random benign execution\n\n\n**0.6**\n\n**0.5**\n\n\n**0.4**\n\n**0.3**\n\n\n**0.2**\n\n**0.1**\n\n\n**0.0**\n\n\n**Ratio of samples for each detection rate**\n\n**Category**\n\n**Malware**\n**PUP**\n**Benign**\n\n**0.0** **0.1** **0.2** **0.3** **0.4** **0.5** **0.6** **0.7** **0.8** **0.9** **1.0**\n**Detection rate**\n\n|Col1|Category|\n|---|---|\n||Malware PUP|\n|Benign||\n|||\n|||\n|||\n|||\n\n\n(b) Using all benign execution\n\nFigure 9: Amount of samples for the ratio of machines with\nanomalous file writes.\n\n\n-----\n\n25[th] **percentile** **Median** 75[th] **percentile**\n**Mal** **PUP** **Ben** **Mal** **PUP** **Ben** **Mal** **PUP** **Ben**\n\nPath 0.1            - 0.3 0.2 0.5 0.9 0.7 1.0 1.0\nName         -         -         -         - 0.3 0.5 0.8 1.0 1.0\nExt. 0.1 0.1 0.5 0.3 0.7 1.0 1.0 1.0 1.0\n\nPath            -            -            -            - 0.5 0.9 1.0 1.0 1.0\nName         -         -         -         - 0.3 1.0 0.5 1.0 1.0\nExt.             -             -             - 0.3 1.0 1.0 1.0 1.0 1.0\n\nKey Path            -            -            -            - 0.3 1.0 0.75 1.0 1.0\nKey Name          -          -          -          - 0.3 1.0 0.81 1.0 1.0\nValue            -            -            -            -            - 0.5 0.33 0.8 1.0\nDir. Create Path             -             -             -             - 0.6 0.9 0.8 1.0 1.0\nReg. Create Path             -             -             -             -             -             -             - 0.7 1.0\nMtx Create Name           -           - 0.1 0.1 0.4 0.8 0.7 1.0 1.0\nNew Proc. CMD line           -           -           -           -           - 0.5 0.1 1.0 1.0\n\nTable 5: Parameter variability for malware, PUP and benign.\nJaccard index distribution for the top 7 most common actions in our dataset, from week 0 to week 1.\n\n**Median** **ratio >0** **ratio >2** **ratio >5**\n**Mal** **PUP** **Ben** **Mal** **PUP** **Ben** **Mal** **PUP** **Ben** **Mal** **PUP** **Ben**\nAll actions 39 19 8 99% 95% 83% 96% 91% 68% 92% 77% 57%\nFile Create 33 2.8 1.8 92% 74% 64% 84% 58% 45% 71% 36% 35%\nMutex Create 6 3 1.3 93% 78% 65% 80% 68% 37% 57% 33% 20%\nRegistry Set 7.5 3 1 92% 74% 54% 77% 60% 34% 58% 42% 22%\nDirectory Create 4 1 0 82% 55% 41% 56% 36% 16% 42% 12% 8%\nReg. Key Create 1.8 1 0 71% 52% 36% 39% 34% 18% 7% 16% 10%\nPE Create 1.3 0.3 0 76% 50% 35% 28% 29% 18% 6% 13% 12%\nProcess Load 4 1 0 83% 54% 29% 60% 20% 3% 34% 6% 1%\n\nTable 6: IQR variability across machines for malware, PUP and benign samples across different machines. In the last 3\ncolumns we measure the ratio of samples that show a variability greater than 0, 2 and 5 events across machines. The larger the\npercentage value in those columns, the more samples have IQR greater than the threshold.\n\n|Col1|Col2|25th percentile|Median|75th percentile|\n|---|---|---|---|---|\n|||Mal PUP Ben|Mal PUP Ben|Mal PUP Ben|\n|File|Path Name Ext.|0.1 - 0.3 - - - 0.1 0.1 0.5|0.2 0.5 0.9 - 0.3 0.5 0.3 0.7 1.0|0.7 1.0 1.0 0.8 1.0 1.0 1.0 1.0 1.0|\n|PE|Path Name Ext.|- - - - - - - - -|- 0.5 0.9 - 0.3 1.0 0.3 1.0 1.0|1.0 1.0 1.0 0.5 1.0 1.0 1.0 1.0 1.0|\n|Reg.Set|Key Path Key Name Value|- - - - - - - - -|- 0.3 1.0 - 0.3 1.0 - - 0.5|0.75 1.0 1.0 0.81 1.0 1.0 0.33 0.8 1.0|\n||Dir. Create Path|- - -|- 0.6 0.9|0.8 1.0 1.0|\n||Reg. Create Path|- - -|- - -|- 0.7 1.0|\n||Mtx Create Name|- - 0.1|0.1 0.4 0.8|0.7 1.0 1.0|\n||New Proc. CMD line|- - -|- - 0.5|0.1 1.0 1.0|\n\n|Col1|Median|ratio >0|ratio >2|ratio >5|\n|---|---|---|---|---|\n||Mal PUP Ben|Mal PUP Ben|Mal PUP Ben|Mal PUP Ben|\n|All actions|39 19 8|99% 95% 83%|96% 91% 68%|92% 77% 57%|\n|File Create|33 2.8 1.8|92% 74% 64%|84% 58% 45%|71% 36% 35%|\n|Mutex Create|6 3 1.3|93% 78% 65%|80% 68% 37%|57% 33% 20%|\n|Registry Set|7.5 3 1|92% 74% 54%|77% 60% 34%|58% 42% 22%|\n|Directory Create|4 1 0|82% 55% 41%|56% 36% 16%|42% 12% 8%|\n|Reg. Key Create|1.8 1 0|71% 52% 36%|39% 34% 18%|7% 16% 10%|\n|PE Create|1.3 0.3 0|76% 50% 35%|28% 29% 18%|6% 13% 12%|\n|Process Load|4 1 0|83% 54% 29%|60% 20% 3%|34% 6% 1%|\n\n|Action type|Mal|PUP|Ben|\n|---|---|---|---|\n|FileCreated|25|60|27|\n|IESecurity|17|12|12|\n|RegistryValueSet|17|10|16|\n|DirectoryCreated|6.7|9|6.3|\n|ProcessLoad|2.7|5|1.9|\n|RegistryKeyCreated|7.8|4|9.9|\n|PECreation|7.9|3.8|15|\n|ProcessInjection|2.9|1.7|2.6|\n|DesktopShortcut|1.4|1.6|1.5|\n|ProcessManipulationInjection|1.2|1.4|1.2|\n|IEHomePage|1.0|1.1|1.6|\n|InternetProxyServer|1.2|1.0|1.4|\n|Others (mean)|≈1.5|≈0.3|≈1.4|\n\n|Action type|Mal|PUP|Ben|\n|---|---|---|---|\n|FileCreated|0.74|0.88|0.75|\n|DirectoryCreated|0.47|0.73|0.35|\n|RegistryValueSet|0.56|0.67|0.47|\n|ProcessLoad|0.65|0.65|0.79|\n|PECreation|0.42|0.57|0.29|\n|RegistryKeyCreated|0.31|0.41|0.25|\n|OpenService|0.24|0.29|0.15|\n|DesktopShortcut|0.04|0.14|0.02|\n|CreateService|0.01|0|0.01|\n|KeyloggerShield|0.03|0|0.11|\n|ProcessInjection|0.04|0|0.02|\n|IESecurity|0|0|0|\n|Others (mean)|≈0|0|0|\n\n\nTable 7: Average number of actions per execution for executions\nwith at least 1.\n\n\nTable 8: Average appearance of an action type across machines.\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2021/2021-08-13 - When Malware Changes Its Mind - A Study of Variable Program Behaviors.pdf"
    ],
    "report_names": [
        "2021-08-13 - When Malware Changes Its Mind - A Study of Variable Program Behaviors.pdf"
    ],
    "threat_actors": [
        {
            "id": "d90307b6-14a9-4d0b-9156-89e453d6eb13",
            "created_at": "2022-10-25T16:07:23.773944Z",
            "updated_at": "2025-03-27T02:02:09.974695Z",
            "deleted_at": null,
            "main_name": "Lead",
            "aliases": [
                "Casper",
                "TG-3279"
            ],
            "source_name": "ETDA:Lead",
            "tools": [
                "Agentemis",
                "BleDoor",
                "Cobalt Strike",
                "CobaltStrike",
                "RbDoor",
                "RibDoor",
                "Winnti",
                "cobeacon"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "46b3c0fc-fa0c-4d63-a38a-b33a524561fb",
            "created_at": "2023-01-06T13:46:38.393409Z",
            "updated_at": "2025-03-27T02:00:02.822155Z",
            "deleted_at": null,
            "main_name": "APT29",
            "aliases": [
                "The Dukes",
                "Minidionis",
                "Grizzly Steppe",
                "G0016",
                "Blue Kitsune",
                "BlueBravo",
                "SeaDuke",
                "Cloaked Ursa",
                "YTTRIUM",
                "ATK7",
                "Nobelium",
                "UAC-0029",
                "Group 100",
                "COZY BEAR",
                "IRON HEMLOCK",
                "TA421",
                "ITG11"
            ],
            "source_name": "MISPGALAXY:APT29",
            "tools": [
                "QUARTERRIG",
                "SNOWYAMBER",
                "HALFRIG"
            ],
            "source_id": "MISPGALAXY",
            "reports": null
        }
    ],
    "ts_created_at": 1673535970,
    "ts_updated_at": 1743041129,
    "ts_creation_date": 0,
    "ts_modification_date": 1627843689,
    "files": {
        "pdf": "https://archive.orkl.eu/eed8938a335897b01911f778bec91e781b5981ab.pdf",
        "text": "https://archive.orkl.eu/eed8938a335897b01911f778bec91e781b5981ab.txt",
        "img": "https://archive.orkl.eu/eed8938a335897b01911f778bec91e781b5981ab.jpg"
    }
}