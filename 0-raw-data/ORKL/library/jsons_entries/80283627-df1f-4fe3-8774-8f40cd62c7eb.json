{
    "id": "80283627-df1f-4fe3-8774-8f40cd62c7eb",
    "created_at": "2022-10-25T16:48:22.770799Z",
    "updated_at": "2025-03-27T02:05:25.20892Z",
    "deleted_at": null,
    "sha1_hash": "400e04bf19bcfa10af7df51240f27bab15f12644",
    "title": "",
    "authors": "",
    "file_creation_date": "2019-10-29T08:27:38Z",
    "file_modification_date": "2019-10-29T08:27:38Z",
    "file_size": 6839974,
    "plain_text": "# The Discovery of Fishwrap: A New Social Media Information Operation Methodology\n\n**recordedfuture.com/fishwrap-influence-operation**\n\nJune 11, 2019\n\nJune 11, 2019 • Staffan Truvé\n\n_[Click here to download the complete analysis as a PDF.](https://go.recordedfuture.com/hubfs/reports/cta-2019-0612.pdf)_\n\nFor several years, Recorded Future has been developing tools and methodologies for detecting\nand analyzing influence operations by nation-states and others. We have recently upgraded some\nof these tools and applied them to detecting a new kind of influence operation, which recycles old\nnews about terror incidents by publishing them to appear as new. We refer to this technique as\n“Fishwrap.” This operation is also using a special family of URL shorteners that allow attackers to\ntrack click-through from social media posts used in their campaigns.\n\n## Key Findings\n\nWe have developed new algorithms for identifying influence operations. These algorithms\nallow for the detection of “seed accounts,” which can be used to analyze additional\naccounts engaged in an operation.\nBehavioral analytics based on topological methodologies can be used to analyze the\nhighest-likelihood participants in an operation and cluster those with the highest degree of\nsimilarity.\nUsing this methodology, we identified a new kind of influence operation: Fishwrap. This\nmethodology uses old terror news masquerading as new.\nOver 215 social media accounts participating in the Fishwrap operation were analyzed.\nThese social media accounts use a special family of at least 10 different URL shortener\nservices that allow for tracking the effectiveness of the operation. All of these URL services\nare running the same code and are hosted on the same commercial infrastructure.\nThe accounts’ behavioral similarity leads us to believe that they are all part of the same\ninfluence operation.\nSince account holders are most likely fictive and the domains used for the URL shortener\nservices are registered anonymously, attribution is difficult — however, research is ongoing.\n\n\n-----\n\n## Influence Operations\n\n[RAND Corporation defines information operations and warfare, also known as influence](https://www.rand.org/topics/information-operations.html)\noperations, as the collection of tactical information about an adversary, as well as the\ndissemination of propaganda in pursuit of a competitive advantage over an opponent.\n\nRecorded Future and its partners have been engaged in identifying and characterizing influence\noperations for more than five years, with studies including how terror supporters spread\n[propaganda, election hacking in the U.S. in 2016 and 2018, and how China uses social media to](https://www.recordedfuture.com/nation-state-cyber-activity/)\ninfluence U.S. opinion.\n\nInfluence operations are, of course, not an exclusive nation-state activity — political groups use\n[the same mechanisms, as do criminals engaging in stock manipulation activities.](https://www.ft.com/content/a37e4874-2c2a-11e7-bc4b-5528796fe35c)\n\n### The Anatomy of an Influence Operation\n\nInfluence operations that aim to change public opinion (for example, trying to sway the outcome\nof an election) need to reach a large number of people with messages that resonate with their\nbeliefs or fears. Previously, certain groups, including the Nazis in the 1930s all the way to the\nHutus in the 1990s, used radio to obtain massive reach. Today, social media has become the\nchannel of choice for influence operations. Using data-driven profiling, messages can be\n[personalized, as highlighted by the Cambridge Analytica scandal.](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal)\n\nEven though “fake news” has become highly associated with influence operations, in many cases,\n“real news” is also used, but it’s carefully selected to emphasize the opinions the operation\nwishes to foment These news pieces can be distributed in different ways such as through paid\n\n\n-----\n\naccounts that are controlled by humans (so-called trolls ), or with algorithms. Advertising\ncampaigns are hard to detect without access to users’ news feeds, but campaigns using ordinary\nsocial media posts by a large number of accounts will be possible to detect by using a solution\nsuch as Recorded Future.\n\n### Detecting and Tracking Influence Operations\n\nTo detect an influence operation, we need a starting point — a seed. This can be a particular\nstory that is used by the operation engaging in it. Once one or more seeds have been selected,\nwe can broaden the scope of our investigation by finding related topics, such as URLs, hashtags,\nand user accounts. The image below illustrates how our Snowball algorithm can be used to grow\nthe number of potential accounts engaged in an operation, starting with either a few accounts or\nwith some topics believed to be used by an operation.\n\nThe Snowball algorithm generates a large number of candidate accounts, but will also typically\nfind many false positives since “innocent” accounts will repost news from the operation. To decide\nwhich accounts are really part of the operation, we can use behavioral analytics to characterize\nand cluster the activities implicated by the Snowball algorithm.\n\nOur approach to analytics is to define a number of similarity metrics over a large set of social\nmedia accounts. For example, accounts are more similar if they:\n\nPost the same URLs and hashtags — the similarity is stronger if only a few accounts post a\ncertain URL or hashtag within a certain time frame\nPost on the same topics, as defined by the entities and events they mention\nAre using the same URL shorteners\nHave similar temporal behavior — either their overall period of activity or their weekly or\ndaily behavioral patterns\nHave mutually exclusive but adjacent overall periods of activity — this is a weak but not\ninsignificant indication that one account has replaced the other\nHave similar account names, as defined by the editing distance between their names\n\nAll of these different similarity metrics can be used to cluster accounts. We then look at sets of\naccounts which are associated with multiple identical similarity clusters. These accounts are\n\n\n-----\n\njoint membership in one cluster are not very likely to be part of the operation — they are just\nassociated (for example, by having reposted a post originated by the operation).\n\n### Fake News in Influence Operations: 2 Examples\n\nIdentifying fake or biased news can be hard, but by looking at some examples, we can gain some\ninsight. For example, in a timeline view of reports on protests in Sweden in 2018, one event on\nJuly 16, 2018 regarding Muslims protesting against crosses stands out because it is only being\nreported in Russian. This is quite strange for what should be major news in Sweden, and different\nfrom all other major protests during the period.\n\n\n-----\n\nEven though we first found this “news” in Russian, it was also referenced by other social media\naccounts (like U.S. alt-right ones).\n\n\n-----\n\nThis particular story turns out to be entirely fake, and in this case, we could actually use a reverse\nimage search to find the original story, which turns out to be about students protesting in Chile\ntwo years earlier.\n\n\n-----\n\nThis example validates our intuition that important news stories which are reported either only in a\nsingle language, in a surprisingly small volume, or only on social media, are good initial\ncandidates for being fake or hyperpartisan news used in an influence operation.\n\nAs another example, we compared the actual source distribution of a real terror event that took\nplace on November 13, 2015 in Paris (177 thousand references, mostly in mainstream media)\nwith fake news about an event in Paris on March 23, 2019 (only 19 references, mostly on social\nmedia). The account similarity associated with being one of the few accounts posting on March\n23, 2019 is much higher than that associated with being one of thousands of accounts posting on\nNovember 13, 2015.\n\n## Fishwrap: A New Influence Operation\n\nThe fake Paris terror event mentioned above is actually part of an influence operation that we\nhave detected using our new algorithms. This operation is focused on posting old terror news\nstories as if they were new, probably with the goal of spreading general fear and uncertainty. We\ncall this operation Fishwrap, since it uses old news for other purposes.\n\n\n-----\n\nWe first detected Fishwrap through our automatic tracking of terror events only reported by social\nmedia, like the Paris example above. Below is another example of such event reporting.\n\nBy tracking terror events in Recorded Future that were only reported on social media, we were\nable to find a set of about a dozen accounts clearly engaged in spreading old terror news as if it\nwere new. The reason we compared the specific dates in the Paris terror example above is\nbecause of a social media post from March 23, 2019 reporting on an event shown in the image\nbelow.\n\nHowever, the URL-shortened link in the post led us to an article about the original event from\nNovember 13, 2015. While many readers would probably not scrutinize the publication dates, it is\neasy to see how the post could cause concern for those reading it and prompt them to follow the\nlink to validate the news, missing the difference in publication date.\n\n\n-----\n\nBy applying the Snowball algorithm to the small set of identified posts, we could swiftly grow the\nnumber of suspicious activities in this operation to more than a thousand profiles.\n\n### Narrowing It Down\n\nWe then looked at the similarities within this fairly large set of accounts and concentrated on three\nspecific aspects:\n\n1. Temporal behavior\n2. The domain of the URLs referred to in the accounts’ posts\n3. Account status\n\n### Temporal Behavior\n\nBy plotting the activity (postings) of all accounts found in the operation, we can identify different\nactivity periods.\n\n\n-----\n\nWe clearly see three different clusters of accounts:\n\n1. Those active between May 2018 to October 2018\n2. Those active between November 2018 to April 2019\n3. Those active during the entire time period, between May 2018 to April 2019\n\nThese temporal patterns indicate the launch of a number of accounts in May 2018, many of which\nwere shut down in October 2018. These were followed a few weeks later by a new batch of\naccounts with the same behavior and still in operation.\n\n### Topic Similarity: URLs and Domains Used\n\nBy looking at the URLs posted by a subset of the accounts, it is clear that there is some\nrelationship between them since they, to some extent, post identical URLs.\n\n\n-----\n\nThe graph above looks quite complicated. However, if we focus only on the domains of these\nURLs, a much simpler picture emerges, shown below.\n\n\n-----\n\nIt turns out that a lot of the accounts are posting all of their links through a small number of URL\nshortener services (in the example above, pucellina[.]com and bioecologyz[.]com). More\nprecisely, we can identify 10 domains hosting URL shortener services that are used for essentially\nevery single post made by 215 of the identified accounts. As seen in the graph above, some\naccounts use multiple URL shorteners, and thus show that there is an indirect connection\nbetween accounts using different shorteners. Overall, we can see that each domain has a fairly\nlarge number of accounts that has published some reference to it, and a very small number of\naccounts have published a reference to more than one of the domains (based on the data we’ve\ncollected). The exception is the two domains pucellina[.]com and bioecologyz[.]com, where a\nlarger number of accounts have published references to both domains.\n\nIt gets more interesting! Upon inspecting the 10 different URL shortener websites, we immediately\nsee that their appearance is identical.\n\n\n-----\n\nThis is strong evidence linking all 215 accounts to 10 URL shorteners, which in turn appear to be\nrunning the same code. Interestingly, an inspection of the HTML code for the URL shorteners also\nreveals that these domains seem to be tracking all agents that follow the links. This could be to\nmeasure the effectiveness of the operation, but it might also be used for profiling the “captured\naudience” of the operation.\n\nUnfortunately, all of these 10 domains are anonymously registered, and we cannot see who\nregistered or owns them. We did investigate where these services are hosted, and it turns out\nthey are all currently running on dedicated servers on Microsoft Azure.\n\n\n**Men‐**\n**tions**\n\n\n**Domain** **Created** **Updated** **IP**\n\n\n**Selected Historic Name**\n**Server**\n\n\n459.io 2018-0426\n\nn1o.io 2018-0427\n\n3df.me 2018-0427\n\nbki.me 2018-0427\n\ngji.me 2018-0427\n\nxih.me 2018-0427\n\nzk0.io 2018-0427\n\npucellina.com 2018-0917\n\n\n40.117.116.52 24,156\n\n23.98.135.84 14,113\n\n104.209.158.224 20,315\n\n23.96.7.223 adminsky.cn 17,703\n\n104.209.183.79 xincache.com 15,801\n\n13.78.148.193 22.cn 9,353\n\n23.101.187.48 16,549\n\n104.209.246.70 5,983\n\n40.76.26.54 3,215\n\n\nbioecologyz.‐\ncom\n\n\n2018-0917\n\n\n2018-0625\n\n2018-0626\n\n2018-0626\n\n2018-0626\n\n2018-0626\n\n2018-0626\n\n2018-0626\n\n2018-0917\n\n2018-0917\n\n\n-----\n\nf89.me 2018-0426\n\n\n2018-0625\n\n\n13.78.137.38 22.cn 11,232\n\n\nWe can clearly see two clusters of domains corresponding to the two time frames we identified in\nthe temporal analysis above. The first eight domains were created just prior to the observed start\nof the campaign (the red accounts in the temporal analysis above), and the last two domains\nwere created some weeks before the launch of the second wave of the campaign (the orange\naccounts in the temporal analysis above).\n\nWe can also see historic name servers for a couple of domains, but this does not give us much\nadditional information. None of the 10 domains have any risk score in Recorded Future — the\nonly trace of malicious activity is that one of the previous name servers has been associated with\na suspicious IP number and a malware command and control server.\n\nSadly, due to anonymous registration, this is where our trail ends! We can only speculate as to\nwho registered these domains and is running the network of social media accounts that use them.\nThe fact that the operation has been going on for close to a year, and that it is spending money\non numerous domains on dedicated servers, leads us to believe this is not just someone running\nthe operation “for the lulz,” but rather, a political organization or nation-state with an intent to\nspread fear and uncertainty and track followers of the posted links.\n\n### Account Status\n\nUpon closer inspection of the status of the accounts, we note that a fair percentage of them have\nbeen suspended. The degree of suspension varies between different URL shortener service\nclusters, but it is clear that there has been no general suspension of accounts related to these\nURL shorteners. We believe one reason for this is that by posting links related to old, but real,\nterror events, the accounts are not in clear violation of any terms of service, and therefore have\nnot been suspended due to either automatic identification or manual reporting.\n\n## Conclusion\n\n\n-----\n\nBased on a long track record of investigating influence operations, we developed a set of\nalgorithms for identifying and analyzing such operations. Using these algorithms, we have\nidentified a fairly large influence operation that uses old terror events repackaged as breaking\nnews to gain attention. Using behavioral analytics, we have shown how more than 215 accounts\nhave participated in or are participating in this operation. The operation uses a set of dedicated\nURL shortener services to link to old news, and this mechanism allows the operation to track the\nefficiency of its operation and possibly to analyze what kind of audience (at least geographically)\nit succeeds in targeting.\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "5d2b9e7f-cf43-4b54-ba18-065aa3003611",
            "created_at": "2022-10-25T16:06:24.199525Z",
            "updated_at": "2022-10-25T16:06:24.199525Z",
            "deleted_at": null,
            "name": "CyberMonitor",
            "url": "https://github.com/CyberMonitor/APT_CyberCriminal_Campagin_Collections",
            "description": "APT & Cybercriminals Campaign Collection",
            "reports": null
        }
    ],
    "references": [
        "https://github.com/CyberMonitor/APT_CyberCriminal_Campagin_Collections/raw/master/2019/2019.06.11.Fishwrap_Group/The%20Discovery%20of%20Fishwrap_%20A%20New%20Social%20Media%20Information%20Operation%20Methodology.pdf"
    ],
    "report_names": [
        "The Discovery of Fishwrap_ A New Social Media Information Operation Methodology"
    ],
    "threat_actors": [],
    "ts_created_at": 1666716502,
    "ts_updated_at": 1743041125,
    "ts_creation_date": 1572337658,
    "ts_modification_date": 1572337658,
    "files": {
        "pdf": "https://archive.orkl.eu/400e04bf19bcfa10af7df51240f27bab15f12644.pdf",
        "text": "https://archive.orkl.eu/400e04bf19bcfa10af7df51240f27bab15f12644.txt",
        "img": "https://archive.orkl.eu/400e04bf19bcfa10af7df51240f27bab15f12644.jpg"
    }
}