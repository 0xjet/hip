{
    "id": "cd7ea478-942d-4469-a8b3-00a54163fe46",
    "created_at": "2023-01-12T15:08:57.899729Z",
    "updated_at": "2025-03-27T02:05:42.246269Z",
    "deleted_at": null,
    "sha1_hash": "99f5dc057b1ff6acc93d26201f119c2b7bd64f70",
    "title": "2016-07-07 - New threat dubbed Zepto Ransomware is spreading out with a new email spam campaign. It is a variant of the recent Locky Ransomware.",
    "authors": "",
    "file_creation_date": "2016-08-01T12:59:51Z",
    "file_modification_date": "2016-08-01T12:59:51Z",
    "file_size": 1145287,
    "plain_text": "# Commonalities in  Vehicle Vulnerabilities\n\n### Corey Thuen Senior Security Consultant\n\n IOActive\n\n## Abstract\n\nWith the Connected Car becoming commonplace in the market, vehicle\ncybersecurity grows more important by the year. At the forefront of this growing area\nof security research, IOActive has amassed real-world vulnerability data illustrating\nthe general issues and potential solutions to the cybersecurity issues facing today’s\nvehicles.\n\nThis paper explains the differences in testing methodologies, with recommendations\non the most appropriate methods for testing connected vehicle systems. Detailed\nfindings follow, including the impact, likelihood, overall risk, and remediation of\nvulnerabilities IOActive consultants have discovered over the course of thousands of\ntesting hours. The paper concludes with a recommendation for an “ounce of\nprevention” that may ensure more secure vehicle systems in the future.\n\n_© 2016 IOActive, Inc. All Rights Reserved_\n\n\n-----\n\n## Contents\n\nAbstract...................................................................................................................................... 1\n\nIntroduction ................................................................................................................................ 3\n\nThreat Modeling the Connected Car .......................................................................................... 3\n\nHolistic Model ......................................................................................................................... 3\n\nComponent Model .................................................................................................................. 4\n\nTest Methodologies .................................................................................................................... 4\n\nCategorizing Vulnerabilities ........................................................................................................ 4\n\nExample Categorization .......................................................................................................... 6\n\nCommon Vulnerabilities and Analysis ........................................................................................ 8\n\nImpact ..................................................................................................................................... 8\n\nLikelihood ............................................................................................................................... 8\n\nOverall Risk Level ................................................................................................................... 9\n\nAttack Vector .......................................................................................................................... 9\n\nTop Eight Vulnerabilities ........................................................................................................10\n\nCritical Impact Remediation ...................................................................................................12\n\nTesting Method ......................................................................................................................12\n\nImpact on Vehicle ..................................................................................................................14\n\nDefenses and Effectiveness ......................................................................................................15\n\nOunce of Prevention ..............................................................................................................15\n\nConclusion ................................................................................................................................17\n\nFuture Work ..............................................................................................................................17\n\n\n-----\n\n## Introduction\n\nThis paper provides a metadata analysis of the multitude of private vehicle security\nassessments IOActive has conducted, also incorporating other publicly available\nresearch to provide an informative picture of the vulnerabilities that researchers are\nuncovering, the specific systems and attack vectors most prevalently affected, and the\nreal-world significance of these vulnerabilities. This data is extremely useful for\norganizations considering cybersecurity strategy and planning.\n\nVehicle cybersecurity has become a focused and growing area of security research\nfor IOActive. In 2013, the company conducted about 2,000 hours of combined\nresearch and services in the vehicle cybersecurity space, doubling to 4,000 in 2014\nand spiking to 10,000 hours in 2015, and we anticipate that number continuing to grow\ngoing forward.\n\nThis experience puts IOActive’s Vehicle Cybersecurity Division in a unique position to\nprovide valuable insight into common struggles, failures, and solutions facing the\nautomotive and related transportation and components industries. This research uses\nhard data taken from vulnerability assessments of real-world vehicle systems. We\nhave conducted enough of these assessments to properly anonymize the sources of\nthis information and extract the valuable “big-picture” aspects.\n\nFirst, in order to make use of the data we must establish a foundation of cybersecurity\nterminology and comprehension. We will discuss threat modeling, attack vectors, and\nattacker methodologies in order to explain how we discovered these vulnerabilities.\nSecond, we will cover categorization and walk through a vulnerability evaluation.\nFinally, we will look at the data itself and answer some key questions:\n\n       - What kind of vulnerabilities most commonly affect the Connected Car?\n\n       - What attack vectors are most commonly used to compromise a vehicle?\n\n       - What percentage of vulnerabilities would defense product XYZ mitigate?\n\n       - How do automotive and component manufacturers best manage their limited\ncybersecurity resources to maximize effectiveness?\n\n## Threat Modeling the Connected Car\n\nUnderstanding the attack surfaces of the connected car is an important first step. This\nmeans noting the possible ways to attack a target, which might be the entire vehicle or\njust a component therein. A threat model does not focus on attack methods, but rather\nlooks at possible attack vectors.\n\n### Holistic Model\n\nFor a holistic threat model of the Connected Car, we are interested in looking at how\ndata can enter the vehicle, and what the potential impact to the vehicle is if a given\nvulnerability is exploited. These boundaries are where an attacker will focus their\nefforts, and therefore where defense efforts should be prioritized.\n\n\n-----\n\nData can enter vehicles in a variety of ways, including:\n\n   - Cellular Radio   - V2V Radio (802.11p)\n\n   - Bluetooth   - OBDII\n\n   - Wi-Fi   - Infotainment Media\n\n   - Companion Apps   - Zigbee Radio (e.g. TPMS)\n\n### Component Model\n\nThreat modeling is also quite effective at the system or component level. For example,\na threat model of an infotainment unit will include inputs from any media inserted,\nsuch as Bluetooth pairing with smartphones, physical media access via USB, and\npossibly cellular data communications. The threat model aids in organizing the overall\ndevelopment effort. For example, time spent searching an entire code base for buffer\noverflow vulnerabilities may not be as effective as focusing on code that interacts with\nhigher-risk attack vectors, such as Wi-Fi.\n\n## Test Methodologies\n\nCybersecurity researchers will vary their testing methodologies depending on the\ncomponent being analyzed, but most testing falls under two categories: black box and\nwhite box.\n\nBlack box testing is so named because the researcher is given no foreknowledge or\ninsight into how a system operates. The researcher takes the role of an attacker who\nmust evaluate the system, discover how it works, and attempt to find and exploit\nvulnerabilities. This usually involves dynamic testing utilizing protocol fuzzing,\nhardware analysis, chip desoldering, observation of serial bus lines, capturing\nfirmware updates, and other methods.\n\nWhite box (which is often actually grey box - somewhere in between true black box\nand white box) testing is so named because the researcher works with the product\ndeveloper to evaluate the system. The developer will often provide code or a debug\ntesting harness. The methods and “attacker mindset” approach used in white/grey box\ntesting by IOActive are similar to black box testing, but may also include code\nanalysis, protocol specifications, and design review.\n\nIn general, white box testing has provided the best ROI to clients and most of the\nuseful data for this paper. This is because greater insight into a system makes for\nmore meaningful bug reports and more accurate assessments of impact that can be\nbetter aligned to the specific business risks of the client.\n\n## Categorizing Vulnerabilities\n\nTo provide a meaningful, quantitative analysis of its findings, IOActive uses an impactversus-likelihood approach to scoring. For each individual finding, the assessment\nteam assigns two ratings, one for impact and another for likelihood; that is, the\nlikelihood the given vulnerability will be exploited. Each vulnerability is then assigned a\nrating of Critical, High, Medium, Low, or Informational, with corresponding numeric\n\n\n-----\n\nscores ranging from 5 (Critical) to 1 (Informational). This table explains each rating in\nterms of score, impact, and likelihood.\n\n**_Rating (Score)_** **_Impact_** **_Likelihood_**\n\nExtreme impact to vehicle if Vulnerability is almost certain to\nexploited. be exploited.\n\n**Critical (5)** Would receive media attention. Knowledge of the vulnerability and\n\nits exploitation are in the public\ndomain.\n\nMajor impact to vehicle if Vulnerability is relatively easy to\n\n**High (4)** exploited. detect and exploit by an attacker\n\nCould be a regulatory violation. with little skill.\n\nNoticeable impact to vehicle if An expert attacker could exploit\n\n**Medium (3)** exploited. the vulnerability without much\n\ndifficulty.\n\nMinor impact if exploited, or Exploiting the vulnerability would\ncould be used in conjunction require considerable expertise\n\n**Low (2)**\n\nwith other vulnerabilities to and resources.\nperform a more serious attack.\n\nPoor programming practice or Vulnerability is not likely to be\npoor design decision that may exploited on its own, but may be\nnot represent an immediate risk used to gain information for\n\n**Informational (1)** on its own, but may have launching another attack.\n\nsecurity implications if multiplied\nand/or combined with other\nvulnerabilities.\n\n_Table 1 – Vulnerability rating system_\n\nIOActive assigns aggregate risk scores to identified vulnerabilities; specifically, the\nimpact score multiplied by the likelihood score. For example, a vulnerability with high\nlikelihood and low impact would have an aggregate risk score of eight (8); that is, four\n(4) for high likelihood multiplied by two (2) for low impact. The Aggregate Risk Score\ndetermines the finding's Overall Risk Level.\n\n**_Aggregate Risk Score_**\n**_Overall Risk Level_**\n**_(Impact multiplied by Likelihood)_**\n\nCritical **20–25**\n\nHigh **12–19**\n\nMedium **6–11**\n\nLow **2–5**\n\nInformational **1**\n\n_Table 2 - Overall Risk Levels and corresponding Aggregate Risk Scores_\n\nThe Common Vulnerability Scoring System (CVSS) has proven effective for other\norganizations, but experience has taught us that a simpler and more relatable scoring\n\n|Rating (Score)|Impact|Likelihood|\n|---|---|---|\n|Critical (5)|Extreme impact to vehicle if exploited. Would receive media attention.|Vulnerability is almost certain to be exploited. Knowledge of the vulnerability and its exploitation are in the public domain.|\n|High (4)|Major impact to vehicle if exploited. Could be a regulatory violation.|Vulnerability is relatively easy to detect and exploit by an attacker with little skill.|\n|Medium (3)|Noticeable impact to vehicle if exploited.|An expert attacker could exploit the vulnerability without much difficulty.|\n|Low (2)|Minor impact if exploited, or could be used in conjunction with other vulnerabilities to perform a more serious attack.|Exploiting the vulnerability would require considerable expertise and resources.|\n|Informational (1)|Poor programming practice or poor design decision that may not represent an immediate risk on its own, but may have security implications if multiplied and/or combined with other vulnerabilities.|Vulnerability is not likely to be exploited on its own, but may be used to gain information for launching another attack.|\n\n|Overall Risk Level|Aggregate Risk Score (Impact multiplied by Likelihood)|\n|---|---|\n|Critical|20–25|\n|High|12–19|\n|Medium|6–11|\n|Low|2–5|\n|Informational|1|\n\n\n-----\n\nmetric facilitates better understanding of overall vulnerability. CVSS does have scoring\nparameters that are useful for the type of meta-analysis performed for this paper, but\nthe research goes beyond CVSS parameters to evaluate other aspects of a\nvulnerability.\n\nIn order to further categorize vulnerabilities in a meaningful way, IOActive collected\nadditional data for each vulnerability finding. For instance, testers gathered data\nrelating a vulnerability to a particular attack vector, a given methodology, or type (e.g.,\nbuffer overflow, authentication issue, etc.).\n\n### Example Categorization\n\nA recent high-profile example[1] effectively illustrates how a vulnerability is evaluated. A\nvehicle manufacturer released a companion smartphone app to interface with the\nvehicle, enabling users to control climate settings and view fuel status, among a\nmyriad of other capabilities. The application itself requested association with a user\naccount, requiring the user to log in before using the app.\n\nThis example vulnerability shows that HTTPS requests made to the backend servers\nfrom the app did not contain any user authentication information. The application\nmade a request to get battery status (example, fictitious URL):\n\nGET [https://host/BatteryStatus.php?VIN=JNFAAZE0U60XXXXX](https://host/BatteryStatus.php?VIN=JNFAAZE0U60XXXXX)\n\nThis request did not contain any session parameters or use Authentication Headers.\nThe server returns a JSON response. By changing the VIN, an attacker can gain\naccess to information about any vehicle that sends data to the backend servers,\nwithout authentication.\n\nFurther, using the app to turn climate control on and off issues requests with similar\nparameters to the URLs:\n```\n    /orchestration_example/ACRemoteRequest.php?XXX\n    /orchestration_example/ACRemoteOffRequest.php?XXX\n\n```\nWe are looking at a vulnerability that is remote, unauthenticated, low-skill, capable of\ncontrolling or changing a process or system, and automatable.\n\n1 https://www.troyhunt.com/controlling-vehicle-features-of-nissan/\n\n\n-----\n\n- **CATEGORY** - **VALUE**\n\n- **IMPACT** - Critical (4.5). Media attention. API servers can affect all\nvehicles.\n\n- **LIKELIHOOD** - Critical (5). Almost certain to be exploited. Very low skill\nrequired. Easily discoverable.\n\n- **ACCESS VECTOR** - Network\n\n- **ACCESS** - Low\n**COMPLEXITY**\n\n- **AUTHENTICATION** - None\n\n- **IMPACT:** - Complete\n**CONFIDENTIALITY**\n\n- **IMPACT:** - Partial\n**INTEGRITY**\n\n- **IMPACT:** - None\n**AVAILABILITY**\n\n- **LABELS** - Telematics, Web API, App, Insecure By Design\n\n- **EXPLOIT** - Web\n**DELIVERY MEDIA**\n\n- **TOOLS USED** - burp\n\n- **VULNERABILITY** - Unauthenticated API\n\n- **IMPACT: VEHICLE** - Secondary system loss of control, multiple vehicles\ncompromised\n\n- **TESTING METHOD** - Black Box\n\n- **TEMPORARY** - Easy (0). Disable API immediately.\n**REMEDIATION**\n\n- **REMEDIATION:** - Medium (3). Prod server update to remediate the issue but\n**DISSEMINATION** a new app required to use new server API.\n\n- **REMEDIATION:** - Medium (3). Proper security of web API requires\n**COMPLEXITY** authentication/session information and CSRF tokens (if a\n\nweb interface exists).\n\n- **REMEDIATION:** - Medium (3)\n**TIMELINE**\n\n- **EXPLOIT** - Easy (5)\n**AUTOMATABILITY**\n\n_Table 3 – Vulnerability report table_\n\n| IMPACT  LIKELIHOOD  ACCESS VECTOR  ACCESS COMPLEXITY  AUTHENTICATION  IMPACT: CONFIDENTIALITY  IMPACT: INTEGRITY  IMPACT: AVAILABILITY  LABELS  EXPLOIT DELIVERY MEDIA  TOOLS USED  VULNERABILITY  IMPACT: VEHICLE  TESTING METHOD  TEMPORARY REMEDIATION  REMEDIATION: DISSEMINATION  REMEDIATION: COMPLEXITY  REMEDIATION: TIMELINE  EXPLOIT AUTOMATABILITY| Critical (4.5). Media attention. API servers can affect all vehicles.  Critical (5). Almost certain to be exploited. Very low skill required. Easily discoverable.  Network  Low  None  Complete  Partial  None  Telematics, Web API, App, Insecure By Design  Web  burp  Unauthenticated API  Secondary system loss of control, multiple vehicles compromised  Black Box  Easy (0). Disable API immediately.  Medium (3). Prod server update to remediate the issue but a new app required to use new server API.  Medium (3). Proper security of web API requires authentication/session information and CSRF tokens (if a web interface exists).  Medium (3)  Easy (5)|\n|---|---|\n\n\n-----\n\n## Common Vulnerabilities and Analysis\n\nWith this understanding of how we find and classify vulnerabilities we can look at the\nraw data. We evaluate the data organized by:\n\n       - Type\n\n       - Severity\n\n       - Attack Vector\n\n       - Methodology\n\n       - Remediation Difficulty\n\n       - Vehicle Impact\n\n### Impact\n\nLooking at the impact level across all vulnerabilities, we see that half are Critical or\nHigh impact. This means half of the vulnerabilities result in a compromise of\ncomponents, communications, or data that causes complete or partial loss of control\nover the system. Business impact, such as regulatory violation fees or negative media\nattention, is also factored into the generic Impact score.\n\n\n**Medium**\n\n**23%**\n\n**Low**\n**13%**\n\n\n**Critical**\n\n**25%**\n\n\n**High**\n\n**Informational**\n**25%**\n\n**14%**\n\n|3% mational|Crit 25 High 25%|\n|---|---|\n\n\n_Figure 1 - Vulnerabilities by Impact_\n\n### Likelihood\n\nThe second generic vulnerability category is Likelihood. Here we see the largest\nclassification is Medium, meaning “an expert attacker could exploit the vulnerability\nwithout much difficulty.” The nature of embedded systems security causes a barrier to\nentry for attackers. The customized nature of each system makes developing attack\ntools difficult. However, in time, more vulnerabilities will shift toward the Critical value\nas more vehicle tools are released to the public and embedded security becomes\nmore accessible to a would-be attacker.\n\n\n-----\n\n_Figure 2 - Vulnerabilities by Likelihood_\n\n### Overall Risk Level\n\nCombining the previous two data segments gives us an Overall Risk Level, a general\nquick look at vulnerability severity. 22% of all vulnerabilities sit in the Critical range.\nThese are the high-priority \"hair on fire\" vulnerabilities that are easily discovered and\nexploited and can cause major impacts to the system or component. These\nvulnerabilities should drive cybersecurity resource allocation and spending.\n\n**Critical (20-25)**\n\n**22%**\n\n**Low (2-5)**\n\n**36%**\n\n**High (12-19)**\n\n**18%**\n\n**Medium (6-11)**\n\n**24%**\n\n_Figure 3 - Vulnerabilities by Overall Risk Level_\n\n### Attack Vector\n\nThe Attack Vector is a useful data set to compare against the threat model for a given\ncomponent or system. The attack surface and trust boundaries should be considered\nduring the design phase of a system. Every decision that increases the attack surface\n\n\n-----\n\nprovides new opportunities for an attacker, and should be evaluated accordingly for\nrisk.\n\n**CANBus** **Cell**\n\n**10%** **Network**\n\n**16%**\n\n**USB**\n\n**13%**\n\n**Local**\n\n**Serial** **17%**\n\n**5%**\n\n**Network**\n\n**39%**\n\n_Figure 4 - Vulnerability Attack Vectors_\n\nWithin our vulnerability set, 39% of vulnerabilities are related to the network. This is a\ngeneral category that includes all network traffic, such as Ethernet or web. However,\ncellular network and CANBus vulnerabilities appear in separate categories,\nrepresenting 16% and 10% (respectively) of the attack vectors for vulnerabilities\nfound.\n\nThe local attack vector requires an attacker to be on the system and obtain privilege\nescalation. At first this may seem like an inconsequential or unlikely attack vector, but\nafter considering the availability of app stores and third-party software modules, this\nattack vector is just as significant as network-based attacks.\n\nUSB and serial attack vectors require an attacker to be physically present at the\nsystem or otherwise chain an attack. For example, malware on a desktop system may\ndeploy a payload to a USB storage device, which the unsuspecting user then delivers\nto the vehicle. These attack vectors generally coincide with a lower likelihood of\nvulnerability.\n\n### Top Eight Vulnerabilities\n\nVulnerabilities related to system design occupy four of the top eight vulnerability types.\nVendor-introduced backdoors, incorrect utilization of the principle of least privilege,\nauthentication systems requiring hardcoded credentials, and accidental information\ndisclosure are all products of the system design process.\n\n\n-----\n\n_Figure 5 – Top Eight Vulnerabilities_\n\nEngineering problems are the root cause of three of the top eight vulnerability types.\nCoding logic errors, such as format strings, buffer overflows, and integer overflows,\naccount for the most vulnerabilities, with buffer overflows the most common sub-type.\nAdditionally, web vulnerability implementation problems fall under this category, which\nare generally High to Critical risk due to widely available attacker tools and knowledge\nbase.\n\nProblems in deployment mechanisms and testing cause vulnerabilities in the\nVulnerable Dependency and Backdoor categories. The Vulnerable Dependency\ncategory stems from technical and cultural issues. It is extremely common for\nembedded systems to use outdated libraries simply because it is hard enough to get a\nsystem to work in the first place, let alone introduce changes that could cause further\nissues by updating a dependency. Further, updating embedded systems is non-trivial,\nso a discovered vulnerability in one of these dependencies is likely to be a fruitful\nvector for attackers for much longer than in the general realm of IT systems.\n\nBackdoors, information disclosure, and hardcoded credentials also stem from issues\nin the deployment process. Production systems should not include developer debug\ninterfaces or other information that enables attackers. Security-aware deployment\nprocedures and testing are necessary to verify removal of these interfaces from\nproduction systems.\n\n\n-----\n\n### Critical Impact Remediation\n\nTo create this chart, we took all vulnerabilities with a Critical impact score and made a\n\"remediation complexity\" estimate to evaluate the difficulty of fixing a specific\nvulnerability. For example, patching code to remove a buffer overflow is relatively\neasy. Interestingly, an overwhelming majority of Critical impact vulnerabilities can be\nremediated with relatively simple fixes.\n\n**High**\n\n**Medium** **8%**\n\n**15%**\n\n**Low**\n**77%**\n\n_Figure 6 - Critical Vulnerability Remediation Difficulty_\n\nHowever, vulnerabilities stemming from design-level issues occupy the Medium and\nHigh remediation complexity categories. These can be extremely difficult, if not\nimpossible, to remediate after the design phase, which results in a system that is\n\"insecure by design\" and can never really be remediated. Some design issues do fall\nin the Low category as a result of not following industry best practices that are widely\npublished with how-to guides (e.g., the use of CSRF tokens to prevent web application\nCSRF attacks).\n\n### Testing Method\n\nThe Testing Method chart below is useful when considering how to proceed with a\nvulnerability assessment. It includes normalized data on Overall Risk Level for\nvulnerabilities, broken down by the testing method used when the vulnerabilities were\ndiscovered. IOActive generally conducts assessments in a black or white box format\nwherein the client will either share (white box) or not share (black box) data, code, or\nother information.\n\nThe usual motivation for a black box test is to evaluate what a \"real-world attacker\"\nwould see or do, but in reality the assessment rarely plays out as intended. Real\nattackers are not limited in scope or timeline when attacking a system. During such an\nassessment, more time is required by researchers to discover and evaluate how to\nuse a system and develop the harnesses or methodologies required for testing. This\nlimits the time remaining for the testing itself.\n\n\n-----\n\nConversely, in a white box test, researchers have more information about the system,\ngiving them three advantages:\n\n1. Researchers can spend less time figuring out how a system works and\n\nmore time discovering vulnerabilities.\n\n2. Researchers are better able to evaluate the impact and likelihood levels for\n\nany vulnerabilities that are discovered.\n\n3. Researchers can provide insight and assistance in areas of the system that\n\nmay not be directly attackable but might be accessible in the future or by\nchaining another vulnerability. Generally, these insights fall into the\nInformational Overall Risk Level.\n\n**Black Box** **White Box**\n\n_Figure 7 - Testing Method_\n\nIn black box testing, most vulnerabilities sit at the Medium Overall Risk Level, and\nfewer vulnerabilities are discovered overall. Comparatively, white box testing uncovers\nfewer Medium and more Critical and Informational vulnerabilities. The additional\ninformation available in white box testing enables researchers to better assess the\nactual risk of a vulnerability. Further, researchers spend less time working to discover\ninformation about the system and how it operates and more time focusing on finding\nand evaluating vulnerabilities.\n\nGrey box testing can be similar in methodology to black box, but benefits from the\naddition of information that enables testers to focus on critical attack paths or\ncomponents, often resulting in the discovery of more Critical or High Overall Risk\nLevel vulnerabilities. However, this also results in fewer Informational level\nvulnerabilities than a white box test, often limiting the additional insights, such as\nincorporating industry best practices, that white box testing can offer.\n\n|Critical High Medium Low Informational Black Box|Critical High Medium Low Informational White Box|\n|---|---|\n\n\n-----\n\nIn summary: white box testing is the most effective at identifying high priority bugs and\nimproving the development process.\n\n### Impact on Vehicle\n\n**Attacker Enabling**\n\n**9%**\n\n**Telematics**\n**Communications**\n\n**24%**\n\n**CANBus Access**\n\n**27%**\n\n**None**\n\n**17%**\n\n**ECU**\n**Compromised**\n\n**8%**\n\n**ECU Disabled**\n\n**Indirect**\n**1%**\n\n**14%**\n\n_Figure 8 - Impact on Vehicle_\n\nAn important point to consider when reviewing this vulnerability data is the actual\nimpact on the vehicle or fleet. Starting from the benign, 17% of vulnerabilities\nevaluated had zero impact on the vehicle itself. These might be vulnerabilities in\nunrelated backend systems or other components that are tangentially connected to\nthe vehicle system being evaluated.\n\nAttacker-enabling impact (9%) encompasses those vulnerabilities that provide\nattackers with additional information or attack chains they can use to gain access or\ntarget another vulnerability. Similarly, indirect impact is a secondary consequence of\nan attack against a vehicle. For example, compromise of a telematics backend might\nallow an attacker to then communicate with the vehicle and gain an additional direct\nattack vector against the vehicle.\n\nVehicle telematics communications are effected in 24% of vulnerabilities. This might\noccur directly on the component, in transit over a cellular or other network, or on the\nbackend systems that gather and utilize the data.\n\n\n-----\n\nMore significantly, 27% of vulnerabilities can be used to gain CANBus Access. This is\nimportant when considering the increasing body of public research showing that an\nattacker on the CANBus can control the vehicle[234].\n\nActual ECU control is gained in 8% of the vulnerabilities IOActive evaluated, while 1%\nof vulnerabilities disable the ECU without gaining actual control. Compromised ECUs\nmay allow the attacker to control all of their normal functionality, add further\nfunctionality, or otherwise result in complete compromise of the vehicle component in\na manner that may be extremely difficult to detect.\n\n## Defenses and Effectiveness\n\nAs part of the assessment process, IOActive provides recommendations to the\ncustomer for vulnerability remediation. This might include re-working of code,\nreplacement or removal of a feature or component, the purchase and implementation\nof a defensive tool or product, or other techniques depending on the nature and\nimpact of the vulnerability.\n\nIn the assessments analyzed for this research, no defensive products were tested.\nThis is likely a result of the relative infancy of automotive cybersecurity products, as\nwell as a general tendency of defensive cybersecurity products to not undergo thirdparty testing. Organizations can improve this deficiency by including testing\nrequirements in any procurement language for such products. This has become\ncommonplace in other industries, such as the Industrial Control System space, where\nmanufacturers must deliver security reports alongside products that are to be\nintegrated into production systems.\n\n### Ounce of Prevention\n\nIn conducting this research, we evaluated an \"Ounce of Prevention\" measure to best\ndetermine what techniques, policies, or methodologies might have prevented a\nvulnerability from existing in the first place.\n\nDisclaimer: while this aspect of the report was a collaborative effort involving opinions\nof multiple researchers, and the results are subjective.\n\n2 http://www.ioactive.com/pdfs/IOActive_Remote_Car_Hacking.pdf\n\n3 http://opengarages.org/handbook/2014_car_hackers_handbook_compressed.pdf\n\n4 https://www.sans.org/reading-room/whitepapers/internet/developments-car-hacking-36607\n\n\n-----\n\n_Figure 9 - An \"Ounce of Prevention\" Analysis_\n\nThe largest category in our analysis is Industry Best Practices. An increasing number\nof security best practices publications are made available every year. The AutoISAC[56], Microsoft, OWASP, ARM, and others publish best-practice documents to help\nsoftware and hardware developers to create secure systems. Utilizing this information\ncould result in up to a 45% decrease in vulnerabilities across the board.\n\nAuthentication design may fall under industry best practices, but becomes its own\ncategory due to its prevalence and the more unique nature of some of the\ncomponents evaluated. Authentication mechanisms may be difficult to change after a\nsystem is deployed and thus should be thoroughly evaluated during system design.\nSimilarly, secure coding practices could help to prevent 11% of vulnerabilities. Simple\nsteps, such as including Microsoft’s “banned.h“ to avoid using insecure coding\npatterns like strcpy(), help to improve overall code security (and consequently overall\nquality) of any system.\n\nThe most difficult category to judge with certainty is Code Review and Testing.\nCatching coding logic errors can be extremely difficult, but following modern software\n\n5 http://www.prnewswire.com/news-releases/automotive-industry-collaborates-in-developing-vehiclecybersecurity-best-practices-to-address-cybersecurity-challenges-300301805.html\n\n6 http://www.autoalliance.org/index.cfm?objectid=1E518FB0-BEC3-11E5-9500000C296BA163\n\n\n-----\n\ndevelopment principles such as test-driven development can do wonders for\nimproving a code base and hardening against unexpected behavior and bugs.\n\nA smaller number of issues stem from poor deployment procedures or patch\nmanagement. Shipping a production system with enabled backdoors (a.k.a. \"debug\nfeatures\") is preventable with a proper deployment procedure and verification.\nSimilarly, shipping products with vulnerable dependencies or a difficult update\nmechanism further increases the risk for attack.\n\nIn evaluating the past few years of vulnerabilities, the old adage rings truer than ever:\nan ounce of prevention is worth a pound of cure. Incorporating industry best practices\nand a secure development lifecycle is crucial to avoiding pitfalls that, frankly, have\nbeen a non-issue in other industries for years. Using modern design principles\nimproves the ability to patch and maintain a codebase, and improves overall code\nquality.\n\n## Conclusion\n\nThe majority of vehicle cybersecurity vulnerabilities are not solvable using “bolt-on”\nsolutions, instead relying on sound engineering, software development practices, and\ncybersecurity best practices. The most effective cybersecurity work occurs during the\nplanning, design and early implementation phases of products, with the difficulty and\ncost of remediation increasing in correlation with product age and complexity.\n\n## Future Work\n\nEmerging vehicle technologies, such as V2V and V2I communication components, are\nunderrepresented in the data thus far. These emerging technologies typically require\nself-funded research, and thus are rarely published.\n\nSimilarly, the data set does not contain any defensive tools or products. It would be\ninteresting to perform a similar analysis on the defensive tools present in the current\nvehicle cybersecurity market. Adding additional components to the system, even if\nthey are designed to improve security, always adds complexity, introducing new attack\nvectors and possibly new vulnerabilities. In the future we should evaluate these\nsystems on their own cybersecurity merit, and add their vulnerabilities to this research.\n\nFor more information on IOActive Transportation Security research and services,\n[please visit http://www.ioactive.com/services/air-auto-rail-satcom-ship-transportation-](http://www.ioactive.com/services/air-auto-rail-satcom-ship-transportation-security.html)\n[security.html](http://www.ioactive.com/services/air-auto-rail-satcom-ship-transportation-security.html) [or contact IOActive at info@ioactive.com.](mailto:info@ioactive.com)\n\n**_About IOActive_**\n\n_IOActive is the industry’s only research-driven, high-end information security services firm with a proven history_\n_of better securing our customers through real-world scenarios created by our security experts. Our world-_\n_renowned consulting and research teams deliver a portfolio of specialist security services ranging from_\n_penetration testing and application code assessment to chip reverse engineering across multiple industries._\n_IOActive is the only security services firm that has a dedicated practice focusing on Smart Cities and the_\n_transportation and technology that connects them. Global 500 companies across every industry continue to_\n_trust IOActive with their most critical and sensitive security issues. Founded in 1998, IOActive is headquartered_\n_in Seattle, US, with global operations through the Americas, EMEA, and Asia Pac regions. Visit_\n_[www.ioactive.com](http://www.ioactive.com/)_ _[for more information. Read the IOActive Labs Research Blog: http://blog.ioactive.com/.](http://blog.ioactive.com/)_\n_Follow IOActive on Twitter: http://twitter.com/ioactive._\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/ICS SCADA/ICS Vulnerabilities/Commonalities in Vehicle Vulnerabilities.pdf"
    ],
    "report_names": [
        "Commonalities in Vehicle Vulnerabilities.pdf"
    ],
    "threat_actors": [],
    "ts_created_at": 1673536137,
    "ts_updated_at": 1743041142,
    "ts_creation_date": 1470056391,
    "ts_modification_date": 1470056391,
    "files": {
        "pdf": "https://archive.orkl.eu/99f5dc057b1ff6acc93d26201f119c2b7bd64f70.pdf",
        "text": "https://archive.orkl.eu/99f5dc057b1ff6acc93d26201f119c2b7bd64f70.txt",
        "img": "https://archive.orkl.eu/99f5dc057b1ff6acc93d26201f119c2b7bd64f70.jpg"
    }
}