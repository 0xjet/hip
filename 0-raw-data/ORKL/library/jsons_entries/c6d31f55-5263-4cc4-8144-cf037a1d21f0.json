{
    "id": "c6d31f55-5263-4cc4-8144-cf037a1d21f0",
    "created_at": "2023-01-12T15:03:17.001055Z",
    "updated_at": "2025-03-27T02:05:58.018753Z",
    "deleted_at": null,
    "sha1_hash": "c12031df25ff97de8769f30eb49bd20bf5c45848",
    "title": "2022-09-29 - Bad VIB(E)s Part Two- Detection and Hardening within ESXi Hypervisors",
    "authors": "",
    "file_creation_date": "2022-10-02T12:26:37Z",
    "file_modification_date": "2022-10-02T12:26:37Z",
    "file_size": 4457474,
    "plain_text": "# Bad VIB(E)s Part Two: Detection and Hardening within ESXi Hypervisors\n\n**[mandiant.com/resources/blog/esxi-hypervisors-detection-hardening](https://www.mandiant.com/resources/blog/esxi-hypervisors-detection-hardening)**\n\nIn [part one, we covered attackers’ usage of malicious vSphere Installation Bundles (“VIBs”)](http://www.mandiant.com/resources/blog/esxi-hypervisors-malware-persistence)\nto install multiple backdoors across ESXi hypervisors, focusing on the malware present\nwithin the VIB payloads. In this installment, we will continue to elaborate further on other\nattacker actions such as timestomping, describe ESXi detection methodologies to dump\nprocess memory and perform YARA scans, and discuss how to further harden hypervisors to\nminimize the attack surface of ESXi hosts. For more details, VMware has released additional\ninformation on protecting vSphere.\n\n## ESXI Logging\n\nBoth VIRTUALPITA and VIRTUALPIE stop the `vmsyslogd process from recording activity`\non startup, but multiple logging processes exist across the hypervisor which can still be used\nto track attacker activity.\n\n### Malicious VIB Installation\n\n\n-----\n\n[It was previously established in part one that ESXi systems do not allow for a falsified VIB file](http://www.mandiant.com/resources/blog/esxi-hypervisors-malware-persistence)\nbelow the minimal set acceptance level, even when the acceptance level was modified in the\nDescriptor XML. To circumvent this, the attacker abused the `--force` [flag to install](https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.install.doc/GUID-6A3AD878-5DE9-4C38-AC86-78CAEED0F710.html#:~:text=are%20case%2Dsensitive.-,Note,-%3A)\nmalicious `CommunitySupported VIBs. This flag adds a VIB or image profile with a lower`\n[acceptance level than required by the host.](https://docs.vmware.com/en/VMware-vSphere/6.5/com.vmware.vsphere.upgrade.doc/GUID-98CAEBD8-424E-4502-B4B4-500E24A0F461.html)\n\nEvidence of the `--force flags usage to install a VIB was found across multiple locations`\non the ESXi hypervisor. The ESXi profile XML file records all VIBs that have been installed\non the system, specifying the date, time, and flags used to install each VIB. This file is found\nunder the path `/var/db/esximg/profile . Figure 1contains an example of the attacker’s`\n```\n--force flag usage logged in the profile XML file.\n\n```\nFigure 1: ESXI Profile XML file with the presence of a --force installation\nThe log file `/var/log/` `esxupdate.log also recorded the usage of the` `--force flag`\nwhen a VIB is installed. Figure 2 contains an event that logged a malicious VIB being\ninstalled with a forced installation.\n\nFigure 2: VIB Installation with force flag in esxupdate.log\n\n### Timestomping\n\n\n-----\n\nMandiant observed that logs surrounding VIB installations with the `--force flag were`\nrecorded as early as October 9, 2013, which did not align with the attack timeline. The log file\n```\n/var/log/vmkwarning.log provided further evidence of the system time being\n\n```\nmanipulated. Figure 3 contains two (2) events that logged the system clock being modified\n[right before and after attacker actions occurred. This behavior suggests timestomping was](https://attack.mitre.org/techniques/T1070/006/)\nbeing performed to cover up the true time the attacker initially installed the VIBs on the\nmachine.\n\nFigure 3: vmkwarning.log recording system time modification\n\n### Creation of sysclog\n\nAnalyzing the VIRTUALPITA sample `rhttpproxy-io`\n(2c28ec2d541f555b2838099ca849f965), it was found that the sample listened over the VMCI\nport number 18098. Once the listener is setup, the malware fetches the system's CID\n(context ID) by issuing IOCTL request code 1977. The PID of the backdoor, CID and\nlistening port are then logged to `/var/log/sysclog in the following format`\n```\n[<date/timestamp>]\\n\\r[!]<<PID>>:<CID>:<port>\\n\\n as seen in Figure 4.\n\n```\nFigure 4: Sample of sysclog\n\n### Guest Machine Interaction\n\nFurther interactions between hypervisors and their respective guest machines were\ndiscovered within multiple logs named `vmware.log . These logs, located at the following`\npath `/vmfs/volumes/…/<virtual machine hostname>/vmware.log, record basic`\noperations between the host and hypervisor that were not logged on the endpoint. Actions\nrecorded by this log include guest machine logins, file/directory creation and deletion,\n\n\n-----\n\ncommand execution, and file transfer between guest machine and hypervisor. To focus on\ninteractions between the hypervisor and its guest machines in the `vmware.log, filter for`\nlines containing GuestOps.\n\n## VIB Verification at Scale\n\nThe previous blog post touched on using the command `esxcli software vib signature`\n```\nverify to identify any VIBs that do not pass the signature verification check made by the\n\n```\nESXi hypervisor. Alternative VIB configurations exist that would be able to circumvent the\nsignature verification check. Mandiant confirmed that when a VIB is installed as\n```\nCommunitySupported, the Signature Verification field will label it as Succeeded if\n\n```\nthe payload is not tampered with after installation. This means a VIB could be created\nwithout any validation from VMWare or its partners and still be labelled as validated.\n\nTo account for properly signed `CommunitySupported VIBs and other anomalous`\nconfigurations which could indicate malicious activity, all VIBs in the environment can be\n[compared with a list of known good VIBs. A matrix created by VMware Front Experience](https://esxi-patches.v-front.de/vm-6.7.0.html)\nbreaks down the names and builds of each VIB expected to be present by default in the\nrespective ESXi build. Each time a VIB is changed across ESXi builds the matrix links to the\nofficial VMware patch release notes which state the adding, modification, or removal of that\nVIB. A sample of this matrix can be seen in Figure 5.\n\nFigure 5: Sample of Known Good VIB Matrix\n\n\n-----\n\n## ESXI Detection Methodologies\n\nWhile ESXi shares many similarities to Linux (commands, directory structure, etc.), it is\nentirely its own operating system known as VMkernel, meaning popular methods to scan the\nfilesystem and dump process memory do not work. Mandiant has formulated alternative\ndetections methods to attempt to provide investigators with better visibility into ESXi\nhypervisors during future incidents.\n\n### Remote ESXi YARA Scanning with SSHFS\n\nMultiple YARA rules were generated for the detection of VIRTUALPITA and VIRTUALPIE\nacross Linux and ESXi environments and can be found in the first part of this blog post.\nThese detections have two caveats to them based on the storage and execution of the\nmalware. If the attacker is launching either malware family from a VIB on ESXi, the sample\nwithin the VIB will not be detected due to being compressed in the .vgz format. Secondly, if\nthe binary is running in memory but deleted from disk, the binary will not be detected by\nYARA’s file system scan.\n\nSince YARA does not run directly on ESXi hosts, Mandiant utilized `sshfs to perform`\nremote YARA scanning of ESXi hypervisors.\n\n**Prerequisites**\n\n_Note: All behaviors of ESXi and the methodology to dump memory have been confirmed_\n_for ESXi 6.7, no other versions at this time have been tested._\n\nBefore scanning the ESXi machine a few prerequisites must be met. For the ESXi machine\nwhich the memory is being dumped, you must have both:\n\nRoot Access to the machine\n[SSH Enabled on the ESXi Server](https://phoenixnap.com/kb/esxi-enable-ssh)\n\nOnce the ESXi machine is correctly configured, a Linux machine must be setup to be able to\ncommunicate over SSH with the ESXi machine. This Linux machine must also install:\n\n[sshfs](https://linux.die.net/man/1/sshfs)\nyara\n\n**Performing the YARA Scan**\n\n\n-----\n\n_Note: Since YARA will naturally recursively scan directories and sshfs pulls files back as_\n_they are accessed, scanning the entire ESXi file system can take a long time depending_\n_on network bandwidth. This method of scanning is only suggested if a strong and stable_\n_network connection is present._\n\nLinux Commands\n\n**Description** **Commands**\n\nCreate a directory to mount the ESXi `> mkdir /mnt/yara`\nmachine on\n\nMount the ESXi root directory to the `> sshfs -o`\nLinux machine mount point using `allow_other,default_permissions`\nsshfs `root@<Insert ESXi IP Address>:/ /mnt/yara`\n\nScan the mount point which the `> yara -r <Provided YARA Rule> <scope of`\nESXi system is attached to `scan>`\n\n### Dumping ESXi Process Memory\n\nWhen attempting to dump the process memory from a ESXi hypervisor like you would a\nLinux machine, it will quickly become apparent that the /proc/ directory will be either empty or\ncontaining a single PID of the commands used to attempt to dump the memory. To recover\nprocess memory from ESXi (and potentially the full binary itself), a mixture of the native tool\n[gdbserver and a github tool called core2ELF64 can be utilized.](https://linux.die.net/man/1/gdbserver)\n\n**Prerequisites**\n\n_Note: All behaviors of ESXi and the methodology to dump memory have been confirmed_\n_for ESXi 6.7, no other versions at this time have been tested._\n\nBefore dumping the process memory a few prerequisites must be met. For the ESXi machine\nwhich, you must have both:\n\nRoot Access to the machine\n[SSH Enabled on the ESXi Server](https://phoenixnap.com/kb/esxi-enable-ssh)\n\nOnce the ESXi machine is correctly configured, a Linux machine must be setup to be able to\ncommunicate over SSH with the ESXi machine. This Linux machine must also install:\n\n\n-----\n\n[gdb](https://linux.die.net/man/1/gdb)\ncore2ELF64\n\n**Dumping Memory**\n\n_Note: The ports to listen and port forward through are arbitrary (Rule of Thumb: Keep_\n_between 1024-25565 to avoid commonly used ports), for this walkthrough the listening_\n_port will be 6000 and the forwarding port will be 7000._\n\nTo dump the ESXi process memory, gdbserver will be utilized to hook into the currently\nrunning process, specified by PID, and listen on an arbitrary port.\n\nESXi Commands\n\n**Description** **Commands**\n\nA preemptive check used to make sure that the PID you will be `> ps` `-Tcjstv`\ncollecting in the next command is the intended one. Please `| grep` `-e “<Binary`\nmake sure that the output of this statement only shows the `to Dump>”`\nprocess you intend to dump the memory for.\n\nAttaches gdbserver to the PID specified in the list processes `> gdbserver –attach`\ncommand, listening on port 6000 for gdb to connect to. `127.0.0.1:6000 `ps`\n```\n                                 Tcjstv |\n                                 grep -e “<Binary\n                                 to\n                                 Dump>” | awk ‘{print\n                                 $1}’ | head -n 1`\n\n```\nOnce listening, the Linux machine will create an SSH tunnel (Port Forward) to the listening\nport on the ESXi server, where gdb will be used to create a core dump of the process\nspecified.\n\nLinux Commands\n\n\n-----\n\n**Description** **Commands**\n\nLaunch gdb `> gdb`\n\nWithin the gdb shell, connect to the gdbserver instance. If at any `(gdb) > target`\npoint you have successfully ran this command and leave the gdb `remote`\nshell, you will need to exit and relaunch the gdbserver process `localhost:1336`\non ESXi to reconnect.\n\nCreate a core dump file of the attach processes' memory in the `?? () > gcore`\nworking directory. The output file should be the following syntax\n\"core.[0-9]{7}\".\n\n**Process Extraction**\n\nOnce a core file is created, the Github project core2ELF64 can be used to reconstruct the\nprogram.\n\nLinux Commands\n\n**Description** **Commands**\n\nSet up an SSH tunnel from the Linux machine to the listening `> core2ELF64 <core`\nport of the ESXi Server gdbserver process. `file> <Desired Output`\n```\n                                Name>\n\n```\nIn the event of the program not being able to recover the first\nsegment, choose the next available segment possible\n(Smallest Number)\n\n**Sources**\n\n[Hooking into ESXi processes with gdbserver](https://straightblast.medium.com/my-poc-walkthrough-for-cve-2021-21974-a266bcad14b9)\n\n## Hardening ESXi\n\n### Network Isolation\n\nWhen configuring networking on the ESXi hosts, only enable VMkernel network adapters on\nthe isolated management network. VMkernel network adapters provide network connectivity\nfor the ESXi hosts and handle necessary system traffic for functionality such as vSphere\nvMotion, vSAN and vSphere replication. Ensure that all dependent technologies such as\n\n\n-----\n\nvSANs and backup systems that the virtualization infrastructure will use are available on this\nisolated network. If possible, use dedicated management systems exclusively connected to\nthis isolated network to conduct all management tasks of the virtualization infrastructure.\n\n### Identity and Access Management\n\nConsider decoupling ESXi and vCenter Servers from Active Directory and use vCenter\nSingle Sign-On. Removing ESXi and vCenter from Active Directory will prevent any\ncompromised Active Directory accounts from being able to be used to authenticate directly to\nthe virtualization infrastructure. Ensure administrators use separate and dedicated accounts\nfor managing and accessing the virtualized infrastructure. Enforce multi-factor authentication\n(MFA) for all management access to vCenter Server instances and store all administrative\ncredentials in a Privileged Access Management (PAM) system.\n\n### Services Management\n\nTo further restrict services and management of ESXi hosts, implement lockdown mode. This\nensures that ESXi hosts can only be accessed through a vCenter Server, disables some\nservices, and restricts some services to certain defined users. Configure the built-in ESXi\nhost firewall to restrict management access only from specific IP addresses or subnets that\ncorrelate to management systems on the isolated network. Determine the appropriate risk\nacceptance level for vSphere Installable Bundles (VIBs) and enforce acceptance levels in the\nSecurity Profiles for ESXi hosts. This protects the integrity of the hosts and ensures unsigned\nVIBs cannot be installed.\n\n### Log Management\n\nCentralized logging of ESXi environments is critical, both to proactively detect potential\nmalicious behavior and investigate an actual incident. Ensure all ESXi host and vCenter\nServer logs are being forwarded to the organization’s SIEM solution. This provides visibility\ninto security events beyond that of normal administrative activity.\n\n## Acknowledgements\n\nSpecial thanks to Brad Slaybaugh, Joshua Kim, Zachary Smith, Jeremy Koppen, and\nCharles Carmakal for their assistance with the investigation, technical review, and creating\ndetections/investigative methodologies for the malware families discussed in this blog post.\nIn addition, we would also like to thank VMware their collaboration on this research.\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "05d7b179-7656-44d8-a74c-9ab34d3df3a2",
            "created_at": "2023-01-12T14:38:44.599904Z",
            "updated_at": "2023-01-12T14:38:44.599904Z",
            "deleted_at": null,
            "name": "VXUG",
            "url": "https://www.vx-underground.org",
            "description": "vx-underground Papers",
            "reports": null
        }
    ],
    "references": [
        "https://papers.vx-underground.org/papers/Malware Defense/Malware Analysis 2022/2022-09-29 - Bad VIB(E)s Part Two- Detection and Hardening within ESXi Hypervisors.pdf"
    ],
    "report_names": [
        "2022-09-29 - Bad VIB(E)s Part Two- Detection and Hardening within ESXi Hypervisors.pdf"
    ],
    "threat_actors": [],
    "ts_created_at": 1673535797,
    "ts_updated_at": 1743041158,
    "ts_creation_date": 1664713597,
    "ts_modification_date": 1664713597,
    "files": {
        "pdf": "https://archive.orkl.eu/c12031df25ff97de8769f30eb49bd20bf5c45848.pdf",
        "text": "https://archive.orkl.eu/c12031df25ff97de8769f30eb49bd20bf5c45848.txt",
        "img": "https://archive.orkl.eu/c12031df25ff97de8769f30eb49bd20bf5c45848.jpg"
    }
}