{
    "id": "a39e4a85-ce76-4081-842f-97107e842a1e",
    "created_at": "2022-10-25T16:48:14.386532Z",
    "updated_at": "2025-03-27T02:05:47.001313Z",
    "deleted_at": null,
    "sha1_hash": "e9a48a1a62bc4d489aad6ebc74effcd32185c39a",
    "title": "The Discovery of Fishwrap: A New Social Media Information Operation Methodology",
    "authors": "",
    "file_creation_date": "2019-06-15T11:29:34Z",
    "file_modification_date": "2019-06-15T11:29:44Z",
    "file_size": 4941437,
    "plain_text": "# The Discovery of Fishwrap: \n## A New Social Media Information Operation Methodology\n\n### By Staffan Truvé\n\n\n-----\n\nFor several years, Recorded Future has been developing tools and\nmethodologies for detecting and analyzing influence operations\nby nation-states and others. We have recently upgraded some of\nthese tools and applied them to detecting a new kind of influence\noperation, which recycles old news about terror incidents by\npublishing them to appear as new. We refer to this technique as\n“Fishwrap.” This operation is also using a special family of URL\nshorteners that allow attackers to track click-through from social\nmedia posts used in their campaigns.\n\n#### Key Findings\n\n - We have developed new algorithms for identifying influence\n\noperations. These algorithms allow for the detection of “seed\naccounts,” which can be used to analyze additional accounts\nengaged in an operation.\n\n - Behavioral analytics based on topological methodologies\n\ncan be used to analyze the highest-likelihood participants\nin an operation and cluster those with the highest degree of\nsimilarity.\n\n - Using this methodology, we identified a new kind of influence\n\noperation: Fishwrap. This methodology uses old terror news\nmasquerading as new.\n\n - Over 215 social media accounts participating in the Fishwrap\n\noperation were analyzed.\n\n - These social media accounts use a special family of at least\n\n10 different URL shortener services that allow for tracking\nthe effectiveness of the operation. All of these URL services\nare running the same code and are hosted on the same\ncommercial infrastructure.\n\n - The accounts’ behavioral similarity leads us to believe that\n\nthey are all part of the same influence operation.\n\n - Since account holders are most likely fictive and the\n\ndomains used for the URL shortener services are registered\nanonymously, attribution is difficult — however, research is\nongoing.\n\n\n-----\n\n#### Influence Operations\n\nRAND Corporation [defines information operations and warfare,](https://www.rand.org/topics/information-operations.html)\nalso known as influence operations, as the collection of tactical\ninformation about an adversary, as well as the dissemination of\npropaganda in pursuit of a competitive advantage over an opponent.\n\nRecorded Future and its partners have been engaged in identifying\nand characterizing influence operations for more than five years,\n[with studies including how terror supporters spread propaganda,](https://www.recordedfuture.com/isis-twitter-activity/)\n[election hacking in the U.S. in 2016 and 2018, and how China uses](https://www.recordedfuture.com/nation-state-cyber-activity/)\n[social media to influence U.S. opinion.](https://www.recordedfuture.com/china-social-media-operations/)\n\nInfluence operations are, of course, not an exclusive nation-state\n[activity — political groups use the same mechanisms, as do criminals](https://www.ft.com/content/a37e4874-2c2a-11e7-bc4b-5528796fe35c)\nengaging in stock manipulation activities.\n\n\n-----\n\n#### The Anatomy of an Influence Operation\n\nInfluence operations that aim to change public opinion (for example,\ntrying to sway the outcome of an election) need to reach a large\nnumber of people with messages that resonate with their beliefs or\nfears. Previously, certain groups, including the Nazis in the 1930s\nall the way to the Hutus in the 1990s, used radio to obtain massive\nreach. Today, social media has become the channel of choice for\ninfluence operations. Using data-driven profiling, messages can be\n[personalized, as highlighted by the Cambridge Analytica scandal.](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal)\n\nEven though “fake news” has become highly associated with\ninfluence operations, in many cases, “real news” is also used, but it’s\ncarefully selected to emphasize the opinions the operation wishes\nto foment. These news pieces can be distributed in different ways,\nsuch as through paid advertising (as in the Cambridge Analytica\ncase), by using a large number of social media accounts that\nare controlled by humans (so-called “trolls”), or with algorithms.\nAdvertising campaigns are hard to detect without access to users’\nnews feeds, but campaigns using ordinary social media posts by\na large number of accounts will be possible to detect by using a\nsolution such as Recorded Future.\n\n#### Detecting and Tracking Influence Operations\n\nTo detect an influence operation, we need a starting point — a seed.\nThis can be a particular story that is used by the operation engaging\nin it. Once one or more seeds have been selected, we can broaden\nthe scope of our investigation by finding related topics, such as\nURLs, hashtags, and user accounts. The image below illustrates\nhow our Snowball algorithm can be used to grow the number of\npotential accounts engaged in an operation, starting with either\na few accounts or with some topics believed to be used by an\noperation.\n\n\n-----\n\nThe Snowball algorithm generates a large number of candidate\naccounts, but will also typically find many false positives since\n“innocent” accounts will repost news from the operation.\nTo decide which accounts are really part of the operation, we can\nuse behavioral analytics to characterize and cluster the activities\nimplicated by the Snowball algorithm.\n\nOur approach to analytics is to define a number of similarity metrics\nover a large set of social media accounts. For example, accounts\nare more similar if they:\n\n - Post the same URLs and hashtags — the similarity is stronger\n\nif only a few accounts post a certain URL or hashtag within a\ncertain time frame\n\n - Post on the same topics, as defined by the entities and events\n\nthey mention\n\n - Are using the same URL shorteners\n\n - Have similar temporal behavior — either their overall period\n\nof activity or their weekly or daily behavioral patterns\n\n - Have mutually exclusive but adjacent overall periods of\n\nactivity — this is a weak but not insignificant indication that\none account has replaced the other\n\n - Have similar account names, as defined by the editing\n\ndistance between their names\n\n\n-----\n\nAll of these different similarity metrics can be used to cluster\naccounts. We then look at sets of accounts which are associated\nwith multiple identical similarity clusters. These accounts are highly\nlikely to be part of the same influence operation. Accounts which\nare related only through joint membership in one cluster are not\nvery likely to be part of the operation — they are just associated (for\nexample, by having reposted a post originated by the operation).\n\n#### Fake News in Influence Operations: 2 Examples\n\nIdentifying fake or biased news can be hard, but by looking at some\nexamples, we can gain some insight. For example, in a timeline view\nof reports on protests in Sweden in 2018, one event on July 16, 2018\nregarding Muslims protesting against crosses stands out because\nit is only being reported in Russian. This is quite strange for what\nshould be major news in Sweden, and different from all other major\nprotests during the period.\n\n\n-----\n\nEven though we first found this “news” in Russian, it was also\nreferenced by other social media accounts (like U.S. alt-right ones).\n\nThis particular story turns out to be entirely fake, and in this case,\nwe could actually use a reverse image search to find the original\nstory, which turns out to be about students protesting in Chile two\nyears earlier.\n\n\n-----\n\nThis example validates our intuition that important news stories\nwhich are reported either only in a single language, in a surprisingly\nsmall volume, or only on social media, are good initial candidates for\nbeing fake or hyperpartisan news used in an influence operation.\n\nAs another example, we compared the actual source distribution of\na real terror event that took place on November 13, 2015 in Paris\n(177 thousand references, mostly in mainstream media) with fake\nnews about an event in Paris on March 23, 2019 (only 19 references,\nmostly on social media). The account similarity associated with\nbeing one of the few accounts posting on March 23, 2019 is much\nhigher than that associated with being one of thousands of accounts\nposting on November 13, 2015.\n\n\n-----\n\n#### Fishwrap: A New Influence Operation\n\nThe fake Paris terror event mentioned above is actually part of an\ninfluence operation that we have detected using our new algorithms.\nThis operation is focused on posting old terror news stories as if\nthey were new, probably with the goal of spreading general fear\nand uncertainty. We call this operation Fishwrap, since it uses old\nnews for other purposes.\n\nWe first detected Fishwrap through our automatic tracking of terror\nevents only reported by social media, like the Paris example above.\nBelow is another example of such event reporting.\n\n\n-----\n\nBy tracking terror events in Recorded Future that were only\nreported on social media, we were able to find a set of about a\ndozen accounts clearly engaged in spreading old terror news as\nif it were new. The reason we compared the specific dates in the\nParis terror example above is because of a social media post from\nMarch 23, 2019 reporting on an event shown in the image below.\n\nHowever, the URL-shortened link in the post led us to an article\nabout the original event from November 13, 2015. While many\nreaders would probably not scrutinize the publication dates, it is\neasy to see how the post could cause concern for those reading it\nand prompt them to follow the link to validate the news, missing\nthe difference in publication date.\n\nBy applying the Snowball algorithm to the small set of identified\nposts, we could swiftly grow the number of suspicious activities in\nthis operation to more than a thousand profiles.\n\n\n-----\n\n#### Narrowing It Down\n\nWe then looked at the similarities within this fairly large set of\naccounts and concentrated on three specific aspects:\n\n1. Temporal behavior\n\n2. The domain of the URLs referred to in the accounts’ posts\n\n3. Account status\n\n#### Temporal Behavior\n\nBy plotting the activity (postings) of all accounts found in the\noperation, we can identify different activity periods.\n\nWe clearly see three different clusters of accounts:\n\n1. Those active between May 2018 to October 2018\n\n2. Those active between November 2018 to April 2019\n\n3. Those active during the entire time period, between May 2018\n\nto April 2019\n\n\n-----\n\nThese temporal patterns indicate the launch of a number of\naccounts in May 2018, many of which were shut down in October\n2018. These were followed a few weeks later by a new batch of\naccounts with the same behavior and still in operation.\n\n#### Topic Similarity: URLs and Domains Used\n\nBy looking at the URLs posted by a subset of the accounts, it is clear\nthat there is some relationship between them since they, to some\nextent, post identical URLs.\n\nThe graph above looks quite complicated. However, if we focus only\non the domains of these URLs, a much simpler picture emerges,\nshown below.\n\n\n-----\n\nIt turns out that a lot of the accounts are posting all of their links\nthrough a small number of URL shortener services (in the example\nabove, pucellina[.]com and bioecologyz[.]com). More precisely, we\ncan identify 10 domains hosting URL shortener services that are\nused for essentially every single post made by 215 of the identified\naccounts. As seen in the graph above, some accounts use multiple\nURL shorteners, and thus show that there is an indirect connection\nbetween accounts using different shorteners. Overall, we can see\nthat each domain has a fairly large number of accounts that has\npublished some reference to it, and a very small number of accounts\nhave published a reference to more than one of the domains (based\non the data we’ve collected). The exception is the two domains\npucellina[.]com and bioecologyz[.]com, where a larger number of\naccounts have published references to both domains.\n\n\n-----\n\nIt gets more interesting! Upon inspecting the 10 different URL\nshortener websites, we immediately see that their appearance is\nidentical.\n\nThis is strong evidence linking all 215 accounts to 10 URL shorteners,\nwhich in turn appear to be running the same code. Interestingly, an\ninspection of the HTML code for the URL shorteners also reveals\nthat these domains seem to be tracking all agents that follow the\nlinks. This could be to measure the effectiveness of the operation,\nbut it might also be used for profiling the “captured audience” of\nthe operation.\n\nUnfortunately, all of these 10 domains are anonymously registered,\nand we cannot see who registered or owns them. We did investigate\nwhere these services are hosted, and it turns out they are all\ncurrently running on dedicated servers on Microsoft Azure.\n\n\n-----\n\n|Domain|Created|Updated|IP|Selected Historic Name Server|Mentions|\n|---|---|---|---|---|---|\n|459.io|2018-04-26|2018-06-25|40.117.116.52||24,156|\n|n1o.io|2018-04-27|2018-06-26|23.98.135.84||14,113|\n|3df.me|2018-04-27|2018-06-26|104.209.158.224||20,315|\n|bki.me|2018-04-27|2018-06-26|23.96.7.223|adminsky.cn|17,703|\n|gji.me|2018-04-27|2018-06-26|104.209.183.79|xincache.com|15,801|\n|xih.me|2018-04-27|2018-06-26|13.78.148.193|22.cn|9,353|\n|zk0.io|2018-04-27|2018-06-26|23.101.187.48||16,549|\n|pucellina.com|2018-09-17|2018-09-17|104.209.246.70||5,983|\n|bioecologyz.com|2018-09-17|2018-09-17|40.76.26.54||3,215|\n|f89.me|2018-04-26|2018-06-25|13.78.137.38|22.cn|11,232|\n\n\nWe can clearly see two clusters of domains corresponding to the\ntwo time frames we identified in the temporal analysis above. The\nfirst eight domains were created just prior to the observed start of\nthe campaign (the red accounts in the temporal analysis above),\nand the last two domains were created some weeks before the\nlaunch of the second wave of the campaign (the orange accounts\nin the temporal analysis above).\n\nWe can also see historic name servers for a couple of domains, but\nthis does not give us much additional information. None of the 10\ndomains have any risk score in Recorded Future — the only trace of\nmalicious activity is that one of the previous name servers has been\nassociated with a suspicious IP number and a malware command\nand control server.\n\n\n-----\n\nSadly, due to anonymous registration, this is where our trail ends!\nWe can only speculate as to who registered these domains and is\nrunning the network of social media accounts that use them. The\nfact that the operation has been going on for close to a year, and that\nit is spending money on numerous domains on dedicated servers,\nleads us to believe this is not just someone running the operation\n“for the lulz,” but rather, a political organization or nation-state\nwith an intent to spread fear and uncertainty and track followers\nof the posted links.\n\n#### Account Status\n\nUpon closer inspection of the status of the accounts, we note that\na fair percentage of them have been suspended. The degree of\nsuspension varies between different URL shortener service clusters,\nbut it is clear that there has been no general suspension of accounts\nrelated to these URL shorteners. We believe one reason for this\nis that by posting links related to old, but real, terror events, the\naccounts are not in clear violation of any terms of service, and\ntherefore have not been suspended due to either automatic\nidentification or manual reporting.\n\n\n-----\n\n#### Conclusion\n\nBased on a long track record of investigating influence operations,\nwe developed a set of algorithms for identifying and analyzing\nsuch operations. Using these algorithms, we have identified a fairly\nlarge influence operation that uses old terror events repackaged\nas breaking news to gain attention. Using behavioral analytics,\nwe have shown how more than 215 accounts have participated\nin or are participating in this operation. The operation uses a\nset of dedicated URL shortener services to link to old news, and\nthis mechanism allows the operation to track the efficiency of its\noperation and possibly to analyze what kind of audience (at least\ngeographically) it succeeds in targeting.\n\n**About Recorded Future**\n\nRecorded Future arms security teams with the only complete threat intelligence\n\nsolution powered by patented machine learning to lower risk. Our technology\n\nautomatically collects and analyzes information from an unrivaled breadth of\n\nsources and provides invaluable context in real time and packaged for human\n\nanalysis or integration with security technologies.\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "5d2b9e7f-cf43-4b54-ba18-065aa3003611",
            "created_at": "2022-10-25T16:06:24.199525Z",
            "updated_at": "2022-10-25T16:06:24.199525Z",
            "deleted_at": null,
            "name": "CyberMonitor",
            "url": "https://github.com/CyberMonitor/APT_CyberCriminal_Campagin_Collections",
            "description": "APT & Cybercriminals Campaign Collection",
            "reports": null
        }
    ],
    "references": [
        "https://github.com/CyberMonitor/APT_CyberCriminal_Campagin_Collections/raw/master/2019/2019.06.11.Fishwrap_Group/cta-2019-0612.pdf"
    ],
    "report_names": [
        "cta-2019-0612"
    ],
    "threat_actors": [],
    "ts_created_at": 1666716494,
    "ts_updated_at": 1743041147,
    "ts_creation_date": 1560598174,
    "ts_modification_date": 1560598184,
    "files": {
        "pdf": "https://archive.orkl.eu/e9a48a1a62bc4d489aad6ebc74effcd32185c39a.pdf",
        "text": "https://archive.orkl.eu/e9a48a1a62bc4d489aad6ebc74effcd32185c39a.txt",
        "img": "https://archive.orkl.eu/e9a48a1a62bc4d489aad6ebc74effcd32185c39a.jpg"
    }
}