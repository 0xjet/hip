{
    "id": "185e5393-60b4-4ec0-a967-3cbac5d635b3",
    "created_at": "2022-10-25T16:48:19.19734Z",
    "updated_at": "2025-03-27T02:16:25.776239Z",
    "deleted_at": null,
    "sha1_hash": "aff2aa7c9402dd04739cebd6abd32f5cac462c47",
    "title": "The MITRE Systems Engineering Guide",
    "authors": "",
    "file_creation_date": "2014-05-09T13:41:45Z",
    "file_modification_date": "2014-05-16T11:39:57Z",
    "file_size": 8403737,
    "plain_text": "#### MITRE\n\n## Systems Engineering Guide\n\nCollected wisdom from MITRE’s\nsystems engineering experts\n\n\n-----\n\n-----\n\n### Systems Engineering Guide\n\n##### MITRE\n\n\n### Systems Engineering Guide\n\n##### MITRE\n\n\n-----\n\n© 2014 by The MITRE Corporation. All rights reserved.\n\nProduced by MITRE Corporate Communications and Public Affairs\n\nInternational Standard Book Number: 978-0-615-97442-2\n\nPrinted in the United States of America on acid-free paper\n\nThe views, opinions, and/or findings contained in this book are those of The MITRE\n\nCorporation and should not be construed as an official government position, policy, or\n\ndecision, unless designated by other documentation. This book discusses observations and\n\nideas, and The MITRE Corporation expressly disclaims any warranty of any kind. Although\n\nproducts are discussed in this book, nothing in this book should be construed as an\n\nendorsement of any kind. The trademarks used herein belong to their respective holders.\n\nApproved for public release; distribution unlimited. Case Numbers 10-4072 and 12-1089.\n\nThe MITRE Corporation\n\n202 Burlington Road • Bedford, MA 01730-1420 • (781) 271-2000\n\n7515 Colshire Drive • McLean, VA 22102-7539 • (703) 983-6000\n\n[www.mitre.org](http://www.mitre.org)\n\n[segteam@mitre.org](mailto:segteam@mitre.org)\n\n\n-----\n\n##### Acknowledgments\n\nThe MITRE Systems Engineering Guide (SEG) was first launched in\n\nMarch 2010 as an internal MITRE resource. In late 2010, a government\nonly version was rolled out in response to many requests from MITRE\n\nstaff to use it as a shared resource with their customers. In June 2011,\n\nan HTML version was published on www.mitre.org as a contribution to\n\nthe wider systems engineering community. The rollout of the public SEG\n\nresulted in requests for it to be made available for popular mobile plat­\n\nforms, and in September 2012 an eBook version was posted on www.\n\nmitre.org in formats for the iPad, iPhone, Android, Kindle, and compatible\n\ndevices. The SEG has been visited hundreds of thousands of times by\n\nindividuals across the world. Now it is available in hardcopy form.\n\nThe SEG is the result of an effort involving nearly 200 individuals from\n\nacross MITRE.\n\nThe seminal idea for capturing the corporation’s accumulated wis­\n\ndom on a wide variety of important and timely systems engineering\n\ntopics in a single, central location came from MITRE Corporate Chief\n\nEngineer Dr. Louis S. Metzger. He inspired the vision of the SEG as a\n\nresource that provides an “in the trenches” view of the typical prob­\n\nlems, pitfalls, conundrums, and tight corners that practicing systems\n\nengineers are likely to find themselves in, together with best practices\n\nand lessons learned to avoid or mitigate the problems and enhance the\n\n\n-----\n\nlikelihood of success. In this way, the SEG complements the many excellent systems\n\nengineering resources currently available.\n\nAn undertaking this ambitious requires a core team to organize and orchestrate the\n\nmyriad details—large and small—that are essential to overall success. Special thanks are\n\ndue to SEG core team members: Robin A. Cormier, Spurgeon T. Norman, Jr., Deborah L.\n\nSchuh, Peter A. Smyton, Dr. Robert S. Swarz, and Frederick C. Wendt.\n\nThe vast majority of contributors authored individual articles and are among the most\n\nsenior and respected MITRE technical staff. The members of the core team stand on\n\nthe shoulders of these experts.\n\nGeorge Rebovich, Jr.\n\nTeam Leader\n\n\n-----\n\n###### SE Guide Contents\n\nThe Evolution of Systems Engineering 1\n\nThe Essence of MITRE Systems Engineering 11\n\n**Enterprise Engineering** 19\n\nComprehensive Viewpoint 27\nSystems Thinking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n\nSystems Engineering Strategies for Uncertainty and Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nTools to Enable a Comprehensive Viewpoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n\nEnterprise Planning and Management 54\nIT Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n\nPortfolio Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n\nHow to Develop a Measurement Capability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n\nEnterprise Technology, Information, and Infrastructure 82\nIT Infrastructure Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n\nIT Service Management (ITSM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n\nInformation and Data Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n\nRadio Frequency Spectrum Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n\nEngineering Information-Intensive Enterprises 113\nArchitectures Federation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n\nDesign Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n\nComposable Capabilities On Demand (CCOD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\n\nOpen Source Software (OSS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n\nPrivacy Systems Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n\nSystems Engineering for Mission Assurance 155\nCyber Mission Assurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n\nCrown Jewels Analysis (CJA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n\nCyber Threat Susceptibility Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\n\nCyber Risk Remediation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n\nSecure Code Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n\nSupply Chain Risk Management (SCRM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\n\nTransformation Planning and Organizational Change 201\nPerforming Organizational Assessments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n\nFormulation of Organizational Transformation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n\nStakeholder Assessment and Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n\nEffective Communication and Influence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n\n\n-----\n\nPlanning for Successful User Adoption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n\nEnterprise Governance 238\nCommunities of Interest (COI) and/or Community of Practice (COP) . . . . . . . . . . . . . . . . . . . . . . . . 242\n\nStandards Boards and Bodies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\n\nPolicy Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\n\nMITRE FFRDC Independent Assessments 257\nPlanning and Managing Independent Assessments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260\n\n**SE Life‑Cycle Building Blocks** 269\n\nConcept Development 275\nOperational Needs Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\n\nConcept of Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\n\nOperational Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\n\nHigh-Level Conceptual Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\n\nRequirements Engineering 301\nEliciting, Collecting, and Developing Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\n\nAnalyzing and Defining Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314\n\nSpecial Considerations for Conditions of Uncertainty:\nPrototyping and Experimentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\n\nSystem Architecture 324\nArchitectural Frameworks, Models, and Views . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\n\nApproaches to Architecture Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\n\nArchitectural Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n\nSystem Design and Development 347\nDevelop System-Level Technical Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\n\nDevelop Top-Level System Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361\n\nAssess the Design’s Ability to Meet the System Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\n\nSystems Integration 378\nIdentify and Assess Integration and Interoperability (I&I) Challenges . . . . . . . . . . . . . . . . . . . . . . . . 381\n\nDevelop and Evaluate Integration and Interoperability (I&I) Solution Strategies . . . . . . . . . . . . 386\n\nAssess Integration Testing Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390\n\nInterface Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396\n\nTest and Evaluation 402\nCreate and Assess Test and Evaluation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\n\nAssess Test and Evaluation Plans and Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413\n\nVerification and Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419\n\nCreate and Assess Certification and Accreditation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425\n\n\n-----\n\nImplementation, O&M, and Transition 433\n\nOther SE Life-Cycle Building Blocks Articles 436\nSpanning the Operational Space—How to Select Use Cases and\nMission Threads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438\n\nAcquiring and Incorporating Post-Fielding Operational Feedback into\nFuture Developments: The Post-Implementation Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\n\nTest and Evaluation of Systems of Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451\n\nVerification and Validation of Simulation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461\n\nAffordability, Efficiency, and Effectiveness (AEE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\n\n**Acquisition Systems Engineering** 483\n\nAcquisition Program Planning 491\nPerforming Analyses of Alternatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .496\n\nAcquisition Management Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502\n\nAssessing Technical Maturity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n\nTechnology Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514\n\nLife-Cycle Cost Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518\n\nIntegrated Master Schedule (IMS)/Integrated Master Plan (IMP) Application . . . . . . . . . . . . . . . 524\n\nPerformance Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528\n\nComparison of Investment Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536\n\nSource Selection Preparation and Evaluation 543\nPicking the Right Contractor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548\n\nRFP Preparation and Source Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551\n\nProgram Acquisition Strategy Formulation 559\nAgile Acquisition Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563\n\nEvolutionary Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568\n\n“Big-Bang” Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572\n\nContractor Evaluation 576\nData-Driven Contractor Evaluations and Milestone Reviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n\nEarned Value Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585\n\nCompetitive Prototyping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593\n\nRisk Management 599\nRisk Management Approach and Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604\n\nRisk Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n\nRisk Impact Assessment and Prioritization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620\n\nRisk Mitigation Planning, Implementation, and Progress Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . 627\n\nRisk Management Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634\n\n\n-----\n\nConfiguration Management 643\nHow to Control a Moving Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649\n\nConfiguration Management Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653\n\nIntegrated Logistics Support 658\nReliability, Availability, and Maintainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 664\n\nManaging Energy Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 670\n\nQuality Assurance and Measurement 675\nEstablishing a Quality Assurance Program in the Systems Acquisition or\nGovernment Operational Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 681\n\nHow to Conduct Process and Product Reviews Across Boundaries . . . . . . . . . . . . . . . . . . . . . . . . 686\n\nContinuous Process Improvement 690\nImplementing and Improving Systems Engineering Processes for the\nAcquisition Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n\nMatching Systems Engineering Process Improvement Frameworks/\nSolutions with Customer Needs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 698\n\nIndex 703\n\n\n-----\n\n###### Introduction\n\nWelcome to the MITRE Systems Engineering Guide (SEG). The primary purpose of the SEG is\nto convey The MITRE Corporation’s accumulated wisdom on a wide range of systems engi­\nneering subjects—sufficient for understanding the essentials of the discipline and for translat­\ning this wisdom into practice in your own work environment.\n\nThe MITRE Systems Engineering Guide (SEG) has more than 600 pages of content and\ncovers more than 100 subjects. It has been developed by MITRE systems engineers for MITRE\nsystems engineers. Systems engineering is a team sport, so although the SEG is written “to”\na MITRE systems engineer, most of the best practices and lessons learned are applicable to\nall members of a government acquisition program team, whatever their particular role or\nspecialty.\n\nThis introduction provides guidance on how to navigate the pages of the SEG and ben­\nefit from doing so. It covers the practical matters—the organization, use, and roots of the\nSEG, what you should (and should not) expect from its articles, and how you can access and\nrespond to the latest SEG information on MITRE’s website.\n\n###### How the SEG Is Organized\n\nSetting the Context for the Systems Engineering Guide\n\n###### �[The Evolution of Systems Engineering—\u0003][provides a working definition of the discipline ]\n\nand traces its evolutionary arc into the future.\n###### �[The Essence of MITRE Systems Engineering—\u0003][introduces how our sponsors perceive ]\n\nMITRE systems engineering roles and responsibilities, and how we at MITRE interpret\nthose expectations.\n###### �[The Systems Engineering Guide—\u0003][Three “meaty” sections partitioned into topics and ]\n\narticles:\n\n      - **Enterprise Engineering—\u0003explains how to take a comprehensive view of systems**\n\nengineering activities at different scales of the customer enterprise, offers techniques\nfor engineering information-intensive enterprises that balance local and global needs,\nand covers how to provide systems engineering support to governance activities.\n\n      - **Systems Engineering Life-Cycle Building Blocks—\u0003is organized around the funda­**\n\nmentals of setting up engineering systems regardless of the specific life-cycle method­\nology used by the supporting sponsor or customer.\n\n      - **Acquisition Systems Engineering—\u0003is centered on how MITRE systems engineering**\n\nfits into and supports government acquisition programs.\n\n\n-----\n\n###### How to Use the Systems Engineering Guide (SEG)\n\nThe first time you access the SEG, read the two expository pieces—The Evolution of Systems\nEngineering and The Essence of MITRE Systems Engineering—in their entirety. They are\nintended to set the context for the material in the three major sections.\n\nThen, take some time to familiarize yourself with the SEG by reading the section-level\nintroductions and sampling a topic or two and a few articles.\n\nTo support your work program or SE educational activities, come back to specific topics\nand articles in the SEG as needed.\n\n###### Systems Engineering Competency Model\n\n[The SEG organization and perspective were inspired by and based on the MITRE Systems](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n[Engineering Competency Model (SECM). MITRE uses the SECM primarily for competency](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\nassessments (self and manager) and development activities, including an internal systems\nengineering curriculum. The competency model is included with the SEG on www.mitre.org.\n\nEach article in the SEG contains a brief MITRE Systems Engineering Roles & Expectations\nstatement distilled from the competency model. Although we believe that much in the SEG\nand SECM is applicable to others, the articles should be used as references to be tailored to\nyour specific objectives and circumstances.\n\n###### What You Will Find in an Article\n\nThe articles are written as if the author is speaking directly to a MITRE technical staff mem­\nber involved in an FFRDC-related systems engineering activity on a government program or\nto someone who wants to learn more about a particular systems engineering perspective. The\nauthors are MITRE systems engineering practitioners with substantial experience in a particu­\nlar subject area.\n\nEach article attempts to convey where MITRE systems engineering typically fits in the\nbig picture of government participants and commercial contractors and clarifies how MITRE’s\nrole differs from that of the other players. Each article follows the same basic construct:\n\n###### �[The authors were asked, “What are the common problems, pitfalls, conundrums, and ]\n\ntight corners that MITRE systems engineers are likely to find themselves in when work­\ning in this subject area?”\n###### �[For each problem or conundrum, the authors answered the question, “What wisdom is ]\n\nthere to convey to avoid or mitigate problems or enhance the likelihood of success?”\n###### �[The wisdom is conveyed in a set of succinct best practices and lessons learned.] �[When an important conundrum is identified, when possible, potential approaches are ]\n\nsuggested for solving the problem.\n\n\n-----\n\n###### �[Each article cites references and resources for additional reading. Be sure to check them ]\n\nout if you are interested in more details.\n\n###### What the SEG Is Not\n\nThe SEG is not intended to provide guidance on every possible issue under the “systems\nengineering sun.” A complete discussion on even one topic could probably fill volumes.\nNor is it intended to serve as a compendium of Systems Engineering 101 tutorials. A rich set\nof resources, within MITRE and beyond, can be tapped into for educational purposes. And\nthough the SEG is based on the collective experience of MITRE systems engineers across the\ncompany, it is not intended to serve as a resource on detailed sponsor- or customer-specific\nsystems engineering policies, practices, or processes.\n\nSystems engineering is a dynamic and evolving discipline, and we are actively evolving\nthe SEG to keep pace with that change. Be sure to visit MITRE’s online version of the SEG\n[periodically at www.mitre.org to see what’s new.](http://www.mitre.org)\n\nFinally, we hope that you find this material of interest. If you have comments or feedback,\n[please contact us at segteam@mitre.org.](https://register.mitre.org/seg/)\n\n###### Setting the Context for the Systems Engineering Guide\n\nThe next section presents two expository pieces—The Evolution of Systems Engineering and The\n_Essence of MITRE Systems Engineering—detailing, respectively, how systems engineering has_\nevolved as a discipline and how MITRE’s systems engineering practice is shaped by our role\nas an operator of Federally Funded Research and Development Centers (FFRDCs). Together,\nthese pieces are intended to set the context for your use of MITRE’s Systems Engineering\nGuide (SEG).\n\n\n-----\n\n-----\n\n###### The Evolution of Systems Engineering\n\nThe twenty-first century is an exciting time for the field of systems engineering. Advances in\nour understanding of the traditional discipline are expanding. At the same time, new forms\nof systems engineering are emerging to address the engineering challenges of systems-ofsystems (SoS) and enterprise systems. Even at this point in their evolution, these new forms\nare evincing their own principles, processes, and practices. Some are different in degree than\nengineering at the system level, whereas others are different in kind.\n\nAlthough it is impossible to predict how the traditional and new forms of systems engi­\nneering will evolve, it is clear even now that a long and robust future lies ahead for all.\nIncreases in technology complexity have led to new challenges in architecture, networks,\nhardware and software engineering, and human systems integration. At the same time, the\nscale at which systems are engineered is exceeding levels that could have been imagined only\na short time ago. As a consequence, all forms of systems engineering will be needed to solve\nthe engineering problems of the future, sometimes separately but increasingly in combination.\n\nWhat Is Systems Engineering?\n\nThe term systems engineering can be traced back at least to the 1940s, but to this day no single,\nuniversal definition of the term exists. Frequently, systems engineering is defined by the con­\ntext in which it is embedded. One definition of the classical practice of systems engineering\nis, “an interdisciplinary approach to translating users’ needs into the definition of a system,\nits architecture and design through an iterative process that results in an effective operational\nsystem. Systems engineering applies over the entire life cycle, from concept development to\nfinal disposal [1].”\n\nSystems Engineering Life Cycle\n\nSystems engineering models and processes are usually organized around the concept of a life\n_cycle. Like the definition of systems engineering, the detailed conceptualization of life cycle is_\nby no means unique across the communities that employ the discipline.\n\nThe International Council on Systems Engineering (INCOSE) systems engineering process\nis a widely recognized representation of classical systems engineering [2]. ISO/IEC 15288 [3]\nis an international systems engineering standard covering processes and life-cycle stages. It\ndefines a set of processes divided into four categories: technical, project, agreement, and enter­\nprise. Sample life-cycle stages include concept, development, production, utilization, support,\nand retirement. The U.S. Department of Defense uses the following phases: materiel solution\nanalysis, technology development, engineering and manufacturing development, production\nand deployment, and operations and support [4].\n\n\n-----\n\nAlthough the detailed views, implementations, and terminology used to articulate the\nsystems engineering life cycle differ across MITRE’s sponsors and customers, they all share\nfundamental elements, depicted in Figure 1 in the V-model [5]. This is a common graphical\nrepresentation of the systems engineering life cycle. The left side of the V represents concept\ndevelopment and the decomposition of requirements into functions and physical entities that\ncan be architected, designed, and developed. The right side of the V represents integration of\nthese entities (including appropriate testing to verify that they satisfy the requirements) and\ntheir ultimate transition into the field, where they are operated and maintained.\n\nThe model of systems engineering used in this guide is based on the “V” representation.\nNote, however, that the system life cycle is rarely, if ever, as linear as this simplified discus­\nsion might imply. There are often iterative cycles, skipped phases, overlapping elements, etc.\nAdditionally, important processes and activities apply to more than one phase in a system life\ncycle, which are better envisioned as threading through or overarching the other building\n\n**Concept** **Transition**\n**Development** **Operation &**\n**Maintenance**\n\n**Requirements** **Test &**\n**Engineering** **Evaluation**\n\n**System** **System**\n**Architecture** **Integration**\n\n**System Design**\n**& Development**\n\nFigure 1. V-Model of Systems Engineering Life Cycle\n\n\n-----\n\nblocks. Risk identification and management is one example. Consult the SE Life-Cycle\nBuilding Blocks and Acquisition Systems Engineering sections for details.\n\nNumerous variations on the classical systems engineering life cycle can be found, includ­\ning incremental or spiral developments that mitigate uncertainties in long-range requirements\nor funding of the system under development as well as evolutionary approaches for navigat­\ning uncertainties in enabling technology maturity. All three sections of the guide—Enterprise\nEngineering section, SE Life-Cycle Building Blocks section, and Acquisition Systems\nEngineering section—contain discussions on these variations.\n\n###### Conditions for Effective Systems Engineering\n\nAs already noted, systems engineering is normally defined and shaped by the context or\nenvironment in which it is embedded. The classical systems engineering approach is tailored\nto and works best in situations in which all relevant systems engineering factors are largely\nunder the control of or can at least be well understood and accommodated by the systems\nengineering organization or the program manager. In general terms, this is when system\nrequirements are relatively well established, technologies are mature, the system is being\ndeveloped for a single or relatively homogeneous user community, and a single individual has\nmanagement and funding authority over the program. Even then, these conditions, while nec­\nessary, are rarely sufficient to ensure success. What is needed, however, are a strong govern­\nment program office capable of a peer relationship with the contractor; effective architecting,\nincluding problem definition, evaluation of alternative solutions, and analysis of execution\nfeasibility; careful attention to program management and systems engineering foundational\nelements; selection of an experienced, capable contractor; and effective performance-based\ncontracting.\n\n###### A Changing Landscape—Systems of Systems\n\nWith the increased emphasis on capabilities and networking, MITRE’s sponsors and custom­\ners are recognizing the criticality of effective end-to-end performance of SoS to meet opera­\ntional user needs. Though most government acquisition policies and processes continue to\nfocus on the development and evolution of individual systems, their requirements are increas­\ningly based on assessments of gaps in user capabilities that require integration across indi­\nvidual systems to be enabled. Increasingly, the role of systems engineering is turning to the\nengineering of SoS to provide these capabilities.\n\nOne working definition of SoS is “a set or arrangement of systems that results when inde­\npendent and useful systems are integrated into a larger system that delivers unique capabili­\nties [6].” Both individual systems and SoS are considered systems because each consists of\nparts, relationships, and a “whole” that is greater than the sum of the parts. However, not all\n\n\n-----\n\nsystems are SoS. Rather, SoS systems engineering deals with “[the] planning, analyzing, orga­\nnizing and integrating of the capabilities of a mix of existing and new development systems\ninto an SoS capability greater than the sum of the capabilities of the constituent parts [6].”\nSoS may deliver capabilities by combining multiple collaborative, autonomous-yet-interacting\nsystems. The mix of systems may include existing, partially developed, and yet-to-be-designed\nindependent systems.\n\nSoS can take different forms, as shown in Table 1 [7, 8]. The Global Information Grid is\nan example of a virtual SoS. Communities of interest are examples of a collaborative SoS. The\nMissile Defense Agency Ballistic Missile Defense System is an example of an acknowledged\nSoS, and the U.S. Army Future Combat System is an example of a directed SoS.\n\nIncreasingly, MITRE sponsors and customers are facing the challenges of acknowledged\nSoS, defined in Table 1. This calls for capability management and SE at the SoS level while\nmaintaining the management and technical autonomy of systems contributing to the SoS\ncapability objectives.\n\nTable 1. Types of Systems of Systems\n\n|Type|Definition|\n|---|---|\n|Virtual|Virtual SoS lack a central management authority and a centrally agreed- on purpose for the system of systems. Large-scale behavior emerges— and may be desirable—but this type of SoS must rely on relatively invisible mechanisms to maintain it.|\n|Collaborative|In collaborative SoS, the component systems interact more or less volun­ tarily to fulfill agreed-on central purposes. The Internet is a collaborative system. The Internet Engineering Task Force works out standards but has no power to enforce them. The central players collectively decide how to provide or deny service, thereby providing some means of enforcing and maintaining standards.|\n|Acknowledged|Acknowledged SoS have recognized objectives, a designated manager, and resources. However, the constituent systems retain their independent ownership, objectives, funding, development, and sustainment approaches. Changes in the systems are based on collaboration between the SoS and the system.|\n|Directed|Directed SoS are those in which the integrated system of systems is built and managed to fulfill specific purposes. It is centrally managed during long- term operation to continue to fulfill those purposes as well as any new ones the system owners might want to address. The component systems main­ tain an ability to operate independently, but their normal operational mode is subordinated to the central managed purpose.|\n\n\n-----\n\nA typical strategy for providing end-to-end support for new capability needs is to add\nfunctionality to assets already in the inventory. In most cases, these systems continue to be\nused for their original requirements. Consequently the ownership or management of these\nsystems remains unchanged, and they continue to evolve based on their own development\nand requirements processes and independent funding.\n\nThe resulting dual levels of management, objectives, and funding create management\nchallenges for both the SoS and the systems, especially when their objectives are not well\naligned. In turn, these management challenges pose technical challenges for systems engi­\nneers, especially those working on the SoS. Table 2 summarizes differences between systems\nand acknowledged SoS that have particular implications for engineering SoS.\n\nThe differences summarized in Table 2 lead to differences in SoS engineering. Some are\ndifferences in degree, and others are differences in kind. These are briefly outlined here, and\nthe references provide a more detailed discussion.\n\n###### �[SoS systems engineers must be able to function in an environment where the SoS man­]\n\nager does not control all of the systems that impact the SoS capabilities and where the\nstakeholders have interests beyond the SoS objectives [9, pp. 11–12].\n###### �[SoS SE must balance SoS needs with individual system needs [9, p. 12].] �[SoS SE planning and implementation must consider and leverage development plans of ]\n\nthe individual systems [9, pp. 13–14].\n###### �[SoS SE must address the end-to-end behavior of the ensemble of systems, addressing ]\n\nkey issues affecting the behavior [9, pp. 14–15].\nThe discipline of SoS systems engineering is still in its infancy. Nevertheless, a set of SoS\nsystems engineering principles is beginning to emerge from a U.S. Department of Defense\n(DoD) initiative to understand and differentiate engineering of these complex, increasingly\ncommon entities from individual systems [10]. These guiding principles are briefly noted here\nand are discussed in more detail in the references.\n\n###### �[Address organizational as well as technical issues when making SE trades and decisions ]\n\n[9, p. 21].\n###### �[Acknowledge the different roles of systems engineers at the system vs. the SoS level\n]\n\nand the relationship between the different SE approaches taken at each of the levels\n\n[9, pp. 21–22].\n###### �[Conduct balanced technical management of the SoS [9, p. 22].] �[Use an architecture based on open systems and loose coupling [9, p. 23].] �[Focus on the design strategy and trade-offs when the formal SoS is first established and ]\n\nthroughout the SoS evolution [9, p. 23].\n\n\n-----\n\nTable 2. Comparison of Systems and Systems of Systems [9, p. 13]\n\n|Aspect of Environment|System|Acknowledged System of Systems|\n|---|---|---|\n|Management & Oversight|||\n|Stakeholder Involvement|Clearer set of stakeholders|Stakeholders at both system level and SoS levels, including system owners with com­ peting interests and priorities. In some cases, the system stakeholder has no vested inter­ est in the SoS; all stakeholders may not be recognized.|\n|Governance|Aligned program manage­ ment and funding|Added levels of complexity due to manage­ ment and funding for both the SoS and indi­ vidual systems. SoS does not have authority over all of the systems.|\n|Operational Environment|||\n|Operational Focus|Designed and developed to meet operational objectives|Called on to meet a set of operational objec­ tives using systems whose objectives may or may not align with the SoS objectives.|\n|Implementation|||\n|Acquisition|Aligned with acquisition milestones, documented requirements, program has a systems engineering plan|Added complexity due to multiple system life cycles across acquisition programs, involv­ ing legacy systems, developmental systems, new developments, and technology insertion. Typically they have stated capability objectives upfront which may need to be translated into formal requirements.|\n|Test & Evaluation|Test and evaluation of the system is generally possible|Testing is more challenging due to the diff­i culty of synchronizing across multiple sys­ tems’ life cycles, given the complexity of all the moving parts and potential for unintended consequences.|\n|Engineering & Design Considerations|||\n|Boundaries & Interfaces|Focuses on boundaries and interfaces for the single system|Focus is on identifying systems that contribute to the SoS objectives and enabling the flow of data, control, and functionality across the SoS while balancing needs of the systems.|\n|Performance & Behavior|Performance of the system to meet specified objectives|Performance across the SoS satisfies SoS user capability needs while balancing needs of the systems.|\n\n\n-----\n\n###### Engineering the Enterprise [11, 12]\n\nMITRE’s sponsors, customers, and the users of the operational systems we help engineer are\nin the midst of a major transformation driven by and deriving largely from advances in infor­\nmation technology.\n\nThe rate of technical change in information processing, storage, and communications\nbandwidth is enormous. Expansions in other technologies (e.g., netted sensors) have been\nstimulated and shaped by these changes. The information revolution is reducing obstacles to\ninteractions among people, businesses, organizations, nations, and processes that were previ­\nously separated in distance or time. Somewhat paradoxically, future events in this information\nabundant world are harder to predict and control, with the result that our world and our role\nas systems engineers are becoming increasing complex.\n\nThis new complexity is a consequence of the interdependencies that arise when large\nnumbers of systems are networked together to achieve some collaborative advantage. It is fur­\nther intensified by rapid technology changes. When networked systems are each individually\nadapting to both technology and mission changes, then the environment for any given system\nor individual becomes essentially unpredictable. The combination of large-scale interdepen­\ndencies and unpredictability creates an environment that is fundamentally different from that\nat the system or SoS level.\n\nExamples in which this new complexity is evident include the U.S. Federal Aviation\nAdministration’s National Airspace System, the DoD’s Global Information Grid, the Internal\nRevenue Service’s Tax Systems, and the Department of Homeland Security’s Secure Border\nInitiative’s SBInet.\n\nAs a result, systems engineering success expands to include not only that of an individual\nsystem or SoS, but of the network of constantly changing systems as well. To successfully\nbring value to these enterprise system users requires the disciplined methods and “big pic­\nture” mindset of the classical forms of systems engineering, plus new methods and mindsets\naimed at addressing the increased complexity.\n\nBecause our customers’ needs are driving the trend toward collaborative advantage and\nadaptability, we must evolve our methods to these changing conditions. This situation is char­\nacterized by several specific characteristics:\n\n###### �[Our customers face extremely complex problems in which stakeholders often disagree ]\n\non the nature of the problems as well as the solutions (i.e., technical and social).\n###### �[Their missions are changing rapidly and unpredictably—thus systems must interoperate ]\n\nin ways that their original developers never envisioned.\n###### �[Even without a predefined direction, the systems will keep evolving and responding to ]\n\nchanging needs and emerging opportunities—the network is inherently adaptive.\n\n\n-----\n\n###### �[People are integral parts of the network, and their purposeful behavior will change ]\n\nthe nature of the network—individual systems must be robust to changes in their\nenvironment.\nThus the systems that we help engineer are facing additional, fundamentally different\nchallenges. Nevertheless, when a system is bounded with relatively static, well-understood\nrequirements, the classical methods of systems engineering are applicable and powerful. It is\nthe increased complexity of problems and solutions that has caused us to extend the systems\nengineering discipline into a domain we call enterprise systems engineering.\n\nWhat do we mean by an enterprise? Enterprise refers to a network of interdependent\npeople, processes, and supporting technology not fully under the control of any single entity.\nIn business literature, an enterprise frequently refers to an organization, such as a firm or\ngovernment agency, and in the computer industry, it refers to any large organization that uses\ncomputers. The MITRE definition emphasizes the interdependency of individual systems and\neven systems of systems. We include firms, government agencies, large information-enabled\norganizations, and any network of entities coming together to collectively accomplish explicit\nor implicit goals. This includes the integration of previously separate units. The enterprise\ndisplays new behaviors that emerge from the interaction of the parts. Examples of enterprises\ninclude:\n\n###### �[A military command and control enterprise of organizations and individuals that ]\n\ndevelop, field, and operate command and control systems, including the acquisition\ncommunity and operational organizations and individuals that employ the systems.\n###### �[A chain hotel in which independent hotel properties operate as agents of the hotel enter­]\n\nprise in providing lodging and related services, while the company provides business\nservice infrastructure (e.g., reservation system), branding, etc.\nWhat do we mean by enterprise systems engineering? This domain of systems engineer­\ning concentrates on managing uncertainty and interdependence in an enterprise. It encom­\npasses and balances technical and non-technical aspects of the problem and the solution.\nIt fits within the broad, multidisciplinary approach of systems engineering and is directed\ntoward building effective and efficient networks of individual systems to meet the objectives\nof the whole enterprise.\n\nIn performing enterprise systems engineering, we engineer the enterprise and we engi­\nneer the systems that enable the enterprise. In particular, we help customers shape their\nenterprises, aligning technology to support goals. We support their business planning, policymaking, and investment strategies. We also determine how the individual systems in the\nenterprise perform and how they affect each other.\n\n\n-----\n\nAt MITRE, we consider enterprise systems engineering as a domain that focuses on\ncomplexity in the broader practice of systems engineering. It is not a replacement for classical\nmethods, and often both classical systems engineering and enterprise systems engineering\napproaches must be applied in combination to achieve success.\n\nWe are learning and evolving enterprise systems engineering as we are doing it. Several\nbasic tenets in the practice are apparent even at this early stage of its evolution:\n\n###### �[Systems thinking:][ Seeing wholes, interrelationships, and patterns of change.] �[Context awareness:][ Being mindful of the political, operational, economic, and technical ]\n\ninfluences and constraints.\n###### �[Accepting uncertainty:][ Acknowledging that some problems cannot be solved by pre­]\n\nscriptive or closed-form methods.\n###### �[Complex systems evolution:][ Drawing from the fundamental principles in the sciences ]\n\nof evolution, ecology and adaptation (e.g., considering variety, self-organization, and\nselection).\n###### �[Matching practice to the problem:][ Knowing when and under what circumstances to ]\n\napply prescriptive methods and when to apply complex systems principles and associ­\nated practices.\nThe SEG’s Enterprise Engineering section and the references provided in the articles are\nthe primary source for enterprise systems engineering subjects. This is a rapidly changing\ndomain of systems engineering.\n\n###### References and Resources\n\n[1. Committee on Pre-Milestone A Systems Engineering, 2009, Pre-Milestone A and Early-Phase](http://www.nap.edu/catalog.php?record_id=12065)\n\n_[Systems Engineering: A Retrospective Review and Benefits for Future Air Force Acquisition,](http://www.nap.edu/catalog.php?record_id=12065)_\nThe National Academies Press.\n\n2. International Council on Systems Engineering (INCOSE), INCOSE Systems Engineering\n\nHandbook.\n\n[3. ISO/IEC 15288, 2002, Systems Engineering—System Life Cycle Processes.](http://ieeexplore.ieee.org/Xplore/defdeny.jsp?url=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D4040510%26isnumber%3D4040509&userType=inst)\n\n[4. Department of Defense Instruction Number 5000.02, December 8, 2008, Operation of the](http://www.dtic.mil/whs/directives/corres/pdf/500002p.pdf)\n\n[Defense Acquisition System.](http://www.dtic.mil/whs/directives/corres/pdf/500002p.pdf)\n\n[5. Wikipedia contributors, “V-Model,” Wikipedia (accessed January 13, 2010).](http://en.wikipedia.org/w/index.php?title=V-Model&oldid=328588516)\n\n6. Department of Defense, October 14, 2004, “System of Systems Engineering,” Defense\n\nAcquisition Guidebook, Washington, DC.\n\n7. Maier, M., 1998, “Architecting Principles for Systems-of-Systems,” Systems Engineering,\nVol. 1, No. 4, pp 267–284.\n\n\n-----\n\n8. Dahmann, J., and K. Baldwin, April 7–10, 2008, “Understanding the Current State of US\n\nDefense Systems of Systems and the Implications for Systems Engineering,” IEEE Systems\nConference, Montreal, Canada.\n\n9. Office of the Undersecretary of Defense for Acquisition, Technology and Logistics (OUSD\n\n[AT&L), August 2008, Systems Engineering Guide for Systems of Systems, Washington, DC.](http://www.acq.osd.mil/se/docs/SE-Guide-for-SoS.pdf)\n\n10. Baldwin, K., June 28, 2007, “Systems of Systems: Challenges for Systems Engineering,”\n\nINCOSE SoS SE Panel.\n\n[11. The MITRE Corporation, August 2007, Evolving Systems Engineering, Bedford, MA.](http://www.mitre.org/news/pdfs/mitre_ese.pdf)\n\n[12. Rebovich, G., Jr., April 2006, “Systems Thinking for the Enterprise: New and Emerging](http://www.mitre.org/work/tech_papers/tech_papers_06/06_0391/)\n\n[Perspectives,” Proceedings of the 2006 IEEE International Conference on Systems of Systems.](http://www.mitre.org/work/tech_papers/tech_papers_06/06_0391/)\n\n\n-----\n\n###### The Essence of MITRE Systems Engineering\n\nThe previous section, The Evolution of Systems Engineering, notes that the systems engineer­\ning discipline is defined by the context or environment in which it is embedded. This com­\npanion section describes more specifically how the distinctive attributes of MITRE systems\nengineering are shaped by the expectations of our sponsors and customers and further\nformed by our corporate interpretation of the quality systems engineering required to meet\nthose expectations.\n\n###### Sponsor Expectations for MITRE Systems Engineering\n\nThe U.S. Federal Acquisition Regulation (FAR) part 35.017 sets forth federal policy on the\nestablishment and use of Federally Funded Research and Development Centers (FFRDCs) and\nrelated sponsoring agreements [1]. A portion is excerpted below.\n\n35.017 Federally Funded Research and Development Centers.\n(a) Policy.\n...\n...\n\n(2) An FFRDC meets some special long-term research or development need which can­\nnot be met as effectively by existing in-house or contractor resources. FFRDC’s enable\nagencies to use private sector resources to accomplish tasks that are integral to the\nmission and operation of the sponsoring agency. An FFRDC, in order to discharge its\nresponsibilities to the sponsoring agency, has access, beyond that which is common\nto the normal contractual relationship, to Government and supplier data, including\nsensitive and proprietary data, and to employees and installations equipment and real\nproperty. The FFRDC is required to conduct its business in a manner befitting its spe­\ncial relationship with the Government, to operate in the public interest with objectivity\nand independence, to be free from organizational conflicts of interest, and to have full\ndisclosure of its affairs to the sponsoring agency. It is not the Government’s intent that\nan FFRDC use its privileged information or access to installations equipment and real\nproperty to compete with the private sector.\n...\n...\n\n(4) Long-term relationships between the Government and FFRDC’s are encouraged in\norder to provide the continuity that will attract high-quality personnel to the FFRDC.\nThis relationship should be of a type to encourage the FFRDC to maintain currency in\n\n\n-----\n\nits field(s) of expertise, maintain its objectivity and independence, preserve its familiar­\nity with the needs of its sponsor(s), and provide a quick response capability.\n\nSome phrases from this excerpt stand out as particularly important factors that influence\nthe way in which MITRE executes its systems engineering roles and responsibilities:\n\n###### �[Meets some special long-term research or development need which cannot be met ]\n\n[otherwise]\n###### �[Private sector resources ] �[Access, beyond that which is common to the normal contractual relationship ] �[Operate in the public interest with objectivity and independence ] �[Free from organizational conflicts of interest ] �[Full disclosure of its affairs to the sponsoring agency ] �[Not...compete with the private sector ] �[Currency in its field(s) of expertise ] �[Familiarity with the needs of its sponsor(s) ]\nMITRE’s individual FFRDC sponsoring agreements further shape how we perceive and\npractice systems engineering [2, 3, 4, 5]. The FFRDC sponsoring agreements for the NSEC\n\n[National Security Engineering Center], CAASD [Center for Advanced Aviation System\nDevelopment], CEM [Center for Enterprise Modernization], and SEDI [Homeland Security\nSystems Engineering and Development Institute] further delineate the purpose and role of\neach FFRDC, its core work, relationship to the sponsoring organization, and other details of\nits operation. Despite obvious differences among the sponsoring agreements, two consistent\nthemes are evident: Each FFRDC is expected to be doing appropriate work that answers the\nnation’s needs, and that work needs to be done well. Within MITRE, we sometimes use the\nshorthand “do the right work” when referring to the former and “do the work right” when\nreferring to the latter. These two fundamental characteristics of quality systems engineering\nare understood and practiced by MITRE. The following excerpts from each of the four spon­\nsoring agreements illustrate these aspects of MITRE systems engineering.\n\n###### Do the Right Work\n �[The work performed...will...be...of both long-term and immediate homeland security ]\n\nconcern...\n###### �[Identification of critical capability gap[s]...particularly in areas where technology...]\n\ncontribute[s] substantially to solutions.\n###### �[Subjects integral to the mission and operations of the sponsoring offices.] �[Provid[e] technical and integration expertise...particularly in the evolution of the most ]\n\ncomplex and critical homeland security programs.\n\n\n-----\n\n###### �[Promote compatibilities across the various homeland security platforms and equip­]\n\nment...through...improved interoperability and information sharing within the home­\nland security enterprise.\n###### �[Work on the most complex homeland security systems that will evolve capabilities...] �[Help the Department develop a DHS system of systems approach... ] �[Address the long- and short-term evolutionary change necessary to modernize the NAS. ] �[Development and evaluation of plans for the evolution and integration of ATM system ]\n\ncapabilities.\n###### �[Problems that do not stand alone but are so linked to others that highly specific analysis ]\n\nmay be misleading.\n###### �[Issues that cannot be formulated sharply enough in advance. ] �[Unprecedented problems that require unique research methods. ] �[Perform studies, analysis and concept formulation for continued...modernization and ]\n\ndevelopment of the NAS.\n###### �[Works with DoD [Department of Defense] to research, develop, integrate, field, sus­]\n\ntain and modernize timely, affordable and interoperable C4ISR [Command, Control,\nCommunications, Computers, Intelligence, Surveillance, and Reconnaissance] solutions,\nsystems and technology.\n###### �[Provid[e] enterprise systems engineering and integration support throughout the C4ISR ]\n\nmission area.]\n###### �[Help identify, define, and recommend solutions to problems as a trusted partner of the ]\n\nSponsors’ management team.\n###### �[Focus...on core work that promotes C4ISR integration/interoperability.] �[[Maintains] an end-to-end understanding of the C4ISR mission area with emphasis on ]\n\nenterprise architectures that enable increasingly advanced and more fully integrated\nsystems of systems, system acquisition (including technical support to source selection),\nintegration of commercial and military technologies and interoperability.\n\n###### Do the Work Right\n �[Produces high-quality work of value to the sponsors ] �[...performance of objective, high-quality work...] �[Provide the government with the necessary expertise to provide best lifecycle value... ] �[Develop and promote standardization of effective and efficient system engineering best ]\n\npractices...\n###### �[The work performed...will...be authoritative…] �[...purpose is to provide special technical expertise]\n\n\n-----\n\n###### �[Simultaneously direct...efforts to the support of individual programs and projects for ]\n\nenterprise modernization, assuring that these individual programs and projects oper­\nate effectively with one another and efficiently support the business objectives of the\nGovernment.\n###### �[Provide exceptional technical competence in support of the Sponsors’ design and pur­]\n\nsuit of mission goals.\n###### �[Partner with the Sponsors in pursuit of excellence in public service. ] �[Maintain a commitment to technical excellence...in everything it does. ] �[Promotion of technical excellence...will be paramount. ] �[...shall be responsible to the FAA with regard to the progress and quality of...NAS devel­]\n\nopment efforts undertaken by it.\n###### �[...staff...encouraged to publish...in professional journals...to have the quality of such ]\n\nwork subject to peer scrutiny.\n###### �[maintaining objectivity and high technical quality. ] �[...maximize value... ] �[...while serving the immediate needs of the many individual programs it supports, ]\n\nthe C3I FFRDC aligns its work program to assist in achieving integrated enterprise\ncapabilities...\n###### �[...information as an enterprise asset to be shared... ]\n\n MITRE Expectations for Quality in Systems Engineering [6]\n\nQuality in MITRE’s systems engineering includes aspects of both delivering an inherently\ngood product or service and meeting external expectations. For MITRE, external expectations\nare set by multiple stakeholders, including not only our immediate customers but also the end\nusers of the capabilities we help create, our FFRDC sponsors (and those above them who set\nexpectations for FFRDCs more generally), and our Board of Trustees (who are external to dayto-day company affairs). For the most part, the higher level expectations from our sponsors\nand Board align with each other and with our internal aspirations for “good” as embodied by\nour strategic framework. They also align with how MITRE can and should uniquely contrib­\nute to meeting end user needs. These alignment points include:\n\n1. Working in the public interest on issues of critical national importance by...\n2. Proactively applying systems engineering and advanced technology to bring...\n3. Timely and innovative/creative solutions to key, hard problems, balancing...\n4. Technical feasibility with economic and political practicality, and leveraging...\n5. Breadth and depth of engineering with mission/business domain knowledge, while...\n6. Providing an integrating perspective across boundaries, and always...\n7. Retaining objectivity and being cost effective in our work.\n\n\n-----\n\nTo meet these expectations we need to be doing appropriate work that answers the\nnation’s needs, and we need to do it well. This is the key requirement that cuts across our four\nsponsoring agreements. We also need to satisfy our immediate customers. And we need to\ninvest in developing quality relationships with decision makers, stakeholders, and our custom­\ners, to shape our work and present results so that they have the impact they deserve. Meeting\nour customers’ expectations requires that we provide value in the quality of our contributions.\n\nTherefore, quality in MITRE systems engineering can be defined as the degree to which\nthe results of systems engineering meet:\n\n1. The higher level expectations for our FFRDCs—resulting in usability and value for end\n\nrecipients.\n2. Expectations of our immediate customers—service and performance.\nThe pressures on our customers often lead them to ask MITRE for quick-reaction\nresponses. To the extent that a quick response is practical, we must provide it. (When the\nimposed constraints make an informed response impractical, we need to define the extent to\nwhich we can make an informed response, explain why we cannot go further, and refuse the\nremainder of the task.) Our processes for identifying and leveraging applicable past analyses\nand data, informed professional judgments, and relevant experiences (either within or exter­\nnal to MITRE) need to be focused on enabling the highest quality response within the con­\nstraints imposed. Whenever possible, we should document our delivery (even after the fact)—\nthe assumptions made, the methods used, and the results conveyed. We also must develop\nour knowledge base to continually improve our ability to respond to future requests related to\nour core competencies.\n\nMoreover, we must assess the risks of quick responses to understand the possible issues\nwith their accuracy and completeness, including the potential consequences of these issues—\nand so inform the customer. When the risk is high, we should strongly recommend a plan for\na more complete, fact-based analysis, using, as needed, trade-space exploration, modeling and\nsimulation, experimentation, proof-of-concept prototyping, etc. Clearly, circumstances requir­\ning in-depth study, especially if associated with key national capability outcomes, demand\nthe highest quality work. This entails careful planning and work shaping, appropriate staff­\ning and resources, peer and management consultation and review throughout the execution\nof the work, and socializing and delivering the results so that they are correctly interpreted\nand acted on. It is important to note that the higher level expectations on MITRE can only be\nmet when a significant fraction of our work goes beyond quick response activities, so finding\nourselves in these circumstances should be relatively common.\n\nThe higher level expectations on MITRE push us beyond responding to customer requests\ntoward proactively identifying key issues on which we can make a difference. These often\ninvolve enterprise objectives such as integration and interoperability for information sharing\n\n\n-----\n\nacross the government (and, at times, beyond), which may exceed the bounds of an individual\ncustomer’s purview. When these proactive initiatives lead to substantive efforts, they also\ndemand the highest quality work, applying all the same attributes discussed above to their\nplanning, execution, and delivery.\n\nMITRE needs to provide its customers with “quick and dirty” products when necessary,\nmaking them as “clean” as possible but conveying a clean/dirty assessment with the product.\nHigher level expectations for MITRE’s FFRDC contributions often require us to work more\nsubstantively, with an even greater emphasis on quality for our work. Quality, then, involves\nboth doing enough of the right work, and doing all of our work (but especially the higher\nimpact work) right. It also includes building relationships so that high impact is, in fact,\nrealized. These objectives are reachable only if we all understand the expectations, are frank\nand open about assessing the work we’re asked to do, foster a culture that values quality and\nlearns from both mistakes and successes, follow through (internally and with customers) on\nresource allocations, and pay attention to important relationships. Upper management needs\nto take the lead, but we all need to contribute. Especially with the immediate customer, it’s\noften the project staff that have the frequent connections that influence the customer’s percep­\ntion of our quality and the acceptance of our recommendations.\n\n###### The Successful MITRE Systems Engineer\n\nWhat does successful systems engineering look like at MITRE? What is the secret for­\nmula for it? As noted early in the companion section to this one—The Evolution of Systems\nEngineering—there is no single definition of systems engineering and so there is no single\ndefinition of success. Much depends on the context in which the systems engineering is being\npracticed. Nevertheless, the following high-level criteria strongly correlate with successful\nMITRE systems engineers.\n\nCriteria for Successful MITRE Systems Engineers\n\n_Successful MITRE Systems Engineers:_\n\n###### �[Define the sponsor’s and customer’s problem or opportunity from a comprehensive, ]\n\nintegrated perspective.\n###### �[Apply systems thinking to create strategies, anticipate problems, and provide short- and ]\n\nlong-term solutions.\n###### �[Adapt to change and uncertainty in the project and program environment, and assist ]\n\nthe sponsor, customer, and other stakeholders in adapting to these.\n###### �[Propose a comprehensive, integrated solution or approach that: ]\n\n      - Contributes to achieving the sponsor’s, customer’s and other stakeholders’ strategic\n\nmission objectives in a changing environment.\n\n\n-----\n\n      - Can be feasibly implemented within the sponsor’s and customer’s political, organiza­\n\ntional, operational, economic, and technical context.\n\n      - Addresses interoperability and integration challenges across organizations.\n\n      - Shapes enterprise evolution through innovation.\n###### �[Cultivate partnerships with our sponsors and customers to work in the public interest. ] �[Bring their own and others’ expertise to provide sound, objective evidence and advice ]\n\nthat influences the decisions of our sponsors, customers, and other stakeholders.\nExcerpted from the MITRE Systems Engineering Competency Model [7].\n\n###### References and Resources\n\n1. Office of Management and Budget, November 13, 2009, U.S. Federal Acquisition\n\nRegulation (FAR), 35.017.\n\n2. November 21, 2008, DoD Sponsoring Agreement with The MITRE Corporation to Operate\n\nthe C3I FFRDC.\n\n3. September 25, 2005, Sponsoring Agreement between the FAA and The MITRE\n\nCorporation for the Operation of the CAASD FFRDC.\n\n4. February 7, 2008, Sponsoring Agreement Among the IRS and Department of Veterans\n\nAffairs and The MITRE Corporation Operating the FFRDC formally known as the Center\nfor Enterprise Modernization.\n\n5. March 3, 2009, Sponsoring Agreement between DHS and The MITRE Corporation to\n\nOperate the Homeland Security System Engineering and Development Institute FFRDC.\n\n6. MITRE, May 2008, Systems Engineering Quality at MITRE.\n\n7. [MITRE, September 2007, MITRE Systems Engineering Competency Model, ver. 1.](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n\n\n-----\n\n-----\n\n### Enterprise Engineering\n\n##### MITRE\n\n\n### Enterprise Engineering\n\n##### MITRE\n\n\n-----\n\n###### Introduction\n\nDid you ever wonder if your work needs to be enabled to support an international community?\nHave you anticipated that the security features of your engineering will have to interoperate\nwith other federal agencies or organizations in the same department? Do performance charac­\nteristics of capabilities beyond your control impact the performance of your endeavor?\n\n“Enterprises” are interwoven sets of mission and business endeavors that need to\ncoexist in a rapidly changing and evolving world. MITRE systems engineers (SEs)\nare expected to bring an enterprise perspective to their activities at whatever scale of\nthe enterprise they operate: subsystem, system, system of systems, or enterprise. SEs\nshould take a comprehensive viewpoint across technical and non-technical aspects\nof the problem space, and use systems thinking to ask probing questions and trace\nthe implications of potential answers across the enterprise. SEs work with ambigu­\nous issues and partial information to frame the essence of the problem; create strate­\ngies that consider all aspects of the problems and needs of the customer, sponsor,\nand beyond; and engineer scalable, adaptable, and evolvable enterprise solutions that\nconsider the larger stakeholder community.\n\n###### Background\n\nIn the article “Evolving Systems Engineering,” MITRE staff considered the topic of “enter­\nprise” definition and came up with the following working definition:\n\nBy “enterprise” we mean a network of interdependent people, processes, and sup­\nporting technology not fully under the control of any single entity. In business lit­\nerature, an enterprise frequently refers to an organization, such as a firm or govern­\nment agency; in the computer industry, it refers to any large organization that uses\ncomputers. Our definition emphasizes the interdependency of individual systems and\neven systems of systems. We include firms, government agencies, large informationenabled organizations, and any network of entities coming together to collectively\naccomplish explicit or implicit goals. This includes the integration of previously sepa­\nrate units. The enterprise displays new behaviors that emerge from the interaction of\nthe parts [1].\n\nMITRE works on projects supporting specific customer needs and their required capabili­\nties. To be successful, MITRE staff must also understand the enterprise context associated\nwith these specific activities. Our customers truly value the enterprise perspective we provide.\nMITRE has worked on our customers’ enterprise and specific needs from our inception. With\nthe SAGE [Semi-Automatic Ground Environment] project, we focused early in our history on\nthe needs of the national enterprise for defense and formulated specific radar solutions to\n\n\n-----\n\nimplement the required protection. As MITRE has worked on enterprise challenges over time,\nwe’ve come to realize:\n\nEnterprise engineering is based on the premise that an enterprise is a collection of\nentities that want to succeed and will adapt to do so. The implication of this statement\nis that enterprise engineering processes are more about shaping the space in which\norganizations develop systems so that an organization innovating and operating to\nsucceed in its local mission will—automatically and at the same time—innovate and\noperate in the interest of the enterprise. Enterprise engineering processes are focused\nmore on shaping the environment, incentives, and rules of success in which classi­\ncal engineering takes place. Enterprise engineering coordinates, harmonizes, and\nintegrates the efforts of organizations and individuals through processes informed or\ninspired by natural evolution and economic markets. Enterprise engineering manages\nlargely through interventions instead of controls [2].\n\nMajor topics and considerations for MITRE staff engineering enterprise solutions are:\n\n###### �Taking a comprehensive viewpoint �Enterprise planning and management �Enterprise technology, information, and infrastructure �Addressing the complex issues associated with information-intensive environments �Engineering systems for mission assurance �Transformation planning and organizational change �Understanding the enterprise’s governance operations along with related assumptions\n\nand constraints\n###### �Independent engineering assessments\n\n Comprehensive Viewpoint\n\nA comprehensive viewpoint helps the MITRE engineer create a solution that considers and\naccounts for the many factors associated with an advantageous path across an enterprise and\nthe environment where the enterprise must operate. There are many complexities to assess\nand negotiate as we evaluate a comprehensive perspective of the solution space. MITRE\nengineers can apply a variety of tools to help gain an understanding of the uncertain envi­\nronment that affects their enterprise. Articles in this topic area include “Systems Thinking,”\n“Systems Engineering Strategies for Uncertainty and Complexity,” and “Tools to Enable a\nComprehensive Viewpoint.”\n\n\n-----\n\n###### Enterprise Planning and Management\n\nEnterprise planning and management takes a strategic view of the major plans and pro­\ncesses needed for a federal government organization to achieve its mission. The legislative\nbranch does not often get into details about which components of an executive branch\nagency will execute each aspect of the mission, or how they will operate. Therefore, at the\nstrategic level, each agency must plan, manage, and account for both how and to what\nextent it achieves that mission. MITRE engineers are sometimes asked by sponsors to help\ndevelop and execute these strategic-level plans and processes. Articles in this topic area\ninclude “IT Governance,” “Portfolio Management,” and “How to Develop a Measurement\nCapability.”\n\n###### Enterprise Technology, Information, and Infrastructure\n\nThe term “enterprise technology, information, and infrastructure” refers to the concept\nof information technology (IT) resources and data that are shared across an enterprise.\nEmbodied in this concept are technical efforts such as infrastructure engineering for build­\ning, managing, and evolving shared IT; IT or infrastructure operations for administering\nand monitoring the performance of the IT service being provided to the enterprise; IT ser­\nvices management; and information services management. Articles in this topic area include\n“IT Infrastructure Engineering,” “IT Service Management (ITSM),” “Information and Data\nManagement,” and “Radio Frequency Spectrum Management.”\n\n###### Engineering Information-Intensive Enterprises\n\nMITRE’s role in operating systems engineering Federally Funded Research and Development\nCenters (FFRDCs) places us in an environment where our solutions are predominantly used\nfor information-intensive capabilities. Part of our work program may lead us to hardware or\nplatform considerations for enhancing the capabilities of our customers, but typically the\nemphasis is on the information needs of the missions and decision makers we support. As\nsuch, we need to provide solutions that meet the information needs of our customers:\n\n###### �Solutions that consider the architectures of the enterprise and how to federate the ele­\n\nments to provide integrated capabilities\n###### �Solutions that consider the complexity of the comprehensive viewpoint and formulate\n\napproaches to take advantage of design patterns and agile techniques while planning an\nevolutionary strategy to satisfy the longer term enterprise needs\n###### �Solutions that can be created on-demand for the particular challenge at hand using\n\navailable resources such as open system capabilities while meeting the rapidly changing\nand real-time events of the nation\n\n\n-----\n\nArticles in this topic area include “Architectures Federation,” “Design Patterns,”\n“Composable Capabilities On Demand (CCOD),” “Open Source Software (OSS),” and “Privacy\nSystems Engineering.”\n\n###### Systems Engineering for Mission Assurance\n\nThe concept of engineering a system that can withstand purposeful or accidental failure\nor environmental changes has a long history in the discipline of designing systems for\nsurvivability. In the Internet era, engineering systems for mission assurance has been\nfurther expanded to include engineering for information assurance and cyber security.\nIn this guide, the definition of “systems engineering for mission assurance” is the art of\nengineering systems with options and alternatives to accomplish a mission under different\ncircumstances and the capability to assess, understand, and balance the associated risks.\nOptions and alternatives will normally take the form of a blend of technical and operational\nelements, which requires the systems engineer to have an intimate understanding of the\ntechnical details and limitations of the system, the doctrine and operations for its use, and\nthe environmental conditions and threats that will or may be encountered. Taken together,\nthe various dimensions of mission assurance pose some of the most difficult challenges in\nengineering systems today. The systems engineering community does not yet have complete\nanswers to its myriad questions.\n\nThe articles in this topic are focused on what we know about systems engineering for\nmission assurance today. It is a rapidly evolving field, so check back often for updates and\nadditional material. Articles in this topic area include “Cyber Mission Assurance,” “Crown\nJewels Analysis (CJA),” “Cyber Threat Susceptibility Assessment,” “Cyber Risk Remediation\nAnalysis,” “Secure Code Review,” and “Supply Chain Risk Management.”\n\n###### Transformation Planning and Organizational Change\n\nTransformational planning and organizational change is the coordinated management of\nchange activities that enable users to adopt a new vision, mission, or system. MITRE sys­\ntems engineers assist in formulating a strategy and plans, and in leading and communicat­\ning change. Articles in this topic area include “Performing Organizational Assessments,”\n“Formulation of Organizational Transformation Strategies,” “Stakeholder Assessment and\nManagement,” “Effective Communication and Influence,” and “Planning for Successful User\nAdoption.”\n\n\n-----\n\n###### Enterprise Governance\n\nMITRE engineers need to understand the mechanisms used by the government to “govern”\nsystems engineering and the capabilities required to accomplish the tasks of the enterprise.\n\nGovernance is the activity of governing. It relates to decisions that define expecta­\ntions, grant power, or verify performance ... governance relates to consistent man­\nagement, cohesive policies, processes and decision-rights for a given area of respon­\nsibility [3].\n\nIT Governance primarily deals with connections between business focus and IT\nmanagement. The goal of clear governance is to assure the investment in IT general\nbusiness value and mitigate the risks that are associated with IT projects [4].\n\nGovernance engineering requires MITRE staff to work on the social engineering\nand social networking aspects of systems engineering by using, and sometimes working\naround, the governance structures. Governance in this area is defined as where the inter­\ndependent people, processes, and technology come together to accomplish the required\nactions to implement the needs of and evolve the enterprise.\n\nArticles in this topic area include “Communities of Interest and/or Community of\nPractice,” “Standards Boards and Bodies,” and “Policy Analysis.”\n\n###### MITRE FFRDC Independent Assessments\n\nMITRE systems engineers perform many types of independent assessments, which are\nknown by various names including independent reviews, red teams, appraisals, audits, and\ncompliance assessments. Very often independent assessments are done to identify risks to\na program. They provide value to government organizations because the MITRE FFRDC\nrole promotes independence, objectivity, freedom from conflicts of interest, and technical\nexpertise. Related to Contractor Evaluation, this topic area includes the article “Planning\nand Managing Independent Assessments.”\n\n###### Other Enterprise Engineering Articles\n\nIn the future, any articles on subjects of relevance to enterprise engineering but that don’t\nneatly fit under one of the section’s existing topics will be added in a separate topic, Other\nEnterprise Engineering Articles. Such articles are likely to arise because the subject matter\nis at the edge of our understanding of systems engineering, represents some of the most\ndifficult problems MITRE systems engineers work on, and has not yet formed a sufficient\ncritical mass to constitute a separate topic.\n\n\n-----\n\n###### References and Resources\n\n[1. The MITRE Corporation, 2011, Evolving Systems Engineering.](http://www.mitre.org/news/pdfs/mitre_ese.pdf)\n\n[2. Rebovich, G., March 2007, Engineering the Enterprise.](http://www.mitre.org/work/tech_papers/tech_papers_07/07_0434/)\n\n[3. Wikipedia contributors, “Governance,” Wikipedia, accessed January 27, 2010.](http://en.wikipedia.org/wiki/Governance)\n\n[4. Smallwood, D., March 2009, “IT Governance: A Simple Model,” ebiz.](http://www.ebizq.net/blogs/insurance/2009/02/it_governance_a_simple_model.php)\n\n\n-----\n\n###### Enterprise Engineering Contents\n\nComprehensive Viewpoint 27\nSystems Thinking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nSystems Engineering Strategies for Uncertainty and Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\nTools to Enable a Comprehensive Viewpoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n\nEnterprise Planning and Management 54\nIT Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\nPortfolio Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\nHow to Develop a Measurement Capability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n\nEnterprise Technology, Information, and Infrastructure 82\nIT Infrastructure Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\nIT Service Management (ITSM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\nInformation and Data Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\nRadio Frequency Spectrum Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n\nEngineering Information-Intensive Enterprises 113\nArchitectures Federation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\nDesign Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\nComposable Capabilities On Demand (CCOD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\nOpen Source Software (OSS) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\nPrivacy Systems Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n\nSystems Engineering for Mission Assurance 155\nCyber Mission Assurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\nCrown Jewels Analysis (CJA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\nCyber Threat Susceptibility Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\nCyber Risk Remediation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\nSecure Code Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\nSupply Chain Risk Management (SCRM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197\n\nTransformation Planning and Organizational Change 201\nPerforming Organizational Assessments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\nFormulation of Organizational Transformation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\nStakeholder Assessment and Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\nEffective Communication and Influence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\nPlanning for Successful User Adoption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n\nEnterprise Governance 238\nCommunities of Interest (COI) and/or Community of Practice (COP) . . . . . . . . . . . . . . . . . . . . . . . . 242\nStandards Boards and Bodies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248\nPolicy Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252\n\nMITRE FFRDC Independent Assessments 257\nPlanning and Managing Independent Assessments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260\n\n\n-----\n\n##### Comprehensive Viewpoint\n\nDefinition: A broad understanding of the context and environment in which the\n\n_systems engineering activity or problem is embedded and to be solved. A com­_\n\n_prehensive viewpoint enables the ability to develop solutions that consider all_\n\n_aspects of a problem, their relationships and interactions, including current and_\n\n_future needs of the user, customer, and sponsor as well as political, organizational,_\n\n_economic, operational, and technical issues._\n\nKeywords: agility, complexity, domain, enterprise, systems, systems thinking, tools,\n\n_users_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to develop a broad\n\nunderstanding of their problem context and environment. They should\n\nconsider current and future needs of the sponsor, customer, and opera­\n\ntional user, and take into account political, organizational, economic,\n\noperational, and technical aspects of the problem and its potential\n\nsolutions. They are expected to use this comprehensive view to develop,\n\nrecommend, and lead systems engineering activities in the enterprise. In\n\ndoing so, MITRE SEs consider:\n\n###### �Operational needs and the changing global environment that the\n\nnation and our operational users must work within, including the col­\n\nlection of systems with which our individual projects interact\n\n\n-----\n\n###### �Technical environment, its rapid evolution and how it influences feasible implementa­\n\ntion approaches\n###### �Economic constraints and processes that influence solutions and their implementation �Agendas and perspectives of the stakeholder community (in the customer chain and\n\nacross the mission and domain areas)\n###### �International partners and the policies that govern how we work in the international\n\ncommunity\n###### �Data and information needs, processing, security, and applications that are required to\n\nget results.\n\n###### Comprehensive Viewpoint: The Sponsors’ Requirement\n\nAs a corporation that operates federally funded research and development centers (FFRDCs),\nMITRE is required to take a comprehensive viewpoint of all of our work. This requirement is\nspecifically delineated in the individual FFRDC sponsoring agreements, as shown in the fol­\nlowing excerpts:\n\n###### �Department of Defense (DoD) Command, Control, Communications and Intelligence\n\n**(C3I) FFRDC Sponsoring Agreement: “While serving the immediate needs of the many**\nindividual programs it supports, the C3I FFRDC aligns its work program to assist in\nachieving integrated enterprise capabilities [1, p. 3].”\n###### �Federal Aviation Administration (FAA) Sponsoring Agreement: “CAASD [Center for\n\nAdvanced Aviation System Development] is ... to solve problems that are too broad and\ntoo complex to ... stand alone but are so linked to others that a highly specific analysis\nmay be misleading [2, p. 5].”\n###### �Center for Enterprise Modernization (CEM) Sponsoring Agreement: “... simultane­\n\nously direct its efforts to the support of individual programs and projects for enterprise\nmodernization, assuring that these individual programs and projects operate effectively\nwith one another and efficiently support the ... objectives of the Government [3, p. 3].”\n###### �Homeland Security Systems Engineering and Development Institute [SEDI]\n\n**Sponsoring Agreement: “... shall promote compatibilities across the various homeland**\nsecurity platforms and equipment ... through, among other things, improved interoper­\nability and information sharing within the homeland security enterprise [4, p. 2].”\n\n###### Comprehensive Viewpoint: Leveraging the Corporation\n\nMITRE’s sponsoring agreements not only direct us to take a comprehensive viewpoint across\nthe sponsor’s enterprise, but extend across all of our FFRDCs to ensure we are formulating\nnational solutions to hard problems. The following excerpts from the CEM sponsoring agree­\nment illustrate this. The other sponsoring agreements contain similar language.\n\n\n-----\n\n“... ensure that the FFRDC’s work programs can be accomplished in a complementary\nmanner that draws on the entire range of corporate competencies [3, pp. 3-4].”\n\n“[MITRE’s several FFRDCs] ... are operated in such a way as to enhance the technical\nquality and objectivity of each [3, p. 3].”\n\nWithin MITRE, we often refer to this requirement as “bringing the corporation to bear.”\nIt is for this reason that MITRE emphasizes collaboration and networking across the corpora­\ntion. More recently, we have extended this concept to “bringing the world to bear,” by which\nwe emphasize collaboration beyond our corporate boundaries to wherever the greatest exper­\ntise to solve a problem resides—other FFRDCs, academia, industry, and international partners.\n\n###### Articles Under This Topic\n\nThe article “Systems Thinking” provides a general introduction to the art and practice of\nexamining the totality of a problem, including the environment in which the problem is\ncontained, as well as the linkages and interactions among the problem’s parts. Systems think­\ning is used in problems in which cause and effect are not closely related in space or time, as\nwell as problems in which the relationships among elements are nonlinear. Systems thinking\nenables alignment of purposes, which is so important to successful engineering of enterprise\ncapabilities because it enables the systems engineer to ask purposeful questions and trace the\nimplications of potential answers across their enterprise.\n\nIncreasingly, the complexity we encounter in the enterprises and systems that MITRE\nhelps engineer requires a spectrum of systems engineering techniques. When a system\nis bounded with relatively static, well-understood requirements, the classical methods of\nsystems engineering are applicable and powerful. At the other end of the spectrum, when\nsystems are networked and each is individually reacting to technology and mission changes,\nthe environment for any given system becomes essentially unpredictable. The article “Systems\nEngineering Strategies for Uncertainty and Complexity” discusses the nature and sources of\nuncertainty in engineering IT-intensive, networked systems and suggests strategies for manag­\ning and mitigating their effects.\n\nThere are a variety of cognitive tools to help apply a systems thinking perspective\nto the increasingly complex problems MITRE encounters. The article “Tools to Enable\na Comprehensive Viewpoint” describes a set of tools to help MITRE systems engineers\nunderstand and characterize the nature and source of uncertainty and complexity in their\nenvironment.\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nLook for opportunities to contribute to solving the\n\nbroader integration and interoperability challenges\n\nacross your enterprise at the same time you solve\n\nyour particular project’s problems.\n\nAs you do your day-to-day work, keep your head\n\nup to understand where and how your particular\n\nactivity fits into the larger context.\n\nUnderstand MITRE’s systems engineering quality\n\nconstruct [5], and use it to guide the execution of\n\nyour work activities.\n\n###### References and Resources\n\n\nRecognize and act on the understanding that\n\na locally optimal solution for a problem may be\n\nsuboptimal for the enterprise and less advanta­\n\ngeous overall than other solutions. For example,\n\nworking a data strategy across a broader com­\n\nmunity may preclude a more elegant solution to\n\na particular system application, but the increased\n\nvalue of data sharing and interoperability across\n\nthe broader community outweighs the benefits of\n\na program-centric solution.\n\n\n1. November 21, 2008, DoD Sponsoring Agreement with The MITRE Corporation to Operate\n\nthe C3I FFRDC.\n\n2. December 22, 2010, Sponsoring Agreement Between the FAA and The MITRE Corporation\n\nfor the Operation of the CAASD FFRDC.\n\n3. February 7, 2008, Sponsoring Agreement Among the IRS and Department of Veterans\n\nAffairs and The MITRE Corporation Operating the FFRDC formally known as the Center\nfor Enterprise Modernization.\n\n4. March 3, 2009, Sponsoring Agreement between DHS and The MITRE Corporation to\n\nOperate the Homeland Security System Engineering and Development Institute FFRDC.\n\n5. Metzger, L., May 2009, Systems Quality at MITRE, The MITRE Corporation.\n\n###### Additional References and Resources\n\n“Comprehensive Viewpoints,” MITRE Systems Engineering Competency Model, The MITRE\nCorporation.\n\n“FFRDC Role and Public Interest,” MITRE Project Leadership Handbook, The MITRE\nCorporation.\n\n\n-----\n\nDefinition: The ability and\n\n_practice of examining the whole_\n\n_rather than focusing on isolated_\n\n_problems (P. Senge) [1]. The_\n\n_act of taking into account the_\n\n_interactions and relationships_\n\n_of a system with its containing_\n\n_environment (Y. Bar Yam, New_\n\n_England Complex Systems_\n\n_Institute)._\n\nKeywords: holism, holistic, inter­\n\n_actions, multidimensionality,_\n\n_multiple perspectives, relation­_\n\n_ships, synthesis, synthetic, sys­_\n\n_tem thinking, systems thinking_\n\n\nCOMPREHENSIVE VIEWPOINT\n###### Systems Thinking\n\n**MITRE SE Roles and Expectations:**\n\nMITRE systems engineers are expected to:\n\n(a) understand the linkages and interactions\n\namong the elements of their system or enter­\n\nprise and its connecting entities; (b) align goals\n\nand purposes across the enterprise; and (c) ask\n\nprobing questions and trace the implications\n\nof potential answers across the enterprise.\n\n\n-----\n\n###### Background\n\nThe recently renewed interest in systems thinking in government circles and the engineering\ncommunity has been fueled, in part, by the movement to apply systems science and complex­\nity theory to problems of large-scale, heterogeneous, information technology-based systems.\n\nSystems thinking is a framework for solving problems based on the premise that a com­\nponent part of an entity can best be understood in the context of its relationships with other\ncomponents of the entity, rather than in isolation. The way to fully understand why a problem\noccurs and persists is to understand the “part” in relation to the “whole.” A focus of systems\nthinking is on understanding the linkages and interactions among the elements that compose\nthe entirety. Systems thinking is often used in problems in which cause and effect are not\nclosely related in space or time, as well as problems in which the relationships among compo­\nnents are nonlinear (also see the SEG article “Systems Engineering Strategies for Uncertainty\nand Complexity”).\n\nSystems thinking requires knowledge and understanding—both analysis and synthesis—\nrepresented in the same view. The ability to combine analytic and synthetic perspectives in a\nsingle view enables alignment of purposes, which is so important to successful engineering\nof enterprise capabilities. It allows the systems engineer to ask purposeful questions and trace\nthe implications of potential answers across the enterprise. Would a change in performance at\nthe subsystem level result in a change at the enterprise level? If so, how, and is it important?\nHow would a new enterprise-level need be met?\n\nThe following concepts are important in applying systems thinking:\n\n###### �Analysis: The ability to decompose an entity into deterministic components, explain each\n\ncomponent separately, and aggregate the component behaviors to explain the whole. If\nthe entity is a system, then analysis answers the question, “How does the system work?”\nAnalysis results in knowledge of an entity; it reveals internal structure. For example, to\nknow how an automobile works, you analyze it—that is, you take it apart and determine\nwhat each part does. This is essential to important activities like repairing automobiles or\ndiagnosing and repairing problems of other, more complicated systems.\n###### �Synthesis: The ability to identify the whole of which a system is a part, explain the\n\nbehavior or properties of the whole, and disaggregate the whole to identify the role or\nfunction of the system in the whole. Synthesis answers the “Why is it what it is?” ques­\ntion. Synthesis is the mode of thought that results in the understanding of an entity (i.e.,\nan appreciation of the role or function an entity plays in the larger system of which it is\na part). For example, the reason why the American automobile was originally designed\nfor six passengers is because the average family size at the time was 5.6. Every MITRE\nsystems engineer who has defined a system performance specification against mission\nor operational requirements has used synthetic thinking.\n\n\n-----\n\n###### Example\n\nTo analyze an unmanned aerial vehicle (UAV), we would logically decompose it into sub­\nsystems that perform certain roles or functions (e.g., the host platform, sensors that perform\nground target detection and identification, communication systems for receiving commands\nfrom controlling entities, transmitting onboard sensor data to control systems, etc.) down to a\nlevel of granularity sufficient to answer the question at hand.\n\nTo understand the UAV system synthetically, the first step is to identify the larger system\nof which the UAV is a part, (e.g., a situational awareness [SA] system of systems [SoS]). The\nsecond step is to describe the containing system (e.g., the SA SoS delivers high-quality loca­\ntion and identification information on militarily significant objects of interest in a surveillance\nvolume, with an emphasis on ground, sea surface, and low-altitude air vehicles). The third\nstep is to disaggregate the whole to identify the role or function in the larger system of which\nthe UAV is a part (e.g., the organization employing the SA SoS has a mission that focuses\non the detection, location, and identification of ground and sea surface vehicles. The UAV in\nquestion is equipped with sensors and processing tailored to ground vehicle detection.). Taken\ntogether, this synthetic view explains why the organization has ground vehicle detection UAVs\nin its SA SoS and provides a basis for asking and answering “what if?” questions about the\nUAV, like: “What if the organization’s mission shifted away from ground vehicle detection or\nmoved more toward it?”\n\nCombining the analytical and synthetic perspectives in a single view allows the systems\nengineer to ask questions and draw implications of potential answers across the enterprise. If\nthe organization’s mission shifted to ultra-light airborne vehicle detection, how would SA be\naccomplished? Could the existing UAVs be re-engineered or refitted with new sensors to detect\nand identify the new target types? Would a change in performance at the UAV system level\nresult in a change at the SA SoS or mission level? If so, how, and is it important?\n\n###### Government Interest and Use\n\nThe need to apply systems thinking continues to be pervasive across MITRE. It is expected of\nMITRE by our sponsors. Reference to it is made in our sponsoring agreements:\n\n###### �From the FAA Sponsoring Agreement: “CAASD is uniquely qualified...to solve problems\n\nthat are too broad and too complex to become the focus of a competitive procurement...”\n###### �From the DoD Sponsoring Agreement: “While serving the immediate needs of the\n\nmany individual programs it supports, the C3I FFRDC aligns its work program to assist\nin achieving integrated DoD-wide enterprise capabilities...”\n###### �From the IRS Sponsoring Agreement: “The FFRDC shall simultaneously direct its\n\nefforts to the support of individual programs and projects for tax modernization, and to\n\n\n-----\n\nassuring that these individual programs and projects operate effectively with each other\nand efficiently support the business objectives of the government...”\n\n###### Systems Thinking Best Practices\n\n\nSystems engineering and systems thinking have\n\nalways been about asking good questions and\n\nforming conclusions and recommendations\n\nbased on the answers. When performing your\n\nMITRE systems engineering activities, consider\n\nasking these sorts of questions:\n\nWhat is my enterprise? What elements of it do\n\nI control? What elements do I influence? What\n\nare the elements of my environment that I do not\n\ncontrol or influence but which influence me? [2,\n\npp. 2-3 – 2-4]\n\nCan a balance be achieved between optimizing\n\nat the system level and enabling the broader\n\nenterprise? If the balance comes at the expense\n\nof the smaller system, can that be offset or miti­\n\ngated? How?\n\nIs interdependence of performance measures\n\n(variables) in a system or enterprise hidden by\n\n###### References and Resources\n\n\nslack? Is the inability to make progress in one\n\nmeasure, except at the expense of others, an indi­\n\ncation that the slack among them has been used\n\nup? Can a redesign of the system or enterprise\n\nremove interdependence or provide additional\n\nslack? [2, pp. 4-9 – 4-10]\n\nHow can analytic and synthetic perspectives\n\nbe combined in a single view to enable align­\n\nment of purposes across the enterprise? Would\n\na change in performance at the subsystem level\n\nresult in a change at the enterprise level? If so,\n\nhow, and is it important? How would a new enter­\n\nprise-level requirement be met and how would it\n\ninfluence its constituent systems?\n\nCan the solution space of a seemingly intrac­\n\ntable problem be expanded by viewing it in its\n\ncontaining whole? How? [2, pp. 4-3 – 4-4]\n\n\n1. Senge, P., et al., 1994, The Fifth Discipline Fieldbook, New York, NY, Doubleday.\n\n[2. Rebovich, G., 2005. Systems Thinking for the Enterprise: New and Emerging Perspectives,](http://www.mitre.org/work/tech_papers/tech_papers_06/05_1483/05_1483.pdf)\n\nThe MITRE Corporation.\n\n###### Additional References and Resources\n\nAckoff, R., November 1993, “From Mechanistic to Social Systemic Thinking,” System\nThinking in Action Conference.\n\nAxelrod, R., and M. D. Cohen, 2000, Harnessing Complexity: Organizational Implications of a\n_Scientific Frontier, New York, NY, Basic Books._\n\n\n-----\n\nGharajedaghi, J., 1999, Systems Thinking: Managing Chaos and Complexity. Boston, MA,\nButterworth Heinemann.\n\nRebovich, G., 2006, “Systems Thinking for the Enterprise: A Thought Piece,” International\nConference on Complex Systems, Bedford, MA, The MITRE Corporation.\n\n[Rebovich, G., 2006, “Systems Thinking for the Enterprise: New and Emerging Perspectives,”](http://www.mitre.org/work/tech_papers/tech_papers_06/06_0391/06_0391.pdf)\n_Proceedings of 2006 IEEE International Conference on Systems of Systems._\n\n\n-----\n\nDefinitions: External uncer­\n\n_tainty includes changes in the_\n\n_market, the operating environ­_\n\n_ments, business processes, and_\n\n_threats. Internal uncertainties_\n\n_include program/project execu­_\n\n_tion as well as design, imple­_\n\n_mentation, and performance_\n\n_challenges [1]. Complexity is the_\n\n_interactions and interdepen­_\n\n_dencies among people, orga­_\n\n_nizations, technologies, tools,_\n\n_techniques, procedures, and_\n\n_economics that create patterns_\n\n_that transcend the goals of any_\n\n_one group. Complex interac­_\n\n_tions can result in resilience and_\n\n_robustness but also in cascad­_\n\n_ing failures [2, 3]._\n\nKeywords: adaptability, agility,\n\n_complex systems, complexity,_\n\n_ecosystem, emergent behav­_\n\n_ior, fitness, flows, interactions,_\n\n_interdependency, robustness,_\n\n_selection, uncertainty, variety_\n\n\nCOMPREHENSIVE VIEWPOINT\n###### Systems Engineering Strategies for Uncertainty and Complexity\n\n**MITRE SE Roles and Expectations:**\n\nMITRE ­systems engineers are expected\n\nto understand the nature and sources of\n\nuncertainty, lack of effective control [4],\n\nand complexity [5] in their environment and\n\nthen select and apply appropriate strategies\n\nfor managing or mitigating their effects.\n\n\n-----\n\n###### Introduction\n\nThe complexity we are seeing in the enterprises and systems that MITRE helps engineer\nrequires a spectrum of systems engineering techniques. When a system is bounded with\nrelatively static, well-understood requirements, the classical methods of systems engineering\nare applicable and powerful. At the other end of the spectrum, when systems are networked\nand each is individually reacting to technology and mission changes, the environment for any\ngiven system becomes essentially unpredictable.\n\nThe metaphor of the watchmaker and gardener is sometimes used to describe the differ­\nences between engineering in the two types of environments [6]. Classical systems engineer­\ning is like watchmaking. Its processes, techniques, and tools are applicable to difficult prob­\nlems that are essentially deterministic or reductionist in nature. Like gardening, engineering\nfor the enterprise draws on the fundamental principles of evolution, ecology, and adaptation.\nIt uses techniques to increase the likelihood of desirable or favorable outcomes in complex\nenvironments that are characterized by uncertainty and that may change in unpredictable\nways. Engineering for the enterprise is not a replacement for classical systems engineering.\nIncreasingly, both disciplines must be used in combination to achieve success.\n\nThis article begins with a discussion of ecosystems and includes a large number of foot­\nnotes and references for the interested reader. This will strike some as an odd place to start.\nBut in many ways the point of view required to understand ecology is analogous to the one\nneeded to comprehend the complex environment in which MITRE systems engineers find\nthemselves today. In fact, a number of the emerging best practices and lessons learned dis­\ncussed later in this article draw on ecology or evolutionary biology for their inspiration. Last,\nthe best practices and lessons learned are organized around important conundrums, needs, or\nissues that systems engineers face in complex environments.\n\nBecause engineering in complex environments is an emerging and rapidly changing field,\nMITRE systems engineers and others are developing its processes, techniques, and tools as\nthey execute their program responsibilities. As a result, in many cases, there is an inherent\nuncertainty about the right wisdom to recommend. But pointing them out has value even if\nwe don’t yet know exactly the right wisdom to convey about solving them. When it has been\npossible to suggest at least potential approaches to dealing with a problem, the article does so.\n\n###### Background\n\nPeople exist within an ecosystem. We have a sense of what that means and understand the\nnatural world around us as elements in an ecosystem. Our technical systems exist within\necosystems as well. We need to unwrap what it means to be systems engineers within an\necosystem; and, thus, understand the nature and sources of uncertainty, lack of control, and\ncomplexity in our environment [7].\n\n\n-----\n\nMost people have a keen sense of what a natural ecosystem consists of, and how it\nmorphs over time in response to changes of (and to) its constituent members. Ecosystems are\ndynamic. Their aggregate state is arrived at through various interactions among the elements\npresent (some connected directly, others indirectly through some transitive path), and how\nthe elements push and pull all the time along many dimensions. Any apparent stability is a\ndynamic stability which, when one of the interacting elements is altered, changes the stabil­\nity point of the aggregate—and the changes ripple through the connected pieces (sometimes\nrather rapidly—and unexpectedly) until another dynamic stability point is found.\n\nEcosystems are distributed and also have no leader; no one’s “in charge.” This is nothing\nthat needs to be “fixed”—in fact, it can’t be fixed [8].\n\nAll of the systems we work on at MITRE have always existed in this type of ecosystem\nand have been subject to this type of push-and-pull. But three things have changed:\n\n1. In the past we have used people as the “impedance matching” element between the\n\nartificial elements (traditionally, fully formed, and conceived systems); now artificial\nelements are connecting to other artificial elements (machine to machine).\n2. The wide potential interconnections we now accept (and demand) among the artificial\n\nelements (composition on demand [9]).\n3. The engineering we are now expected to perform at large scopes and scales (enterprise\n\nengineering).\nWe now find our systems to be primary elements of the ecosystems in which they reside,\nrather than augmentations to the primary elements (i.e., the people using them), and we must\nfactor that into our requirements, analyses, and designs [10]. They must be able to respond to\nchanges in the context they find themselves within, rather than relying on people to be the\nelements that change in response to context changes (i.e., the environment).\n\nNote also that this environment is changing at rapid and unpredictable rates, and in\nplaces we didn’t necessarily predict. The technology itself is also changing at unprecedented\nrates. Thus we are finding that agility is most desired. The systems themselves must be agile;\nnot just the users of the systems [11, 12]. Most important, isolation (or attempted isolation)\ndoesn’t work.\n\nHaving made the argument for variety and interaction, it is important to add the guiding\nfactor: selection. Arbitrary and random change merely leads to chaos. However, the environ­\nment guides or channels change by selecting the winners and the losers among those present.\nThose chosen are said to be “more fit” for the environment. Thus fitness, and its measure­\nment, might be something to pursue [13].\n\nGiven multiple interdependencies, rippling change, an unknown (and possibly unknow­\nable) future, and selection among choices, then, clearly, we can expect uncertainty and there­\nfore agility is a top need. But agility of what?\n\n\n-----\n\n###### �Agility of the aggregate: “Systems” and “systems of systems” are nothing more than\n\ncollections of technical elements, people, and processes that perform useful work. It is\nin the aggregate where this useful work is realized. To be agile, the aggregates must be\nable to adapt (and possibly, to be assembled) near the point of need, and in a timeframe\nthat allows the potential solution (the aggregate) to be applied to the need in a timely\nway.\n###### �Agility of the elements: Each element within an aggregate must itself be able to evolve.\n\nThe rate of its evolution—or its ability to change rapidly with the emergence of a new\nneed—may well define its value to the enterprise. Thus adaptability becomes a strong\ndesign aspect.\nIt is within this environment, and with these challenges, that MITRE systems engineers\nare expected to perform. One needs to have this understanding and mindset. It is within this\nmindset that we can see users as arbiters of “fitness” and the greater stakeholder community\nas the environment [14].\n\n###### Government Interest and Use\n\nThe government has a direct interest in seeing that systems built are agile and composable\nin order to meet the changing ecosystem in which our government customers live. Examples\nof capabilities MITRE has built this way are in the SEG article, “Special Considerations for\nConditions of Uncertainty: Prototyping and Experimentation.” Being agile and composable\nsatisfies the ability to change quickly as conditions, technologies, missions, and proce­\ndures change. It also suggests that we may be able to achieve more (re)usability and thus\nmore effectively manage cost. Uncertainty becomes less of a problem if agility is possible.\nIt allows rapid reaction to current conditions rather than prediction of future conditions\nfollowed by subsequent reaction/change. Best practices and lessons learned fall along these\nlines [15, 16, 17, 18, 19].\n\n###### Best Practices and Lessons Learned\n\n\nTechnology\n\nGiven an unknown future, MITRE systems engi­\n\nneers are expected to consider and recommend\n\nthe value of building options into designs [20].\n\nThey are expected to envision possible system or\n\nenterprise extensions in advance, the likelihood\n\nof whether and when they would be needed, and\n\n\nthe cost of extending the design versus creating a\n\nreplacement.\n\nPartition design by both functionality and time\n\ndifferences of change. Traditional design tends\n\nto partition primarily by function. However, parti­\n\ntioning also by rate of change allows us to isolate\n\nelements that change quickly (or might change\n\n\n-----\n\nquickly) from those elements that are more stable\n\nand will change slowly [21].\n\nEncapsulate change. A basic tenet of design that\n\nhas weathered the test of time is to isolate things\n\nthat change. Key to this is the use of interfaces\n\nas a method to isolate the partitions from each\n\nother, yet allow interaction.\n\nCarefully choose “bow ties” [22]. In the design,\n\nidentify and codify those key decoupling points\n\nthat divide designs into coherent layers. These\n\nshould be small in number and ruthlessly\n\nenforced. It is the essence of workable architec­\n\ntures. A small number of connection/decoupling\n\npoints of very low variety (i.e., goal of one) allows\n\nhigh variety on each side of these strategic points.\n\nThe key decoupling points should use well-known\n\nand popular protocols and methods to ensure\n\nthey have “legs.”\n\nBuilding an enterprise element while building\n\na local system. Understand your offering to the\n\nenterprise:\n\n###### � [What does it do (the single thing that pro­]\n\nvides value to the enterprise)?\n###### � [How do others interact with it? ] � [Where/how is it available? ] � [How do others find it? ]\n\nRefactoring for the enterprise. Once local ele­\n\nments are discovered and used by the enterprise\n\n(i.e., by consumers outside of those originally\n\nanticipated by the program originators), refac­\n\ntoring their appearance and presentation to the\n\nenterprise is likely warranted. This could mean:\n\n###### � [Splitting a system into two or more (allow­]\n\ning each part to change at its own rate, or\n\n\npermitting access and interaction to only a\n\npiece of the original whole).\n###### � [Substituting one element for another ]\n\n(allowing a new element to perform a\n\nrole previously provided by another). This\n\nallows evolution and change and is the\n\nfundamental idea behind interface imple­\n\nmenter substitution.\n###### � [Augmenting a system with new elements ]\n\n(adding on new elements may allow new\n\nroles for the system).\n###### � [Inverting element dependencies to alter ]\n\nbusiness/political considerations (consider\n\nthe different political/business dynamics\n\nresulting from using a design pattern such\n\nas subclass/inheritance vice containment/\n\ndelegation).\n\nThe actions in the previous bullets have been\n\nargued to be design primitives [23].\n\nFlows [24] and their emergence. Information\n\nflows are the essence of command and control\n\nsystems. Often we used defined flows in the past\n\nwithin our designs to decide what elements in\n\na system need to connect together to realize a\n\nsystem’s behavior. To achieve agility, however, we\n\nneed to create designs that allow technical ele­\n\nments to join and leave existing flows dynamically,\n\nand which will enable the creation of new flows.\n\nStructure and Organization\n\nMITRE systems engineers are expected to con­\n\nsider, recommend, and apply systems engineering\n\nstrategies such as early prototyping, exploratory\n\nintegration test-beds, field trials, and experi­\n\nments to support early and continuous discovery\n\n\n-----\n\nactivities in situations in which the required\n\nbehavior of the deployed system(s) is difficult to\n\npredict.\n\n###### � [Development networks.][ Mimic the real ]\n\nworld as much as possible.\n\nyy Providing vetted access to online\navailable software services that are also\nfound in the fielded system allows third\nparties to learn about and use aspects\nof the system-of-record that would\notherwise need to be guessed at.\n\nyy Third-party developers who use the\n\nresources available on the development\nnetwork will require less integration,\nhand-holding, and rework, thus speed­\ning fielding and holding costs under\ncontrol.\n###### � [Developmental spirals.][ Because the ]\n\nfuture is difficult to predict, using spi­\n\nrals (smaller scope, shorter duration) to\n\nsharpen the focus on future requirements\n\nlowers uncertainty and risk.\n###### � [Modeling and simulation.][ People are ]\n\npoor at predicting patterns formed from\n\nthe interactions of elements (e.g., rules,\n\ncomputing artifacts, etc.). The only way\n\nthat we may fairly, and without introducing\n\nadditional bias, elicit patterns (other than\n\nthe choices and assumptions that go into\n\na model, which should be explicit) is to use\n\nmodeling and simulation to explore the\n\ninteractions (be they operational, techni­\n\ncal, or systemic).\n\nPiloting integration strategies. MITRE systems\n\nengineers are expected to consider, recom­\n\nmend, and implement integration strategy pilots\n\nto explore terminology, operational patterns,\n\n\ntechnology, and desired features when interop­\n\nerating systems cross multiple seams and lack a\n\nhistory of effectively working together.\n\nUsing “technical intimacy”—from casual relations\n\nto deep commitment, we are most likely to use\n\n(and depend on) an external element when it:\n\n###### � [Already exists. ] � [Is available. ] � [Is likely to remain available. ] � [Is understandable. ] � [Makes small demands on our resources. ] � [Requires few unique agreements. ] � [Appears to be part of the environment. ]\n\nReplaceability vs. reusability. Focus on designs\n\nthat offer the ability to replace elements with\n\nother (similar) elements as experience is gained\n\nwith a system, and/or as requirements change,\n\nrather than seeking or designing elements that\n\npurport to include all future needs. We can start\n\nwith small sets of known functionality, then grow\n\nit. This lowers risk greatly.\n\nPartnerships build trust [25]. Forming partner­\n\nships among both consumers and producers of\n\nservices builds trust. Activity taking place on a\n\ndevelopment network can provide pointers to\n\npotential partnering opportunities that may not\n\nhave been obvious.\n\nBusiness and Economic [26, 27, 28]\n\nReduce uncertainty [29]. MITRE systems engi­\n\nneers are expected to understand the elements\n\nthat may drive uncertainty in the tasks they’re\n\nsupporting. Uncertainty may come from require­\n\nments and/or technologies and MITRE engineers\n\n\n-----\n\nmust help customers understand this environ­\n\nment and help mitigate the uncertainty.\n\n###### � [Where a project is stable in both require­]\n\nments and technologies, we are able to\n\nplan ahead, then execute the plan.\n###### � [Where the project is dominated by new ]\n\nor emerging technologies, we should\n\nconsider a strategy of a “portfolio of small\n\nbets.”\n###### � [Where a project is dominated by evolving ]\n\nrequirements, we should consider a strat­\n\negy of “staged commitments.”\n###### � [Where all characteristics exist, we need a ]\n\nhybrid strategy.\n\nReduce uncertainty in cost estimation. MITRE\n\nsystems engineers are expected to understand\n\nthe principles underlying good cost estima­\n\ntion and be able to recommend and implement\n\ntechniques to mitigate cost uncertainty, to include\n\ndeveloping design alternatives as bases for cost.\n\nMITRE’s CASA organization has many methods\n\nto help MITRE engineers with cost estimating and\n\nassociated decision analysis.\n\nThere are two “truths” in conflict. We need to\n\nknow what to build before building it, and things\n\nalways change. Thus the idea that requirements\n\nmust be known before building is desired, but the\n\nrequirements themselves may be changing; so, if\n\nthings always change, knowing what to build may\n\nbe fuzzy. But “what to build” needs to be known to\n\nestimate well.\n\nIf it’s fuzzy, tighten it up, either in time or scope.\n\nCan we define what will be done this year? This\n\nmonth? This week? Find a time slice where this\n\nis clear, outcomes are definite, and the method\n\n\nto achieve them is known. Where things become\n\nfuzzy, this may well be a point where there’s a\n\nlogical branching of possibilities, and a perfect\n\nopportunity for “Real Options” [30] to be devel­\n\noped. This is good for interfaces in which details\n\ncan be deferred.\n\nWith respect to estimation:\n\n###### � [The smaller it is, the easier it is to ]\n\nestimate.\n###### � [The simpler it is, the easier it is to ]\n\nestimate.\n###### � [The more mature the technology is, the ]\n\neasier it is to estimate.\n###### � [The more that is supplied by others, the ]\n\nless needs to be done (i.e., the smaller\n\nit is).\n\nThere are many approaches we can take for\n\nan estimation methodology. They all share one\n\nkey characteristic: none is able to satisfy all. This\n\ngoes from agile and lean techniques [31], which\n\nmeasure team velocity delivering story points,\n\nto function points, and the classic SLOC (source\n\nline of code) counts. Be very wary of whatever\n\ntechnique is chosen. Don’t automatically accept\n\nit—always seek supporting and refuting evidence\n\non the estimates during execution.\n\nEstablishing baselines. The baselines should\n\nbe appropriate for the estimation method and\n\nthe development measurement methods. For\n\nexample, “done done” in agile methods should\n\nbe ruthlessly watched. This fits well with defining\n\nearned value milestones (EVM) [32]. A potential\n\nbenefit of EVM is that it demands a crisp defini­\n\ntion of a milestone and provides early hints when\n\nthe cost and schedule assumptions are being\n\n\n-----\n\nviolated. This may provide a tool for knowing when\n\nto abandon one option and pick another.\n\nA hidden problem with service-oriented\n\napproaches [33]. Ironically, although service\noriented approaches offer the potential agility\n\nand composability desired, the manner in which\n\nwe contract with developers may erect barriers\n\nto realizing the benefits. Consider the situation\n\nin which a program offers a service that delivers\n\nsome of its information bundled in a collection.\n\nSuppose further that this is discovered and found\n\nuseful by many outside the originally planned\n\nusers and stakeholders. Under these circum­\n\nstances, we might expect the use of the service\n\nto be greatly beyond the planned use. However,\n\ntransaction densities may exceed the design limits\n\nand degrade the performance. Whose problem is\n\nthis, and how is it mitigated?\n\n###### References and Resources\n\n\nContract types. Consider using contract\n\nstructures for which the default behavior is not\n\ncontinuation. We might do this using an indefinite\n\ndelivery/indefinite quantity contract with a series\n\nof tasks that build on one another, yet where each\n\nhas a clear ending.\n\nConsider using “supplier” models in contrast\n\nwith “developer” models. Payout based on use.\n\nThere are many challenges to working the uncer­\n\ntainty and complexity of MITRE’s customer envi­\n\nronment. The need to manage these challenges\n\nhas become more prevalent in the tasks MITRE\n\ntakes on as we continue to work the strategic\n\nand hard problems of our customers and their\n\nenterprises. The practices listed can help work\n\nthis critical area—as more experience is gained by\n\nMITRE staff, these practices will evolve as well in\n\nour uncertain and complex MITRE world.\n\n\n1. As characterized in Stevens, R., September 24, 2009, “Acquisition Strategies to Address\n\nUncertainty: Acquisition Research Findings,” from MITRE-Sponsored Research,\n[“Enterprise Systems Acquisition Using Venture Capital Concepts.”](http://www.mitre.org/news/events/exchange08/3188.pdf)\n\n2. Dorner, D., 1996, The Logic of Failure, Basic Books.\n\n3. Mitchell, M., 2009, Complexity: A Guided Tour, Oxford.\n\n4. “Lack of control” includes many conditions and situations, but the most general sense of\n\nits use here is the inability to set the desired state, or vector, of an element under one’s\nauthority, and which one is expected to exercise control over. The “traditional” approach\nto ensuring control is isolation of a system—both in development and in use. With more\ninterconnected and interdependent elements and systems, this presumption (and tech­\nnique) is violated.\n\n5. “Complex” has become a term of art in engineering and science, and its meaning is\n\nslightly different than how it is used in the vernacular. At the risk of oversimplifying,\n“complicated” as a word means difficult to understand, whereas “complex” means stable\n\n\n-----\n\ncollections and patterns arising from (or emerging) from simple interactions among com­\nponent pieces. See almost any of the references for more description.\n\n6. Metzger, L. M., April 27, 2009, “MITRE Systems Engineering: So What’s It All About?”\n\n7. Bar-Yam, Y., 2004, Making Things Work: Solving Complex Problems in a Complex World,\nKnowledge Press.\n\n8. Johnson, S., 2001, Emergence: The Connected Lives of Ants, Brains, Cities, and Software,\n\nScribner.\n\n9. Also see the SE Guide article “Composable Capabilities On Demand (CCOD)” in\n\nEngineering Information-Intensive Enterprises.\n\n10. DeRosa, J. K., et al., 2008,”The Requirements Conundrum in Complex Systems\n\nEngineering,” ICSSEA 2008, Paris.\n\n11. Watts, D., 2003, Six Degrees: The Science of a Connected Age, Norton.\n\n12. Barabasi, A-L., 2002, Linked: The New Science of Networks, Perseus.\n\n13. Perhaps only the recognition of fitness as a process is sufficient, and the understanding\n\nand management of choices and “choice-spaces” is where we can make our engineering\nprocesses reflect fitness. If we were to be able to understand and quantify fitness, it might\ngive us a predictive tool that is currently absent.\n\n14. There are many efforts that attempt to quantify fitness in engineering systems. In our\n\nown community, there are attempts to define measures of effectiveness focusing on opera­\ntional metrics.\n\n[15. Norman, D. O., and B. E. White, 2008, “Asks the Chief Engineer: ‘So What Do I Go Do?!,’”](http://www.mitre.org/work/tech_papers/tech_papers_07/07_1347/07_1347.pdf)\n\nSysCon 2008, IEEE International Systems Conference.\n\n16. Norman, D. O., and M. Kuras, 2006, “Engineering Complex Systems,” Complex Engineered\n\n_Systems, Springer, Chapter 10._\n\n17. Norman, D. O., 2009, “Engineering Complex Systems: challenges in the theory and prac­\n\ntice,” Organizing for a Complex World, CSIS Press, Chapter 7.\n\n18. Friedman, T. L., 2006, The World Is Flat, Farrar, Straus, and Giroux.\n\n19. Ackoff, R. L., F. Emery, 2006, On Purposeful Systems, Transaction.\n\n20. Options provide variety. And, variety is absolutely necessary to promote evolution. This\n\nmay seem counterintuitive to those who view variety as mere redundancies. It must be\nrecognized that variety (also diversity) requires selection to lead toward effective evolu­\ntion. Variety is explained nicely by Ross Ashby in “Law of Requisite Variety” in his book\n_[Introduction to Cybernetics, 1956, Chapman and Hall, London. Also see Diversity Collapse:](http://www.santafe.edu/events/workshops/index.php/Diversity_Collapse:_Causes_Connections_and_Consequences_-_Agenda)_\n_[Causes, Connections, and Consequences.](http://www.santafe.edu/events/workshops/index.php/Diversity_Collapse:_Causes_Connections_and_Consequences_-_Agenda)_\n\n\n-----\n\n21. Think about automobiles. If we didn’t allow for the removal and replacement of the\n\nwheels/tires, we would need to wait for a different (redesigned) vehicle in order to operate\nthe vehicle in a different environment—like loose sand rather than asphalt. By recog­\nnizing that wheels/tires can be changed quicker than the vehicle is replaced, we allow\nchange at that point, and the evolution of the whole can occur more rapidly.\n\n22. Also see the SE Guide articles, “Architectures Federation” and “Design Patterns” in\n\nEngineering Information-Intensive Enterprises.\n\n23. Baldwin, C., and K. Clark, 2000, Design Rules: The Power of Modularity, Vol. 1, MIT Press.\n\n24. Holland, J., 1995, Hidden Order: How Adaptation Builds Complexity, Perseus.\n\n25. Moore, J. F., 1996, The Death of Competition: Leadership and Strategy in the Age of Business\n\n_Ecosystems, Harper Business._\n\n26. Beinhocker, E., 2006, The Origin of Wealth: Evolution, Complexity, and the Radical\n\n_Remaking of Economics, HBS Press._\n\n27. Wheatley, M. J., 1999, Leadership and the New Science: Discovering Order in a Chaotic\n\n_World, Berrett Koehler._\n\n28. Also see the SEG topic Acquisition Program Planning in Acquisition Systems Engineering\n\nSection.\n\n29. Stevens, R., September 24, 2009, “Acquisition Strategies to Address Uncertainty:\n\n[Acquisition Research Findings,” from her MITRE-Sponsored Research “Enterprise Systems](http://www.mitre.org/news/events/exchange08/3188.pdf)\n[Acquisition Using Venture Capital Concepts.”](http://www.mitre.org/news/events/exchange08/3188.pdf)\n\n30. Real Options research at MITRE.\n\n31. Shore, J., and S. Warden, 2008, The Art of Agile Development, O’Reilly.\n\n32. Fleming, Q., and J. Koppelman, 2005, Earned Value Project Management, 3rd ed., Project\n\nManagement Institute.\n\n33. Martin, J., 1995, The Great Transition, Macon.\n\n\n-----\n\nDefinition: A comprehensive\n\n_view takes a look at a situa­_\n\n_tion and helps describe the_\n\n_complexity of an enterprise and_\n\n_identify the activities necessary_\n\n_to balance interests across_\n\n_potentially competing perspec­_\n\n_tives throughout the enterprise,_\n\n_such as interconnected mission_\n\n_needs, business requirements,_\n\n_technological enablers, cultural_\n\n_environments, economic_\n\n_constraints, and others. Various_\n\n_tools can be used to formulate_\n\n_a comprehensive view of an_\n\n_enterprise that captures and_\n\n_compares the important drivers,_\n\n_influences, and risks affecting_\n\n_the establishment of desired_\n\n_capabilities._\n\nKeywords: comprehensive\n\n_viewpoint, enterprise, fed­_\n\n_eration, POET, principles, SE_\n\n_Profiler, stakeholder analysis,_\n\n_TEAPOT, tools, value impact,_\n\n_value metrics_\n\n\nCOMPREHENSIVE VIEWPOINT\n###### Tools to Enable a Comprehensive Viewpoint\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers are expected to analyze\n\nand understand a customer’s enterprise or\n\ncross-agency environment in the context of\n\ncustomer and stakeholder needs and challenges.\n\nMITRE systems engineers are also expected to\n\nformulate and adjust plans and steps needed\n\nto effectively provide thought leadership,\n\nenhance enterprise integration, identify politi­\n\ncal challenges, recognize mission/operational\n\ngaps, mitigate risks, and ensure delivery.\n\n\n-----\n\n###### Enabling a Comprehensive Viewpoint\n\nA comprehensive viewpoint of the customer’s environments should portray strengths, weak­\nnesses, challenges, and constraints in all areas pertinent to the work program/project. It is\ncrucial to take a holistic approach when establishing a view of the customer’s environments in\nthe context of the intended program/project [1] (also see the SEG article “Systems Thinking”).\nA well-analyzed and balanced perspective not only provides the facts and information for\nMITRE systems engineers to devise plans and activities necessary to meet the intended\nrequirements and objectives, it also renders indications for adjustments, improvements, and\nenhancements [2, 3]. It is important to establish a set of “program basics” to best depict the\ncurrent state of the working environments as well as associated elements that would assist/\nimpact the success of the program/project. As starting points for analysis, consider this set of\nprogram basics:\n\n###### �Scope of work program/project �Work program/project relevant to customer’s mission and strategic objectives �MITRE roles and responsibilities �Work program/project environments �Relationships with the customer �Work program/project management (initiation, planning, execution, and closing) �Work program status\nTo adequately portray the current and desired state of the environments, the analyses\nshould be conducted with integrity, objectivity, and consistency. Tools are available for con­\nducting such analyses that can appropriately articulate the states of the customer’s environ­\nments throughout the program/project life cycle (see Table 1).\n\nDepending on the size and complexity of the program/project, tools can be applied either\nindependently or collectively to describe the strengths, weaknesses, gaps, risks, and issues of\nthe environments being analyzed. Additionally, it is crucial to recognize/identify the interde­\npendencies of the findings to best assist the formulation of the corrective plans and actions.\nFor instance, the root cause of some technical challenges encountered may be the results of\ndeficient stakeholder analyses and ill-defined requirements.\n\nThe following tools have been proven useful and effective in analyzing the working envi­\nronments, devising feasible enhancement/corrective actions, and formulating execution plans\nand steps.\n\n###### POET\n\nThe Political, Operational, Economic, and Technical (POET) analysis technique was developed\nby TRW, Inc., in 1999. It was created to assess challenges and opportunities associated with\nlarge-scale programs consisting of systems-of-systems. However, it can be used to assess or\n\n\n-----\n\ndevise programs, customer challenges, or strategies, regardless of the size and complexity of\nthe program. The analysis uses the POET categories to construct the program basics, identify\nthe program challenges and constraints, and devise action plans accordingly.\n\n###### �Political: Assess and articulate associated leadership, mission/business decision drivers,\n\norganizational strengths/weaknesses, policies, governance, expectation management\n(e.g., stakeholder relationship), program management approach, etc.\n###### �Operational: Obtain and evaluate mission capabilities, requirements management,\n\noperational utility, operational constraints, supporting infrastructure and processes,\ninteroperability, supportability, etc.\n###### �Economic: Review capital planning and investment management capabilities, and\n\nassess the maturity level of the associated processes of budgeting, cost analysis, pro­\ngram structure, acquisition, etc.\n###### �Technical: Assess and determine the adequacy of planned scope/scale, technical matu­\n\nrity/obsolescence, policy/standards implementation, technical approach, etc.\n\n###### TEAPOT\n\nThe Center for Enterprise Modernization furthers the POET analysis disciplines to promote\ntechnical accuracy, economic feasibility, actionable recommendations, political insightfulness,\noperational reality, and timely delivery (TEAPOT) [4].\n\nIn addition to assessing and presenting the challenges and deficiencies, TEAPOT empha­\nsizes the need to define actions and activities to be performed to enhance/improve the current\nstate and to demonstrate the breadth and depth of MITRE’s federally funded research and\ndevelopment center role and responsibilities. Here are some examples of TEAPOT application:\n\n###### �Technical accuracy: Use mature technologies and methodologies to assess the sound­\n\nness of technical requirements and/or solutions; review compatibility among new and\nlegacy systems; determine extensibility and scalability for future changes in scope and\nrequirements, etc.\n###### �Economic feasibility: Determine if the total cost of the program/project is within the\n\ncustomer’s available funding and proportional to expected benefits; ensure the acquisi­\ntion/sourcing strategies are adequate, etc.\n###### �Actionable recommendations: Present direct and clear recommendations that target\n\nidentified deficiencies, documented findings, and recommendations objectively and\nprofessionally; provide level of detail appropriate for the customer (e.g., executive vs.\ntechnical briefings), etc.\n###### �Political insightfulness: Recognize the strength and weakness of the organiza­\n\ntional culture and socialize findings to ensure understanding and acceptance; make\n\n\n-----\n\nrecommendations that are in compliance with the mandates; balance the competing\npriorities of key stakeholders, etc.\n###### �Operational reality: Consider customer’s resource constraints such as staff, systems,\n\nfunding, etc.\n###### �Timely delivery: Plan and deliver on time as scheduled.\n\n Systems Engineering (SE) Profiler\n\nThe MITRE-developed Systems Engineering Profiler is used to characterize systems in con­\ntext and for visualizing system integration problems along multiple dimensions. This tool is\nparticularly useful and effective for programs/projects that involve designing systems that\ncan perform as components of large-scale, complex enterprises. MITRE systems engineers are\nadvised to look beyond the system, and consider the characteristics of the enterprise in which\nthe system will function and the context in which the system is being developed and acquired\n(see [5, 6, 7, 8] for detailed how-to suggestions).\n\n###### MITRE Value Impact Assessment: Collaborative Tool to Use with POET, TEAPOT, and SE Profiler\n\nValue metrics charts were developed in 2004 to portray MITRE’s range of relationships with\na particular customer and the scope and nature of MITRE’s work for that customer [9]. Two\nmain types of value metrics have been developed to: (1) address criticality of the mission\nneed vs. the nature of MITRE’s work (i.e., highly repeatable vs. advancing the state of the art);\nand (2) address MITRE’s relationship with a customer compared to the scope of our work for\nthem. Value metrics charts can be generated from inputs prepared in Excel.\n\nThe primary goal for using the MITRE Value Impact Assessment is to strengthen work\nprogram content, customer relationships and satisfaction, and MITRE’s impact. This tool is\noften used to identify future directions for MITRE’s engagement model and differentiation\nwith a customer (e.g., projecting MITRE to take on a more strategic role, or in some circum­\nstances, transferring a repeatable role to a government contractor to maintain).\n\n###### Stakeholder Analysis Process: Collaborative Tool to Use with POET, TEAPOT, and SE Profiler\n\nThe stakeholder analysis process is used to strengthen relationships among key stakehold­\ners by establishing why different stakeholder types behave differently and why they behave\nthe way they do. Stakeholder analysis enables tailoring strategies for key stakeholders to take\ngreater advantage of opportunities and avoid or mitigate unwanted risks when they become\napparent.\n\n\n-----\n\nThough the direct customer relationship is a high priority, it is important to determine\nwhich other stakeholder types are of a priority and undertake relationship improvement\nefforts with them.\n\nOnce the key stakeholders have been established, a relationship management program\nstarts by developing a relationship management plan. The tips for the customer relationship\ncan be adapted in planning, executing, and assessing a relationship management program\nwith other key stakeholder types.\n\n###### Enterprise Principles: Collaborative Tool to Use with POET, TEAPOT, and SE Profiler\n\nEnterprise principles are enduring guidelines that describe the way an organization fulfills its\nmission. Principles express an organization’s intentions and fundamental values so that deci­\nsions can be made from a common understanding.\n\nPrinciples are driven by functional capability and/or organizational visions, strategic\nplans, enterprise direction, and policy directives, which in turn are generally driven by presi­\ndential executive orders, legislation, and other external mandates and directives (see [10] for\nadditional details).\n\nThe primary intended audience for enterprise principles includes mission capability pro­\nponents, chief information officers, chief architects, and program managers.\n\n###### Models for Enterprise Federation Analysis: Collaborative Tool Used with POET, TEAPOT, and SE Profiler\n\nFederal Enterprise Architecture\n\nWhile the federal government is organized into agencies, departments, and other organiza­\ntional structures, many of the government’s functional missions cross agency boundaries and\nauthorities. To address the need to coordinate efforts and plans across federal agencies and\nto share information and services, the Office of Management and Budget (OMB) has estab­\nlished the Federal Enterprise Architecture (FEA). The structure of the FEA is maintained by\nOMB, but portions of it, called segments, are developed and maintained by agency leads in\ncoordination with other agencies. Cross-agency FEA segments are documented by OMB in the\nFederal Transition Framework [11], which is used in life-cycle planning activities of agencies\nand their budget submissions. Agencies are responsible for submitting segment architectures\nto OMB. A federal segment architecture methodology was developed to provide guidance and\ndirection to agencies for developing their segment architectures; it consists of a collection of\nbest practices, tools, techniques, templates, and examples of the various elements that may be\nincluded in a segment architecture [12].\n\n\n-----\n\nTable 1. Summary of Analysis and Collaborative Tools\n\n**Analysis Tools** **Topics/Areas to Address and Analyze**\n\nScope of work program/project\n\nWork program/project relevant to customer’s mission and strategic\nobjectives\n\nMITRE roles and responsibilities\n\nPOET\n\nWork program/project environments (political, operational, economic,\n\nTEAPOT\n\nand technical)\n\nSE Profiler\n\nRelationships with the customer\n\nWork program/project management (planning, implementing, and\nmonitoring)\n\nWork program status (accomplishments, actions, and timeliness)\n\n**Collaborative Tools** **Topics/Areas to Address and Analyze**\n\nWork program/project relevant to customer’s mission and strategic\n\nMITRE Value Impact\n\nobjectives\n\nAssessment\n\nMITRE roles, responsibilities, and impacts\n\nWork program/project environments\n\nMITRE internal stakeholders\n\nStakeholder Analysis Customer stakeholders\n\nProgram stakeholders\n\nRelationships with stakeholders\n\nWork program/project compliance to customer’s enterprise objectives\n\nWork program/project environments (e.g., standards, integration, sharing,\n\nEnterprise Principles etc.)\n\nWork program/project management (initiation, planning, execution, and\nclosing)\n\nModels for Enter­\nprise Federation Work program/project compliance to mandates and policies\nAnalysis\n\nDepartment of Defense (DoD) as a Federated Enterprise\n\nThe DoD, like many agencies, has missions to perform that cut across its organizational\nelements. In addition, there are common business functions, such as financial, man­\nagement, and IT infrastructure needs that cut across both missions and organizational\nelements. To address the many potential relationships, and ultimately both complemen­\ntary and competing interests, the DoD has been developing and employing a federated\n\n|Analysis Tools|Topics/Areas to Address and Analyze|\n|---|---|\n|POET TEAPOT SE Profiler|Scope of work program/project Work program/project relevant to customer’s mission and strategic objectives MITRE roles and responsibilities Work program/project environments (political, operational, economic, and technical) Relationships with the customer Work program/project management (planning, implementing, and monitoring) Work program status (accomplishments, actions, and timeliness)|\n|Collaborative Tools|Topics/Areas to Address and Analyze|\n|MITRE Value Impact Assessment|Work program/project relevant to customer’s mission and strategic objectives MITRE roles, responsibilities, and impacts|\n|Stakeholder Analysis|Work program/project environments MITRE internal stakeholders Customer stakeholders Program stakeholders Relationships with stakeholders|\n|Enterprise Principles|Work program/project compliance to customer’s enterprise objectives Work program/project environments (e.g., standards, integration, sharing, etc.) Work program/project management (initiation, planning, execution, and closing)|\n|Models for Enter­ prise Federation Analysis|Work program/project compliance to mandates and policies|\n\n\n-----\n\nenterprise approach to provide consistent context and disciplines for accomplishing the\nmission of the Department collectively [13, 14, 15], as are other federal agencies [16, 17,\n18, 19, 20].\n\n###### References and Resources\n\n1. “Stakeholder Analysis and Relationships,” MITRE Project Leader Handbook, The MITRE\n\nCorporation.\n\n[2. MITRE Systems Engineering (SE) Competency Model, September 1, 2007, Ver. 1, The](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\nMITRE Corporation,\n\n3. The MITRE Corporation Center for Enterprise Modernization, February 10, 2009, Quality\n\nHandbook, Ver. 2.0, pp. 4–33.\n\n4. The MITRE Corporation “TEAPOT Chart: Characterize Systems Engineering Output.”\n\n[5. Carlock, P. G., S. C. Decker, and R. E. Fenton, Spring/Summer 1999, “Agency-Level](http://www.is.northropgrumman.com/about/ngtr_journal/assets/TRJ-1999/SS/99SS_Carlock.pdf)\n\n[Systems Engineering for ‘Systems of Systems,’” Systems and Information Technology Review](http://www.is.northropgrumman.com/about/ngtr_journal/assets/TRJ-1999/SS/99SS_Carlock.pdf)\n_Journal, pp. 99–110._\n\n6. Stevens, R., July 2010, Engineering Mega-Systems: The Challenge of Systems Engineering in\n\n_[the Information Age, CRC Press.](http://www.crcpress.com/product/isbn/9781420076660)_\n\n7. [Stevens, R., “Profiling Complex Systems,” The MITRE Corporation.](http://www.mitre.org/work/tech_papers/tech_papers_08/07_0072/)\n\n8. Stevens, R., “Managing Uncertainty,” The MITRE Corporation.\n\n9. The MITRE Corporation, “Use Value Metrics to Assess Potential Technical Work.”\n\n[10. The Open Group Architecture Framework (TOGAF), “Architecture Principles,” TOGAF](http://pubs.opengroup.org/architecture/togaf8-doc/arch/chap29.html)\n\nversion 8.1.1, Pt. IV, Resource Base, Ch. 29.\n\n[11. Federal Transition Framework (FTF), Ver. 2.0, www.whitehouse.gov, January 2008,](http://www.whitehouse.gov/sites/default/files/omb/assets/fea_docs/FTF_Metamodel_Reference_Ver_2_0.pdf)\n\n12. Office of Management and Budget, Federal Enterprise Architecture (FEA), www.white­\n\nhouse.gov.\n\n13. DoD Deputy Chief Information Officer, “DoD Architecture Framework 2.0, Architecture\n\n[Development, Enterprise Architecture,” http://dodcio.defense.gov/dodaf20.aspx, retrieved](http://dodcio.defense.gov/dodaf20.aspx)\nJuly 29, 2010.\n\n[14. Golombek, A., and W. Okon, “EA Federation and Building the DoD EA—Briefing to OMG,”](http://www.updm.com/document/document.htm)\n\nDoD CIO, September 16, 2009. UPDM is an Object Management Group (OMG) initiative\nto develop a modeling standard that supports both the DoDAF and the UK Ministry of\nDefence Architecture Framework (MODAF). The modeling standard is called the Unified\nProfile for DoDAF and MODAF (UPDM).\n\n\n-----\n\n15. DoD CIO, “DoD Governance: Architecture Federation,” July 29, 2010 (requires Intelink\n\nusername and password).\n\n16. The MITRE Corporation, December 3, 2003, United States Coast Guard Enterprise\n\nArchitecture Framework, Ver. 0.3\n\n[17. Federal Health Architecture, www.healthit.gov.](http://healthit.hhs.gov/portal/server.pt?open=512&objID=1181&parentname=CommunityPage&parentid=26&mode=2&in_hi_userid=11113&cached=true)\n\n18. Mullins, K., December 15, 2005, DOJ Litigation Case Management (LCM) Target LCM\n\nArchitecture.\n\n[19. Department of Defense Office of Chief Information Officer, May 2009, Defense Information](http://dodcio.defense.gov/Home/Initiatives/DIEA.aspx)\n\n[Enterprise Architecture, Ver.1.1.](http://dodcio.defense.gov/Home/Initiatives/DIEA.aspx)\n\n20. Grasso, D., and M. B. Burkins, December 1, 2009, “Holistic Engineering Education Beyond\n\nTechnology,” Springer, Ch. 5.\n\n\n-----\n\n##### Enterprise Planning and\n Management\n\nDefinition: Enterprise planning and management addresses agency and program\n\n_direction, monitoring, and resource allocation to achieve goals and objectives at_\n\n_the strategic level._\n\nKeywords: governance, performance management, portfolio management, pro­\n\n_gram management, resource allocation, strategic planning_\n\n###### Introduction\n\nEnterprise planning and management takes a strategic view of the major\n\nplans and processes needed for a federal government organization to\n\nachieve its mission. The legislative branch does not often get into details\n\nabout which components of an executive branch agency will execute\n\neach aspect of the mission, or how they will operate. Therefore, at the\n\nstrategic level, each agency must plan, manage, and account for both\n\nhow and to what extent it achieves that mission. MITRE engineers may\n\nbe asked by sponsors to help develop and execute these strategic-level\n\nplans and processes.\n\nAn awareness and working knowledge of enterprise planning and\n\nmanagement is needed by all systems engineers whether or not their\n\ndaily activities directly support enterprise-level government activities.\n\nNearly all government development programs or those undergoing\n\nsignificant modifications are already interfacing to a number of other\n\n\n-----\n\nsystems, networks, databases, and data sources over the Web, or are part of a family or sys­\ntem of systems. Therefore, at whatever level of the enterprise MITRE systems engineers oper­\nate, enterprise planning and management provides the context for or environment in which\nthey execute their activities.\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to bring an enterprise perspective to their sup­\nport of customer planning and management activities at whatever scale of the enterprise they\noperate: subsystem, system, system of systems, or enterprise.\n\nWhen directly supporting enterprise planning and management activities, MITRE\nsystems engineers are expected to understand the central role systems engineering plays\nin effectively planning and managing the evolution or modernization of government enter­\nprises. MITRE SEs are expected to tailor and adapt systems engineering principles, processes,\nand concepts to match the scope and complexity of the government overall effort as well\nas the agency or department acquisition regulations, policies, and governance approaches.\nMITRE SEs need to be cognizant of enterprise management challenges/issues so they can\nassume appropriate accountability for the success of the activities they support. MITRE staff\nare expected to coordinate extensively across the corporation, other FFRDCs, academia, and\nindustry to ensure that all aspects of the problem are considered in shaping products or deci­\nsions. MITRE contributions should provide an enterprise perspective, be product and vendor\nneutral, and anticipate future missions and technologies.\n\n###### Best Practices and Lessons Learned\n\n\nDo the right things and do them well. “The\n\ngreatest waste in business is doing the wrong\n\nthing well”—Henry Ford.\n\nThe primary objective of enterprise planning and\n\nmanagement is to ensure that the enterprise is\n\ndoing the right things—directing its resources with\n\nmaximum impact for achieving its mission. Doing\n\nthe right things well is more a tactical conern, with\n\nprogram and project execution. The best prac­\n\ntices and lessons learned apply to planning to do\n\nthe right things, and then monitoring how (and\n\nwhether) doing those things is leading toward the\n\nend goals.\n\n\nProvide the right focus. Focus organizational\n\nresources on achieving the goals outlined in the\n\nstrategic plan.\n\nImportance of senior leadership role. An\n\nessential component of success is the commit­\n\nment and active involvement of the organization’s\n\nsenior leadership.\n\nThe articles under this topic provide more detailed\n\ndescriptions of best practices.\n\n\n-----\n\n###### Articles Under This Topic\n\nThe “IT Governance” article outlines government enterprise investment management poli­\ncies and goals and describes best practices for governing those investments in the federal\ngovernment.\n\nThe “Portfolio Management” article describes how MITRE provides technical advice and\nrecommendations to support the customer in making resource allocation decisions to achieve\ndesired outcomes within funding and other business constraints.\n\nThe “How to Develop a Measurement Capability” article describes the general principles\nand best practices of performance measurement methods and systems and how to use per­\nformance measures to assess progress toward achieving strategic goals and objectives and to\ninform decisions about resource allocation.\n\n###### References and Resources\n\n[1. U.S. Government Accounting [now Accountability] Office (GAO)/General Government](http://www.gao.gov/archive/1997/gg1016.pdf)\n\n[Division, May 1997, Agencies’ Strategic Plans Under GPRA: Key Questions to Facilitate](http://www.gao.gov/archive/1997/gg1016.pdf)\n[Congressional Review, Ver. 1, GAO/GGD-l0.l.16.](http://www.gao.gov/archive/1997/gg1016.pdf)\n\n[2. GAO, Information Technology Investment Management: A Framework for Assessing and](http://www.gao.gov/new.items/d04394g.pdf)\n\n[Improving Process Maturity, 2004 (GAO-04-394G).](http://www.gao.gov/new.items/d04394g.pdf)\n\n[3. GAO, Government Performance: GPRA Modernization Act Provides Opportunities to Help](http://www.gao.gov/new.items/d11466t.pdf)\n\n[Address Fiscal, Performance, and Management Challenges, March 2011 (GAO-11-466T).](http://www.gao.gov/new.items/d11466t.pdf)\n\n\n-----\n\nDefinition: Governance is the\n\n_responsibilities, structures, and_\n\n_processes by which organiza­_\n\n_tions are directed and controlled_\n\n_[1]. It defines how business,_\n\n_engineering, and operations_\n\n_decisions are made to support_\n\n_business strategy. Enterprises_\n\n_have many interrelated layers of_\n\n_governance that differ in scope_\n\n_and decisions. Governance_\n\n_definitions vary, but they have_\n\n_some elements in common:_\n\n_It is about making decisions_\n\n_to support business strategy._\n\n_It requires a framework that_\n\n_defines roles and responsi­_\n\n_bilities, processes, policies,_\n\n_and criteria for sound decision_\n\n_making. It requires identifying_\n\n_the right people to make and_\n\n_be held accountable for tough_\n\n_decisions._\n\nKeywords: business process,\n\n_framework, governance,_\n\n_strategy_\n\n\nENTERPRISE PLANNING AND MANAGEMENT\n###### IT Governance\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to understand\n\nwhy IT governance is a critical issue for the federal\n\ngovernment and the integral role IT governance\n\nserves within organizational strategic planning.\n\nThey are expected to assist the customer in\n\nadhering to the requirements of the organization’s\n\ngovernance program, establishing appropriate\n\nroles and responsibilities, and following mandates\n\nand best practices for governing IT investments\n\nin the federal government. MITRE SEs also should\n\nplay a role in helping an organization achieve real\n\nvalue from IT investments by ensuring alignment\n\nto the enterprise strategies and governance\n\nprogram. MITRE SEs’ role is to increase the value\n\nof the IT investments by providing feedback and\n\nlessons learned on how the governance program\n\n\n-----\n\nFigure 1. Fundamental Phases of the IT Investment Approach\n\nis functioning and where improvements should be made. MITRE SEs are expected to estab­\nlish a foundation on which good decisions can be made by deriving and analyzing data for\nspecific decisions (e.g., those related to business cases, reference architectures, policies, stan­\ndards, formats, processes, and life cycles needed to establish governance). This may require\nan understanding of organizational change and transformation, risk management, and\ncommunications planning. For more information on both of those topics, see the SEG topic\nTransformation Planning and Organizational Change.\n\n###### Background\n\nEnterprise governance is a set of responsibilities and practices exercised by “a board and\nexecutive management with the goal of providing strategic direction, ensuring that objectives\nare achieved, ascertaining that risks are managed appropriately, and verifying that the enter­\nprise’s resources are used responsibly [2].” All other types of governance within an organiza­\ntion—IT governance, interagency governance, program governance, project governance—are\nwithin the context of enterprise governance.\n\nInformation technology (IT) governance is an integral part of enterprise governance and\nconsists of the leadership, structures, and processes that ensure that an organization’s IT\nsustains and extends its strategies and objectives [2]. IT governance requires a structure and\nprocesses to support repeatable decision making, alignment of IT activities to the enterprise’s\nstrategic goals and objectives, and a clear understanding of authority and accountability.\n\n\n-----\n\nFigure 2. Sample OMB Circulars and Guidance\n\nAs with any governance body within an organization, IT governance cannot be viewed,\nassessed, modified, or changed without considering the rest of the organization’s governance\nbodies and practices.\n\nIT governance affects the degree to which an organization will get value from its IT\ninvestments. The goals of IT governance are to ensure IT investments generate business value\nand to mitigate IT risks [6]. Research among private sector organizations has found that “top\nperforming enterprises succeed in obtaining value from IT where others fail, in part, by imple­\nmenting effective IT governance to support their strategies and institutionalize good prac­\ntices [3].” This principle can be extended to the goals of the enterprise at large. Whereas the\npurpose of enterprise governance is to effectively derive value from the enterprise resources\nfor all the constituents in the enterprise, based on defined enterprise goals and strategy, the\npurpose of IT governance is to ensure the effective and efficient management and delivery\nof goods and services aligned to enterprise strategies [6]. For more information on Enterprise\nStrategy, see the article in this section on Strategic Planning. Also, see related articles under\nthe Enterprise Technology, Information, and Infrastructure topic in this section.\n\nFor nearly two decades, the federal government has been trying to adopt investment and\nusage best practices from private industry to ensure that IT enables government to better\nserve the American people. Through legislation, executive orders, and guidance, the federal\n\n\n-----\n\ngovernment requires that agencies apply rigor and structure to the selection and manage­\nment of IT in order to achieve program benefits and meet agency goals. In 1996, Congress\npassed the Clinger-Cohen Act, which required, among other things, that senior government\ndecision makers become involved in the decisions concerning the value and use of IT in the\norganization.\n\n###### IT Investment Management\n\nIn 2004, the U.S. Government Accountability Office (GAO) published Information Technology\nInvestment Management (ITIM): A Framework for Assessing and Improving Process Maturity\n\n[4]. ITIM is a maturity model built around the select/control/evaluate approach outlined in the\nClinger-Cohen Act. ITIM establishes requirements for IT management and is used to assess\nhow well an agency is selecting and managing its IT resources (Figure 1). In many agen­\ncies today, the IT investment life cycle includes a fourth phase: Pre-Select. In this phase, the\norganization plans and evaluates its strategy and mission needs before the select phase and\n“pre-selects” those that best help the organization meet this strategy before final selection of\ninvestments.\n\nThe Office of Management and Budget (OMB) has issued executive orders and circulars to\nhelp improve agency management of IT resources to support and govern the intent of ClingerCohen and other legislation. (See Figure 2.) These circulars approach the problem of the use\nof IT through the budget process requiring that requests for funds for IT investments meet\nspecific requirements.\n\nEstablishing effective governance starts with addressing three questions:\n\n\nI\n\nI\n\nI\n\n\nWhat decisions must be made to ensure\n\nWhat are the desired outcomes?\n\neffective management and use of IT?\n\nWho should make these decisions? Who is responsible and accountable?\n\nHow will these decisions be made and How should the process work?\nmonitored?\n\nDesigning the IT Governance Process should be done after the organization has identified its\ndesired outcomes\n\nFigure 3. Establishing Effective Governance\n\n\n-----\n\nMost recently, OMB issued its 25 Point Implementation Plan to Reform Federal IT\nManagement [5], once again addressing the concerns around federal agencies’ ability to\nachieve effectiveness and efficiency from its IT investments. The 25 Points institutes a\n“TechStat” process within the federal government to take a close look at poor or underper­\nforming programs.\n\n###### Best Practices and Lessons Learned\n\n\nThe governance program must have clear goals\n\nand defined outcomes tied to strategic goals.\n\nOne of the first actions in standing up a gover­\n\nnance program is to clearly define and articulate\n\nthe scope of what is being governed and the\n\ndesired outcomes of governance decision making.\n\nThe outcome of the governance process should\n\nbe aligned to the organization’s strategic goals\n\nand clearly communicated to all stakeholders in\n\nthe organization. The focus on outcomes will drive\n\nall other decisions surrounding the establish­\n\nment of the governance program, including what\n\ndecisions need to be made, who should make the\n\ndecisions, and what data and analysis are needed.\n\n(See Figure 3.)\n\nOften, an organization does not articulate the\n\nreal objectives of the governance program, or\n\nthe governance efforts are focused solely on\n\ncomplying with federal laws and guidance. It is\n\nnot uncommon for an organization to spend\n\nconsiderable resources developing charters,\n\nprocesses, and governance structures without\n\na clear and universal understanding of the goal.\n\nAnd, although compliance is certainly important,\n\nif it is the only focus of the program, it is not likely\n\nprovide real value to the organization. Accepting\n\na broader view on the need for governance, an IT\n\ngovernance body could have goals focused on\n\n\nvalue delivery, resource management, and/or risk\n\nmanagement where compliance objectives are\n\nsimply part of overall decision making.\n\nEnsure reliable information for decision mak­\n\ning. Successful and effective governance relies\n\non the availability of reliable data and information\n\non which to base decisions. Organizations often\n\ndo not have the right information about projects\n\nor investments to make good decisions. The\n\nresult can be “garbage in, garbage out.” Once an\n\norganization has defined its desired outcomes for\n\nthe process, it can begin to identify the informa­\n\ntion needed to make decisions to achieve these\n\noutcomes. This type of information would include,\n\nfor example, a project’s actual cost, schedule,\n\nand scope performance against the estimated or\n\nprojected performance. IT management docu­\n\nmentation, service management monitoring, and\n\nconfiguration management also inform the deci­\n\nsion-making process. Data for IT decision making\n\nincludes assessment factors such as return on\n\ninvestment, total cost of ownership, performance\n\nmeasurements, IT security, and enterprise archi­\n\ntecture; development of scoring algorithms; and\n\nguidelines and methodology, as required, for con­\n\nsistency in scoring. SEs can assist by investigat­\n\ning alternative courses of action, determining the\n\napplicable measures of effectiveness, and relating\n\n\n-----\n\nthese to assessments of risk (including techni­\n\ncal maturity and applicability to the task at hand),\n\ncost, schedule, and performance. If the informa­\n\ntion is not readily available, executive sponsors\n\ncan help support a process for getting the right\n\ninformation to decision makers in a predictable\n\nmanner.\n\nGovernance programs must gain and retain the\n\nexecutive sponsorship needed. Lack of leader­\n\nship for establishing and maintaining a governance\n\nprogram is a challenge to sustaining it over time.\n\nA related issue is changing leadership. Often a\n\nfederal executive establishes and puts full weight\n\nbehind a program, only to leave behind a succes­\n\nsor who does not support the cause as vigor­\n\nously. This underscores the need for a sustained,\n\ndocumented, and formalized program focused\n\non clear IT outcomes aligned to organizational\n\nstrategy. The program needs to provide oppor­\n\ntunities to revisit it for updates and to ensure that\n\nteam members and stakeholders are sufficiently\n\nengaged.\n\nGovernance requires a structure, defined and\n\nrepeatable processes, policy, and criteria.\n\nOnce the desired outcomes of governance are\n\nidentified, an organization needs to establish\n\nthe decision-making authority and the partici­\n\npants’ roles and responsibilities. This involves\n\nthe development of a governance structure that\n\nestablishes the authority of governance bodies,\n\nprocesses that establish repeatable criteria and\n\ndecision making, and preparation of charters, or\n\nsimilar type of documents, to describe the scope,\n\nduties, structure, and selection process of mem­\n\nbers, roles, and responsibilities. For governance to\n\nbe effective over a sustained period of time, it is\n\n\nmore likely to succeed if it reflects the culture and\n\ndecision-making style of the organization and is\n\nintegrated with existing decision making, toler­\n\nance of risk, and operational management pro­\n\ncesses. The governance processes should not be\n\nburdensome, but can and should be tailored and\n\ndeveloped to ensure a “fit to purpose” by matching\n\nthe size and scope of the program/organization\n\nbusiness needs and strategic goals to the climate,\n\nrisk tolerance acceptance levels, and governance\n\nmaturity level of the organization.\n\nPerformance measures are critical to effective\n\nIT governance. Many organizations find it difficult\n\nto measure the performance of their IT gover­\n\nnance programs because the programs often\n\ndon’t function in the context of governance goals\n\nbut instead focus on individual IT project goals.\n\nIn these situations, the lack of effective gover­\n\nnance measurements limits the understanding\n\nof how well the process is performing in meet­\n\ning the decision-making needs of the organiza­\n\ntion. Successful governance activities maintain\n\nreporting or tracking of measures that indicate the\n\nvalue of the governance program for its intended\n\npurpose toward meeting defined goals. Examples\n\nof IT governance performance measures focused\n\non improving the process include increasing\n\ntransparency of IT investment decisions, demon­\n\nstrating an increase in IT innovation investments\n\nwith a decrease in IT sustainment spending, and\n\nincorporating flexibility in IT infrastructure to\n\nreact to changes in regulation and policy envi­\n\nronment [7]. Regular reporting not only serves to\n\nshow value, but also helps maintain the focus of\n\nthe governance program as it executes. MITRE\n\nSEs can help customers measure and report on\n\n\n-----\n\nperformance indicators to enable governance\n\nbodies to make decisions about projects and pro­\n\ngrams in the context of the organization’s goals.\n\nArticulate the value of governance to balance\n\nits perception as a burden. Because organiza­\n\ntions often have the notion that governance is too\n\nburdensome, in order to meet release or develop­\n\nment schedules, their governance processes are\n\n###### Summary\n\n\noften short-cut or bypassed altogether. This may\n\nappear to provide short-term rewards, but experi­\n\nence has shown it is inefficient in the long term.\n\nAs organizations try to balance resources across\n\nmany efforts, their visibility into the programs\n\ndiminishes and, as result, they lose opportunities\n\nfor consolidation or more effective enterprise\n\noperations that would have been achieved if they\n\nhad had a functioning governance process.\n\n\nTo be successful, IT governance must be integrated and aligned with the organization’s enter­\nprise governance. The decisions for IT investments must have a direct connection to support­\ning goals defined by the organization and to the allocation of resources to meet those goals.\nIT governance decisions should have a clear line of sight to the agency’s goals and intended\nstrategic outcomes. IT governance activities provide focus and create a path forward to meet­\ning the information management challenges faced by the agency.\n\nThere are many approaches to implementing effective governance. The exact approach\ndepends on the strategy and results the organization is trying to achieve as well as the culture\nwithin which the organization operates. A review of governance practices suggests that spe­\ncific foundational elements must be in place for governance to be effective:\n\n###### �Strong executive sponsorship of the process �Clear and well-communicated strategic goals �Clear, well-defined roles and responsibilities �Standardized data and information transparency �Measurement and planned review of the governance practices to ensure value\nGovernance frameworks that may be of interest include CoBIT, ITIL, CMMI, and ISO38500.\n\n###### References and Resources\n\n[1. International Standard ISO/IEC 38500:2008(E), 1st Ed., 2008-06-01.](http://www.iso.org/iso/catalogue_detail?csnumber=51639)\n\n[2. ITGI Board Briefing on IT Governance, 2nd Ed.](http://www.itgi.org/Template_ITGIc9a4.html?Section=About_IT_Governance1&Template=/ContentManagement/ContentDisplay.cfm&ContentID=6658)\n\n[3. Weill, P., “Don’t Just Lead, Govern: How Top Performing Firms Govern IT,” Center for](http://csc-studentweb.lr.edu/swp/Berg/PhD%20Backgound%20material%20-%20dissortation/Figures%20and%20misc/PhD%20class%20and%20study%20notes/dont%20just%20lead-govern-how%20top%20performing%20firms%20govern%20it.pdf)\n\nInformation Systems Research, Sloan School of Management, Massachusetts Institute of\nTechnology, 2004.\n\n[4. GAO Executive Guide, Information Technology Investment Management: A Framework](http://www.gao.gov/new.items/d04394g.pdf)\n\n[for Assessing and Improving Process Maturity, March 2004. GAO-04-394G.](http://www.gao.gov/new.items/d04394g.pdf)\n\n\n-----\n\n[5. Office of Management and Budget, 25 Point Implementation Plan to Reform Federal](http://www.cio.gov/documents/25-point-implementation-plan-to-reform-federal%20it.pdf)\n\n[Information Technology Management, December 9, 2010.](http://www.cio.gov/documents/25-point-implementation-plan-to-reform-federal%20it.pdf)\n\n6. R. Brisebois, G. Boyd, and Z. Shadid, “Canada - What is IT Governance? And Why is it\n\nImportant for the IS auditor,” The IntoSAI IT Journal, No. 25, pp. 30–35, August 2007.\n\n7. Fink, K., and Ploder, C. Decision support framework for the implementation of\nIT-governance. Hawaii International Conference on System Sciences, pp. 432–441,\nJanuary 2008.\n\n###### Additional References and Resources\n\nWeill, P., and J. W. Ross, IT Governance: How Top Performers Manage IT Decision Rights for\n_Superior Results. Harvard Business Press, 2004._\n\n\n-----\n\nDefinition: A continuous and\n\n_persistent process that enables_\n\n_decision makers to strategi­_\n\n_cally and operationally man­_\n\n_age resources to maximize_\n\n_accomplishment of desired_\n\n_outcomes (e.g., mission results,_\n\n_organizational improvements,_\n\n_enhancement of operational_\n\n_capabilities) within given con­_\n\n_straints and constructs such_\n\n_as regulations, interdependent_\n\n_architectures, budgets, concept_\n\n_of operations, technology, and_\n\n_mission threads._\n\nKeywords: capability, capa­\n\n_bilities, optimize, outcomes,_\n\n_portfolio analysis, portfolio_\n\n_management, portfolios_\n\n\nENTERPRISE PLANNING AND MANAGEMENT\n###### Portfolio Management\n\nMITRE SE Roles and Expectations: MITRE\n\nsystems engineers are expected to understand\n\nand keep abreast of sponsor and customer\n\nportfolio management (PfM) challenges, themes,\n\nand strategies. They are expected to recommend\n\nand apply systems engineering approaches to\n\naddress PfM opportunities and issues, includ­\n\ning data-driven analysis, incremental baseline\n\ninnovation, time-certain and price-driven\n\nagile acquisition, and the exploitation of com­\n\nmercial development methods and products.\n\n\n-----\n\n###### Overview of Portfolio Management\n\nModern portfolio theory provides foundational concepts that are useful in multiple portfolio\nmanagement environments. Portfolio management is about aggregating sets of user needs into\na portfolio and weighing numerous elements to determine the mix of resource investments\nexpected to result in improved end-user capabilities. The key elements that portfolio manage­\nment must assess are overall goals, timing, tolerance for risk, cost/price, interdependencies,\nbudget, and change in the enterprise environment over time.\n\nAccountability for and transparency of government expenditures has been a significant\nfocus during the last two decades. More recently, it has become important that these expen­\nditures address key enterprise (agency, mission) outcomes efficiently, effectively, and collec­\ntively rather than as independent and unrelated initiatives. Portfolio management is a key tool\nfor supporting this form of fiscal accountability. A simplified overview of portfolio manage­\nment activities is provided in Figure 1. Various laws, directives, and guides relate to portfolio\nmanagement.\n\nAt present there are two major, definitive types of portfolio management: (1) informa­\ntion technology (IT) portfolio management and (2) capability portfolio management (CPM).\nIT portfolio management deals with investment analysis from a hardware and software\nperspective for an enterprise: dealing with the configurations and evolution for IT assets,\nre-capitalization, savings through concepts like regionalization, virtualization, shared assets,\ncloud capabilities, etc. CPM deals with managing the end user capabilities (applications, data,\nservices) as investment options and selecting the best set of functional capabilities to invest\nresources in and to evolve over time. Government organizations are currently in various\nstages of implementation with multiple approaches, and they have met with different levels\nof success. MITRE systems engineers can use our knowledge to help analyze the best way\nforward for successful customer IT architectures and implementations, as well as use our\nknowledge of the operational needs and associated capabilities within the enterprises to help\n\nAnalysis Select Control Evaluate\nLinks objectives to Identifies and selects Ensures investments Measures actual\nvision, goals, best mix of invest- within portfolios are contributions of\npriorities; develops ments to achieve managed and portfolio toward\nperformance (capability) goals and monitored to improved capability\nmeasures; identifies objectives across the determine whether to and supports\ngaps and risks portfolio continue, modify, or adjustment to the\n\nterminate investment mix\n\nFigure 1. Simplified View of Portfolio Management Activities\n\n\n-----\n\ncustomers performing CPM. There is a place for both types of portfolio assessments. As an\nenterprise conducts CPM, they will undoubtedly need to construct the best IT environment to\nsupport the capabilities.\n\n###### Stakeholder Engagement, Roles, and Responsibilities\n\n**Portfolio management requires leadership commitment. Leadership must endorse portfolio**\nmanagement goals, a rigorous and analytical process, and the willingness to make difficult\nrecommendations and decisions such as investment termination. MITRE can help with lead­\nership commitment by analyzing the options and formulating courses of action that define\nthe best investment and use of resources and by highlighting the cost-benefit and return on\ninvestment from the recommended application of resources.\n\n**Engage all stakeholders early and often. Due to the significant number of portfolio**\ncapability providers, as well as the organizational constructs/governance structures that\nmay divide the decision maker and the portfolio managers, it is important to identify all\nstakeholders and to understand the magnitude of their stake in the portfolio and how spe­\ncific stakeholder groups might drive portfolio components and the portfolio. Understanding\nthe different roles, responsibilities, and perspectives of the stakeholders (including those\nof your particular customer) helps in devising strategies to ensure objective assessment\nof potential investments, stakeholder buy-in, viable and affordable recommendations, and\nminimization of “back-door” efforts. Knowing how each stakeholder group drives the port­\nfolio can suggest the needed level of attention that must be paid to each. A minority stake­\nholder may drive a single requirement that drives solutions to be significantly more complex\nand costly than would a majority stakeholder holding 90 percent of the requirements—a\nsituation that the government must avoid.\n\n###### Recommended Tools and Techniques to Use in Portfolio Management\n\nUsing the process of Figure 1, the following sections describe the tools and techniques along\nwith the actions of systems engineers to help accomplish portfolio management.\n\n###### Analysis Tools and Techniques\n\n**Establish the common set of operational needs over time. Understand what requirements,**\ncapabilities, goals, or outcomes need to be achieved, when they must be delivered, how they\nare measured, and how they are prioritized. This information provides specific and common\ntargets for each element of the portfolio. However, the collection of this data is not normally\nstandardized and sustained in a meaningful way in all organizations. The development and\nmaintenance of this information should be a goal of the collective stakeholders of the portfo­\nlio. The SEG topics Concept Development and Requirements Engineering provide articles on\n\n\n-----\n\nhow to help manage the concepts, needs, and requirements of users and can clarify the priori­\nties of these as input to portfolio management.\n\n**Establish an analytic process. The government needs to move away from using force­**\nfully conveyed arguments that appeal to group emotions to using an analytical foundation for\nportfolio decisions. To build a compelling case for portfolio selection (and continuation), help\nestablish a well-understood and transparent analytic process with a framework that addresses\nspecified portfolio goals/outcomes, incorporates key contextual elements (e.g., scenarios) and\ncriteria/metrics important to the decision, uses appropriate data sets, and allows consistent\ncomparison across the portfolio options. Execution of this process should provide insight\ninto the trade-offs and define how the portfolio will be chosen or modified. Figure 2 provides\nan example of what is called an “efficient frontier” created using MITRE’s Portfolio Analysis\nMachine (PALMA™) optimization tool. The efficient frontier showcases the portfolio options\n(black and red points) that provide the most “bang for the buck” (defined here by an overall\nbenefit measure for a given budget). It also shows gray points that are a subset of the less\nefficient portfolios. MITRE systems engineers should understand the mechanics and value of\nusing tools such as PALMA to better understand trade-offs and should call on MITRE experts\nsuch as those in MITRE’s Center for Acquisition and Systems Analysis and other analysis\norganizations like the National Security Analysis Group to help with the analyses.\n\n**Be data driven. Organizations generally do not have a repository that provides a single,**\nconsistent, and robust database to support organizational portfolio management. Based on\ntime and availability, the team (including MITRE systems engineers) should develop the best\ndata set to assess criteria and create con­\nsistent comparisons across the investments 100\nconsidered. Though the most objective data\nis sought out, the best data available may\nactually come from subject matter experts 75\n(SMEs). MITRE systems engineers can help\nfacilitate cost-benefit assessments with\n\n50\n\ngroups of SMEs by providing questionnaires\nand facilitated discussions of proposed\ncapability benefits, prioritization, process 25\nimprovements, resource savings, etc.\n\n\n**Understand the contents of a portfolio.**\nA full understanding of the investments in\na portfolio, as well as of those that may be\nrelated to or impacted by your portfolio,\nis required in order to define the correct\n\n\nFigure 2. Example Efficient Frontier\n\n\n0\n\n\n0 100 200 300 400 500 600\n\nCost\n\n\n-----\n\ntrade-offs for decision making. A common understanding of the portfolio content by the deci­\nsion makers, reviewing bodies, stakeholders, and systems engineers is critical. MITRE sys­\ntems engineers can help lay out the contents of the portfolio, their interconnections, feasibility,\nrisks, etc. Modeling and simulation and in some cases actual prototyping and experimentation\ncan be used by MITRE systems engineers to highlight the features of the critical drivers in the\nportfolio.\n\n**Determine dependencies. Current operations require multiple systems, system compo­**\nnents, etc., to work together to create appropriate capabilities, which in turn creates dependen­\ncies within portfolios that must be identified and understood. The SEG Risk Management topic\nprovides some guidelines on dependency risks. Multiple data sources, artifacts, policies, etc.,\nmust be considered in total, as well as an understanding of the links across these elements,\nto gain full comprehension of the complexity of the portfolio. For example, we need to ensure\nthat we understand the connection between elements such as requirements, capabilities, mis­\nsion threads, architectures, and systems.\n\n###### Selection Tools and Techniques\n\n**Know the baseline. Based on an understanding of user needs, the team must understand how**\ncurrent needs are being met before recommending changes to the portfolio. In some cases this\nis called development of the “baseline.” MITRE systems engineers help establish the baseline\nusing techniques like federated architectures (see the SEG article, “Architectures Federation”)\nwhere the baseline and subsequent evolution of the proposed portfolio can be captured.\n\n**Adequacy of the options. A robust set of options allows key insights into the trade-offs**\nand their drivers to address portfolio offset drills and changes in funds. Various levels of the\noptions may be addressed, including alternate acquisition strategies, different risk profiles,\nand different cost-effectiveness profiles. MITRE SEs can help the customers understand the\npros and cons of each option, including feasibility, risk, performance, cost, and schedule\nconsiderations.\n\n###### Control Tools and Techniques\n\n**It’s more than technology. The business and programmatic aspects (including cost, acquisi­**\ntion strategies, business models, and risk) of the entire portfolio and its components are as\nimportant as the technical aspects.\n\n**How to buy is as important as what to buy. Defining options within the portfolio should**\ninclude consideration of how the option should be acquired with consideration of timing and\ncost. MITRE systems engineers typically help acquisition organizations with strategies and\nmethods and help them extend and apply this knowledge at the enterprise and its portfolio\nlevel.\n\n\n-----\n\n**Establish an integrated master schedule (IMS). An IMS with a life-cycle view is essen­**\ntial to reflect key technical and programmatic interdependencies of portfolio components and\nallows for focus on synchronization and integration points. System integration analyses can\nbe performed by MITRE systems engineers to determine the schedule dependencies, impacts\nof slippages, ability to synchronize with other efforts, and assessment of when the portfolio\nneeds to be re-assessed based upon schedule challenges. See the articles “Identify and Assess\nIntegration and Interoperability (I&I) Challenges” and “Develop and Evaluate Integration and\nInteroperability (I&I) Solution Strategies” for insights on systems integration.\n\n###### Evaluation Tools and Techniques\n\n**Establish outcomes for the portfolio and appropriate metrics to monitor progress. Metrics**\nare difficult to establish, in part because they must reflect common recognition of outcomes\nacross the portfolio. But they are critical to measuring and tracking efficiency and effective­\nness. MITRE systems engineers can help formulate metrics through knowledge of the enter­\nprise’s operational concepts, needs, requirements, mission accomplishment and assurance,\nand the operational and technical trade-offs for these needs.\n\n**What’s the value proposition? Each investment, program, or resource used must deter­**\nmine its mission as well as how it supports the outcomes/products of the portfolio in which\nit resides. The cost and funding profile, effectiveness, timeliness of delivery, and risks of each\ncomponent in relation to the portfolio must be understood. MITRE systems engineers usually\nfocus on the results of getting capabilities to the end users to conduct this mission. Having\nthis knowledge and emphasis and encouraging this perspective across the portfolio stakehold­\ners will help keep the emphasis on the users’ value.\n\n###### Issues and Challenges Impacting Successful Portfolio Management\n\nMITRE system engineers should understand the big picture when it comes to portfolio\nmanagement and ensure that appropriate perspectives, information, analysis, and tools are\nbrought to bear on the issues.\n\nMITRE systems engineers should understand where the system, system of systems,\nenterprise, and organization they support fits in the relevant portfolio(s); how it impacts or is\nimpacted by the portfolio investment decisions and its elements; what overarching outcomes\nneed to be supported/achieved and why; and the statutory, regulatory, and policy environ­\nment affecting the portfolio decisions for the enterprise. MITRE systems engineers should\nensure that appropriate analytics, tools, data sets, and strategies are brought to bear and that\nappropriate consideration is paid to stakeholders.\n\n\n-----\n\nThe issues related to portfolio management include:\n**Dueling Directives. There may be multiple directives within portfolio management that**\nmust be understood in the context of your program or portfolio. These directives may have\ncome from various governing organizations and have conflicting and inconsistent guidance\nthat is difficult to apply to the portfolio assessment. Knowing how your program, portfolio,\nand/or organization fits into one or more of these structures should help identify these dis­\nconnects and support your work to ensure progress and appropriate accountability. MITRE\nsystems engineers can help highlight the inconsistencies and work with the responsible orga­\nnizations to provide clear and consistent guidance and governance to the whole enterprise.\n\n**Multiple Taxonomies. For many government organizations/agencies, there may be mul­**\ntiple taxonomies that define the portfolio structure. Typically, a single taxonomy has not been\nadopted, nor has an approach been developed to allow the taxonomies to be used together\neffectively to support the goals of portfolio management. Given this, the MITRE systems engi­\nneer may need to map across multiple taxonomies to correlate equivalent or similar perspec­\ntives/areas of interest.\n\n**Budget Authority. Budget authority may not rest with the portfolio manager, making the**\nportfolio manager more of an advisory resource than a decision maker. The Clinger-Cohen\nAct suggests IT budget authority rests with the Secretary, the CFO, and the CIO of the particu­\nlar federal department. In the Department of Defense, for example, budget authority gener­\nally resides with the Military Services (Title X) and not the capability portfolio managers. In\ncases where the portfolio manager also has budget authority, many times the execution of the\ninvestment plan can be streamlined. In cases where the responsibilities are in separate organi­\nzations, MITRE staff can help the portfolio managers create a persuasive case for the preferred\nportfolio and highlight the value/cost-benefits of applying the portfolio resources needed.\n\n**Budget Process. In many government agencies, budgets are planned for and executed at a**\nlower level than a portfolio (e.g., program, program element, budget line, appropriation). This\nadds complexity, making portfolio management execution more difficult because the change\nrecommendations may be at a more granular level than is reflected in budgetary accounting.\nMITRE systems engineers can maintain the investment profile using the detailed, data-driven\nanalyses previously described and performing sensitivity analyses of changes to the individ­\nual components that comprise the portfolio.\n\n**Culture. Portfolio management is a “greater good,” or enterprise process, and is not sup­**\nported within a program acquisition culture rewarded for individual program success rather\nthan enterprise success. This is partly because it takes up-front investment to achieve a longer\nterm “greater good” outcome. In addition, the mission success or portfolio savings benefits\nfrom portfolio changes are not adequately accounted for or attributed to the portfolio changes,\nmaking change a difficult proposition. MITRE systems engineers can demonstrate the greater\n\n\n-----\n\ngood by presenting the value that the portfolio choice provides to the enterprise’s concepts/\nneeds, federated architecture, business planning, collective integration, design and develop­\nment activities, and life-cycle cost.\n\n**Political Factors. Politics has consistently been an element of investment decision mak­**\ning, as have operational, economic, technical, and other factors. There may be cases where\ncertain solutions are technically elegant and affordable but not politically feasible. It is impor­\ntant for MITRE to ensure consideration of all factors within the decision-space and to under­\nstand the implications of each. Substantive and validated analysis can illuminate whether an\ninvestment supports desired portfolio outcomes.\n\n**“Pet Rocks.” In some cases, particular solutions may be favored by leadership. This may**\nresult in a less valuable investment being selected, which can undermine the ability to secure\nthe most cost-effective portfolio. Analysis can and should inform an understanding of the\n“value proposition” of investments; however, it may be trumped by leadership preferences.\nMITRE SEs may recommend a solution that is not acted on for reasons outside of their control.\nWhen these situations arise, MITRE SEs should continue to highlight the risk and to provide\nan independent view of the situation while helping the government plan and execute their\nselected alternative.\n\n**Poor Life-Cycle Cost Analysis. Cost overruns are rampant in the government. This is par­**\ntially due to the low levels of confidence inherent in the original cost estimates of the individ­\nual programs. Portfolio management further complicates these already “narrow” cost analyses\nby altering the foundational assumptions in the cost estimates. For example, a new innovation\nor a new approach to capability delivery may affect the development or sustainment costs of\nentire suites of investments. Integrating the effects of portfolio changes on the initial projected\nlife-cycle cost estimate is confounded by flaws in the original cost estimates and by their\ninability to accommodate the PfM innovation. See the SEG article “Life-Cycle Cost Estimation”\nfor practices on cost estimating that could be evaluated for use across portfolios.\n\n###### References and Resources\n\nGovernment Guidance, Policies, Regulations\n\n40 U.S.C. 1401 et seq, Clinger-Cohen Act of 1996, Division E, National Defense Authorization\nAct for FY1996.\n\n[Air Force Instruction 33-141, Air Force Information Technology Portfolio Management and IT](http://www.e-publishing.af.mil/)\n[Investment Review, December 23, 2008.](http://www.e-publishing.af.mil/)\n\nCJCSI 8410.01, 22 Jun 07, Warfighting Mission Area Information Technology Portfolio\nManagement and Net-Centric Data Sharing.\n\n\n-----\n\n_Department of Defense Chief Information Officer Desk Reference, August 2006, Foundation_\nDocuments, Vol. 1.\n\nDepartment of Defense Directive 7045.20 USD(P), September 25, 2008, Capability Portfolio\n_Management._\n\nDepartment of Defense Directive 8115.01, October 10, 2005, Information Technology Portfolio\n_Management, ASD(NII)/DoD CIO._\n\nDepartment of Defense Directive 8115.02, October 10, 2005, Information Technology Portfolio\n_Management Implementation, ASD(NII)/DoD CIO._\n\nFederal CIO, Capital Planning, and IT Management Committee, Smart Practices\nSubcommittee, Performance Management Area, A Summary of First Practices and Lessons\n_Learned in IT Portfolio Management, 2002._\n\nGAO-04-394G, Information Technology Investment Management, A Framework for\n_Assessing and Improving Process Maturity, March 2004, U.S. Government Accounting (now_\nAccountability) Office, Executive Guide, Ver. 1.1.\n\nGAO-07-388, An Integrated Portfolio Management Approach to Weapon Systems Investment\n_Could Improve DoD’s Acquisition Outcomes, March 2007, U.S. Government Accountability_\nOffice, Report to the Committee on Armed Services, U.S. Senate.\n\n[Health and Human Services Policy for IT – CPIC.](http://www.hhs.gov/ocio/policy/2005-0005.001.html)\n\n_Net-Centric Functional Capabilities Board Endorsement of Capability Delivery Increments_\n_(CDI) Document, June 22, 2009, The Joint Staff, Memo._\n\n[OMB Circular A-11, August 2009, Preparation, Submission, and Execution of the Budget;](http://www.whitehouse.gov/sites/default/files/omb/assets/a11_current_year/a_11_2009.pdf)\n[Exhibit 53, Agency IT Investment Portfolio.](http://www.whitehouse.gov/sites/default/files/omb/assets/a11_current_year/s53.pdf)\n\nOSD Memorandum, 12 January 09 (predecessor: February 14, 2008), Joint Capability Areas.\n\nJournal Articles, Technical Notes, White Papers\n\n[Enterprise Scale Portfolio Analysis at the National Oceanic and Atmospheric Administration](http://portal.acm.org/citation.cfm?id=1543229)\n[(NOAA).](http://portal.acm.org/citation.cfm?id=1543229)\n\nMarkowitz, H., March 1952, “Portfolio Selection,” The Journal of Finance, Vol. VII, No.1.\n\n\n-----\n\nArticles, Books\n\n[Bhadra, D., and F. Morser, 26–28 September 2005, “Analysis of System-wide Investment in the](http://www.mitre.org/work/tech_papers/tech_papers_06/05_1151/05_1151.pdf)\n[National Airspace: A Portfolio Analytical Framework and an Example,” American Institute of](http://www.mitre.org/work/tech_papers/tech_papers_06/05_1151/05_1151.pdf)\nAeronautics and Astronautics, 5th ATIO/AIAA Conference.\n\nThe MITRE Corporation, Integrated Architecture-Based Portfolio Investment Strategies, techni­\ncal paper.\n\nMaizlish, B., and R. Handler, 2005, IT Portfolio Management Step-By-Step, John Wiley & Sons,\nInc., Hoboken, NJ.\n\n[Moynihan R., Investment Analysis using the Portfolio Analysis Machine (PALMA) Tool.](http://www.mitre.org/work/tech_papers/tech_papers_05/05_0848/05_0848.pdf)\n\nProgram Formulation and Project Planning,” MITRE Project Leadership Handbook, The\nMITRE Corporation.\n\nSanwal, A., April 2007, Optimizing Corporate Portfolio Management: Aligning Investment\n_Proposals with Organizational Strategy, John Wiley & Sons, Inc., Hoboken, NJ._\n\nChecklists, Toolkits\n\n[Corporate Executive Board, Portfolio Management.](http://www.executiveboard.com/information-technology/cio-executive-board/research-library/it-governance/portfolio-management/index.html)\n\n[DAU IT Portfolio Management Community.](https://acc.dau.mil/CommunityBrowser.aspx?id=102095)\n\n\n-----\n\nDefinition: The GAO defines\n\n_performance measurement as_\n\n_the ongoing monitoring and_\n\n_reporting of program accom­_\n\n_plishments, particularly progress_\n\n_toward pre-established goals,_\n\n_typically conducted by program_\n\n_or agency management. They_\n\n_may address the type or level_\n\n_of program activities conducted_\n\n_(process), the direct products_\n\n_and services delivered (outputs),_\n\n_or the results of those products_\n\n_and services (outcomes). A_\n\n_program may be any activity,_\n\n_project, function, or policy with_\n\n_an identifiable purpose or set of_\n\n_objectives._\n\nKeywords: evaluation,\n\n_Government Performance_\n\n_and Results Act, logic model,_\n\n_measurement, outcome mea­_\n\n_sures, outcomes, performance_\n\n_management, performance_\n\n_measurement, performance ref­_\n\n_erence model, strategic planning_\n\n\nENTERPRISE PLANNING AND MANAGEMENT\n###### How to Develop a Measurement Capability\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to under­\n\nstand the general principles and best practices\n\nof performance measurement methods and\n\nsystems. They are expected to assist spon­\n\nsors in developing a measurement capability in\n\nthe systems acquisition and/or the operational\n\norganization. They assist in collecting and using\n\nperformance measures to assess progress\n\ntoward achieving strategic goals and objectives\n\nand to inform decisions about resource allocation.\n\n\n-----\n\n###### Background\n\nCongress required performance measures of all federal agencies starting in 1999. The legisla­\ntion containing those requirements is the Government Performance and Results Act (GPRA),\npassed in 1993. The only federal legislation that requires strategic planning and performance\nmeasurement, GPRA requires each agency to develop a five-year strategic plan (to be updated\nat least every three years), an annual performance plan, and an annual performance report.\nIn specifying what must be included in those documents, GPRA requires that a strategic plan\nmust show the link between strategic goals and performance goals. A strategic plan must also\ncontain an evaluation plan that includes performance measures.\n\nGPRA designates the Office of Management and Budget (OMB) as the agency respon­\nsible for executing GPRA. OMB’s process for evaluating agencies is called the Performance\nAssessment Rating Tool (PART). Two of the four sections of PART examine performance\nmeasures. Although PART is likely to change somewhat, the current Administration has\nannounced that the fundamentals of that process will remain unchanged. Agencies must\nreport performance, showing results (the “R” in GPRA). The Administration is increasing its\nemphasis on evaluation, which is a way to make sure that what matters is measured, that\nwhat is measured is really what is intended to be measured, and that results reported are\ncredible.\n\nCongress and the Administration are placing increased emphasis on performance and\nresults for a simple reason: it is the best solution for achieving success when money is tight.\nUnless performance improves, there are basically three other, highly unpopular directions:\n(a) raise taxes, (b) cut programs, or (c) increase debt. MITRE can expect to see more requests\nfor assistance with performance.\n\n###### The Single Most Common Problem (and Its Solution)\n\nAt MITRE we are often asked to develop performance measures for a government program or\nother initiative. The most common problem about program performance cited in research—and\nthat we have seen at MITRE—is that the program’s goals/objectives have not been identified. It\n_is impossible to develop measures of progress if we do not know where we are trying to go._\n\nThe first step in developing measures of progress is to identify the desired end point\nor goal. One of the most useful tools for identifying goals, and for developing performance\nmeasures, is the logic model. A logic model is a map, a one-page bridge between planning and\nperformance.\n\nThe logic model shown in Figure 1 should be read from left to right.\n\n###### �The problem that the program was created to solve is identified on the left. �The agency’s strategic priorities—its goals/objectives—are next and should directly\n\nrelate to the problem.\n\n\n-----\n\n###### �The next three columns are basic input-process-output. Inputs are people, funding, and\n\nother resources. Outputs are results of processes or activities. Output measures answer\nthe question: “How do you know they really did that?” Outputs are usually expressed in\nnumbers of units produced or units of service provided.\nOutcomes are all about impact. They are answers to the question: “So what?” What dif­\nference did your product or service make? An initial outcome is softer, usually near-term,\nand might be measured by before/after tests of understanding if a training service were the\noutput. Customer satisfaction is a common short-term outcome measure. An intermediate out­\ncome might include changes in behavior, and it might be measured by finding out how many\nof those who received training are actually using their new skills. (Note: Often, short-term and\nintermediate outcomes are combined as intermediate outcomes.) Long-term outcomes are the\nconditions the program/agency is trying to change and should be a mirror image of the prob­\nlem on the left of the logic model. Thus measures of long-term outcomes can be relatively easy\nto identify. A program established to address the problem of homelessness among veterans, for\nexample, would have an outcome measure that looks at the number and percent of veterans\nwho are homeless. (Defining “homeless” may be a separate issue to be addressed in a later\nimplementation of data collection and reporting.)\n\nSituation Priority\nAnalysis Setting Inputs Activities Outputs Outcomes Impacts\n\nProblem Mission Resources Work conduc- Products Changes in individuals, groups,\nidentification goals and ted to achieve and systems, and communities.\nObjectives contributions objectives services Outcomes may be intended or\n\ndelivered unintended.\n\nActivities are Initial Intermediate Long-term\ndirectly linked Learning Action Conditions\nto outputs Awareness Behavior Social\n\nKnowledge Practice Economic\nAttitude Policies Civic\nSkills Social action Environment\nOpinions Decision\nAspirations making\nMotivations\n\nEnvironment: External and contextual factors that influence the program\n\n_Sources: GAO-02-923, Strategies for Assessing How Information Dissemination Contributes to Agency Goals._\n_GAO/GGD-00-10, Managing for Results: Strengthening Regulatory Agencies’ Performance Management,_\n_Ellen Taylor-Powell, 2000. A Logic Model: A Program Performance Framework, University of Wisconsin,_\n_Cooperative Extension Program Evaluation Conference._\n\nFigure 1. Defining Performance Measures with Logic Models\n\n\n-----\n\nEnvironmental factors can influence all stages of a program and need to be identified in\nagencies’ strategic plans.\n\nThe extreme left and extreme right of the model are easiest to define. The hard part is to\ndevelop measures for outputs (although those are easiest) and especially for outcomes. How\nwould you know you are making progress toward achieving your long-term goal before you\nget to that goal? What would tell you that you are on the right or wrong track? How would\nyou know whether you need to make course corrections to get to the destination you want?\nIn developing outcome measures, keep asking like a four-year-old child, “Why? ... Why? ...\nWhy?”\n\nThe further away from outputs you measure, the more likely that conditions outside the\nagency’s/program’s control are affecting the results observed. Factors such as the economy or\nthe weather can affect long-term outcomes. And that is where third-party evaluation can be\nhelpful to analyze the performance data, as well as other quantitative and qualitative informa­\ntion, to assess the impact of the agency/program on the outcomes.\n\nThe benefits of using a logic model are numerous:\n\n###### �It is the strategic plan on a page. The measures can be derived directly from a logic\n\nmodel.\n###### �A logic model can be a highly effective tool for communicating with stakeholders and\n\nfor making sure that the activities, outputs, and outcomes are accurate in terms of their\nmission and business. Program people seem to “get it,” and they help refine the model\nvery quickly.\n###### �It makes the connection between inputs, activities, outputs, and outcomes transparent\n\nand traceable.\n###### �Most important, it shows in a nutshell where you want to go.\n\n Related Problems and Pitfalls\n\n**Clients tend to focus on outputs, not outcomes. Output measures are much easier, and they**\nare under the agency’s control. People know what they do, and they are used to measuring\nit. “I produced 2,500 widgets last year” or “On the average we provided two-second turn­\naround time.” They can find it harder to answer the question: “So what?” They are not used\nto looking at the outcomes, or impact, of what they do. We need to keep asking “So what?” or\n“Why?” Move toward what would show impact or progress toward solving the problem the\nagency, program, or project was created to address.\n\n**Goals and objectives are often lofty, and not really measurable. “The goal is to conduct**\nthe best census ever.” How do you measure that? Make the goals concrete enough that we can\nknow whether they have been achieved and whether we are making progress toward them.\n\n\n-----\n\n**There are tons of reports with measures that are ignored; no one knows how to use**\n**them. There is no plan to actually use the measures to make decisions about resource alloca­**\ntion. This is where agencies need to move from performance measurement to performance\nmanagement, using the performance data to make resource allocation decisions based on\ncredible evidence and including evaluations, analysis of agency culture, new directives from\nCongress or higher levels of the Administration, etc.\n\n**The client wants to use measures that they already produce, regardless of whether**\n**those are actually useful, meaningful, or important. This is the argument that “we already**\nreport performance data and have been doing it for years.” These are probably outputs, not\noutcomes, and even so, they need to be reviewed in light of the strategic goals/objectives to\ndetermine whether they show progress toward achieving end outcomes.\n\n**They want to identify a budget as an output or outcome. A budget is always an input.**\nJust don’t let the conversation go there.\n\n###### Best Practices and Lessons Learned\n\n\nYou need clear goals/objectives to even begin\n\nto start developing performance measures.\n\nWithout clear goals, you can only measure activi­\n\nties and outputs. You can show, for example, how\n\nmany steps travelers have taken along a path, how\n\nmuch food was consumed, and how long they\n\nhave been traveling. But you cannot know whether\n\nthey are any nearer their destination unless you\n\nknow the destination. They might be walking in\n\ncircles.\n\nPerformance measures are derived from stra­\n\ntegic plans. If the agency does not have a plan,\n\nit needs to develop one. There is much guidance\n\nand many examples on developing a plan.\n\nComplete a logic model for the whole program.\n\nYou can develop outcomes or measures as you go\n\nor wait until the end, but the measures help keep\n\nthe goals/objectives and outcomes real.\n\n\nTo the maximum extent possible, ground\n\nthe logic model in bedrock. Bedrock includes\n\nthe following, in the priority listed: Legislation,\n\nCongressional committee reports, executive\n\norders, regulations, agency policies, and agency\n\nguidance. Legislation is gold. The Constitution\n\nis platinum (e.g., the requirement for a decennial\n\ncensus).\n\nLong-term outcomes, or impact, are relatively\n\nstraightforward to identify. It should reflect the\n\nproblem that the program, agency, or project was\n\ncreated to solve. That is what you are trying to\n\nmeasure progress toward. If your program was\n\ncreated to address the problem of homeless­\n\nness, the long-term outcome is a reduction of\n\nhomelessness, regardless of how you decide to\n\nmeasure it.\n\nUse caution in interpreting what the measures\n\nshow. Performance measures tell you what is\n\n\n-----\n\nhappening; they do not tell you why something is\n\nhappening. You need to plan for periodic evalu­\n\nations to get at causality. It is possible that your\n\nprogram kept things from being worse than they\n\nappear or that the results measured might have\n\nhappened even without your program.\n\nFewer is better; avoid a shotgun approach to\n\ncreating measures. Agencies tend to want to\n\nmeasure everything they do rather than focus on\n\nthe most important few things. Goals might need\n\nto be prioritized to emphasize the most important\n\nthings to measure.\n\nLook at similar agencies and programs for\n\nexamples of performance measures. Two types\n\nof outcomes are particularly difficult to measure:\n\n(a) prevention and (b) research and development.\n\nHow do you measure what did not happen, and\n\nhow do you measure what might be experimental\n\nwith a limited scope? The solution for the first is\n\nto find a proxy, and the best place to look might\n\nbe at similar programs in other agencies. The\n\nDepartment of Health and Human Services does\n\na lot of prevention work and is a good place to\n\nlook for examples. The solution to the second\n\noften takes the form of simply finding out whether\n\nanyone anywhere is using the results of the\n\nresearch.\n\nThe organization responsible for an agency’s\n\nperformance should be closely aligned with\n\nthe organization responsible for its strategic\n\n###### References and Resources\n\n\nplanning. Otherwise, strategic plans and/\n\nor performance reports tend to be ignored.\n\nPerformance management operationalizes an\n\norganization’s strategic plan.\n\nMore frequent reporting tends to be better\n\nthan less frequent. Agencies often have a hard\n\ntime getting their performance reports done on\n\nan annual basis, and the data is so out of date that\n\nit is not helpful for resource allocation decisions.\n\nThe current OMB Director is calling for perfor­\n\nmance reporting more often than weekly, which\n\nseems daunting for agencies that have trouble\n\nreporting annually, but it could actually become\n\neasier if processes are put in place to streamline\n\nreporting the few most important data items. This\n\nfrequency is already being required for reporting\n\nunder the Recovery Act.\n\nEfficiency is output divided by input; effective­\n\nness is outcome divided by input. You need\n\na denominator in both cases to achieve these\n\ncommon measures of program performance.\n\nEfficiency is about producing more output with\n\nless input, but efficient does not always mean\n\neffective. Effectiveness is about results and\n\ntherefore uses outcome measures. The logic\n\nmodel helps show clearly those relationships.\n\nSee the SEG articles “Earned Value Management”\n\nand “Acquisition Management Metrics” in the\n\nAcquisition Systems Engineering section for\n\nrelated information.\n\n\n[1. Advanced Performance Institute, Strategic Performance Management in Government and](http://www.ap-institute.com/)\n\n_Public Sector Organizations._\n\n\n-----\n\n2. MITRE-supported performance measurement products:\n\nU.S. Census Bureau, August 18, 2009, Strategic Plan for the 2020 Census, Ver. 1.0.\nUnited States Visitor and Immigrant Status Indicator Technology Program, April 16, 2009,\nUS-VISIT Strategic Plan 2009–2013.\n\n[3. Steinhardt, B., July 24, 2008, Government Performance: Lessons Learned for the Next](http://www.gao.gov/new.items/d081026t.pdf)\n\n[Administration on Using Performance Information to Improve Results, Testimony](http://www.gao.gov/new.items/d081026t.pdf)\n(statement) before the Subcommittee on Federal Financial Management, Government\nInformation, Federal Services, and International Security, Committee on Homeland\nSecurity and Governmental Affairs, U.S. Senate, U.S Government Accountability Office,\nGAO-08-1026T.\n\n[4. The MITRE Institute, September 1, 2007, MITRE Systems Engineering (SE) Competency](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n\n[Model, Ver. 1, p. 46.](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n\n[5. U.S. Commodity Futures Trading Commission, Commodity Futures Trading Commission](http://www.cftc.gov/reports/strategicplan/2012/)\n\n[Strategic Plan 2007-2012, accessed February 24, 2010.](http://www.cftc.gov/reports/strategicplan/2012/)\n\n6. U.S. Government Accounting (now Accountability) Office/General Government\n\n[Division, May 1997, Agencies’ Strategic Plans Under GPRA: Key Questions to Facilitate](http://govinfo.library.unt.edu/npr/library/gao/gpraqu.pdf)\n[Congressional Review, Ver. 1, GAO/GGD-l0.l.16.](http://govinfo.library.unt.edu/npr/library/gao/gpraqu.pdf)\n\n7. [U.S. Government Accountability Office, May 2003, Program Evaluation: An Evaluation](http://www.gao.gov/new.items/d03454.pdf)\n[Culture and Collaborative Partnerships Help Build Agency Capacity, GAO-03-454.](http://www.gao.gov/new.items/d03454.pdf)\n\n[8. U.S. Government Accountability Office, May 2005, Performance Measurement and](http://www.gao.gov/new.items/d05739sp.pdf)\n\n[Evaluation: Definitions and Relationships, GAO-05-739SP.](http://www.gao.gov/new.items/d05739sp.pdf)\n\n[9. W.K. Kellogg Foundation Logic Model Development Guide.](http://www.wkkf.org/knowledge-center/resources/2006/02/WK-Kellogg-Foundation-Logic-Model-Development-Guide.aspx)\n\n\n-----\n\n##### Enterprise Technology, Information, and Infrastructure\n\nDefinition: Enterprise technology, information, and infrastructure refers to infor­\n\n_mation technology (IT) resources and data shared across an enterprise—at least_\n\n_across a sponsor’s organization, but also cross-organizational (multi-agency,_\n\n_Joint/DoD). It includes such efforts as infrastructure engineering for building, man­_\n\n_aging, and evolving shared IT; IT or infrastructure operations for administering and_\n\n_monitoring the performance of IT services provided to the enterprise; IT services_\n\n_management; and information services management. IT strategy and portfolio_\n\n_management and IT governance help the concept function effectively._\n\nKeywords: information and data management, IT infrastructure, IT service man­\n\n_agement, service management_\n\n###### Context\n\nFormer U.S. Chief Information Officer (CIO) Vivek Kundra’s February\n\n2011 paper on Federal Cloud Computing [1] states: “Cloud computing\n\ndescribes a broad movement to treat IT services as a commodity with\n\nthe ability to dynamically increase or decrease capacity to match usage\n\nneeds. By leveraging shared infrastructure and economies of scale,\n\ncloud computing presents federal leadership with a compelling business\n\nmodel. It allows users to control the computing services they access,\n\nwhile sharing the investment in the underlying IT resources among\n\nconsumers. When the computing resources are provided by another\n\n\n-----\n\norganization over a wide-area network, cloud computing is similar to an electric power utility.\nThe providers benefit from economies of scale, which in turn enables them to lower indi­\nvidual usage costs and centralize infrastructure costs. Users pay for what they consume, can\nincrease or decrease their usage, and leverage the shared underlying resources. With a cloud\ncomputing approach, a cloud customer can spend less time managing complex IT resources\nand more time investing in core mission work.”\n\nDespite this endorsement, IT management and the associated shift toward common,\nshared resources are large concerns for many of our sponsors. MITRE system engineers (SEs)\nare increasingly supporting sponsors who are in the process of procuring new IT systems,\nmigrating existing IT-based systems to a common or shared infrastructure, or upgrading\ntheir own internal business systems. Although most aspects of this shift are technical, we are\nrecognizing that many are non-technical, and our systems engineering skills need to expand\nto address those aspects (i.e., governance, increased complexity of sharing resources across\norganizations, data ownership, service management. and life cycle).\n\nIn addition, at the center of this shift are data (or information) and the need to share it\nappropriately. Data is the “life blood” of an organization—as it flows among systems, data­\nbases, processes, and people, it carries with it the ability to make the organization smarter\nand more effective. The migration toward shared IT resources needs to accommodate the\nintended business operations being supported as well as the data usage, including appropriate\naccess control and protection.\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers are expected to understand the systems engineering principles\nto be applied to the enterprise-level IT programs they support. They are also expected to\nunderstand the larger enterprise context in which the programs operate. For a particular\nenterprise-level program, MITRE may be asked to play a role in helping the customer define\nor refine business processes, such as technical or systems engineering aspects of portfolio\nmanagement, or operational constructs for shared infrastructure. For mid- and senior-level\nMITRE staff, the role often involves recommending how to apply engineering analysis, advice,\nprocesses, and resources to achieve desired portfolio-level outcomes. Understanding the\ninterconnections and dynamics across the different levels of an organization or multi-agency\ngovernance structure is important to providing thoughtful, balanced recommendations.\n\nEnterprise-level efforts require many skills. MITRE SEs may be expected to support\nenterprise architecture, technical evolution, preliminary design of data centers or infra­\nstructure components, implementation, monitoring and operations of infrastructure, and\ntechnical governance. Critical areas of focus normally include information assurance, data\nstrategy, interoperability, application integration, information exchange, networks, and\n\n\n-----\n\ncommunications services (voice, video, and data). MITRE SEs may assist sponsors with ini­\ntiatives for application migrations, infrastructure upgrades, and consolidation of computing\ninfrastructure. Other skills involve quantifying the performance across enterprise resources\nand enabling service-level agreements. In cases where deep, focused technical knowledge is\nrequired, MITRE SEs must to be able to identify the need and bring in the required skills to\nmatch the challenge at hand.\n\n###### Best Practices and Lessons Learned\n\n\nIn complex environments such as enterprise\nlevel IT programs, three important factors should\n\nbe taken into consideration: the stakeholders,\n\nthe technology, and the mission the IT supports.\n\nFailure in even one of these factors can cause\n\ntotal program failure.\n\nKnow the stakeholders. An “enterprise” usually\n\ninvolves a set of constituents with various goals,\n\nrequirements, and resources. Sometimes, these\n\nconstituents’ considerations are at odds with\n\none another. Vital elements of the non-technical\n\naspects of enterprise IT are understanding the\n\nvarious stakeholders and being able to articulate\n\nneeds from their perspective. Several methods\n\nexist for analyzing stakeholders. For instance, a\n\nsimple POET (Political, Operational, Economic,\n\nTechnical) analysis can be used to clearly articu­\n\nlate issues associated with stakeholders (see\n\nthe “Stakeholder Assessment and Management”\n\narticle). Understanding the kind of governance\n\nrequired to make an enterprise function is also\n\nnecessary (see the “IT Governance” article).\n\nGovernance is what enables the stakeholders to\n\ncommunicate their needs and participate in the\n\nenterprise definition, evolution, and operation. The\n\nneed for strong governance cannot be overstated.\n\n\nKnow the technology. A wide array of technology\n\nis associated with enterprise IT programs, from\n\nnetworking details to cloud computing and data\n\ncenters. Keeping abreast of the current trends in\n\nthe appropriate areas of your IT program allows\n\nyou to address disruptive technology concerns\n\nand apply sound technical practice to the job.\n\nBecause computing is so prevalent in today’s\n\nsociety and it takes many forms from desktop PCs\n\nto handheld mobile devices, everyone touches\n\ntechnology and has expectations from it—often\n\nunrealistic. Our sponsors and other program\n\nstakeholders are no different. The key to manag­\n\ning technical expectations is knowing the technol­\n\nogy and its applicability and having the trust of the\n\nsponsor so you can help them recognize when\n\nsomething is too immature for implementation\n\nand not a shrink-wrapped, off-the-shelf bargain.\n\nIn addition to knowing the technology itself is\n\nknowing how to apply good IT management\n\ntechniques. IT service efforts have frameworks\n\nand best practices to leverage. A fairly complete\n\nand commonly referenced framework is the\n\nInformation Technology Service Management\n\nand Information Technology Infrastructure Library\n\n(ITIL) [2]. The “IT Service Management (ITSM)”\n\narticle details this further. In addition, NIST [3]\n\nprovides many useful references for IT, cloud\n\n\n-----\n\ncomputing, security, and the Federal Information\n\nSecurity Management Act.\n\nKnow the mission being supported. It is very\n\nimportant to understand the mission(s) that the\n\ninfrastructure supports. The ability to articulate\n\nthe technical implications of mission needs is\n\narguably the most valuable systems engineering\n\ntalent to bring to bear on customer programs.\n\nEnterprise technology succeeds by anticipating\n\n###### Articles Under This Topic\n\n\nend-user needs and proactively addressing\n\nthem, not waiting for breakage or unhappy users\n\nto complain that they are not being supported.\n\nThis is a complex and difficult thing to do for an\n\nenterprise, but it is necessary as computing and\n\ninfrastructure become more commoditized. The\n\nEngineering Information-Intensive Enterprises\n\nsection of this guide addresses ways to sup­\n\nport the mission through enterprise systems\n\nengineering.\n\n\n“IT Infrastructure Engineering” provides insight into the complexities of developing, manag­\ning, and operating IT infrastructure (networks and communications equipment, data centers,\nshared computing platforms, etc.) within an enterprise environment.\n\n“IT Service Management (ITSM)” describes frameworks, processes, and models that\naddress best practices in managing, supporting, and delivering IT services.\n\n“Information and Data Management” includes best practices and lessons learned for engi­\nneering enterprise data and information.\n\n“Radio Frequency Spectrum Management” discusses the analytical, procedural, and\npolicy approaches to planning and managing the use of the electromagnetic spectrum.\n\n###### References and Resources\n\n1. Kundra, V., U.S. Chief Information Officer, Federal Cloud Computing Strategy, February 8,\n\n2011.\n\n2. IT Infrastructure Library (ITIL) OGC website, http://www.itil-officialsite.com/AboutITIL/\n\nWhatisITIL.aspx.\n\n3. National Institute for Standards and Technology (NIST), www.nist.gov.\n\n\n-----\n\nDefinitions: Infrastructure engi­\n\n_neering builds, manages, and_\n\n_evolves the environment sup­_\n\n_porting the processes, physical_\n\n_and human resources needed_\n\n_to develop, operate, and sustain_\n\n_IT. Infrastructure operations_\n\n_address day-to-day manage­_\n\n_ment and maintenance of IT_\n\n_services, systems, and applica­_\n\n_tions—plus their infrastructures_\n\n_and facilities. Processes include_\n\n_systems and network adminis­_\n\n_tration, data center operations,_\n\n_help desks, and service-level_\n\n_management._\n\nKeywords: cloud computing,\n\n_continuity of operation, data_\n\n_center, data center operations,_\n\n_disaster recovery, end-to-end_\n\n_computing infrastructure, IT_\n\n_infrastructure, servers, service_\n\n_management, storage area_\n\n_networks, unified communica­_\n\n_tions, virtualization, wide area_\n\n_networks_\n\n\nENTERPRISE TECHNOLOGY, INFORMATION,\nAND INFRASTRUCTURE\n###### IT Infrastructure Engineering\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are challenged with the\n\nrapid changes in the emerging technology of\n\nIT infrastructure. They are expected to support\n\narchitecture, preliminary design, analysis, imple­\n\nmentation, and operations of the infrastructure.\n\nCritical areas of focus include information assur­\n\nance, data strategy, interoperability, application\n\nintegration, information exchange, networks,\n\nand communications services (voice, video, and\n\ndata). MITRE SEs assist sponsors with initia­\n\ntives for data centers, application migrations,\n\ninfrastructure architecture, and consolidation of\n\ncomputing infrastructure. MITRE SEs develop\n\ncompetencies in data center operations, infra­\n\nstructure platforms, and IT service delivery.\n\nTechnical specialties to which they should reach\n\n\n-----\n\nback include local and wide-area network design, servers, storage, backup, disaster recov­\nery, continuity of operation (COOP), performance monitoring, virtualization, cloud comput­\ning, modeling, visualization, voice over Internet protocol (VoIP), IPv6, and other emerging\ntechnologies.\n\n###### Background\n\nMITRE SEs are expected to take a total life-cycle approach to assist operational users in apply­\ning IT infrastructure, operations, maintenance, and management techniques to meet their\nchallenges.\n\nInfrastructure Engineering and the associated Operations and Service Management\nexpertise includes:\n\n###### �Implementation of Information Technology Service Management and Information\n\nTechnology Infrastructure Library (ITIL) concepts and policies (for more details, see the\narticle, “IT Service Management (ITSM)” under this topic)\n###### �Development of infrastructure strategy and IT operational policies, standards, and pro­\n\ncesses tailored to agency o-r department missions\n###### �Development of infrastructure and operational requirements in all phases of the system\n\ndevelopment life cycle\n###### �Development of asset management processes that support the provisioning, tracking,\n\nreporting, ownership, and financial status of IT assets\n###### �Data center operations, consolidations, and relocations; planning, implementing, and\n\ntesting for disaster recovery; daily operations and data center management, including\nserver and systems migrations\n###### �Service desk, help desk, and contact and call center development, implementation,\n\noperations, and process improvement\n###### �Service-level management through the development of processes, people, technology,\n\nand service-level and operating-level agreements\n###### �Technical strategy, architecture, and design incorporating emerging technologies such\n\nas virtualization, cloud and utility computing, IP telephony, and IPv6 planning and\nmigration\n###### �Infrastructure and operations security, such as network and application firewalls,\n\nauthentication, identity and privilege management, and intrusion detection and\nprevention\n###### �Beyond technical deliverables, assist with various road shows, technical exchange meet­\n\nings, and conferences to promote the importance of a solid infrastructure\n\n\n-----\n\n###### Government, Industry, and Commercial Interest in IT Infrastructure\n\nIn December 2010, the U.S. Federal Government Chief Information Officer released a 25 Point\nIT Management Reform Plan that concentrates on areas to reduce IT operating costs and to\nbring greater value through IT consolidation. The emphasis is on reducing data centers and\nmigrating to lean and agile IT computing services [1].\n\nThe National Institute of Standards and Technology (NIST) took the lead to define cloud\ncomputing in the context of cost savings and “increased IT agility.” This effort provided the\nmomentum to challenge the rising and unsustainable costs in response to “difficult economic\nconstraints.” NIST is partnering with all stakeholders (including MITRE) to face the chal­\nlenges of security, privacy, and other barriers that have hindered a broad adoption of cloudbased IT infrastructure [2, 3].\n\nThe U.S. General Services Administration (GSA) sought and adopted lightweight and\nagile IT infrastructure to support their common enterprise infrastructure (e.g., enterprise\nemail) while reducing the costs and increasing efficiency of the associated acquisition and\ndeployment. Additionally, GSA is taking a lead role in deploying Software as a Service (SaaS)\nthrough the apps.gov portal [4]. This effort emphasizes compliance with Certification and\nAccreditation and FISMA [5] Moderate Impact Data security requirements prior to loading\ntheir applications to the store for distribution.\n\n###### Best Practices and Lessons Learned\n\n\nTranslating business objectives into IT infra­\n\nstructure needs. The most difficult part of\n\ninfrastructure engineering is identifying the\n\ninfrastructure requirements implied by the spon­\n\nsor’s business objectives. Business objectives,\n\nby definition, are not technological. Deriving the\n\ntechnical requirements for the IT infrastructure\n\nneeded to support business objectives is a criti­\n\ncal technical contribution. For example, translat­\n\ning a business need for enhanced distributed\n\ncapabilities may require the development of a\n\nNetwork Design guide where the technical prin­\n\nciples for switching (e.g., VLANs, Ethernet, STP),\n\nrouting (e.g., RIP, EIGRP, OSPF, ISIS, BGP), Quality\n\nof Service (QOS), and wiring/physical infrastruc­\n\nture are mapped to the business objectives. By\n\n\ncreating such a guideline, the client is then able\n\nto make technically supported decisions to meet\n\ntheir objectives.\n\nGovernance. Because infrastructure supports\n\nthe entire range of an enterprise’s IT needs, it\n\nrequires a broad level of coordination. Every\n\ndepartment and function in the enterprise needs\n\nto be represented in the governance of the\n\ninfrastructure. Plan for significant investment\n\nof time and resources in governance boards,\n\noutreach programs. and socialization of change.\n\n(For more details on governance, see the SEG\n\narticles, “Enterprise Governance,” “IT Governance,”\n\nand “Transformation Planning and Organizational\n\nChange.”)\n\n\n-----\n\nInfrastructure evolution. Infrastructure\n\nEngineering is distinguished from other IT efforts\n\nby the almost absolute necessity of incremental\n\nevolution. It is extremely rare for an enterprise\n\nto be able to switch from one infrastructure to\n\nanother in one fell swoop. Plan and organize\n\nbased on incremental change. Provision for oper­\n\nating both old and new infrastructure compo­\n\nnents in parallel. (For more details, see the articles\n\non Configuration Management.)\n\nService-level agreements. Because the\n\ninfrastructure supports the entire enterprise,\n\nit is impractical and inappropriate to organize\n\ninterfaces around traditional interface control\n\ndocuments. Users (and potential users) of an\n\ninfrastructure or shared core function demand a\n\ndifferent kind of performance guarantee based\n\non the one-to-many relationship between the\n\nowners of the infrastructure or shared function\n\nand their customers. This guarantee is captured\n\nin a service-level agreement (SLA) that docu­\n\nments the expected performance and behav­\n\nior of the infrastructure for use. Because the\n\nSLA is, in effect, an internal contract between\n\nthe infrastructure and its users, Infrastructure\n\nEngineering must provide for precise measur­\n\ning, monitoring, and reporting of the function’s\n\nbehavior in the design and in the operation—to\n\nthe degree that the SLA can be enforced. This\n\nrequires significantly more detail and rigor than is\n\nusually applied to just developing an infrastruc­\n\nture by itself.\n\nVersioning and provisioning. Our sponsor’s\n\nenterprise is usually large, complex, and widely\n\ndistributed. As a consequence, it is virtually\n\nimpossible to change every physical instance of\n\n\nan infrastructure component at one time. Plan\n\nfor operating multiple versions of any infrastruc­\n\nture component being updated or replaced. It is\n\ncommon for a physically distributed enterprise\n\nto be operating two, three, or even four different\n\nversions of a single component at the same time.\n\nAccount for multiple versions, not just for brief\n\nperiods but continuously as the infrastructure\n\nevolves. (For more details, see the articles under\n\nConfiguration Management in the Acquisition\n\nSystems Engineering section.)\n\nBaseline infrastructure assessment. Assessing\n\nan operational environment is often a first step in\n\nan infrastructure engineering effort. The focus of\n\nthe assessment should be based on the customer\n\nneeds and requirements. Two examples are:\n\n###### � [Assess a baseline configuration of an ]\n\nexisting operational environment to use for\n\ngap analysis of an “AS-IS” versus a “TO\nBE” architecture.\n###### � [Compare a baseline configuration of an ]\n\nexisting operational environment against a\n\nsecure configuration standard for a secu­\n\nrity assessment.\n\nCommon security processes. Perform trusted,\n\nindependent vulnerability assessments to\n\nhighlight issues and help remedy and mitigate\n\nrisk based on NIST, NSA, and leading industry\n\npractices in the information assurance and secu­\n\nrity realm. Document security vulnerabilities and\n\nprovide recommendations for resolution, map­\n\nping the findings to NIST 800-53 [6] controls and\n\nproviding a risk level report. Promote a standard\n\nset of commercial tools such as NetDoctor,\n\nNessus®, or Wireshark where applicable. These\n\ntools reuse a “Findings Dictionary” to document\n\n\n-----\n\ncommon vulnerabilities and provide a consistent\n\napproach across assessors and assessment\n\norganizations—multiple systems engineers from\n\ndifferent organizations can all perform the same\n\nscience, technology, and engineering for different\n\ncustomers in the enterprise following the same\n\ndocumented processes.\n\nTechnology transition testing. Leverage the\n\neffort of industry experts by partnering with\n\naccredited test laboratories. For example,\n\npreparing for changes to computer networks to\n\nsupport the IPv6 addressing plan requires a part­\n\nnership with NIST, federal agencies, or govern­\n\nment entities, and the wide range of commercial\n\nnetwork equipment vendors. The IPv6 Transition\n\neffort is based on a “target architecture” to focus\n\non operational testing. Test planning includes\n\nimplementing a test laboratory architecture,\n\nproving out operational Dual Stack configura­\n\ntions, and identifying testing requirements for\n\npilot deployment.\n\nNext-generation network—the evolution con­\n\ntinues. Network technologies and capabilities\n\ncontinue to evolve with the continued growth of\n\nthe Internet. The current trend toward converged\n\nservices is apparent and seen across the federal\n\ngovernment. This shift requires a robust core and\n\nreliable end-to-end services at a minimum. Key\n\nnext-generation network infrastructure attributes\n\ninclude:\n\n###### � [Robust core technologies: ]\n\nyy Multiprotocol label switching\n\nyy High-end routers/switches\n###### � [Convergence: ]\n\n\nyy Voice, video, data on a single\n\ninfrastructure\n\nyy Broadband wireless access (4G/3G)\n\nyy Mobile applications and value-add ser­\n\nvices and applications are drivers\n\nyy Carrier class devices\n\nyy Network is transparent to end user\n###### � [Multi-platform, multimedia, multi-chan­]\n\nnel, multi-purpose platforms—Android,\n\nBlackberry, iPhone, iPad, and Windows\n\nplatforms\n###### � [Security-centric: Sensitive and critical ]\n\ninformation riding on a single infrastructure\n\nrequires SLA and carrier class devices/\n\nservices.\n###### � [Low cost: Economies of scale are pushing ]\n\na low-cost model approach:\n\nyy Virtualization and cloud\n\nyy Infrastructure consolidation\n\nyy Green IT\n###### � [Unified communications: More than just ]\n\nVoIP:\n\nyy Video teleconference, teleconference,\n\nvirtual meeting spaces\n\nyy E-boarding and collaboration\n\nyy Presence and mobility\n\nyy Platform and technology agnostic\n\nyy IP telephony\n\nAn efficient infrastructure. Assess cabling,\n\npower, grounding, heating, ventilation and air\n\nconditioning, raised flooring, load bearing, fire sup­\n\npression, physical access and egress (ADA com­\n\npliance). They follow applicable local codes and\n\nordinances, using the ANSI-EAI, NEMA, and NEC\n\nas references, and create recommendations for\n\n\n-----\n\nsponsors to follow based on standards. Currently,\n\n“green” initiatives cost more than standard infra­\n\nstructure build-outs; however, when life-cycle\n\ncosts can be shown to be equal (or less) based\n\non operating savings (i.e., lower electric bill due\n\nto increased efficiencies), the effort to move to a\n\ngreen infrastructure may be justified. (For more\n\ndetails, see the articles under Integrated Logistics\n\nSupport in the Acquisition Systems Engineering\n\nsection.)\n\nMobile IT management and support. Mobile IT\n\nPlatform diversity complicates IT management\n\nand help desk activities because these plat­\n\nforms are incompatible. IT departments need to\n\nrevise processes for developing applications to\n\naccommodate the new workflow and mobile data\n\nplatforms. Evolving security policies and blurred\n\nlines between the personal and professional role\n\n###### References and Resources\n\n\nof wireless devices require security approaches\n\nthat go beyond traditional firewalls. Most enter­\n\nprise infrastructure architecture mapping efforts\n\nfocus on fixed IT assets and core applications\n\nthat run on them. Mobile devices and applica­\n\ntions are often unaccounted for in future plans of\n\narchitectures. Required infrastructure engineering\n\ncapabilities include:\n\n###### � [Mobile Technology Policy/Security Devel­]\n\nopment Support\n###### � [Mobile IT System Design Support ] � [Mobile IT System Integration Support ] � [Mobile IT Change Management Support ] � [Mobile Workforce Management Support ] � [Mobile IT Performance Management ]\n\nSupport\n\n\n[1. 25 Point Implementation Plan to Reform Federal Information Technology Management.](http://www.cio.gov/documents/25-Point-Implementation-Plan-to-Reform-Federal%20IT.pdf)\n\n[2. The NIST Definition of Cloud Computing, NIST Cloud Computing Program, NIST.](http://www.nist.gov/itl/cloud/upload/cloud-def-v15.pdf)\n\n[3. Cloud-Based Infrastructure as a Service Comes to Government, GSA.](http://www.gsa.gov/portal/content/193441)\n\n[4. U.S. GSA Apps.Gov Portal, GSA.](https://www.apps.gov/cloud/main/home.do?)\n\n[5. Federal Information Security Management Act (FISMA) Implementation Project, NIST.](http://csrc.nist.gov/groups/SMA/fisma/index.html)\n\n[6. Recommended Security Controls for Federal Information Systems and Organizations, NIST.](http://csrc.nist.gov/publications/nistpubs/800-53-Rev3/sp800-53-rev3-final.pdf)\n\n###### Additional References and Resources\n\nHoskins, J., 2004, Building an On Demand Computing Environment with IBM: How to\n_Optimize your Current Infrastructure for Today and Tomorrow, Maximum Press._\n\nFoster, I., and Kesselman, C., eds., 2004, The Grid 2: Blueprint for a New Computing\n_Infrastructure, Elsevier._\n\nSasaki, R., 2005, Security and Privacy in the Age of Ubiquitous Computing, International\nFederation for Information Processing.\n\n\n-----\n\nDefinition: Information\n\n_Technology (IT) Service_\n\n_Management is a generic_\n\n_umbrella for frameworks,_\n\n_processes, and models that_\n\n_address best practices in_\n\n_managing, supporting, and_\n\n_delivering IT services. IT ser­_\n\n_vices may include (as defined_\n\n_by NIST for cloud computing):_\n\n_Software as a Service (SaaS),_\n\n_Platform as a Service (PaaS),_\n\n_and Infrastructure as a Service_\n\n_(IaaS)._\n\nKeywords: CMM, COBIT, infra­\n\n_structure services, ISO 20000,_\n\n_ITIL, ITSM, service delivery,_\n\n_service desk, service manage­_\n\n_ment, service support_\n\n\nENTERPRISE TECHNOLOGY, INFORMATION,\nAND INFRASTRUCTURE\n###### IT Service Management (ITSM)\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) supporting sponsors\n\nprocuring new IT systems, migrating existing\n\nIT-based systems to common or shared infra­\n\nstructure, or upgrading their internal business\n\nsystems must have an understanding of the IT\n\nand associated processes for control, operations,\n\nshared use, and governance. MITRE SEs develop\n\ncomprehensive programs around an ITSM frame­\n\nwork or approach or address specific issues in\n\nparticular process areas. Examples include devel­\n\noping implementation plans for migrating from\n\ndecentralized help desks to centralized service\n\ndesks, recommending process improvements to\n\nimprove overall system availability, designing end\nto-end monitoring systems, developing service\nlevel agreements, and identifying critical support\n\nfactors for service management process areas.\n\n\n-----\n\n###### Background\n\nIT service providers have several frameworks and best practices to leverage, whether they are\ncommercial providers or internal IT organizations. This article focuses on the processes and\npractices defined in the Information Technology Infrastructure Library (ITIL), by far the most\ncomprehensive and widely adopted framework for IT service management.\n\nOther related best practice frameworks for ITSM include:\n\n###### �Control Objectives for Information and related Technology (COBIT), which was devel­\n\n[oped by the ISACA IT Governance Institute.](https://www.isaca.org/)\n###### �IT Service Capability Maturity Model (CMM, CMMI, CMM Services), which provides\n\nstandards for defining process areas and measuring an organization’s level of maturity\nwithin these process areas.\n###### �ISO/IEC20000, which is an international standard, based on the ITIL Framework, for IT\n\norganizations that want formal certification of their service management processes.\nThe ITIL is a framework developed by the United Kingdom Office of Government\nCommerce (OGC). The original ITIL framework was developed in the late 1980s and was then\ndocumented in a collection of books in the 1990s known as ITIL v2. The current version of the\nframework, ITIL v3 was released in 2006.\n\n\nFinancial\nmanagement\n\n\nChange\nmanagement\n\nConfiguration\nmanagement\n\nRelease\nmanagement\n\nTransition planning & support\n\nService validation & testing\n\n\nIncident\nmanagement\n\nProblem\nmanagement\n\nAccess\nmanagement\n\nEvent\nmanagement\n\n\n7-step Process\nImprovement\n\n\nService\ndesk\n\n\nService portfolio\nmanagement\n\n\nService level\nmanagement\n\nAvailability\nmanagement\n\nCapacity\nmanagement\n\nContinuity\nmanagement\n\nInfo security\nmanagement\n\n\nDemand\nmanagement\n\n\nStrategy\noperation\n\n\nService catalog\nmanagement\n\n\nEvaluation\n\n\nRequest\nfulfillment\n\n\nSupplier\nmanagement\n\n\nKnowledge\nmanagement\n\n\nTechnical\nmanagement\n\nApplication\nmanagement\n\nIT operations\nmanagement\n\n\n**Key**\n\n\nProcess\n\n\nFunction\n\n\nFigure 1. ITIL v3 Phases and Process Areas\n\n\n-----\n\nThe ITIL framework is based on the concept that IT organizations provide services, not\ntechnologies, to business customers. The difference between a service and a technology is\nthe added value IT organizations provide to their customers by accepting and managing the\nrisk associated with that technology. In simple terms, an exchange server is a technology.\nElectronic mail or messaging is a service that includes support and management functions\nwhose details are hidden from the end user or customer. ITIL divides these support and man­\nagement functions into 30 process areas that arise through the service life cycle.\n\nAlthough the ITIL does describe the major elements of the 30 process areas that are\nconsidered to be best practice, the ITIL is not prescriptive. For example, the ITIL describes\nContinual Service Improvement processes but does not require the use of one process\nimprovement methodology over another. Practitioners can rely on Lean, Six Sigma, or other\nmethodologies for process improvement. Similarly, the ITIL allows for the use of COBIT, for\nexample, for the change management and governance related processes, and has a simi­\nlar, complementary relationship with CMM. The ITIL also provides descriptions of roles\nassociated with each of the process areas that should be considered within the governance\nstructure.\n\nThe service life cycle is divided into five phases, each with its set of process areas that\nplay critical roles during that phase (see Figure 1). Note that some of the process areas men­\ntioned in the ITIL body of knowledge are relevant throughout the service life cycle. The clas­\nsification of processes under a specific phase is only meant to demonstrate when these process\nareas are most important.\n\n###### Why Implement ITIL?\n\nThe ITIL provides a framework for viewing IT support and service processes. None of the ITIL\nprocess areas is “new” or different from the traditional IT process areas. What is different\nabout the ITIL is the acknowledgment that IT is no longer driving business decisions. To the\ncontrary, IT services have largely become a commodity. This shift has often caused IT organi­\nzations to become separated and marginalized from the business operations they support. The\nITIL framework was designed with the objective of injecting IT back into business decisions;\nthat is, the ITIL aims to reestablish the participation of IT in making business or mission deci­\nsions with the goal of delivering relevant, improved services at a reasonable cost. By involving\nIT at the beginning of the service life cycle, support and service level offerings can become a\nstandard part of every IT service.\n\nPoor-performing IT operations is often a symptom of the problem, but not the problem\nitself. IT operations receive systems from business units, applications development, systems\nengineering, and other parts of the organization. Lack of organizational processes and stan­\ndards can cause IT operations groups to have to manage every version of every platform and\n\n\n-----\n\napplication available on the market. Often this is a consequence of an organizational business\nmodel in which IT operations has no voice in the decision making for the design of systems\nthat they later own after the transition portion of the life cycle. The earlier in the design phase\nlife-cycle management (or sustainment) is built in, the more likely the overall cost and perfor­\nmance objectives can be achieved. This is a critical and often overlooked point.\n\nThe ITIL helps point to the processes that begin from the conceptualization phase of\na new system, continue through acquisition, and then move to change, configuration, and\nrelease management processes that directly impact application development and systems\nengineering teams. Most important, the processes include mission/business representatives as\nan integral part of the service development process.\n\nFinally, the ITIL stresses the importance of metrics, both in measuring the success of\nthe ITIL program itself and for measuring the performance of the IT organization in deliver­\ning customer services. Because ITIL programs are often lengthy, it is critical to demonstrate\nimprovements throughout the duration of the program.\n\n###### Best Practices and Lessons Learned\n\n\nDuring the early 2000s, ITIL became a popular\n\nframework for IT organizations to adopt, includ­\n\ning those within federal government agencies.\n\nHowever, federal government agencies are still\n\ncatching up with the private sector in implement­\n\ning ITIL.\n\nAre we there yet? Implementing an IT Services\n\nManagement framework is a lengthy process.\n\nOrganizations can expect to spend up to two\n\nyears on these efforts, even if they focus on just a\n\nsubset of the ITIL process areas. For this reason,\n\nITIL programs require senior leadership buy-in in\n\norder to be successful. Strong governance is a key\n\ncomponent of even limited success.\n\nIt’s not just about the IT. IT services manage­\n\nment extends beyond IT operations and into all\n\naspects of IT services, including acquisition plan­\n\nning, financial planning, portfolio management,\n\nand release management. Don’t make the mistake\n\n\nof focusing IT services efforts only on IT opera­\n\ntions. As noted, operational performance issues\n\nare usually the symptom, not the root cause of the\n\nproblem.\n\nAre you being served? Often IT organizations are\n\nhesitant to include representatives from outside\n\nof their organization in their IT services efforts.\n\nInstead, they focus exclusively on internal IT pro­\n\ncess improvement efforts. This misses the whole\n\npoint of IT services management, which is to view\n\nstakeholders and especially customers or users as\n\npartners. The shift toward IT’s being a commodity\n\nmeans that bringing the customers or users into\n\nthe project translates to better understanding\n\ntheir needs and level of service required.\n\nMeasuring business value. Defining metrics\n\nfor an IT services program is often overlooked.\n\nIt is not always obvious that an improvement\n\nin change management can directly impact\n\n\n-----\n\navailability of critical systems. Metrics need to\n\nbe closely tied to the strategic goals and value\n\nof the IT program, and they need to be relevant\n\nto the business or mission being supported.\n\nMetrics need to be defined, collected, and\n\n###### References and Resources\n\n\nshared throughout the program. Good sources\n\nof material on metrics useful for IT can be found\n\non Gartner, Corporate Executive Board, and\n\nCIO Executive Council websites (access is for\n\nmembers).\n\n\nJan van Bon, Tieneke Verheijen, 2006, Frameworks for IT Management, Van Haren Publishing\n\n[Office of Government Commerce, 2007, ITIL v3 Library, TSO. (Book)](http://www.itil-officialsite.com/AboutITIL/WhatisITIL.aspx)\n\n[ISO/IEC 20000-1:2011, Information technology—Service management—Part 1: Service man­](http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51986)\n[agement system requirements](http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51986)\n\n[NIST Special Publication 800-145 (Draft), The NIST Definition of Cloud Computing.](http://csrc.nist.gov/publications/nistpubs/800-145/SP800-145.pdf)\n\n\n-----\n\nDefinition: Information and\n\n_data management (IDM) forms_\n\n_policies, procedures, and best_\n\n_practices to ensure data is_\n\n_understandable, trusted, visible,_\n\n_accessible, optimized, and_\n\n_interoperable. IDM includes_\n\n_processes for strategy, plan­_\n\n_ning, modeling, security, access_\n\n_control, visualization, data ana­_\n\n_lytics, and quality. Outcomes_\n\n_encompass improving data_\n\n_quality and assurance, enabling_\n\n_information sharing, and foster­_\n\n_ing data reuse by minimizing_\n\n_data redundancy._\n\nKeywords: business intelligence,\n\n_data, data analysis, data_\n\n_governance, data manage­_\n\n_ment, data mart, data mining,_\n\n_data modeling, data quality,_\n\n_data warehouse, database,_\n\n_database management system_\n\n_(DBMS), information manage­_\n\n_ment, master data manage­_\n\n_ment, metadata, data migration_\n\n\nENTERPRISE TECHNOLOGY, INFORMATION,\nAND INFRASTRUCTURE\n###### Information and Data Management\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) will encounter IDM\nrelated activities on most government programs.\n\nThey are expected to understand the customer\n\norganization’s data requirements and help\n\ndevelop concepts for how to use and man­\n\nage data, as well as how to apply appropriate\n\nIDM mechanisms in the organization’s system\n\nenvironment. The IDM SE role may start before\n\nsystem acquisition, when only general require­\n\nments are known. Typically it encompasses\n\nplanning, training, and operational support for the\n\nawareness, coordination, and integration of data\n\nand information management activities. MITRE\n\nSEs are expected to be able to determine the size\n\nof data, data security and privacy requirements,\n\nand data sharing requirements. This may include\n\n\n-----\n\nspecifying the information needs, data, software, and hardware, as well as the skills and staff­\ning required to support the system’s operational IDM needs. At the end of a system life cycle,\nthe SE may need to consider where and how data is stored or disposed.\n\n###### Discussion\n\nData is the “life blood” of an organization, for as it flows between systems, databases, pro­\ncesses, and departments, it carries with it the ability to make the organization smarter and\nmore effective. The highest performing organizations pay close attention to the data asset, not\nas an afterthought but rather as a core part of defining, designing, and constructing their sys­\ntems and databases. Data is essential to making well-informed decisions that guide and mea­\nsure achievement of organizational strategy. For example, an organization may analyze data\nto determine the optimal enforcement actions that reduce non-compliant behavior. Similarly,\ndata is also at the heart of the business processes. An organization may enhance a process\nto catch fraudulent activities by including historical risk-related data. Over time, this type of\nprocess improvement can result in material savings. Even a single execution of a business\nprocess can translate into substantial benefits, such as using data patterns to stop a terrorist at\na border or filtering a cyber attack.\n\nHow an organization uses and manages the data is just as important as the mechanisms\nused to bring it into the environment. Having the right data of appropriate quality enables the\norganization to perform processes well and to determine which processes have the greatest\nimpact. These fundamental objectives leverage data by transforming it into useful informa­\ntion. The highest performing organizations ensure that their data assets are accessible to the\nprocesses and individuals who need them,\nare of sufficient quality and timeliness, and\n\nConcept /\n\nare protected against misuse and abuse. Define\nSuccessfully leveraging data and informa­\ntion assets does not happen by itself; it\n\nEvaluate / Design /\n\nrequires proactive data management by Retire Engineer\napplying specific disciplines, policies, and\ncompetencies throughout the life of the data.\n\n\nSimilar to systems, data goes through a\nlife cycle. Figure 1 presents the key phases\nof the data life cycle.\n\nEffective data management through all\nof the data life-cycle phases is the founda­\ntion for reliable information. Data may have\n\n\nLeverage / Capture /\nMining Control\n\n\nProtect /\nPrivacy\n\n\nFigure 1. Data Life Cycle\n\n\n-----\n\ndifferent uses at different times and requires different management handling in the life-cycle\nphases. For instance, an organization may consider critical data required for discovery as very\nvaluable during a key event, but when the event is over, the information diminishes in value\nquickly (e.g., data collected for predicting the weather).\n\nData may typically have a longer lifespan than the project that creates it. Though the\nfunding period formally defines the lifespan of most projects, the resultant data may be avail­\nable for many years afterwards. If an organization manages and preserves the data properly,\nthe data is available for use well into the future, increasing the investment made in generating\nit by increasing visibility and usefulness. The time spent in planning and implementing effec­\ntive data management pays dividends far in excess of its investment costs.\n\nIDM is the set of related disciplines that aims to manage the data asset fully, from concep­\ntion to retirement. Figure 2 presents a high-level view of data management disciplines.\n\nData without context has no value; data that consumers never use is worthless, too. The\nvalue of data is in the information it contains and uses. The extraction of information and\n\n\nPolicies, standards,\nSLAs, and metrics\n\n\nDatabase Data\nadministration integration\n\n\nDatabase\nprogramming\n\n\nData\naccessibility\n\n\nData Knowledge\ntransformation Engineering Integration sharing\n\n\nData Data\nmining quality\n\nData\nData analysis Analytics Information & Strategy & governance\n\ndata management planning\n\n\nBusiness Data\nintelligence migration\n\nDatabase Data\n\nTechnology Architecture\n\ntechnologies warehousing\n\n\nPrivacy and\nsecurity\n\n\nData\nmodeling\n\n\nMetadata Data\nmanagement requirements\n\nFigure 2. Data Management Disciplines\n\n\n-----\n\nproviding it in an appropriate format may be summarized as data analysis and reporting.\nHowever, data analysis and reporting encompasses several overlapping disciplines, among\nthem statistical analysis, data mining, predictive analysis, artificial intelligence, and business\nintelligence. IDM has an appreciation for these disciplines and may use the same tools and\nincorporate some of these disciplines. The common ground among all of these disciplines and\nIDM is making good use of data.\n\n###### Knowledge Required\n\nA MITRE SE dealing with data should be knowledgeable in at least one of the following envi­\nronments or disciplines:\n\n###### �Operational data: Operational environments provide core transactional capabilities\n\n(i.e., processing applications, claims, payments, etc.) that typically work with database\nmanagement systems.\n###### �Data exchange: Organizations use data exchanges and data exchange standards to\n\nshare information with internal or external parties. Standardizing exchange formats\nand metadata minimizes impacts to both the sending and receiving systems and\nreduces cost and delivery time. A related discipline is master data management (MDM).\nAn example is a vendor list. The U.S. Treasury requires specific information identifying\ncontractors before the federal government reimburses them. Most federal agencies use\nthis centrally collected list. Exchange, transform, and load (ETL) tools typically support\nthese types of data exchange activities. ETL tools manipulate data and move it from one\ndatabase environment to another.\n###### �Data warehouses [1]: The integration of similar and disparate data from across organi­\n\nzational, functional, and system boundaries can create new data assets. The organiza­\ntions can use the new data to ensure consistent analysis and reporting and to enhance\nthe information needed for decision making. Data may be structured, unstructured, or\nboth. Business intelligence (BI) has become a recognized discipline. It takes advantage\nof data warehouses (or similar large data consolidation) to generate business perfor­\nmance management and reporting.\n###### �Data mining and knowledge discovery: Mining applications explore the patterns\n\nwithin data to discover new insight and predictive models. An organization may use\nspecialized software that applies advanced statistics, neural net processing, graphical\nvisualization, and other advanced analytical techniques against targeted extracts of\ndata. In addition, tools may evaluate continuously streaming data within operational\nsources.\n###### �Database administration [2]: Knowledge in this discipline requires specific training\n\nrelated to a specific DBMS and being certified. A certified database administrator (DBA)\n\n\n-----\n\nis responsible for the installation, configuration, and maintenance of a DBMS (e.g., stor­\nage requirements, backup and recovery), as well as database design, implementation,\nmonitoring, integrity, performance, and security of the data in the DBMS.\n###### �Data architecture: A data architect is responsible for the overall data requirements of an\n\norganization, its data architecture and data models, and the design of the databases and\ndata integration solutions that support the organization. The data structure must meet\nbusiness requirements and regulations. Good communication and knowledge of the\nbusiness must be part of the data architect’s arsenal. A specialized area in data architec­\nture is the role of the data steward. The data steward is usually responsible for a specific\narea of data such as one or more master data.\nNote that:\n\n###### �A database is a collection of related data. It may be stored in a single or several files. It\n\nmay be structured or unstructured.\n###### �A DBMS is software that controls the organization, creation, maintenance, retrieval,\n\nstorage, and security of data in a database. Applications make requests to the DBMS,\nbut they do not manipulate the data directly.\n\n###### Best Practices and Lessons Learned\n\n\nWhat is in it for me? Conveying the importance\n\nof information and data management to federal\n\nexecutives is the most common challenge that\n\nan SE will encounter. Most organizations focus\n\ntheir time and energy on application development\n\nand the technical infrastructure. For information\n\nsystems, at best this approach leads to delays in\n\nimplementation and, at worst, data is not trusted\n\nand system failures occur. The organization needs\n\nto coordinate data and IT staff with the business\n\nstaff to align strategy and improvement initiatives.\n\nThe best approach for long-term success is to\n\ninitiate a program that gradually addresses the\n\nmultifaceted challenges of data management.\n\nAn effective data management program begins\n\nwith identifying core principles and collaborative\n\nactivities that form the foundation for provid­\n\ning efficient, effective, and sustainable data. The\n\n\norganization should interweave the following core\n\nprinciples throughout all of the data management\n\nactivities:\n\n###### � [Data collected is timely, accurate, relevant, ]\n\nand cost-effective.\n###### � [Data efforts are cost-efficient and pur­]\n\nposeful, and they minimize redundancy\n\nand respondent burden.\n###### � [Data is used to inform, monitor, and con­]\n\ntinuously improve policies and programs.\n###### � [Data activities seek the highest quality of ]\n\ndata and data collection methodologies\n\nand use.\n###### � [Data activities are coordinated within the ]\n\norganization, maximizing the standardiza­\n\ntion of data and sharing across programs.\n\n\n-----\n\nIT Subject Resource\nExperts\n\n###### �[System/data resource ]\n\nexperts\n###### �[IT staff, including application ]\n\ndevelopment, data design,\n\nsecurity, and other data\n###### �[Resource management]\n\n\nExecutive Level\n\n###### �[Data Steering Committee] �[Senior executives] �[Chief Information Officer ]\n\n(CIO)\n###### �[Strategic Level] �[Program Management Office]\n\n\nCollaborative Level\n\n###### �[Data Governance Council] �[Data steward chair for each ]\n\ndataset\n###### �[IT support]\n\nOperational Level\n\n###### �[Operational data stewards, ]\n\nusers\n###### �[Data steward facilitators, data ]\n\ndefiners, producers, users,\n\nSMEs, and other administra­\n\ntive support\n\n\nFigure 3. Data Governance Framework\n\n\n-----\n\n###### � [Partnerships and collaboration with all ]\n\nstakeholders are cultivated to support\n\ncommon goals and objectives around data\n\nactivities.\n###### � [Activities related to the collection and ]\n\nuse of data are consistent with applicable\n\nconfidentiality, privacy, and other laws,\n\nregulations, and relevant authorities.\n###### � [Data activities adhere to appropriate guid­]\n\nance issued by the organization, its advi­\n\nsory bodies, and other relevant authorities.\n###### � [The data management program supports ]\n\nthe framework that facilitates relationships\n\namong the organization’s staff, stakehold­\n\ners, communities of interest (COIs), and\n\nusers. It also provides a plan and approach\n\nto accomplish the next level of work\n\nneeded to implement the technical archi­\n\ntecture. The ultimate goal of the program\n\nis to define a data-sharing environment to\n\nprovide a single, accurate, and consistent\n\nsource of data for the organization.\n\nDesign for use. A simple analogy is to view data\n\nas a set of books. With a small number of books\n\nand only one individual who interacts with them,\n\norganizing the books is a matter of preference.\n\nFurther, finding sections of interest in the books\n\nis manageable. However, as the number of books\n\nincreases and the number of individuals interact­\n\ning with them also increases, additional resources\n\nare required to acquire, organize, and make the\n\nbooks available when requested.\n\nIn the discipline of data management, acquiring,\n\nmanaging, and extracting information are also\n\ntrue for data, but at a more intricate level. The\n\ncomplexity of the tasks related to database 1\n\n\ndesign grows as requirements, number of users,\n\nand data relationships increase. The most com­\n\nmon approach to deal with large amounts of data\n\nwith multiple users is to store data in a Database\n\nManagement System 2 (DBMS). In many cases,\n\nthe DBMS is a relational DBMS (RDBMS), which\n\nreduces reliance on software developers and\n\nprovides an environment to establish data stan­\n\ndards. However, working with any DBMS requires\n\nknowledge of the specific DBMS. In addition,\n\nan SE would have to be proficient in specific\n\ntools such as data modeling, query language, or\n\nothers. A DBMS designer also would take into\n\nconsideration:\n\n###### � [Business requirements ] � [Operational requirements (is it mainly an ]\n\ninteractive system for data collection or is\n\nit for querying?)\n###### � [Access and usage requirements ] � [Performance ] � [Data structure and replications ]\n\nrequirements\n###### � [Interfaces and data-sharing requirements ] � [Reporting and analytical requirements ] � [Data volume ] � [Privacy and security ]\n\nThe complexity of data may require both a data\n\narchitect and a certified DBA. A MITRE SE may\n\nplay these roles or advise someone playing these\n\nroles. A data architect is usually associated with\n\ndata strategy and data modeling. The data archi­\n\ntect may propose a physical data model, but it is\n\nin coordination with the DBA. Though the DBA’s\n\nresponsibilities usually start with the physical\n\ndatabase model, their responsibilities span into all\n\n\n-----\n\nphysical data responsibilities while data is in the\n\nDBMS.\n\nFit for consumption. The Federal Data\n\nArchitecture Subcommittee (DAS) Data Quality\n\nFramework [3] defines data quality as ”the state\n\nof excellence that exists when data is relevant\n\nto its intended uses, and is of sufficient detail\n\nand quantity, with a high degree of accuracy and\n\ncompleteness, consistent with other sources,\n\nand presented in appropriate ways.” A simpler\n\ndefinition is “data fit for its intended use.” A set of\n\ncharacteristics provides the best definition for\n\ndata quality. These are data accessibility, data\n\ncompleteness, data consistency, data definition,\n\ndata accuracy, data relevancy, data timeliness, and\n\ndata validity. Emphasis on one characteristic over\n\nanother depends on the environment. The follow­\n\ning environments introduce key considerations:\n\n###### � [Stand-alone: Usually data from a single ]\n\napplication with limited or no interfaces\n###### � [Enterprise-wide: Data of relevance to the ]\n\nenterprise with no interfaces to the exter­\n\nnal world\n###### � [Multi-enterprise: Data shared outside the ]\n\nenterprise with the need to meet external\n\nregulations\n\nIn a stand-alone environment, obtaining an\n\nacceptable level of data quality is relatively simple.\n\nThe organization can meet most of the charac­\n\nteristics because they are part of the applica­\n\ntion requirements and design. In such a case,\n\ndata quality usually means data accuracy and\n\ndata validity. The organization manages the data\n\nquality by ensuring that data collection meets\n\nrequirements and there are tools (automated or\n\n\notherwise) to control and monitor data validity and\n\naccuracy.\n\nThe picture changes in an enterprise environment\n\nbecause there are competing needs for the same\n\nsets of data. For example, an accounting depart­\n\nment must account for every penny to avoid legal\n\nconsequences, whereas budgeting operations are\n\ntypically not concerned with small dollar varia­\n\ntions. In this environment, all the data quality char­\n\nacteristics are important, but usage determines\n\nwhat is acceptable and what is not. Another com­\n\nmon factor is the variation in terminology, such\n\nas using the same word to mean two different\n\nthings or using different coding lists for equivalent\n\nattributes. A recommended solution to eliminate\n\nor reduce miscommunications is to establish data\n\nstewardships and data governance to facilitate\n\nmediation and conflict management. In addition,\n\nas in most large endeavors, documentation and\n\nstandards are critical for success.\n\nThe multi-enterprise environment adds com­\n\nplexity (i.e., data sharing). An organization may\n\nuse the data in the manner originally intended.\n\nDocumentation of data content is important, and\n\ncontrol of data use is more limited, so standards\n\nare harder to enforce. As an example, the unique\n\nidentification of an individual varies from state\n\nto state. A federal agency integrating data from\n\nstates that do not share unique identifiers may\n\nintroduce data incompatibility issues (e.g., fraud\n\nmay go on unnoticed). This issue is not easily\n\nresolved because one state may mandate the use\n\nof social security number as an identifier, whereas\n\nanother state may forbid it. In such a case,\n\ncompromised data quality will occur until the\n\n\n-----\n\norganization implements an innovative solution\n\nthat ensures uniqueness.\n\n‘Cause they said so. Data governance encom­\n\npasses roles, responsibilities, accountability, policy\n\nenforcement, processes, and procedures that\n\nensure data value, quality improvement, and stan­\n\ndard definitions. It also entails the overall man­\n\nagement of the availability, usability, integrity, and\n\n[security of the data employed in the enterprise.](http://searchdatamanagement.techtarget.com/sDefinition/0,,sid91_gci211894,00.html)\n\nA sound governance program includes a govern­\n\ning council, an accountability structure, a defined\n\nset of procedures, and a plan to execute those\n\nprocedures. The Data Governance Framework\n\npresented in Figure 3 provides an overview of the\n\nexpected governance roles and responsibilities,\n\naccountability, and authority for the strategic,\n\ncollaborative, and operational levels and the IT\n\nsubject matter experts.\n\nThe line of business (LOB) chief has a clear\n\nresponsibility over the business. In addition, the\n\nstaff at the operational level (i.e., data stewards,\n\nSMEs, etc.) receive direction from the LOB chief.\n\nOperational data stewards are responsible for\n\nmanaging data in the best interest of the LOB.\n\nHowever, when several LOBs are dealing with the\n\nsame set of data, conflicts may arise because of\n\ntheir varying needs. Resolution of these issues\n\nrequires collaboration among the LOBs. The most\n\nimportant role of the data governance council\n\n(or equivalent) is conflict resolution. Business\n\nand technical staffs, specifically the collaborative\n\ndata stewards, should define the composition\n\nof the data governance council. The collabora­\n\ntive data stewards should be knowledgeable in\n\nmore than one LOB as part of proposing solutions\n\nthat are best for the enterprise. By promoting\n\n\naccountability for data as an enterprise asset\n\nand providing for efficient collaboration among\n\nstakeholders, the data governance council fosters\n\nan environment that ensures optimal mission\n\nperformance. Even with the best of intentions, the\n\ndata governance council may deadlock. In such\n\ncases, the collaborative steward must escalate\n\nthe issues to the executive/strategic level.\n\nEstablishing a data governance council may be\n\neasy, but an effective council must be commit­\n\nted to collaboration. The role and responsibilities\n\nshould be clear and focused to accomplish what\n\nis best for the enterprise. In some organizations,\n\nthe council is composed of individuals from the\n\nLOBs, whereas in others, a separate indepen­\n\ndent group is established. Success with either\n\napproach depends on the organization.\n\nSecure your belongings. Data security [4]\n\nprotects data from unauthorized access, use,\n\ndisclosure, and destruction, as well as preventing\n\nunwanted changes that can affect the integrity\n\nof data. Ensuring data security requires paying\n\nattention to physical security, network security,\n\nand security of computer systems and files. Data\n\nsecurity is required to protect intellectual property\n\nrights, commercial interests, or to keep sensitive\n\ninformation safe. Security defines the methods of\n\nprotecting information and information sys­\n\ntems from unauthorized access, use, disclosure,\n\ndisruption, modification, or destruction in order\n\nto provide confidentiality, integrity, and availability,\n\nwhether in storage or in transit. Confidentiality\n\nwill prevent the disclosure of information to\n\nunauthorized individuals or systems. Integrity\n\nmeans that the data cannot be modified without\n\nauthorization (i.e., integrity is violated when an\n\n\n-----\n\nindividual accidentally or with malicious intent\n\ndeletes important data files). Availability means\n\nthat the information must be obtainable when a\n\nuser requests the data. These three concepts are\n\ncore principles of information security.\n\nData about data. Informative and relevant meta­\n\ndata (i.e., data about data) supports your orga­\n\nnization and helps everyone that uses your data.\n\nA data steward, working under the direction of a\n\ndata architect and a DBA, is usually responsible\n\nfor managing a portion of the metadata. Metadata\n\ndescribes the definition, structure, and adminis­\n\ntration of information with all contents in context\n\nto ease the use of the captured and archived\n\ndata for further use. The traditional data admin­\n\nistration approach uses metadata to define data\n\nstructures and relationships (e.g., data models)\n\nto support the development of databases and\n\nsoftware applications. In addition to supporting\n\nsystems development, metadata may be associ­\n\nated with all data in the enterprise for the pur­\n\nposes of “advertising” data assets for discovery.\n\nOrganizations have to identify and document\n\nall data to facilitate its subsequent identifica­\n\ntion, proper management, and effective use, and\n\nto avoid collecting or purchasing the same data\n\nmultiple times. There are many types of metadata,\n\nincluding vocabularies, taxonomic structures used\n\nfor organizing data assets, interface specifica­\n\ntions, and mapping tables.\n\nMetadata management not only encapsulates\n\nbasic data dictionary content but also ensures\n\ndata’s ongoing integrity. Metadata aids in the\n\ncomprehension of the data to avoid making\n\nincorrect decisions based on their interpretations.\n\nData lineage, the understanding of data from\n\n\nits inception to its current state, is a foundation\n\ncapability of metadata management. As users\n\nreuse data from an original source system to\n\nthe downstream support systems, they need to\n\nunderstand the lineage of that data. Data longevity\n\nis roughly proportional to the comprehensiveness\n\nof the metadata. For example, during an emer­\n\ngency event, it can be difficult to know where data\n\nis in order to assemble it expeditiously. Access to\n\nthe data is critical when saving time means sav­\n\ning lives. Good metadata can help overcome the\n\nobstacles and get the right information into the\n\nhands of the right people as fast as possible.\n\nGoing to a better place. Data migration is the\n\n[process of transferring data from one system to](http://en.wikipedia.org/wiki/Data)\n\nanother. Migration includes the following steps:\n\n###### � [Identify the migrating legacy data and ]\n\nassociated business rules.\n###### � [Map and match the legacy data to the ]\n\ntarget system.\n###### � [Aggregate, cleanse, and convert legacy ]\n\ndata, as needed, to fit appropriately in the\n\ntarget system.\n###### � [Migrate the data in an appropriate ]\n\nsequence to the target system.\n\nThe most frequent challenges a data migration\n\neffort may face are an underestimation of the\n\ntask and a postponement until the target system\n\nis almost ready to go operational. The complex­\n\nity of a migration effort is in the implementation,\n\nand challenges exist at every step of the process.\n\nIt is easy to reach Step 4 and discover that Step\n\n1 is not complete. In some instances, legacy data\n\ncannot be migrated because it does not meet\n\nbusiness rules in the target system and there may\n\n\n-----\n\nbe a cascading effect on the cleansed data. Data\n\ncleansing is the process of detecting and cor­\n\nrecting or removing corrupt or inaccurate records\n\nfrom a record set, table, or database.\n\nDefining data elements and associated business\n\nrules can be a daunting exercise but a neces­\n\nsary task to ensure a successful migration effort.\n\nLegacy systems may not always document the\n\ndata well, and business rules may not be fully\n\nenforced. For example, the definition of an existing\n\ndata element could change midstream and affect\n\nassociated business rules. Mapping may be pos­\n\nsible, but the business rules may differ significantly\n\nto render legacy data useless. A detailed data\n\ncleansing routine will ease the pain during the\n\ntedious process of weeding out duplicates and\n\nobsolete data, as well as correcting any errors in\n\nthe data.\n\nFinally—and this is a common mistake—never\n\nassume that the previous steps worked perfectly.\n\nRoutines to cleanse, transform, and migrate\n\nthe data have to be run several times and at\n\ntimes modified to ensure completeness. The\n\nbest advice for data migration is to start early\n\n###### References and Resources\n\n[1. The Data Warehousing Institute (TDWI).](http://www.tdwi.org/)\n\n\nin the system migration process. Be prepared.\n\nUnderstand as much as possible what data is\n\navailable (i.e., legacy system) and where data is\n\nmoving (i.e., target system). Be patient, be flexible,\n\nand expect the unexpected.\n\nPlay nice in the sandbox. Information sharing is\n\nthe exchange among individuals, organizations,\n\nsystems, and databases across domains and\n\norganizational boundaries. The goal of informa­\n\ntion sharing is to provide the right data at the right\n\nplace in order to support timely and effective\n\ndecision making. Information-sharing solu­\n\ntions support the collection of data from enter­\n\nprise systems and their assembly into concise,\n\nunderstandable, actionable, and when possible,\n\nunclassified formats. An organization can have\n\nan information-sharing culture that embraces\n\nthe exchange of information and an information\nsharing environment that includes policies,\n\ngovernance, procedures, and technologies that\n\nlink resources (people, process, and technology)\n\nof stakeholders to facilitate information sharing,\n\naccess, and collaboration. A mature organization\n\nwill exhibit continual information sharing in a stan­\n\ndardized manner with guaranteed data quality.\n\n\n[2. DAMA’s Data Management Body of Knowledge (DMBOK).](http://www.dama.org/files/public/di_dama_dmbok_en_v2_1.pdf)\n\n[3. Federal Data Architecture Subcommittee (DAS) Data Quality Framework, v1.0, October](http://semanticommunity.info/@api/deki/files/2388/=Fed_DAS_DQ_FINAL_Release_v1.doc)\n\n2008.\n\n[4. National Institute of Standards and Technology (NIST), Information Security Handbook:](http://csrc.nist.gov/publications/nistpubs/800-100/SP800-100-Mar07-2007.pdf)\n\n[A Guide for Managers Information Security, Special Publication (SP) 800-100, Revision 3](http://csrc.nist.gov/publications/nistpubs/800-100/SP800-100-Mar07-2007.pdf)\n(March 2007).\n\n\n-----\n\nDefinition: Radio Frequency\n\n_Spectrum Management is the_\n\n_analytical, procedural, and_\n\n_policy approach to planning_\n\n_and managing the use of the_\n\n_electromagnetic spectrum._\n\nKeywords: harmful interference,\n\n_policies and procedures, radio_\n\n_frequencies, radio frequency_\n\n_interference analysis, radio_\n\n_spectrum, system acquisition_\n\n\nENTERPRISE TECHNOLOGY, INFORMATION,\nAND INFRASTRUCTURE\n###### Radio Frequency Spectrum Management\n\n**MITRE SE Roles and Expectations: MITRE’s cus­**\n\ntomers are becoming increasingly dependent on\n\nwireless communications, navigation, and surveil­\n\nlance systems in order to support a broad variety\n\nof operational missions in the areas of air traffic\n\ncontrol, national defense, and homeland security.\n\nThe single most critical asset that any wireless\n\nsystem must acquire is the radio frequency (RF)\n\nspectrum in which to operate. Nearly everywhere\n\nin the world, unallocated radio spectrum has\n\nbecome scarce, and as a result, its commercial\n\nvalue has increased dramatically. In the resulting\n\nintense competition for a limited resource, private\n\ncompanies have been winning the “war of words”\n\nassociated with this asset. This makes it increas­\n\ningly difficult for government agencies to acquire\n\nspectrum for new systems and even to keep the\n\nfrequencies they have been using for years.\n\n\n-----\n\nMITRE SEs are being called on to advise government system developers, operational\nunits, and policy organizations on how best to plan for, acquire, use, and retain radio frequen­\ncies. It is essential for MITRE staff involved in systems that depend on RF emissions to have\na working knowledge of this somewhat complex field and to be able to get help from MITRE\nexperts when needed.\n\n###### Government Interest and Use\n\nAll useful regions of the radio frequency spectrum (9kHz–300GHz) are regulated. Worldwide,\nthe International Telecommunication Union (ITU), an entity within the United Nations, main­\ntains a Table of Allocations to which most countries adhere, to a large extent [1]. The ITU has\ndivided the world into three regions, each often having different radio rules and allocations.\nEach nation also has internal spectrum regulators who manage what is universally considered\nto be a sovereign asset within their own borders. Generally a Ministry of Telecommunications\nor similar organization fills this role.\n\nThe ITU is the venue in which deliberations are held to accommodate new types of tele­\ncommunications functions. World Radiocommunication Conferences (WRCs) are held every\nthree or four years to consider changes to the Table of Allocations. Because this process\ntakes several years to complete, spectrum for any new function (e.g., when satellites were\nfirst introduced in significant numbers in the 1970s) has to be planned for many years in\nadvance.\n\nIn the United States, the authority to regulate spectrum use is split between two agencies:\nthe National Telecommunications and Information Administration (NTIA) [2] and the Federal\nCommunications Commission (FCC) [3]. The operating rules of these agencies are extensive\nand are codified into law within Title 47 of the U.S. Code of Federal Regulations.\n\nNTIA is responsible for spectrum matters that involve federal government users in\nall three branches of the government. For a new system, the procuring federal govern­\nment agency must provide the system’s technical characteristics and demonstrate to the\nsatisfaction of NTIA that the system neither causes nor receives harmful interference to or\nfrom other authorized users when placed in its intended operational environment. Once\nthis is accomplished, NTIA issues a Certificate of Spectrum Support, which identifies the\nfrequency band in which the agency can operate and bounds the technical parameters\nthat the system can have. NTIA then issues a frequency authorization allowing the user to\noperate a system on a specific frequency or frequencies at a particular location or within\na defined area. Once a system is fielded, a multitude of radio frequency analysis and\nspectrum management tools are available to plan for and identify frequency assignments.\nUltimate authority, however, to use a frequency must come through an NTIA frequency\n\n\n-----\n\nauthorization or through delegated authority, which is provided by NTIA to specified fed­\neral government agencies for certain bands.\n\nThe FCC is responsible for the spectrum matters of private users as well as state and local\ngovernment users. The FCC first issues a Type Acceptance for new non-government systems,\nidentifying the authorized frequency band and parameter set. For most systems, the FCC then\nissues a radio license that grants a user the right to use a particular frequency or range of\nfrequencies at a given site.\n\nIt is worth noting that this bifurcated approval process can both complicate and protract\nthe system acquisition process for MITRE’s government customers. For example, to develop\nand test a spectrum-dependent system, a private sector vendor must follow the FCC’s rules\nfor doing so—even if the eventual end user is a government agency. The acquiring govern­\nment agency must then go to NTIA to obtain the necessary approvals to use the system in an\noperational environment.\n\n###### Best Practices and Lessons Learned\n\n\nKnow the spectrum policy landscape (part\n\n1). The management—and very often even the\n\ntechnical staff—of most government system\n\nacquisition programs is not acquainted with the\n\nrequirements, policies, and procedures associated\n\nwith the identification, acquisition, and retention\n\nof adequate radio spectrum resources for their\n\nsystems.\n\nKnow the spectrum policy landscape (part 2).\n\nMITRE SEs involved with spectrum-dependent\n\nsystems should have at least a rudimentary\n\nunderstanding of domestic (NTIA and FCC rules)\n\nand international spectrum regulations and\n\npolicy. MITRE SEs supporting the Department\n\nof Defense (DoD) should additionally be famil­\n\niar with DoD Instruction (DODI) 4650.01, “Policy\n\nand Procedures for Management and Use of the\n\nElectromagnetic Spectrum” [4].\n\nKnow the planning horizon (part 1). The time\n\nrequired to obtain spectrum for a new type of\n\n\nsystem is measured in years. Typically, it takes\n\nsix to ten years to get new spectrum to the point\n\nwhere systems can actually use it. The Office of\n\nManagement and Budget requires that federal\n\ngovernment agencies obtain an NTIA Certificate\n\nof Spectrum Support before submitting budget\n\nrequests for “the development or procurement\n\nof major communications-electronics systems\n\n(including all systems employing space satellite\n\ntechniques).” It is thus vitally important to initiate\n\nthe processes to obtain spectrum for new system\n\nprograms as soon as possible.\n\nKnow the planning horizon (part 2). Even if a new\n\nsystem does not represent a new radio service\n\n(e.g., a communication, navigation, or surveillance),\n\nit can take more than a year to obtain the approv­\n\nals to use existing spectrum.\n\nDual approvals needed. Government contrac­\n\ntors must follow FCC rules [3] for spectrum use\n\nduring their design, test, and acceptance phases.\n\n\n-----\n\nThe acquiring agency must then get a separate\n\n(NTIA) approval to use the system on government\n\nfrequencies.\n\nKnow the competition. Competition for radio\n\nspectrum has intensified in recent years, particu­\n\nlarly in bands that are optimal for mobile systems\n\n(approximately 200MHz–4GHz). This factor has\n\nhad a dramatic impact on the perceived (and\n\nactual) value of spectrum and has biased deci­\n\nsions for spectrum re-allocation heavily in favor of\n\nthe private sector.\n\nImportance of justification. Government agen­\n\ncies must develop compelling, operationally based\n\njustifications for both the acquisition of new spec­\n\ntrum and the retention of spectrum they already\n\nhave. Failure to do so will cause spectrum to\n\nbe lost to commercial interests with the result­\n\ning harmful impact on the mission of the federal\n\ngovernment.\n\nDesign and architecture implications.\n\nGovernment agencies typically operate systems\n\nover long life cycles (e.g., 15–30 years or more).\n\n###### References and Resources\n\n\nWith growing scarcity of unused spectrum and\n\nrapid changes in technology, system designs\n\nshould consider wider tuning ranges and modular\n\narchitectures that facilitate upgrading over the life\n\ncycle. Such considerations are especially impor­\n\ntant for systems to be operated overseas in order\n\nto maximize the likelihood that the applicable\n\nhost nation(s) will authorize such equipment to\n\noperate.\n\nLeverage the corporation’s expertise. MITRE\n\nhas a strong capability in spectrum manage­\n\nment that can be brought to the aid of systems\n\nengineers who are working with spectrum\ndependent systems. As one entry point into the\n\nMITRE spectrum community, Dr. Chris Hegarty\n\ncurrently serves as MITRE’s corporate focal point\n\nfor spectrum. [3]\n\nShare your information. MITRE SEs should\n\ninform the corporate focal point for spectrum, in\n\naddition to their own management chain, of any\n\nspectrum-related issues that involve more than\n\none of our sponsors.\n\n\n[1. ITU Radio Regulations are available for purchase at http://www.itu.int/pub/R-REG-RR/en.](http://www.itu.int/pub/R-REG-RR/en)\n\n[2. NTIA’s Manual of Regulations and Procedures for Federal Radio Frequency Management,](http://www.ntia.doc.gov/osmhome/redbook/redbook.html)\n\nwww.ntia.doc.org.\n\n3. The FCC Rules are available on the FCC’s website, http://www.fcc.gov/rulemaking.\n\n[4. Policy and Procedures for Management and Use of the Electromagnetic Spectrum. DoD](http://www.dtic.mil/whs/directives/corres/pdf/465001p.pdf)\n\n[Instruction 4650.01, January 9, 2009.](http://www.dtic.mil/whs/directives/corres/pdf/465001p.pdf)\n\n###### Additional References and Resources\n\nThrough an ongoing, corporately funded initiative, MITRE has developed and maintains\na spectrum management Web collection intended to provide MITRE staff with a basic\n\n\n-----\n\noverview of spectrum management. Links are provided to domestic and international\nregulatory documents and websites. The site also includes a listing of MITRE documents\nrelated to spectrum management, points of contact within MITRE’s staff, and directions\nfor joining the MITRE spectrum shared user distribution list (which currently has over 60\nmembers).\n\n\n-----\n\n##### Engineering Information-Intensive\n Enterprises\n\nDefinition: An enterprise is a network of interdependent people, processes, and\n\n_supporting technology not fully under the control of a single entity. Successful_\n\n_operation of an information-intensive enterprise substantially depends on_\n\n_networked information systems. Engineering an information-intensive enterprise_\n\n_concentrates on managing uncertainty and interdependence in an enterprise;_\n\n_involves engineering both the enterprise and the systems that enable it; and is_\n\n_directed toward building effective and efficient networks of individual systems to_\n\n_meet the objectives of the whole enterprise._\n\nKeywords: architecture, change, composable, design patterns, information-inten­\n\n_sive, innovation, mission assurance, open systems, uncertainty_\n\n###### Context\n\nThe success of our sponsors’ organizations increasingly relies on\n\ninformation. If the right information isn’t available when needed, the\n\nmissions and outcomes of the enterprise will be less effective, efficient,\n\nor successful.\n\nAn enterprise has many components and information that must\n\ncome together for mission success. Data, business rules, applications,\n\ncommunications, and sensors need to be created or composed into\n\ncapabilities within the constraints of the enterprise’s architecture(s),\n\ndesigns, existing systems, and mission assurance requirements. Here\n\nare a few examples:\n\n\n-----\n\n###### �For homeland security, communications capabilities must support the needs of first\n\nresponders and state, local, and tribal partners.\n###### �In the DoD, cyber security threats require careful consideration and close examina­\n\ntion of open capabilities and emerging technologies, such as social networking, before\nemploying them.\n###### �In air traffic management, the need for public trust may drive the business rules associ­\n\nated with free flight and use of unmanned systems in the national airspace.\n###### �For modernization efforts like those at IRS and VA, questions arise about how and when\n\nto insert new technology and capabilities in light of the readiness of the operational\norganizations to absorb them and their associated new processes and procedures.\n\n###### Articles Under This Topic\n\nArticles under this topic are intended to help MITRE staff in engineering information-intensive\nenterprises.\n\nArchitectures are used by and across our customers for a variety of purposes—to support\nunderstanding of operations, help with system design and implementation, and provide basic\nbuilding blocks for enterprise capabilities. A federated architecture helps deal with the magni­\ntude and complexity of engineering cross-enterprise needs to enhance overall mission effec­\ntiveness. The article “Architectures Federation” discusses how federated architectures enable\nlocal innovation, enterprise integration, and evolution across major portions of an enterprise—\nmany of which may be enterprises in their own right.\n\nDesign patterns in software are not concrete pieces of software, but a kind of stencil\nof best practices applied in certain situations. MITRE systems engineers (SEs) are likely to\nencounter and use them in developing or reviewing interfaces between system components\nor, at a higher level, across system boundaries. The article “Design Patterns” describes basic\napproaches, best practices, and lessons learned in using these patterns in engineering serviceoriented environments and interface standardization activities.\n\nThe article “Composable Capabilities On Demand (CCOD)” describes a new and evolv­\ning strategy to enable the rapid piecing together of capabilities to meet end users’ needs, in\nsome cases by the users themselves. CCOD is in the style of many Internet tools that enable\nthe rapid application of various services to data or information to compose a “user-defined” or\ntailored view/perspective to satisfy their needs.\n\nOpen systems approaches enhance the ability to rapidly create capabilities in informationintensive systems. The article “Open Source Software (OSS)” provides an historical perspec­\ntive on OSS, describes the rapidly changing view of OSS and its relationship to engineering\ninformation-intensive enterprises, highlights government interest in and use of OSS, and\n\n\n-----\n\nconcludes with a comprehensive and detailed set of best practices and lessons learned in\napplying open system techniques and using open source software.\n\nMITRE systems engineers should understand the legal requirements that apply to federal\nagencies’ collection, use, maintenance, and disclosure of personally identifiable information.\nThe article “Privacy Systems Engineering” provides guidance on how privacy must be built into\nthe systems engineering life cycle and how technology can be leveraged to protect privacy.\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers are expected to develop enterprise solutions that balance local\ninnovation with global innovation and evolution. They develop solutions that (a) provide cus­\ntomized innovations to meet end-user local needs, and (b) interoperate with, respond to, and\nco-evolve with an environment that itself is constantly changing.\n\n###### Best Practices and Lessons Learned\n\n\nInformation as capital. Treat enterprise data and\n\ninformation as a capital resource that has value\n\nover time. Emphasize the importance of a data\n\nstrategy in your work.\n\nData interoperability. Adopt the view that data\n\ninteroperability should be engineered to ensure\n\nthat cross-enterprise capabilities are realized.\n\nBe attuned to enterprise cycles. There are\n\nlong- and short-term customer cycles. The\n\nformer includes activities like budgeting, require­\n\nments, contracting, and implementing. The latter\n\nincludes responding to urgent operational needs.\n\nUnderstand and differentiate between them, and\n\nadapt systems engineering to them.\n\nConsider capability longevity. Understand the\n\nlikely longevity of the capabilities that users need.\n\nAdapt your perspective and systems engineering\n\napproach to this aspect of the capabilities you\n\nengineer. A capability might be required for the\n\nimmediate situation/environment, but then not\n\nbe needed for the next crisis, or ever again. In a\n\n\ncrisis, consideration of capability evolution might\n\nnot be a critical part of the systems engineering\n\nanalysis, but consideration of future use should\n\nnot be completely set aside. For example, a design\n\npattern could be used to create an immediate\n\ncapability that, at the same time, facilitates use for\n\nfuture crises. A composable capability strategy\n\ncan enable components to be created and be\n\n“on the shelf” to support future situations. Open\n\nsource capabilities can provide a foundation for\n\n“immediate use.”\n\nDon’t throw away “throwaway” thinking. Many\n\ncustomer developments stress that everything\n\nmust be able to be reused by others (and this\n\nhas intensified in the service-oriented world).\n\nAlthough this is often the case, sometimes the\n\nprudent course of action is to build a faster,\n\ncheaper, throwaway capability. Understand the\n\nvalue of reuse within your enterprise and by oth­\n\ners, but also understand that in some situations\n\nbuilding a throwaway version is the better course\n\nof action.\n\n\n-----\n\nDefinition: Architecture federa­\n\n_tion is a framework for enter­_\n\n_prise architecture development,_\n\n_maintenance, and use that_\n\n_aligns, locates, and links sepa­_\n\n_rate but related architectures_\n\n_and architecture information_\n\n_to deliver a seamless outward_\n\n_appearance to users._\n\nKeywords: enterprise architec­\n\n_ture, federated architecture,_\n\n_fit for federation, semantic_\n\n_alignment, tiered accountability,_\n\n_touch point_\n\n\nENGINEERING INFORMATION-INTENSIVE\nENTERPRISES\n###### Architectures Federation\n\n**MITRE SE Roles and Expectations: MITRE**\n\nworks with a variety of government customers to\n\nhelp them build enterprise architectures, often in\n\nthe context of supporting their overall enterprise\n\nmodernization or transformation programs. Many\n\ncustomers are facing the complex problem of\n\nsharing their business processes, information\n\nstores, technical systems, and human resources\n\nin a cohesive and secure way to accomplish a\n\ncommon mission. MITRE systems engineers (SEs)\n\nare expected to understand and apply the prin­\n\nciples of architectures federation to enable local\n\ninnovation, enterprise integration, and evolution\n\nacross major portions of an enterprise architec­\n\nture or multi-agency enterprise architectures. By\n\nhelping them build their respective products to\n\nmeet common prescriptive direction, MITRE’s\n\n\n-----\n\ncustomers will be able to reuse component architectures by “snapping them together” like\nLEGO® bricks to build complex architectures of wider scope and applicability.\n\n###### Introduction\n\nIn recent years, MITRE has been supporting architecture efforts across the federal government\nspectrum. In fact, the federal government now mandates the use of Enterprise Architectures\n(EAs) by agencies seeking to obtain funding for any significant information technology\ninvestment. Customers use architectures to improve warfighting and business capabili­\nties by enhancing the interoperability and integration of U.S. enterprises (e.g., the Air Force\nEnterprise) with Joint and Coalition forces, other Services, and national agencies.\n\nTo accomplish the preceding efforts, MITRE SEs are expected to understand and apply\nthe principles of federated architectures to account for architecture interrelationships and to\nexpress how architectures connect to one another. Federated architectures enable local inno­\nvation, enterprise integration, and evolution across major portions of an enterprise—many of\nwhich may be enterprises in their own right. Principles of architectures federation in practice\nrequire merging, integrating, and federating a large number of diverse organization architec­\ntures such as the Federal Aviation Administration, DoD, DHS, CBP, and the Federal Emergency\nManagement Agency, as well as contributions from industry players like the airlines, airports,\nIT industry, weather bureaus, and others. This article explores the basic concepts of archi­\ntectures federation and offers lessons learned to help MITRE systems engineers understand\nhow the principles of federation can help practitioners build architectures more efficiently and\neffectively.\n\n###### What is enterprise architecture?\n\nArchitecture relates to the structure of components, their relationships to each other and\nto the environment, and the principles guiding the design and evolution of the entity they\ndescribe [1], whether that entity is an organization (e.g., federal department or agency), a\nsystem (e.g., Joint Surveillance Target Attack Radar System), or a functional or mission area\n(e.g., financial management, homeland security). Architecture products and artifacts can\ntake a variety of forms, including models of structured data stored in an architecture tool or\ndatabase repository, graphical depictions of the information in hard copy or electronic format,\nor unstructured data or text.\n\nA good working definition of “enterprise” is any organization or group of organizations\nthat has a common set of goals or principles, or a single bottom line (e.g., a corporation, a\nsingle department, a government entity, a network of geographically remote organizations).\nAn enterprise architecture provides a clear and comprehensive picture of an enterprise. It\nconsists of snapshots of the current operational and technological environment, the target\n\n\n-----\n\nenvironment, and a capital investment roadmap for transitioning from the “as is” to the “to\nbe” environment. In other words, it acts as a roadmap for the way ahead. The snapshots are\nfurther comprised of “views,” each of which consists of one or more architecture products\nthat provide conceptual or logical representations of some part of the enterprise of interest to a\nparticular group of stakeholders [2].\n\n###### What does federated architecture mean?\n\nThe historical approach of developing monolithic, integrated architectures has not worked\nwell, as these products generally become too complex and unwieldy. By contrast, a federated\narchitecture is a framework for enterprise architecture development, maintenance, and use\nthat aligns, locates, and links separate but related architectures and architecture information\nto deliver a seamless outward appearance to users. It enables a complex architecture to be\nbuilt in a piecemeal fashion from component architectures. In this way, a federated architec­\nture approach recognizes the uniqueness and specific purpose of individual architectures, and\nallows for their autonomy and local governance, while enabling the enterprise to benefit from\ntheir collective content.\n\nFederation provides the means to\norganize an enterprise’s body of knowl­\nedge (architecture) about its activities Supported Architecture\n(processes), people, and things within\n\nArchitecture\n\na defined context and current/future Interface\nenvironment. Federated architectures Points\nsupport decision making by linking\n\nSubject Architecture\n\narchitectures across the enterprise, pro­\n\nBusiness\n\nviding a holistic enterprise view that\n\nCompliance\n\nallows for the assessment of such mat­\n\nInformation criteria (for\n\nters as interoperability, identification of architectures\nduplication and gaps, and determina­ and/or\n\nServices\n\nprograms)\n\ntion of reusability [1].\n\nTechnology\n\n###### Why develop architectures that support federation? Architecture\n\nThe ability to integrate and/or fed­ Interface\n\nPoints\n\nerate architectures is essential for\naddressing enterprise issues across Supporting Architecture\na broad domain such as a federal\ndepartment or agency. Federation\nFigure 1. Key Constructs for Architectures Federation\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|||||||||||\n|Foundation|S Bu Info Se Tec|ubj sin rm rvi hno|ect Arc ess ation ces logy|hite|ctu a|re Complia criteria rchitec and/ progra|nc (for ture or ms)|e s||\n|||||||||||\n|||||||||||\n\n\n-----\n\nenables multiple groups to develop architectures with the focus that best meets their imme­\ndiate needs, while providing a means for linking and relating those architectures to address\nissues that cross multiple areas. A single architecture may not be able to address the entire\nenterprise sufficiently to support the kind of analyses needed in a large organization with\na diversity of missions. The ability to federate multiple architectures leads to a more robust\nconstruct for understanding the enterprise in smaller, bite-size chunks.\n\nArchitecture federation serves, in part, as a process for relating subordinate and par­\nent architectures via finding overlaps and establishing mappings between their common\narchitecture information. Federal departments and agencies are also pursuing another use of\nan architectures federation strategy that divides the enterprise into manageable, right-sized\ncomponents, each of which can be described by the communities that are most closely associ­\nated with them [3]. A small set of rules, common terms, and standards are used by everyone\nto maintain consistency so that the component parts can be “snapped together” as needed.\nFor example, department architectures depict department-wide rules and constraints, compo­\nnent architectures depict mission-specific services and capabilities, and solution architectures\ndepict solutions that conform to higher rules and constraints.\n\nThe concept of federation also plays an important role in the development of the environ­\nment and the sharing of information. For example, as federal department and agency enter­\nprises become increasingly networked, federated architectures are proving essential in orga­\nnizing the array of information and complex relationships. Federated architecture metadata is\nalso useful for evaluating portfolios of existing systems and programs to make decisions about\nchanges or additions necessary to achieve desired capabilities.\n\n###### So then, what is federated enterprise architecture?\n\nAs defined by the enterprise scope, federated enterprise architecture is a collective set of archi­\ntectures with the following attributes:\n\n###### �It operates collaboratively, where governance is divided between a central authority and\n\nconstituent units, balancing organizational autonomy with enterprise needs.\n###### �The central authority’s architecture can focus on the dynamics of economies of scale,\n\nstandards, and the well-being of the enterprise.\n###### �Constituent units’ architectures have the flexibility to pursue autonomous strategies and\n\nindependent processes [4].\n\n###### What are the central elements that support architectures federation?\n\nIn a federated approach, responsibility for architecture development is shared at different ech­\nelons within the enterprise. To bring these separate but related efforts together requires:\n\n\n-----\n\n###### �Tiered accountability: Establish a hierarchy of architectures whereby architectures\n\nlower in the hierarchy inherit characteristics from higher-level architectures. Use touch\npoints to relate architectures across the levels or tiers.\n###### �Categorization: Relate and group “like” architectures and artifacts. �Semantic alignment: Use common vocabulary and mapping relationships to establish\n\nshared understanding.\n###### �Reference architectures: Provide parent taxonomies for other architectures to use. �Search and discovery: Allow authorized users to find and access relevant architecture\n\nfor information and reuse [3].\n\n###### What are some key constructs for architectures federation?\n\nThe key constructs for architectures federation are graphically depicted in Figure 1. Each\nconstruct comprises a collection of architecture products of interest to a particular group of\nstakeholders.\n\nThe subject architecture is the architecture that drives solutions for a specific purpose. It\naddresses all the business, information, business services, and technology components needed\nto deliver capabilities. The architectures of those solutions upon which the subject architec­\nture relies are called supporting architectures, whereas the architectures of those solutions\nthat rely on the subject architecture are called supported architectures.\n\nEach architecture interface point (also called touch point) is an abstract representation of\na purposeful connection between two architectures. These architecture interface points are\nabstractions of real-world interfaces that will be embodied in the solutions that implement\nthe corresponding architectures. In simple terms, the interface points are the places where\narchitectures can be joined into a larger federated architecture, so they are key to purposeful\nfederation from an operational perspective [5].\n\n###### What is the role of compliance in federation?\n\nIt is important for an architecture to comply with a set of standards, if it will be shared and\nused to support federation with other architectures (e.g., guiding the development of other\narchitectures or programs). These standards come in the form of prescriptive direction called\ncompliance criteria. Compliance criteria include business rules and processes such as infor­\nmation, service, and technology standards. A program or other architecture must adhere to\nthese for it to comply with a given structure. Compliance criteria are augmented with descrip­\ntions of the ways in which these criteria will be verified. Therefore, the compliance criteria\nexplicitly state what a program or architecture must demonstrate in terms of functionality and\nin terms of adhering to standards and meeting specific qualitative requirements.\n\n\n-----\n\nAn organization can start by creating architectures that meet a minimum set of stan­\ndards, making it easier to share the architectures and positioning them for use in build­\ning a federation of architectures to support the construction of a federation of interoperable\nsolutions.\n\n###### What are some examples of compliance criteria?\n\nFit for Federation is an example of a specific compliance assessment that might be applied\nto any architecture that will become part of an architectures federation. Fit for Federation is\ndetermined by the following compliance criteria:\n\n###### �The architecture’s purpose has been documented and verified by users and usages. �Input has been verified as coming from authoritative source, and the authoritative\n\nsource is recorded.\n###### �The architecture and/or analysis (output) have been verified as fit for purpose. �Supported architecture interface points and associated standards are identified, docu­\n\nmented, and verified.\n###### �Supporting architecture interface points are identified, documented, and negotiated\n\nwith the provider.\n###### �Other compliance criteria (e.g., enterprise-wide standards and/or qualitative require­\n\nments) are established, documented, and verified.\nSome examples of qualitative requirements that might be applied while assessing confor­\nmance to compliance criteria are affordability, dependability, extensibility, performance, and\ntrust.\n\nFor a service-oriented environment, specific compliance criteria would be packaged as\nservice-level agreements (SLAs). A single compliance criterion can distribute to multiple SLAs.\nFor example, supporting a given vocabulary would apply to all services that deal with the\nsubject (domain) vocabulary.\n\n###### Lessons Learned\n\n\nTo federate architectures, there must be semantic\n\nagreement so that pertinent information can be\n\nrelated appropriately. MITRE SEs can recommend\n\nthat their customers achieve semantic agree\nment by:\n\n###### � [Adhering to a common framework, which ]\n\nincludes the use of common data element\n\ndefinitions, semantics, and data structures\n\n\nfor all architecture description entities or\n\nobjects\n###### � [Conforming to common or shared archi­]\n\ntecture standards\n###### � [Using enterprise taxonomies and authori­]\n\ntative reference data.\n\n\n-----\n\nIn general, conforming to common or shared\n\narchitecture standards increases interoperability\n\nand makes it easier to federate. MITRE SEs should\n\nencourage their customers to choose standards\n\nappropriate to their purposes and help them\n\nestablish the means to enforce compliance. For\n\nexample, agreed enterprise taxonomies estab­\n\nlish the context for aligning mission-area activi­\n\nties and associated reference models, and for\n\ncategorizing and organizing component architec­\n\ntures, thereby facilitating semantic understanding\n\nacross the various architectures in the federation.\n\nThe federation of architectures is facilitated by\n\nan environment that enables information sharing.\n\nMITRE systems engineers first must recognize\n\nthat an architecture-sharing environment requires\n\nsound governance and enterprise architec­\n\nture services. They must help their custom­\n\ners establish sound governance structures to\n\napply accountability to the development and\n\nmaintenance of architectures toward set objec­\n\ntives, which will ultimately facilitate their ability\n\nto federate. This approach places responsibility\n\naround processes such as configuration manage­\n\nment and quality assurance. MITRE SEs also must\n\nencourage their customers to establish enterprise\n\narchitecture services to allow for the visibility,\n\naccessibility, and understandability of architecture\n\ninformation in a consistent and efficient manner.\n\n###### Summary\n\n\nThe success of a federation effort also depends\n\non exposing architectures and architecture meta­\n\ndata for potential linkage and reuse by analysts,\n\nplanners, and decision makers at every level.\n\nSharing architectures and services that already\n\nexist helps expedite architecture development\n\nand federation. Registry capabilities [6] provide for\n\nregistration and linking of architecture metadata\n\nto enable the creation of navigable and searchable\n\nfederated enterprise architectures. Enterprise\n\nenforcement policies and governance for archi­\n\ntectures reinforce robust interfaces and data\n\nrelationships [1]. MITRE systems engineers should\n\nassist their customers to actively engage in these\n\narchitecture-sharing venues by reusing artifacts\n\nbefore reinventing them and by posting their own\n\nmetadata and products for reuse by others.\n\nMITRE SEs should promote and foster the\n\ndevelopment of federated architectures within\n\ncustomer organizations to help improve the reli­\n\nability and efficiency of decisions. This will occur\n\nas organizations align semantic and structural\n\ndata across their boundaries so they can ensure\n\nthat the right information is being used to answer\n\nkey decision makers’ questions. MITRE systems\n\nengineers should continue to use federated\n\narchitecture opportunities and improve the flow\n\nof information among stakeholder nodes and\n\nconsequently decision makers.\n\n\nMITRE is working with a wide variety of government customers to help them build their EAs,\nmost often in the context of supporting their overall enterprise modernization or transforma­\ntion programs. A key skill that MITRE systems engineers need to bring is an understanding\nof how business needs, information technology, and people come together in well-constructed\narchitectures.\n\n\n-----\n\nMany of MITRE’s customers are facing the complex problem of multi-agency enterprise\narchitecture. How can different government entities share their business processes, informa­\ntion stores, technical systems, and human resources in a cohesive, secure way to accomplish\na common mission? Architectures federation can foster this kind of sharing. By helping them\nto build their respective products to meet common prescriptive direction, MITRE’s customers\nwill be able to reuse component architectures by “snapping them together” like LEGO® bricks\nto build complex architectures of wider scope and applicability.\n\n###### References and Resources\n\n1. Department of Defense, April 23, 2007, DoD Architecture Framework Version 1.5,\n\n_Volume I: Definitions and Guidelines._\n\n2. Hite, R. C., and G. D. Kutz, March 28, 2003, DoD’s Draft Architecture, GAO-03-571R.\n\n[3. Frey, B., July-September 2008, “Department of the Navy Architecture Federation Pilot,”](http://www.public.navy.mil/usff/chips/Documents/PDFs/chipsjul08.pdf)\n\n_CHIPS, pp. 41–43._\n\n4. Air Force Chief Architect’s Office, December 2007, Air Force Architecture Framework.\n\n[5. COLAB—Collaborative Work Environment, http://colab.cim3.net, accessed January 20,](http://colab.cim3.net)\n\n2010.\n\n[6. Department of Defense, DoD Architecture Registry System, accessed January 20, 2010.](https://dars1.army.mil/IER2/)\n\n###### Additional References and Resources\n\n[Business Transformation Agency, BEA 6.2 Informational Release, http://dcmo.defense.gov/](http://www.bta.mil/products/BEA_6.2/BEA/html_files/federation.html)\nproducts-and-services/business-enterprise-architecture/\n\n[Federal Chief Information Officer Council, February 2001, A Practical Guide to Federal](http://www.gao.gov/bestpractices/bpeaguide.pdf)\n[Enterprise Architecture, Ver. 1.0.](http://www.gao.gov/bestpractices/bpeaguide.pdf)\n\n“System Architecture,” Project Leadership Handbook, The MITRE Corporation.\n\n\n-----\n\nDefinition: Design patterns\n\n_in software are usually short_\n\n_descriptions capturing prac­_\n\n_tices that have proven suc­_\n\n_cessful in the past. They are not_\n\n_concrete pieces of software,_\n\n_but a stencil applied in certain_\n\n_situations. They are generally_\n\n_not prescriptive, but suggestive;_\n\n_include guidance on their most_\n\n_appropriate use; and provide_\n\n_examples from existing sys­_\n\n_tems. Their most important use_\n\n_is to describe the interaction of_\n\n_objects or systems with their_\n\n_environment (i.e., other objects_\n\n_or systems). Design patterns_\n\n_can occur at different levels of_\n\n_system design, from low-level_\n\n_programming to system-of-_\n\n_systems. At the latter level,_\n\n_they are most associated with_\n\n_interface design and coupling._\n\nKeywords: coupling, design\n\n_pattern, interface_\n\n\nENGINEERING INFORMATION-INTENSIVE\nENTERPRISES\n###### Design Patterns\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the general principles and best\n\npractices of design patterns for information\n\ntechnology (IT) intensive systems. They are\n\nexpected to select and recommend the pat­\n\nterns appropriate to the application, under­\n\nstand the challenges and choices that arise,\n\nand understand the issues and challenges of\n\ninterface design in an enterprise environment.\n\n\n-----\n\n###### Background\n\nThe concept of design patterns is usually attributed to the work of the architect Christopher\nAlexander, and was adapted to software by Kent Beck and Ward Cunningham. In 1995, the\npopular book Gang of Four (GOF) [1] established a set of patterns that are in continuous use,\nand provided a “pattern” for describing the patterns. These 23 patterns are divided into cre­\national, structural, and behavioral categories. Many other patterns have been defined, as well\nas other categories, such as user interface.\n\nAs an example, one GOF patterns is the Abstract Factory, a creational pattern that pres­\nents an interface for creating new objects, without the caller knowing the specific type of\nobject being created. This could be used to implement a different look and feel with minimal\nchanges to the program. Other examples are the Proxy structural pattern, in which one object\nbecomes a surrogate for another (with the same interface), often used in remote procedure\ncalls, the Singleton pattern, in which a class allows only one instance of itself to be created,\noften used in managing shared resources, and the Mediator behavioral pattern, which allows\nloose coupling between classes by being the only class that has detailed knowledge of their\nmethods.\n\nDesign patterns enable review and discussion of software design to take place at a higher\nand more abstract level than reviewing specifics of interface calls. We can ask: “Should you\nbe using a Singleton pattern here?” or “Would an Abstract Factory pattern help?”\n\nGOF patterns have several things in common: they are defined in terms of object-oriented\nsoftware, they (usually) describe the interaction of an object with its environment (e.g., other\nobjects), and they are generally used within the internal design of a single application (i.e., a\nlocal calling environment).\n\nPatterns can also be viewed at a broader level of design, however, and MITRE SEs are\nmore often involved in this aspect. MITRE SEs are less likely to be involved in the develop­\nment of the detailed internal workings of system components than in the review of interfaces\nbetween components or, at a higher level, between systems. This calls for a set of design pat­\nterns that focus on the manner in which connections are made across the system boundaries.\nMany GOF patterns will not directly apply.\n\n###### Design Patterns in an Enterprise Engineering Service-Oriented Environment\n\nTwo considerations arise when designing for a large-scale enterprise service environment: (1)\nusers may put services, interfaces, etc., together in ways that designers did not anticipate, and\n(2) any interface changes will affect a larger set of users. Thoughtful use of design patterns\ncan help deal with both of these issues. A third issue with scaling to the enterprise is that a\nservice will generally have to deal with a (currently) unknown and potentially large number\nof users. Design patterns are of less use in dealing directly with this issue.\n\n\n-----\n\nIn an enterprise environment, when considering system-to-system interfaces, the notion\nof design patterns can be broadened to encompass more general guidance on how to manage\nthe coupling in the interface. As a general rule, loose coupling is preferred over tight coupling\nwhenever possible. Loose coupling means that a change in the implementation of one side of\nthe interface does not affect the implementation of the other side. For example, using a code\nin a field with a lookup table that must be distributed to users is not loose coupling. Also, a\nloosely coupled interface should not lock in specific limits that will inhibit scalability. As a\nsimple example of this, in an interface for contact information, allowing for only one (or two)\ntelephone numbers of 10 digits may not be sufficient. A more extensible interface might allow\nfor an arbitrary-length list of telephone numbers of indeterminate length.\n\nLoose coupling insulates users of an interface from changes in the implementation. For\nexample, a well-designed interface should be able to add additional parameters to the inter­\nface, while still generating and accepting messages without the new parameters. This allows\nfor growth and innovation without stranding users of the previous version of the interface.\nOn the flip side, though, this extension mechanism must be managed with discretion, or the\nnumber of supported interfaces that differ just in parameters can grow large, and the mainte­\nnance of these can swamp the value of backward compatibility.\n\n###### Interface Standardization Efforts\n\nCursor on Target (CoT) is an example of an enterprise effort to simplify a collection of inter­\nfaces and provide loose coupling. The Air Force has had a large number of tightly coupled\npoint-to-point interfaces among many components. Gen. Jumper (former Chief of Staff of the\nAir Force) inspired MITRE to come up with a small set of data elements that would give the\nmajority of what most users need. MITRE studied several months’ worth of messages and\nfound that a small number of data elements were used repeatedly. CoT standardized a defini­\ntion of these elements in an XML format that is easy to generate and parse. It provided for\ncompatible extensions so that new elements could be added without disrupting existing users.\n\nUniversal Core [2] (UCORE), developed by the Department of Defense (DoD) and the\nIntelligence Community, built on the CoT philosophy and approach. It is hierarchically\ndesigned to allow the user to choose the level of detail desired in a particular element. Users\ncan find out that an object is a fixed wing aircraft, or drill down and find out the type of air­\ncraft (e.g., F16), or even a unique aircraft identifier such as the tail number. This pattern helps\nto define data elements that are common across multiple communities of interest. It follows\nseveral principles:\n\n###### �Be able to operate at different levels depending on the needs of the user (hierarchical). �Make schemas extensible. �Develop small spirals, making it easier to build innovations.\n\n\n-----\n\n###### Alignment with MITRE Systems Engineering Competency Model (SE CM)\n\nSystems engineering work with design patterns most closely aligns with the “Architecture”\n(Section 2.3) and “Software and Information Engineering” (Section 4.7) competencies in the\nMITRE SE CM [3]. In the former, design patterns can be a useful tool in discussing, visualiz­\ning, comparing, and recording architectural interface decisions. In the latter, because design\npatterns are now a well-established paradigm within software engineering, an understand­\ning of the techniques and terminology is useful in facilitating communication between the\ncustomer/user and software specialist.\n\n###### Best Practices and Lessons Learned\n\n\nThe following rules of practice can be seen as\n\ndesign patterns for interfaces at the enterprise\n\nlevel as well as at the detailed implementation\n\nlevel:\n\nAvoid complexity in the interfaces. Complex\n\ninterfaces typically do not scale well. The com­\n\nplexity is pushed out to all users, and the skill in\n\ndealing with it may vary. For example, rather than\n\nproviding latitude and longitude in 10 poten­\n\ntially different formats, each of which has to be\n\nhandled by the user, provide it in a single format\n\ninstead. If an interface is too complicated, there is\n\na greater possibility that it will be misinterpreted,\n\nor that developers will copy sub-optimal imple­\n\nmentations of the user end. Complexity leads to\n\nerrors, which can lead to poor performance that\n\nmay not be correctable, and may even become\n\nsecurity risks.\n\nUse loosely coupled interfaces wherever pos­\n\nsible. Loose coupling implies that a change in the\n\nimplementation of one side of the interface will\n\nnot affect the implementation of the other side.\n\nThis allows enormous freedom on both sides to\n\nmake improvements and to keep development\n\nschedules disjoint. Tight timing requirements or\n\n\n(unfortunately) software version requirements\n\nmay be considerations that require a reevaluation\n\nand relaxation of this practice, but this should be\n\nmade explicit and documented in such cases.\n\nUse tightly coupled interfaces only if they are\n\nnecessary for performance. Tight coupling can\n\nlead to code that is buggy and fragile. An example\n\nof tight coupling is in the Link-16 interface, which,\n\nbecause it is a tactical link, uses a number to\n\nrepresent the type of an aircraft. This ties the\n\nuser to a particular version of a conversion table.\n\nIf the table is updated on one side, the user may\n\nbe left with a meaningless number until the table\n\nis updated as well. Of course, a more expansive\n\ncommunication protocol could carry all informa­\n\ntion on the aircraft explicitly, but bandwidth limita­\n\ntions may prohibit this as an alternative.\n\nWhen possible start design with loose cou­\n\npling. Even in cases where tight coupling will be\n\nused, initial design can begin with loose coupling\n\ninterfaces. Document why a tight coupling is\n\nbeing used. This is analogous to defining a logi­\n\ncal schema in a database management system\n\n(DBMS)-independent way, but implementing it in a\n\n\n-----\n\nDBMS-dependent physical schema. This may be\n\na useful pattern for systems of systems.\n\nFocus on data conformity in the interfaces\n\nrather than in internal representations. In the\n\n1990s, government organizations tried to enforce\n\ndata uniformity across all applications, even to the\n\npoint of specifying how data was to be repre­\n\nsented within the application and its databases.\n\nThis was never achieved. More recently, the\n\nfocus is on creating common definitions for data\n\nexchange, leaving applications free to choose how\n\nto represent data internally [4, 5]. This has proven\n\nto be an easier goal to reach.\n\nRecognize that differences in the representa­\n\ntion of data result from different uses of the\n\ndata. For example, consider a gun. A shooter\n\nwants to know its range, caliber, etc. A shipper\n\nwants to know its size, weight, etc. Finance wants\n\nto know its cost, estimated lifetime, etc. The same\n\ngun is naturally represented differently in different\n\nsystems. Forcing all characteristics on all sys­\n\ntems would be burdensome. However, unantici­\n\npated, innovative uses of data can be achieved\n\nthrough compositional patterns to create new\n\ndata representations that are built on existing\n\nrepresentations.\n\nIn the design of an interface, consider the\n\n80/20 rule. It may be better to implement 80\n\n###### References and Resources\n\n\npercent (or so) of what most users need most of\n\nthe time, especially if this can be done quickly with\n\na simple interface. This reduces the cost and time\n\nfor implementation.\n\nBuild in the ability to extend the interface.\n\nSome users will need to reach at least part of that\n\nremaining 20 percent, and in any case, interfaces\n\nhave to grow and change over time. A loosely\n\ncoupled interface should build in a mechanism for\n\ncompatible extension, so that changes and addi­\n\ntions can be made without affecting users who do\n\nnot need the extensions.\n\nConsider the governance of the extensible\n\ninterfaces. Extension of an interface creates\n\nmultiple versions/copies that must be managed.\n\nConsider the justification for and understand the\n\nimpact of doing this.\n\nDo not forget about the semantic level of\n\nunderstanding in the interface. It is fine for\n\nsomeone to be able to correctly parse your inter­\n\nface, but there must also be agreement on the\n\nmeanings of the data elements.\n\nInvolve developers in the development of\n\nsystem interfaces. Those who will implement the\n\ninterface should be involved in the design, since\n\nthey may have insight into decisions that could\n\ninhibit scalability or cause other problems.\n\n\n1. Gamma, E., R. Helm, R. Johnson, and J. Vlissides, 1995, Design Patterns—Elements of\n\n_Reusable Object-Oriented Software, Addison-Wesley._\n\n[2. November 2008, UCore: Breaking the Barrier to Information Sharing.](http://dodcio.defense.gov/Portals/0/Documents/ucore/UCore_Overview.pdf)\n\n[3. The MITRE Corporation, The MITRE Systems Engineering Competency Model.](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n\n-----\n\n4. Department of Defense, September 26, 1991 (certified current as of November 21, 2003),\n\n[DoD Data Administration, DoD Directive 8320.1.](https://dap.dau.mil/policy/Documents/Policy/8320-1.pdf)\n\n[5. Department of Defense, December 2, 2004 (certified current as of April 23, 2007), Data](http://www.dtic.mil/whs/directives/corres/pdf/832002p.pdf)\n\n[Sharing in a Net-Centric Department of Defense, DoD Directive 8320.02.](http://www.dtic.mil/whs/directives/corres/pdf/832002p.pdf)\n\n###### Additional References and Resources\n\n“Agile Acquisition,” MITRE Project Leadership Handbook, The MITRE Corporation.\n\nErl, T., 2009, SOA Design Patterns, Prentice Hall.\n\nFowler, M., 2002, Patterns of Enterprise Application Architecture, Addison-Wesley.\n\nHohpe, G. and B. Woolf, 2003, Enterprise Integration Patterns: Designing, Building, and\n_Deploying Messaging Solutions, Addison-Wesley._\n\n[National Information Exchange Model, Accessed September 16, 2009.](http://www.niem.gov/)\n\n“Prototyping/Experimentation/Modeling and Simulation,” MITRE Project Leadership\nHandbook, The MITRE Corporation.\n\n“System Architecture,” MITRE Project Leadership Handbook, The MITRE Corporation.\n\n\n-----\n\nDefinition: Composable\n\n_Capabilities On Demand_\n\n_(CCOD) is a design concept to_\n\n_enable the rapid development_\n\n_of new capabilities, carried_\n\n_out by operators combining_\n\n_services, data, and existing sys­_\n\n_tems to achieve awareness of,_\n\n_or respond to, a new situation_\n\n_or mission._\n\nKeywords: capabilities, com­\n\n_ponents, composability, net_\n\n_centric operations (NCO), net-_\n\n_centric waveform (NCW), reuse,_\n\n_service_\n\n\nENGINEERING INFORMATION-INTENSIVE\nENTERPRISES\n###### Composable Capabilities On Demand (CCOD)\n\n**MITRE SE Roles and Expectations: CCOD**\n\nis a new and evolving concept rooted in net\ncentric principles and effective enterprise-level\n\ndistributed computing tenets (e.g., modularity,\n\nloose coupling, platform independence). CCOD\n\ndisrupts traditional software systems engineer­\n\ning in two ways: the extension of capability\n\ncomposition to the end user as opposed to\n\nthe developer; and enablement of the user to\n\nperform runtime composition of said capabili­\n\nties. CCOD represents a dynamic composition\n\nof existing and emerging components; the\n\nresult may even be a recombination of exist­\n\ning capabilities as opposed to a new system.\n\n\n-----\n\nMITRE SEs attempting to apply CCOD principles must understand and use detailed exper­\ntise in these areas:\n\n###### �Distributed and enterprise software engineering from low-level infrastructure to modu­\n\nlar componentization\n###### �Human systems integration for design, workflow analysis, and capability management,\n\nand acquisition, especially of malleable/componentized net-centric systems as opposed\nto large, monolithic, self-contained systems\n###### �Security, information assurance, and mission assurance, which are especially challeng­\n\ning due to the run-time composition issues\n###### �Governance, including contract models for component development/sustainment, infra­\n\nstructure development/sustainment, testing, and related issues\nBecause CCOD is an evolving concept based on still-evolving technologies and tenets,\nMITRE systems engineers seeking to apply CCOD must be aware that not all programs will be\namenable to such an approach.\n\n###### Background\n\nCCOD is a design concept to enable the rapid development of new capabilities by combining\nservices, data, and existing systems to respond to a new situation or mission. Ideally, CCOD\nshould enable operational users in the field to do this development. CJCSI 3170.01G, Joint\nCapabilities Integration and Development System, defines a capability as “the ability to achieve\n_a desired effect under specified standards and conditions through combinations of means and_\n_ways ... . It is defined by an operational user and expressed in broad operational terms [1].”_\nCCOD supports this definition by providing an environment and associated components to\nenable the operational user to create relevant capabilities when needed.\n\nComposable components may include elements from traditional services and data sources\n(e.g., government programs of record) as well as non-traditional ones (e.g., Twitter, Flickr,\nand other open-source/Web 2.0 technologies that foster the creation of user-generated con­\ntent and capabilities). The resulting capabilities and combinations of services can be tailored\nto the task at hand and either published as a “template” (if useful to others) or discarded.\nComponent pieces can be reused should a similar need arise in a future task.\n\nInteroperability is facilitated by the use of “loose couplers”—common data formats\nthat leverage the information-sharing benefits of the “network” through the exchange of\nsufficient mission data for the most common use. Loose couplers (e.g., Keyhole Markup\nLanguage [KML] and Universal Core [UCore]) are defined and evolved by a stakeholder com­\nmunity, including operational users, developers, and MITRE domain or technology experts.\nExperience to date suggests that a minimalist approach to loose couplers is a key principle.\n\n\n-----\n\nGlobal, complex data standards may prove problematic, due to the large investment in pars­\ning/integrating/adopting them.\n\nAd-hoc composability figures prominently in this development process; consequently, it\nis important that the component technologies be built to high usability standards, and that\ntools, services, and data sources be described in a manner that improves discoverability.\nEventually it may be possible for end users with no specific training to find, combine, and\nuse tools, services, and data sources in a mission-appropriate way via approaches like social\nnetworking. The widespread adoption of the CCOD approach also depends on innovation\nin the development of tools, services, and data sources. Innovation is crucial for ensuring\nthat there is a sufficient variety of components to allow for the development of a missionfocused solution. This innovation can be fostered by the adoption of a layered architectural\napproach, which promotes the creation of multiple, partially overlapping capabilities. This,\nin turn, allows non-programmer end users to select the best components to incorporate into\ntheir solution.\n\nExamples of CCOD capabilities can be found in the Agile Capability Mashup\nEnvironment (ACME) [2]. This environment has supported a variety of customers by rap­\nidly bringing together innovative ideas, approaches, and capabilities to meet challenging\nproblems.\n\n###### Operational Properties of a CCOD Environment\n\nThe objective of CCOD is to provide operational capability to the end user. To make this pos­\nsible, capabilities in the CCOD environment have the following properties:\n\n###### �User-facing: The composed capability will deliver an effect for the end user. This\n\ndoesn’t mean that capabilities will not have middleware components, but rather that\nmiddleware alone is not sufficient to provide CCOD.\n###### �Rapidly integratable: Reusable components in a CCOD environment can be rapidly\n\nintegrated, ideally by the end user.\n###### �Quickly adaptable to the task at hand: Capabilities that are composed can be quickly\n\nadapted to changing operational context.\n###### �Assured: The capabilities that are composed will need to be assured from a mission\n\nperspective; this involves issues such as information security, trust, versioning, and\nverification/validation.\n###### �Integratable with existing operational systems and domain business processes:\n\nComponents designed for CCOD will need to integrate with existing business processes,\ncommand and control systems, and ancillary information technology (IT) systems.\n\n\n-----\n\nThe use of CCOD in government systems development and the acquisition process is\nnascent. CCOD’s run-time and user-facing tenets provide challenges that are rarely considered\nin the systems engineering and acquisition realms.\n\n###### Best Practices and Lessons Learned\n\n\nThese lessons have been derived from hands\non CCOD prototyping and operator-centric\n\nanalyses of current command and control (C2)\n\nsystems and workarounds, filtered through\n\nthe lens of practiced distributed computing\n\nexpertise. However, remember that CCOD is still\n\na budding concept and these lessons are still\n\nforming.\n\nFavor the small and reusable. Using Occam’s\n\nrazor, where there is a choice between develop­\n\ning or using two or more components, usually\n\nthe small, simpler one is preferred. A component\n\nshould be “light weight” to ensure its ease of\n\nadoption and integration across a variety of users\n\nand uses.\n\nMake components discoverable. Ad-hoc\n\nmission-focused composability necessitates\n\nthe ability to find the components and data best\n\nsuited for a particular task in timely manner. A\n\n“marketplace” or “app store” concept is a useful\n\nconstruct for many CCOD environments.\n\nDevelop components with an understanding of\n\nthe end user. Early experience is leaning toward\n\na CCOD design concept that follows a multilevel\n\nproducer/consumer design pattern. An engi­\n\nneer with operational/domain knowledge will still\n\ndevelop/compose some components, but the\n\npromise of CCOD will be fulfilled when the user/\n\noperator composes new functionality from exist­\n\ning components. Throughout the composition\n\n\nand use process, several users have differing roles\n\nand responsibilities:\n\n###### � [Combat coder or “mashup engineer”:]\n\nDevelops, prepares, and publishes the\n\ndata and services for consumption. This\n\nengineer has operational/domain knowl­\n\nedge and can compose data and visua\n\nlizations into raw application components\n\nfor users.\n###### � [Average users/operators:][ Tailor the ]\n\nraw application components to meet\n\ntheir specific needs, responsibilities,\n\nand preferences. This is the first layer of\n\nusers who consume or interpret the data,\n\npotentially adding, modifying, or filtering\n\nit before sending it up the chain. This is a\n\ntypical operator in a mission setting who\n\nhas potentially complex, mission-centric\n\nresponsibilities yet is not a computer\n\nprogrammer.\n###### � [Commander:][ High-level information con­]\n\nsumer who combines data from several\n\nsources to make final decisions.\n\nFocus on reuse. Perhaps the greatest value of\n\ncomposable capability is the reuse of someone\n\nelse’s components and compositions. Each\n\nCCOD component should be reusable and\n\ngeneric, allowing other CCOD projects to use\n\nit, particularly outside the direct composition\n\nenvironment. Each solution should be used by\n\n\n-----\n\nsuccessive CCOD projects to build on previous\n\nexperience and lessons learned. Where possible,\n\nuse existing open source solutions/tools that have\n\nadoption momentum and are adequate to the\n\ntask at hand (e.g., Restlet, PostgreSQL, PostGIS,\n\nSmile, Jetty).\n\nStrongly consider RESTful architectures. This\n\nframework has proven to be robust and flex­\n\nible enough to allow for quick development and\n\nintegration of components. Consider the Web\n\nApplication Description Language (WADL) data\n\nstandard [3], which has been used to facilitate\n\ncommunication between RESTful services. While\n\nWADLs have some potential restrictions as a\n\nresult of their simplicity, these restrictions were\n\nnot a hindrance in a Department of Defense\n\nproject using CCOD. In fact, it was an advan­\n\ntage to the implemented architecture. WADLs\n\nare intuitive, easy to write and understand,\n\nand require minimal effort for cross-service\n\ncommunication.\n\nStrongly consider loose couplers. Design com­\n\nponents and services to be independent of one\n\nanother through use of loose couplers. In some\n\nMITRE CCOD projects, effective loose couplers\n\nproved to be:\n\n###### � [UCore for mission data interoperability [4] ] � [KML for geo-referenced data [5] ] � [WADL for RESTful services ] � [CoT for tactical mission data ]\n\ninteroperability [6]\n\nUse standard industry data formats as loose\n\ncouplers for ease of reuse, integration, and adop­\n\ntion of components. Loose couplers can reduce\n\n\nthe number of required data translations from N2\n\nto 2N.\n\nExplicitly design component granularity.\n\nComponents of a composition must be at the\n\nappropriate abstraction level. For nontechnical\n\nusers to perform composition, the components\n\nmust be abstract enough to be understood, flex­\n\nible, and unusable. Minimize interaction between\n\nthe components by using a loose coupler.\n\nDocument dependencies between components/\n\nservices. It is important to test the component\n\nabstraction level with the intended user popula­\n\ntion (e.g., average user, combat coder, etc.).\n\nPrepare design/artifacts and integration plan\n\nearly to mitigate integration challenges. Due\n\nto the challenge of integrating diverse com­\n\nponents at run time, it’s important to develop\n\nsystem architecture and sequence diagrams\n\nin a CCOD project’s early stages. This helps\n\nbuild a strong foundation when later creating an\n\nintegrated system. Where possible, use approved\n\ndesign patterns to enhance extensibility and\n\nunderstanding. Clearly communicate the design\n\nand goal of the various project elements to the\n\nproject team early in the process. And, have\n\nlower level component tests along the way to\n\ntest each self-contained component, as well\n\nas enterprise-scope testing. Incorporate many\n\niterations, with small increments to functionality,\n\nto enable testing.\n\nEmphasize documentation. Documentation\n\nis crucial for any system implementing CCOD\n\nprinciples. The goal is to have combat coders and\n\naverage users who may be unfamiliar with any\n\ncomponent/service leverage or reuse this func­\n\ntionality. Document early on and throughout the\n\n\n-----\n\ndevelopment process, and provide easy discovery\n\nand navigation of the documentation. Agree on\n\nthe documentation approach at the beginning of\n\nthe project. As appropriate, use tools and formal\n\nmodeling approaches to ease communication\n\noutside the project.\n\nSeparate visualization from function. One\n\nCCOD-based prototype developed at MITRE\n\ncould not naturally conform to a typical Model\nView-Controller (MVC) design pattern, yet sepa­\n\nration of the visualization from data components\n\nremained critical. Data only has to be exposed\n\nin simple standards to be visualized in useful\n\nways. These include Hypertext Markup Language\n\n(HTML) tables, Extensible Markup Language\n\n(XML) (plain, Real Simple Syndication [RSS],\n\ngeoRSS, KML, RESTful Web services), comma\nseparated values, etc.\n\nAccommodate dependencies. A CCOD sys­\n\ntem depends and relies more heavily on other\n\nsystems’ performance and availability. If known\n\nperformance issues exist, you should duplicate\n\ndata sources, where possible, so that no piece of\n\nthe system completely depends on an unreliable\n\ncomponent. Document dependencies using a\n\nmodeling paradigm as appropriate.\n\n###### References and Resources\n\n\nScope the use of CCOD. CCOD represents a very\n\nlarge solution space. It is important to have a well\ndefined use case for CCOD that can be scoped\n\nwithin given resource and time restrictions.\n\nSeek operational context. Seek operational\n\nexpertise early to define the operational scenarios\n\nand use cases to which a CCOD solution will be\n\napplied. Some use cases, situations, or entire\n\nsystems may not be viable candidates for a CCOD\n\napproach (e.g., real-time semi-automated target­\n\ning systems).\n\nBuild from existing components. Where pos­\n\nsible, CCOD will provide capabilities built out of\n\nexisting components discoverable and integrat­\n\nable on the network and/or adaptors interfacing\n\nwith “non-composable” systems of record. Seek\n\nways to provide composable “pieces” of large\n\nsystems.\n\nVerification/validation. All components\n\nshould be tested to ensure that they operate\n\nas designed, prior to being integrated into the\n\ncomponent repository or storefront. Developer\n\ntesting should be documented so that users\n\nof components will be able to better under­\n\nstand the approved use and limitations of\n\ncomponents.\n\n\n[1. Chairman of the Joint Chiefs of Staff Instruction, March 1, 2009, CJCSI 3170.01G, Joint](http://www.dtic.mil/cjcs_directives/cdata/unlimit/3170_01.pdf)\n\n[Capabilities Integration and Development System, accessed November 28, 2009.](http://www.dtic.mil/cjcs_directives/cdata/unlimit/3170_01.pdf)\n\n2. “Agile Capability Mashup Environment (ACME) Lab Drives Innovation and Integration,”\n\n[sidebar in REACT Fine-Tunes Air Force Capabilities Before Live-Flying, accessed April 30,](http://www.mitre.org/news/digest/advanced_research/04_09/react.html)\n2010.\n\n[3. W3C Member Submission, August 31, 2009, Web Application Description Language.](http://www.w3.org/Submission/wadl/)\n\n\n-----\n\n[4. “Universal Core Advances Information Sharing Across Government Agencies,” accessed](http://www.mitre.org/news/digest/defense_intelligence/11_09/universal.html)\n\nApril 30, 2010.\n\n[5. Open Geospatial Consortium (OGC), KML Standard, accessed February 9, 2010.](http://www.opengeospatial.org/standards/kml/)\n\n[6. “Creating Standards for Multiway Data Sharing,” accessed April 30, 2010.](http://www.mitre.org/news/the_edge/summer_04/harding.html)\n\n###### Additional References and Resources\n\nAlberts, D., J. Gartska, and F. Stein, July 2002, Netcentric Warfare, CCRP Publication Series.\n\n\n-----\n\nDefinition: Open source\n\n_software (OSS) is commercial_\n\n_software for which full owner­_\n\n_ship rights are obtained by_\n\n_agreeing (without immediate_\n\n_third-party verification) to abide_\n\n_by an attached OSS license._\n\n_Agreeing to the license lets an_\n\n_individual, company, or govern­_\n\n_ment entity replicate, distribute,_\n\n_and run the application as often_\n\n_and as broadly as desired;_\n\n_obtain its human-readable_\n\n_source code; and (subject to_\n\n_release requirements that vary_\n\n_from license to license) expand_\n\n_or extend the OSS application._\n\n_Payment is indirect, usually_\n\n_consisting of agreeing to share_\n\n_value (e.g., application fixes and_\n\n_extensions) with the community_\n\n_maintaining the application._\n\nKeywords: FOSS, free and open\n\n_source software, open source_\n\n_software, OSS_\n\n\nENGINEERING INFORMATION-INTENSIVE\nENTERPRISES\n###### Open Source Software (OSS)\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to understand\n\nthe potential benefits, risks, and limits of applying\n\nopen source software (OSS) and associated sup­\n\nport processes to the construction of large sys­\n\ntems and to systems of systems. To ensure com­\n\npliance with federal regulations requiring selection\n\nand use of applicable commercial software over\n\nnew development, they should understand how\n\nand where OSS capabilities apply to systems\n\nintegration, end-user support, and configurability.\n\nThey should be aware of how OSS compares to\n\nother forms of commercial software in terms of\n\nacquisition costs, long-term support, scalability,\n\nadaptability, security, and resilience in the face of\n\nchanging requirements. SEs should be aware in\n\nparticular of the security properties of OSS at the\n\n\n-----\n\nengineering and process levels, and how those properties compare with other types of com­\nmercial and government software. They should be aware of the certification status of major\nOSS packages, and how certification works in the distributed ownership model of OSS. They\nshould understand how to interact successfully and productively with OSS support commu­\nnities, which use a barter-based economy in which payments are made in terms of software\nfixes and application capability extensions instead of fees.\n\n###### Background\n\nFew topics in the software engineering domain of systems engineering, and in engineering\ninformation-intensive enterprises, are more likely to engender strong reactions than open\nsource software. Such reactions stem mainly from the community-based ownership model of\nOSS, in which anyone who agrees to follow an associated OSS license receives the same own­\nership rights as any other user. This dispersal of engineering change authority violates one\nof the oldest and most deeply held assumptions of software engineering: High-quality, highreliability, trustworthy software is possible only if that software has been developed using a\nwell-controlled, authority-centered, top-down development process. OSS not only discards the\nneed for a centralized development authority, but turns the concept upside down by placing\ncontrol of the development process in the hands of loose collaborations of coders. Since coders\nare often viewed in software engineering as the participants least likely to understand needs\nand most likely to violate rules intended to ensure system integrity, quality, maintainability,\nand security, it is not surprising that a process that relegates change control over to coders\nwould tend to be viewed with distrust.\n\nHowever, this view is changing for three reasons. The first is the growing realization that\njust as planned market economies cannot compete with free-market economies at encourag­\ning innovation and efficient use of resources, tightly centralized management of very large\nsoftware development efforts are more likely to fail than approaches that encourage local\ninnovation and adaptation. OSS encourages such local innovation, and moreover makes the\nhuman-readable results of local innovation readily available for any desired level of inspection\nand analysis.\n\nThe second reason is the growing recognition that developing high-quality software\nunavoidably requires the use of experienced coders who have a strongly mathematical, proveit-as-you-code-it (“correct by construction”) approach to code development. Just as maintain­\ning the correctness of a complex mathematical proof requires the use of mathematicians\nwho understand those proofs fully, maintaining the correct-by-construction quality features\nof good software requires the use of experienced coders who understand fully the internal\ncorrectness features and properties of that software. OSS development relies on a wiki-like\nprocess that encourages continued participation by coders who have the theorem-proving\n\n\n-----\n\nskills needed to maintain and improve the internal correctness of well-designed software.\nIn contrast, non-wiki approaches such as waterfall development actively seek to move code\nsupport as quickly as possible to personnel who may be skilled in testing, but who are not\nusually given an opportunity to learn the structure of the software well enough to maintain\nits internal correctness properly.\n\nThe final reason is pragmatic. OSS use is widespread in both private and government sys­\ntems, and has been for many years [1]. The communication software (TCP/IP) that first made\nthe Internet possible was OSS, as were many of the early server systems that provided useful\ndata. Microsoft is one of many examples of commercial companies that make extensive use of\nopen source software to build and expand their product line. Internet Explorer is an example of\na notable Microsoft utility that is based heavily on OSS. Essentially all modern Apple products,\nfrom Macs to iPods and iPhones, are built on OSS with a thin layer of customized software on\ntop. Google is another industry leader that uses OSS heavily both internally and in its commer­\ncial products. Apple and Google are also both good examples of how effective use of OSS can\nenable more and faster innovation by keeping costly designers focused not on maintaining old\ncode, but on developing entirely new capabilities. Finally, nearly every network appliance and\ncustom hardware box sold in the open market today is built mostly or entirely using OSS. OSS\nis extremely popular with appliance vendors due to its low cost, easy scalability, flexible adapta­\ntion to new environments, broad range of available functions, and field-proven reliability.\n\n###### Government Interest and Use\n\nOn October 16, 2009, the U.S. Department of Defense (DoD) issued an updated policy on the\nuse of open source software (OSS) [2]. The policy emphasizes and explains the legal status of\n[OSS as a form of commercial software, which means that it falls under U.S. law (10 USC 2377),](http://www.law.cornell.edu/uscode/uscode10/usc_sec_10_00002377----000-.html)\nPreference for acquisition of commercial items [3]. Not including assessments of OSS options\nwhen examining commercial options for reducing costs and improving quality in DoD sys­\ntems can inadvertently violate this law. A good example of the seriousness of the commitment\nof the executive branch to assessing, selecting, and using commercial OSS is the White House\n[website http://whitehouse.gov/, which is based in part on the OSS blogging tool [4, 5, 6].](http://whitehouse.gov)\n\n###### Best Practices and Lessons Learned\n\n\nRead and understand the U.S. DoD Web page on\n\nfree and open source software (FOSS) [7]. The\n\nU.S. Department of Defense spent years creating\n\nthree documents analyzing and elaborating the\n\nrole of OSS in DoD systems. The site addresses\n\n\nDoD policy toward open source, frequently asked\n\nquestions about the federal role and legal status\n\nof open source, and a survey on the widespread\n\nprevalence and importance of OSS to the DoD as\n\nearly as 2003. The Web page is written generically\n\n\n-----\n\nand applies with very little change to other federal\n\ndepartments and agencies. MITRE systems\n\nsoftware engineers working with the DoD should\n\nin particular make sure they have looked at the\n\nOctober 16, 2009, DoD policy statement at the site.\n\nThe larger the community supporting OSS, the\n\ngreater reduction in long-term support costs.\n\nThis rule of thumb is at the heart of how OSS can\n\nprovide significant cost and capability benefits\n\nwhen building large systems. Notably, it has noth­\n\ning to do with the ability to modify code per se,\n\nand in fact can easily be seriously undermined by\n\na premature project interested in modifying OSS\n\nsource code. Because OSS support works like a\n\nconsortium, its cost benefits to individual members\n\nare highest when the consortium size is as large as\n\npossible. These cost benefits increase even further\n\nif the OSS support community is large enough\n\nto include world-class experts in specific OSS\n\nfeatures, since such members often can resolve\n\ndifficult problems in a tiny fraction of the time that\n\nwould be required by more generalized support.\n\nAvoid proliferating OSS licenses. There are\n\nalready far too many OSS licenses. However\n\ntempting it may be for an organization to create its\n\nown unique OSS license, each license simply fur­\n\nther confuses the developers, lawyers, and project\n\nmanagers who must deal with it, and also tends to\n\nsubdivide the pool of developers available to sup­\n\nport such new licenses. Four major license types\n\nare typically sufficient:\n\n###### � [GNU General Public License (GPL):][ This ]\n\npopular license requires that any new\n\nsource code made using GPL source\n\ncode must itself be licensed as GPL; that\n\nis, it must be donated back to the OSS\n\n\ncommunity that created the first source\n\ncode. Although this feature makes GPL\n\ncontroversial, it also makes it very good\n\nat stabilizing the deep infrastructure of a\n\nsystem or network by removing any profit\n\nincentive to change it arbitrarily. The Linux\n\nkernel was created in part using a GPL\n\nlicense, and demonstrates another feature\n\nof GPL: Standard interface to GPL com­\n\nponents can be used without any need for\n\nthe software that uses it to be GPL.\n###### � [GNU Lesser General Public License ]\n\n(LGPL): This is a variant of the GPL that\n\nallows GPL components to be embed­\n\nded as “library components” in non-GPL\n\ncode. It is popular with small companies\n\nthat like the GPL model but do not want to\n\nkeep businesses from using or buying their\n\nsoftware components.\n\n_Note: GNU (GNU’s Not UNIX) is a UNIX-like_\n\noperating system developed by the free\n\nsoftware movement starting in 1984. In 1992,\n\nthe almost-complete GNU system was\n\ncombined with the Linux kernel, producing\n\nthe GNU/Linux system. The GNU project\n\ndeveloped many of the core programs in\n\nGNU but also included available free soft­\n\nware such as the X Window System and TeX.\n\n###### � [Berkeley Software Distribution (BSD)/]\n\nApache: These forgiving licenses allow\n\ncompanies to “capture” copies of source\n\ncode and treat those copies and any\n\nchanges they make to them as proprietary.\n\nApple has made use of this feature of BSD\n\nlicense in creating its current Mac per­\n\nsonal computer, iPod, and iPhone product\n\n\n-----\n\nlines. Due to the high cost of individually\n\nmaintaining large sets of source code, the\n\nmajority of participants on BSD/Apache\n\nlicenses continue to support their OSS\n\nproducts under a community model. For\n\nsystems engineers, BSD and Apache\n\nlicenses should be viewed as tools for\n\nensuring that small businesses participat­\n\ning in a large system-of-systems effort\n\nwill have a strong cost incentive to adapt\n\nOSS features provided under a BSD or\n\nApache license. For example, the selection\n\nof a BSD-like licensing model for the initial\n\nrelease of the Internet communications\n\nsoftware (TCP/IP) was instrumental in get­\n\nting dozens of small and large businesses\n\nwith unique networks to accept the code\n\nand initiate the first working Internet.\n###### � [No license (government code):][ This is ]\n\nthe legally required status of code devel­\n\noped by government employees. While\n\napproximating a BSD or Apache license\n\nby allowing anyone to use it, it can cause\n\nconsiderable confusion if a person or\n\ncompany chooses to copyright the entire\n\nwork “as is” without acknowledging its\n\ngovernment origins.\n\nDon’t assume that the lawyers involved will\n\nunderstand OSS licenses. Lawyers who are not\n\ndeeply familiar with software, and more specifi­\n\ncally, how it is converted from readable source\n\ncode into executable machine code, will have a\n\nvery difficult time even reading the GPL license\n\nand LGPL licenses, let alone understanding them.\n\nBSD and Apache licenses avoid details of soft­\n\nware structure, and are far easier for lawyers to\n\n\nunderstand. Often, BSD and Apache are favored\n\nby lawyers for that reason alone: They under­\n\nstand them. This unfortunate state of affairs is\n\nslowly improving, but in the case of GPL and LGPL,\n\nprogrammers still often understand the meanings\n\nand implications of these licenses far better than\n\nthe lawyers who are responsible for assessing\n\ntheir implications. Systems engineers should be\n\naware of this possible disconnect, and if possible,\n\npoint lawyers toward relevant documents such as\n\nthe Frequently Asked Questions (FAQ) [3] on the\n\nDoD FOSS website [7].\n\nUse OSS to stabilize shared infrastructure.\n\nInfrastructure here means the software compo­\n\nnents of a large system or system-of-systems\n\nthat establish basic functions such as networking\n\nand data sharing. As demonstrated by the history\n\nof the most successful of all system-of-system\n\ninfrastructure projects, the Internet, using OSS\n\nto encourage sharing basic capabilities can be\n\na powerful tool for promoting the emergence of\n\nmore complex and often unanticipated new capa­\n\nbilities on top of that infrastructure. OSS can also\n\nhelp stabilize large systems by removing the profit\n\nincentive for companies to change features arbi­\n\ntrarily or lock customers into unique feature sets.\n\nFinally, since infrastructure is often the code that\n\nis least innovative, using OSS frees up intellectual\n\nresources for more innovative new design work.\n\nUse OSS to help focus costly resources on\n\ninnovation. The end result of factoring out\n\n“solved” problems from large systems and moving\n\nthem into OSS is shown in the pyramid-like struc­\n\nture in Figure 1. The main concept in this figure is\n\nthat by factoring out capabilities that are stable,\n\nchanging relatively slowly, and well-supported by\n\n\n-----\n\nOSS communities, an organization can pull criti­\n\ncally needed designers and coders from support\n\nroles. They can move them into more innovative\n\npositions focused on the most critical needs of\n\nthe organization, typically making use of many of\n\nthe prototyping and exploratory features of OSS\n\n(see next two paragraphs).\n\nEncourage use of OSS liaison positions. An\n\nOSS liaison is a technically proficient program­\n\nmer who has been tasked to track, participate in,\n\nand make use of a related suite of OSS applica­\n\ntions. An experienced OSS liaison both helps\n\nmake sure that the needs of an organization are\n\nunderstood and sustained by its support com­\n\nmunity, and provides quickly available internal\n\nadvice on whether and how a combination of\n\nOSS capabilities might meet or support an iden­\n\ntified system-level need. OSS liaison positions\n\nare non-standard in terms of standard software\n\nengineering job profiles, but provide one of the\n\nmost effective approaches to ensuring that a\n\n\nbroad need does not end up being translated\n\ninappropriately into a long-term software devel­\n\nopment project that will at best only replicate\n\nfeatures already available through OSS.\n\nUnderstand the advantages of OSS for explor­\n\natory prototyping. One of the most powerful\n\nfeatures of OSS is its ability to support experi­\n\nmental prototyping, including research develop­\n\nment of new features. Because OSS is developed\n\nand supported by distributed groups consisting\n\nmostly of individual coders, new features tend to\n\nget generalized quickly to make them usable by\n\nthe entire group of OSS supporters. When this\n\neffect is multiplied through multiple levels of code\n\nand across many types of systems, the result\n\ntends to be an overall set of capabilities that is\n\nunusually easy to combine in new ways and adapt\n\nto new situations. Since the source code is avail­\n\nable, it is also far easier for developers to under­\n\nstand how critical features work and the concepts\n\nbehind them. All of these features make OSS\n\n\nFigure 1. The I-4 Architecture Pyramid\n\n\n-----\n\nexceptionally well suited for exploratory prototyp­\n\ning and research into how to build entirely new\n\ncapabilities. Apple iPhones are a good example\n\nof how the highly composable interface capabili­\n\nties of OSS made it easier for Apple to prototype\n\nand develop new approaches to interacting with a\n\nmobile phone unit.\n\nUnderstand the advantages of OSS for systems\n\nand systems-of-systems integration. For many\n\nof the same reasons that OSS makes a power­\n\nful exploratory prototyping tool, it also provides\n\na powerful approach to handling the integration\n\nproblems typical of large and complex systems\n\nengineering problems. OSS includes packages\n\nand languages designed specifically to help trans­\n\nlate between diverse and often unexpected types\n\nof data and communication protocols. One of the\n\nmost important advantages of such translation\nsupporting OSS tools for large, complex systems\n\nis that they can be used to help simulate the\n\ninputs and interactions expected by older and\n\nout-of-date components of such systems. Since\n\nchanging such older components is often both\n\nvery costly and highly risky, the ability to build such\n\n“soft” interfaces to older systems while maintain­\n\ning current protocols and standards in the rest\n\nof a system-of-systems can be very valuable for\n\nminimizing overall levels of risk to the project.\n\nTreat OSS licenses like any other proprietary\n\nlicenses. It would be very unusual for a large\n\nfederal development project to consider seri­\n\nously violating the license agreements it has\n\nmade with large proprietary software companies\n\nsuch as Oracle, IBM, or Microsoft. Yet, ironically,\n\nand in part due to widespread misconceptions\n\nthat OSS means “free” software with no strings\n\n\nattached whatsoever, it is surprisingly common\n\nfor developers to violate OSS licenses, such as by\n\nstripping OSS licenses from source code. This is\n\nnot just an extremely bad idea from a develop­\n\nment quality and long-term support perspective,\n\nit is also illegal, unethical, and can result in legal\n\naction from watchdog groups such as the Free\n\nSoftware Foundation (FSF) [8]. More important,\n\nit undermines the consortium-style share-and\ncontribute model that is where the real cost\n\nreduction potential of OSS resides. Systems engi­\n\nneers should do what they can to ensure that on\n\nany given project, OSS licenses will be treated with\n\nthe same degree of respect they would give to any\n\nother commercial license.\n\nWhen building large systems, try to minimize\n\nthe need for new software. Historically, software\n\nprojects have used lines-of-code written as a way\n\nof gauging schedule progress, which has resulted\n\nin a tendency to think that more code is a good\n\nthing. However, because every new line of code\n\nentails a long-term cost and reliability burden that\n\nmay endure for decades, the real goal should be\n\nthe opposite: Systems engineers should always\n\nlook for ways to reduce the need for new, unique\n\nsoftware to an absolute minimum. Code usually will\n\nbe needed, but it should be relatively small in size,\n\npowerful in capabilities, and uniquely specific to the\n\nsystem being created. Common features such as\n\nordinary file access or standard network commu­\n\nnications never fall within this category, and should\n\nalways be handled by instead acquiring stable, well\nstandardized OSS or proprietary software.\n\nStrongly discourage any view of OSS as “free\n\nsource code” for speeding up internal develop­\n\nment. Long-term code support costs always dwarf\n\n\n-----\n\ninitial software costs for any type of software. For\n\nthis reason alone, viewing OSS as “free” source\n\ncode to speed up short-term development goals\n\nis a short-sighted and frankly dangerous perspec­\n\ntive. A good analogy is this: If your organization\n\nwas offered all of the source code for Microsoft\n\nWindows without charge, but with the stipulation\n\nthat you would have to fix all bugs and make all\n\nenhancements yourself from then on, would you\n\naccept the offer? Large OSS packages are at least\n\nas complex as Windows, so casually adopting such\n\nsource code into small internal projects creates\n\nthe support-cost equivalent of a very large and\n\nfast-ticking time bomb. A useful three-step rule\n\nof thumb is this: Try executables first, community\n\nsupport second, and new code last:\n\n###### � [Always try using the executable version ]\n\nof the OSS first: OSS tends to be more\n\nconfigurable than alternatives, so the first\n\nstep is to consult with experts to see if\n\nan OSS application can be configured to\n\nmeet a need simply by downloading the\n\n“standard release” binary form of the OSS.\n\n(For security reasons, you may still want to\n\ndownload and compile the source code\n\nlocally.)\n###### � [Next, try submitting non-sensitive ]\n\nchanges to the supporting community: If\n\nsome feature of an OSS application abso­\n\nlutely must be changed or extended to the\n\nsource code level, the next step is to try to\n\nexpress the changes needed in a way that\n\ncan be submitted directly to the commu­\n\nnity that is supporting the application. This\n\napproach not only reduces the need for\n\nlong-term support of the source code but\n\n\ncan also help build a stronger relationship\n\nwith the supporting community.\n###### � [As a last resort only, develop your ]\n\nown modules: In rare cases where code\n\nchanges absolutely cannot be made pub­\n\nlic, look for ways to develop independent\n\nmodules. If possible, avoid any inclusion of\n\nOSS source in such modules. Every line of\n\nOSS source code in a new module must\n\nbe checked and updated whenever the\n\noriginal OSS is updated, and it can also\n\nneedlessly complicate the legal status of\n\nthe module.\n\nAvoid overly casual release of government code\n\nas OSS. Government projects responsible for\n\nnonsensitive government off-the-shelf (GOTS)\n\nproducts often find the community support fea­\n\ntures of OSS highly attractive and wonder whether\n\nthey can simply release their GOTS products under\n\nOSS licenses to take advantage of the lower costs,\n\nfaster bug fixes, and improved long-term support\n\nseen in some OSS projects. The answer to this\n\nquestion is easy: No. The valuable properties of\n\nOSS support emerge from having a community of\n\npeople already interested in the product, and the\n\nvaluable modularity and flexibility of OSS emerges\n\nfrom it having been developed over a period of\n\nyears by such a community. Simply making GOTS\n\nproducts OSS by changing their license and post­\n\ning them on a website exposes all of their flaws to\n\nany interested hackers, without necessarily attract­\n\ning the interest of supporters who will in any case\n\nalmost certainly be mostly baffled by the unfamiliar\n\nsource code. A better approach to replacing GOTS\n\napplications is to look for configurations of existing\n\nOSS tools that could be used to approximate the\n\n\n-----\n\nGOTS features. Then try to start building a new\n\ncommunity around that configuration to build it\n\nup into a full-featured analog or extension of the\n\noriginal GOTS tool.\n\nEncourage a realistic understanding of security\n\nin all commercial software. If software is sold\n\nor released in the form of binary code, its secu­\n\nrity situation in the modern world is no different\n\nfrom software that has been released in the form\n\nof human-readable source code. The reason is\n\nthat modern hacking tools work directly against\n\nthe binary forms of software to attempt to crack\n\nit, making the binary form in some ways prefer­\n\nable over the human-readable form that would\n\nbe hugely slower to analyze. Thus the commonly\n\nexpressed fear that OSS cannot be made secure\n\nbecause “the source code is available” is just\n\nnonsense.\n\nConversely the argument that OSS is always more\n\nsecure because “thousands of eyes” are looking at\n\nit is also faulty for a simple reason: Just because\n\nsource code is posted on a website doesn’t mean\n\nanyone is looking at it at all. Proprietary software\n\nmay also be touted as more secure because\n\nit has been “certified” in one way or another.\n\nUnfortunately because no software certification\n\nprocesses in existence has ever been demon­\n\nstrated in a scientifically assessed field study to\n\nproduce software that is measurably more secure\n\nor reliable than uncertified software, it is not clear\n\nthat such certifications mean anything beyond\n\nthat the companies involved were able to afford\n\nthe high cost of such certifications. Certifications\n\nare applied inconsistently, with federal desktops\n\ntypically running systems and applications that\n\nhave never been certified, or which were certified\n\n\nso long ago that the certifications no longer apply.\n\nOSS is sometimes assumed to be non-secure\n\nbecause “anyone can insert a change” into the\n\ncode. Although it is true that anyone can make a\n\nchange to their own copy of OSS source code, in\n\nactuality, large, active OSS projects such as Linux\n\nhave closely guarded and highly automated code\n\ncontrol processes that only allow code to enter\n\ninto the main build process after it has been scru­\n\ntinized literally by some of the world’s top experts\n\non operating system kernels—a level of verification\n\nthat would put most proprietary code control and\n\nreview processes to shame.\n\nConversely, many proprietary processes that keep\n\nsource code secluded are riddled at multiple levels\n\nwith opportunities for people to insert changes\n\nthat could gravely damage the security of such sys­\n\ntems. The bottom line is this: Security is a process\n\nthat is best assessed based on the actual details\n\nof each case; whether it was developed using pro­\n\nprietary or OSS community methods changes the\n\nissues that must be examined. Proprietary meth­\n\nods have the advantage of bringing more money to\n\nbear, while community methods are more visible to\n\nmore users. When active and global in scope, OSS\n\ncan also bring to bear experts who are far more\n\nlikely to spot infiltration attempts.\n\nSoftware certifications: Look for them, support\n\ngetting them, but never rely on them. As noted\n\nearlier, there is no scientific evidence that software\n\ncertifications make any difference in the field-level\n\nreliability or security of software. They are nonethe­\n\nless required in many cases. For OSS, companies\n\nsuch as IBM have helped provide certifications.\n\nSystems engineers therefore should look for certi­\n\nfications of relevant OSS in case they are available,\n\n\n-----\n\nand see how they compare to proprietary equiva­\n\nlents. It is also possible for interested projects\n\nto help OSS groups get certifications, such as\n\nthrough the small proprietary companies that often\n\nhandle the business side of the use of a particular\n\nOSS component. Finally, regardless of whether a\n\n###### References and Resources\n\n\ncommercial software component is OSS or not\n\nand certified or not, the overall security of a net­\n\nworked software system should never be assumed\n\nto be proven; multiple layers of security are an\n\nabsolute necessity.\n\n\n1. The MITRE Corporation, 2003, Use of Free and Open-Source Software (FOSS) in the U.S.\n\nDepartment of Defense.\n\n2. Department of Defense, October 16, 2009, Clarifying Guidance Regarding Open Source\n\nSoftware (OSS).\n\n3. DoD NII-CIO, DoD Open Source Software Frequently Asked Questions, accessed February\n\n12, 2010.\n\n4. O’Reilly, T., October 25, 2009, “Thoughts on the Whitehouse.gov Switch to Drupal.”\n\n5. The White House, October 24, 2009. (The White House website switched to OSS.)\n\n6. Drupal.org (The White House website switched to OSS), accessed February 12, 2010.\n\n7. DoD NII-CIO, Free Open Source Software (FOSS), accessed February 12, 2010.\n\n8. Free Software Foundation, accessed February 12, 2010.\n\n###### Additional References and Resources\n\n[“Berkeley Software Distribution,” Wikipedia, accessed February 12, 2010.](http://en.wikipedia.org/wiki/Berkeley_Software_Distribution)\n\n[Clarke, G., October 27, 2009, “US DoD snuffs open-source ‘misconceptions’,” The Register.](http://www.theregister.co.uk/2009/10/27/department_defense_free_open_source/)\n\n[Coopersmith, A., November 24, 2009, “Sun relicensing to current X.org license,” Sun.com.](http://lists.x.org/archives/xorg-devel/2009-November/003670.html)\n\n[FORGE.MIL, accessed February 12, 2010.](http://www.disa.mil/Services/Enterprise-Services/Applications/Forge-Mil)\n\n[Hart, K., October 27, 2009, “Defense Department wants more open source software,” The Hill.](http://thehill.com/blogs/hillicon-valley/605-technology/65033-defense-department-wants-more-open-source-software)\n\n[Linux Distributions—Facts and Figures, DistroWatch.com, accessed February 12, 2010.](http://distrowatch.com/stats.php?section=popularity)\n\n[Military Open Source Software, accessed February 12, 2010.](http://groups.google.com/group/mil-oss?hl=en)\n\n[Open Source FAQ, Microsoft, accessed January 21, 2011.](http://www.microsoft.com/opensource/faq-all.aspx)\n\n[Open Source Initiative, accessed February 12, 2010.](http://www.opensource.org/)\n\n[“Open-source license,” Wikipedia, accessed February 12, 2010.](http://en.wikipedia.org/wiki/Open_source_license)\n\n[Ryan, J., November 25, 2009, “Sun Leaves License Behind,” Linux Journal.](http://www.linuxjournal.com/content/sun-leaves-license-behind)\n\n[SourceForge, About SourceForge, SourceForge Directory, accessed February 12, 2010.](http://sourceforge.net/)\n\n\n-----\n\nDefinition: Privacy is individuals’\n\n_claim to determine when, how,_\n\n_and to what extent their infor­_\n\n_mation is communicated [1]._\n\n_Privacy concerns the collection,_\n\n_use, maintenance, and disclo­_\n\n_sure of personally identifiable_\n\n_information (PII)—any informa­_\n\n_tion any agency has about an_\n\n_individual, including information_\n\n_that can be used to distinguish_\n\n_or trace an individual’s identity_\n\n_(name, SSN, date and place of_\n\n_birth, mother’s maiden name,_\n\n_biometric records) or that is_\n\n_linkable to an individual (medi­_\n\n_cal, educational, financial, and_\n\n_employment records) [2]._\n\nKeywords: E-Government Act,\n\n_Fair Information Practices_\n\n_(FIPs), personally identifi­_\n\n_able information (PII), privacy,_\n\n_Privacy Act, privacy impact_\n\n_assessments (PIA) record,_\n\n_system of record_\n\n\nENGINEERING INFORMATION-INTENSIVE\nENTERPRISES\n###### Privacy Systems Engineering\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the basic concept of privacy and\n\nbe able identify PII and the situations in which\n\nprivacy issues may arise. They should under­\n\nstand the legal requirements that apply to\n\nfederal agencies’ collection, use, maintenance,\n\nand disclosure of PII, and how these require­\n\nments relate to the systems engineering life\n\ncycle (SELC). Further, systems engineers are\n\nexpected to develop, implement, and maintain\n\ntechnical controls to be included in informa­\n\ntion technology (IT) systems, which help\n\nensure that privacy requirements are met.\n\n\n-----\n\n###### Background\n\nPrivacy is based on the implementation of Fair Information Practices (FIPs), which were ini­\ntially developed in 1973 by a federal advisory committee, commissioned because of concern\nover the harmful consequences that computerized data systems could have on the privacy\nof personal information. A revised version of the FIPs, developed by the Organization for\nEconomic Cooperation and Development in 1980, has been widely adopted and forms the\nbasis for many privacy laws worldwide (see Table 1) [3].\n\nTable 1. The Fair Information Practices\n\n**Principle** **Description**\n\nThe collection of personal information should be limited,\n\nCollection limitation should be obtained by lawful and fair means, and, where\n\nappropriate, with the knowledge or consent of the individual.\n\nPersonal information should be relevant to the purpose for\n\nData quality which it is collected, and should be accurate, complete, and\n\ncurrent as needed for that purpose.\n\nThe purposes for the collection of personal information\nshould be disclosed before collection and upon any change\n\nPurpose specification\n\nto that purpose, and its use should be limited to those pur­\nposes and compatible purposes.\n\nPersonal information should not be disclosed or otherwise\n\nUse limitation used for other than a specified purpose without consent of\n\nthe individual or legal authority.\n\nPersonal information should be protected with reasonable\n\nSecurity safeguards security safeguards against risks such as loss or unauthorized\n\naccess, destruction, use, modification, or disclosure.\n\nThe public should be informed about privacy policies and\n\nOpenness practices, and individuals should have ready means of learn­\n\ning about the use of personal information.\n\nIndividuals should have the following rights: to know about\nthe collection of personal information, to access that infor­\n\nIndividual participation\n\nmation, to request correction, and to challenge the denial of\nthose rights.\n\nIndividuals controlling the collection or use of personal infor­\n\nAccountability mation should be accountable for taking steps to ensure the\n\nimplementation of these principles.\n\n_Source: Organization for Economic Cooperation and Development_\n\n|Principle|Description|\n|---|---|\n|Collection limitation|The collection of personal information should be limited, should be obtained by lawful and fair means, and, where appropriate, with the knowledge or consent of the individual.|\n|Data quality|Personal information should be relevant to the purpose for which it is collected, and should be accurate, complete, and current as needed for that purpose.|\n|Purpose specification|The purposes for the collection of personal information should be disclosed before collection and upon any change to that purpose, and its use should be limited to those pur­ poses and compatible purposes.|\n|Use limitation|Personal information should not be disclosed or otherwise used for other than a specified purpose without consent of the individual or legal authority.|\n|Security safeguards|Personal information should be protected with reasonable security safeguards against risks such as loss or unauthorized access, destruction, use, modification, or disclosure.|\n|Openness|The public should be informed about privacy policies and practices, and individuals should have ready means of learn­ ing about the use of personal information.|\n|Individual participation|Individuals should have the following rights: to know about the collection of personal information, to access that infor­ mation, to request correction, and to challenge the denial of those rights.|\n|Accountability|Individuals controlling the collection or use of personal infor­ mation should be accountable for taking steps to ensure the implementation of these principles.|\n\n\n-----\n\nThe centerpiece of the federal government’s legal framework for privacy protection, the\nPrivacy Act of 1974, is based on the FIPs and provides safeguards for information maintained\nby federal agencies. Specifically, the act places limitations on agencies’ collection, disclo­\nsure, and use of personal information maintained in systems of records. The act describes\na “record” as any item, collection, or grouping of information about an individual that is\nmaintained by an agency and contains his or her name or another personal identifier. It also\ndefines “system of records” as a group of records under the control of any agency from which\ninformation is retrieved by the name of the individual or by an individual identifier. The\nPrivacy Act requires that when agencies establish or make changes to a system of records,\nthey must notify the public through a notice in the Federal Register. Agencies are allowed to\nclaim exemptions from some of the provisions of the Privacy Act if the records are used for\ncertain purposes, such as law enforcement. However, no system of records can be exempted\nfrom all of the Privacy Act’s provisions. For example, agencies must publish the required\nnotice in the Federal Register for all systems of record, even those that involve classified\ninformation. This ensures that the federal government does not maintain secret systems of\nrecords—a major goal of the act [4].\n\nMore recently, in 2002, Congress enacted the E-Government Act to, among other things,\nenhance protection for personal information in government information systems or informa­\ntion collections by requiring that agencies conduct privacy impact assessments (PIAs). A\nPIA is an analysis of how personal information is collected, stored, shared, and managed in\na federal system; it is used to identify privacy risks and mitigating controls to address those\nrisks. Agencies must conduct PIAs (1) before developing or procuring information technol­\nogy that collects, maintains, or disseminates information that is in identifiable form, or (2)\nbefore initiating any new data collections of information in an identifiable form that will be\ncollected, maintained, or disseminated using information technology if the same questions\nare asked of 10 or more people [5]. Individual PIAs may vary significantly depending on\nthe specifics of department/agency guidance and the scope and sensitivity of PII collected,\nused, and disseminated.\n\nNote that privacy is not synonymous with security. (See Figure 1.) Whereas privacy\nfocuses on the individual’s ability to control the collection, use, and dissemination of their\nPII, security provides the mechanisms to ensure confidentiality and integrity of information,\nand the availability of information technology systems. The concepts of privacy and secu­\nrity, however, do intersect. Specifically, certain IT controls established to ensure confidenti­\nality and integrity from a security perspective also support privacy objectives. For example,\naccess controls ensure that only authorized individuals can read, alter, or delete data. Such\ncontrols help achieve confidentiality and integrity from a security standpoint. In addition,\nwhen a system processes or stores PII, these IT controls ensure that users can access only\n\n\n-----\n\nAssures that information is disclosed only\nto authorized individuals and systems\n\n\n_Source: MITRE_\n\n\nAssures that information\nsystems — and the data\ncontained in them — are\navailable to authorized users\nwhen needed\n\nFigure 1. Privacy vs. Information Security\n\n\nGuards against improper\ninformation modification\nor destruction\n\n\nthe specific PII needed to perform their jobs; this helps ensure that use of PII is limited\nto authorized purposes (purpose specification) and protected from unauthorized access,\ndestruction, and disclosure (security safeguards). Although establishing good security prac­\ntices helps protect privacy, these practices are not, in and of themselves, sufficient to fully\naddress the FIPs.\n\n###### Best Practices and Lessons Learned\n\n\nPrivacy must be “built into” the systems\n\nengineering life cycle. Consideration of privacy,\n\nincluding requirements to conduct PIAs, should\n\nbe built into the agency systems engineering life\n\ncycle. This helps ensure that privacy requirements\n\nare considered early in the development of IT\n\nsystems and that the technology is leveraged to\n\nprovide privacy protections. At a minimum, the\n\n\nSELC should include the requirement to conduct\n\na PIA as early in the development process as\n\npracticable. In addition, the SELC should contain\n\nprocedures that require the completion of the\n\nPIA before the system is authorized to operate.\n\nConsidering privacy requirements early in the\n\ndevelopment process avoids difficult and expen­\n\nsive retrofitting to address them later.\n\n\n-----\n\nAll appropriate stakeholders must be involved\n\nin assessing privacy risks. In conducting the PIA,\n\nit is critical that the privacy office, the systems\n\ndeveloper, and the business process owner all\n\nbe involved. Having the appropriate stakeholders\n\ninvolved ensures that all privacy risks are identi­\n\nfied and that alternative mitigating controls are\n\nconsidered.\n\nTechnology can be leveraged to protect privacy.\n\nAlthough many of the privacy risks identified\n\n\nthrough the PIA will be mitigated by establish­\n\ning administrative controls—such as providing\n\nadditional public notice, establishing policies and\n\nprocedures to allow individuals access to the\n\ninformation held about them, or providing privacy\n\ntraining to system users—certain risks can be\n\nmitigated by technical system controls. Table 2\n\nprovides examples of how a system should be\n\ndesigned to protect privacy.\n\n\nTable 2. How System Engineers Can Implement FIPs\n\n|Principle|Guidance for Systems Engineers|\n|---|---|\n|Collection limitation|Design the system to use only the minimum amount of PII neces­ sary to accomplish the system’s purpose. The key question to ask for each field of PII is: Can the purpose of the system be served without this particular field of PII?|\n|Data quality|Develop the system to meet the data quality standards estab­ lished by the agency.|\n|Purpose specification|Develop systems that interact directly with the public such that the purpose for the collection of PII is made available.|\n|Use limitation|Develop the system such that each field of PII is used only in ways that are required to accomplish the project’s purpose. Each pro­ cess associated with each field of PII should be reviewed to deter­ mine whether that use directly fulfills the project’s purpose. If not, the function should not be developed.|\n|Security safeguards|Implement information security measures for each field of PII to prevent loss, unauthorized access, or unintended use of the PII. Use encryption, strong authentication procedures, and other security controls to make information unusable by unauthorized individuals. Note: OMB guidance directs that only cryptographic modules certified by the National Institutes for Standards and Technology (NIST) are to be used. See NIST’s website at http://csrc.nist.gov/ cryptval/ for a discussion of the certified encryption products [7].|\n\n\n-----\n\n|Openness|Design the system to provide both a security and privacy state­ ment at every entry point. Develop mechanisms to provide notice to the individual at the same time and through the same method that the PII is collected; for example, if PII is collected online, notice should also be provided online at the point of collection.|\n|---|---|\n|Individual participation|Design the system to allow identification of all PII associated with an individual to allow correction of all PII, including propagating the corrected information to third parties with whom the information was shared.|\n|Accountability|Accountability can be encouraged, in part, by the use of audit logs that are capable of supporting a comprehensive audit of collection and use of all fields of PII to ensure that actual collection and use is consistent with the notice provided. Audit logs should not contain the actual content of fields of PII, to limit unnecessary disclosure of this information. The audit log should contain sufficient detail to identify (1) the source of each field of PII, (2) when each field of PII was accessed, (3) the uses of each field of PII, and when and by whom this infor­ mation was used, (4) when each piece of PII was last updated and why, and (5) any suspicious transactions related to any field of PII and, if these occurred, the nature of the suspicion and the specific users involved. If the use of Social Security numbers (SSNs) is authorized, sys­ tems engineers should create mechanisms to log access to SSNs and implement periodic reviews of the audit logs for compliance with the authorization. The audit logs should also record sufficient information to sup­ port an audit of whether each field of PII was shared pursuant to a determination that the recipients needed the field of PII to suc­ cessfully perform their duties, possessed the requisite security clearance, and provided assurance of appropriate safeguarding and protection of the PII.|\n\n\n_Source: MITRE and Department of Homeland Security [6]_\n\nNote that this list is not intended to be exhaustive. The specific risks and mitigating con­\ntrols for an IT system or information collection should be identified by conducting a PIA.\n\nFinally, systems engineers should consider the use of enterprise privacy-enhancing\ntechnologies (ePETs). ePETs are enterprise-oriented, data stewardship tools that help\norganizations achieve their business goals while appropriately managing PII throughout\n\n\n-----\n\nthe information life cycle. These technologies may or may not be privacy-specific.\nePETs include tools that can desensitize static data in databases by applying a variety\nof transformations, including masking and obfuscation. Desensitized data can then be\nused for testing and other purposes without unnecessarily disclosing individuals’ PIIs.\nAnother example is enterprise digital rights management, which can be used to impose\nlimitations on the usage of digital content and devices, and can also be used to enforce\nlimitations on the use of PII [8].\n\n###### References and Resources\n\n1. Westin, A., 1967, Privacy and Freedom, NY: Athenaeum.\n\n[2. Office of Management and Budget (OMB), July 12, 2006, Reporting Incidents Involving](http://www.whitehouse.gov/sites/default/files/omb/assets/omb/memoranda/fy2006/m06-19.pdf)\n\n[Personally Identifiable Information, M-06-19.](http://www.whitehouse.gov/sites/default/files/omb/assets/omb/memoranda/fy2006/m06-19.pdf)\n\n3. Office of Economic Cooperation and Development, September 23, 1980, OECD\n\nGuidelines on the Protection of Privacy and Transborder Flows of Personal Data.\n\n4. The Privacy Act of 1974, as amended, 5 U.S.C. § 552a.\n\n5. The e-Government Act of 2002, Public Law 107-347.\n\n[6. Privacy Office of the Department of Homeland Security, August 16, 2007, Privacy](http://www.dhs.gov/xlibrary/assets/privacy/privacy_guide_ptig.pdf)\n\n[Technology Implementation Guide.](http://www.dhs.gov/xlibrary/assets/privacy/privacy_guide_ptig.pdf)\n\n7. OMB, May 7, 2007, Safeguarding Against and Responding to the Breach of Personally\nIdentifiable Information, M-07-16.\n\n8. Shapiro, S., February 27, 2009, A Gentle Introduction to Privacy Enhancing Technologies,\n\nPowerPoint Presentation.\n\n###### Additional References and Resources\n\nCannon, J. C., 2004, Privacy: What Developers and IT Professionals Should Know,\nAddison-Wesley.\n\nMcEwen, J., and S. Shapiro (eds.). 2009, U.S. Government Privacy: Essential Policies and\n_Practices for Privacy Professionals. York, ME: International Association of Privacy_\nProfessionals.\n\nMITRE has also developed several methodologies to support privacy engineering. For\nexample, Privacy-Based Systems Analysis (PBSA) supports systematic strategic pri­\nvacy analysis and planning at the program or mission level. It enables organizations\nto rigorously consider the alignment between privacy objectives and business and\ntechnical architectures. Privacy Risk Maps provide a view of enterprise privacy risk\n\n\n-----\n\nby documenting where PII resides and how it moves within the organization and across\norganizational boundaries. MITRE has also developed the Privacy RIsk Management\nEngine (PRIME), a Web-based PIA tool to support more effective privacy risk analysis and\nmitigation.\n\n\n-----\n\n##### Systems Engineering for Mission\n Assurance\n\nDefinition: Mission assurance means operators achieve the mission, continue\n\n_critical processes, and protect people and assets under internal/external attack_\n\n_(physical and cyber), unforeseen environmental or operational changes, or_\n\n_system malfunction. Across the acquisition life cycle, systems engineering for_\n\n_mission assurance enables awareness of changing adversarial strategies and_\n\n_environmental and system conditions, options to achieve a mission under differ­_\n\n_ent circumstances, tools to assess and balance advantages/risks of options, and_\n\n_transition to an option while continuing the mission._\n\nKeywords: assurance, attack, cyber, dependability, information, mission, opera­\n\n_tional, quality, resilience, risk, success, supply, threat_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to be conversant in mis­\n\nsion operations, advanced persistent threats, unforeseen environmental\n\nchanges, and system malfunctions that can cause missions to fail. They\n\nare expected to be familiar with the basic principles for building and\n\noperating systems that can sufficiently fight or operate through these\n\nobstacles. They should be knowledgeable in the effects that the mis­\n\nsion/operators are attempting to achieve and the various options and\n\nalternatives that systems or combinations of systems can provide to\n\nachieve these effects. MITRE SEs need to understand methods for\n\n\n-----\n\ndetermining vulnerabilities, countermeasures, and residual risks to mission accomplishment\nbased on available system options and alternatives. They are expected to be able to effectively\nconvey these methods and the results of applying them to system stakeholders and decision\nmakers. MITRE SEs are expected to recommend requirements, strategies, and solutions for\nmission assurance capabilities, including consideration of technical and operational dimen­\nsions across all phases of the system life cycle, from concept development through deployment\nand operations. They are expected to encourage and facilitate active participation of end users\nand other stakeholders in developing capabilities that will ensure mission success. They are\nexpected to monitor and evaluate contractor mission assurance technical efforts and recom­\nmend changes when warranted. MITRE SEs are also expected to keep abreast of the evolving\ndiscipline of systems engineering for mission assurance.\n\n###### Context\n\nThe concept of engineering a system that can withstand purposeful or accidental failure or\nenvironmental changes has a long history in the discipline of designing systems for surviv­\nability. In the Cold War era, designing for survivability meant nuclear hardening of command\ncenters, creating alternate command centers, engineering electronic countermeasures into\ncommunications and sensor systems, building redundant backup components, and engineer­\ning fallback modes of operation and switchover capabilities among them. More recently, the\nnotion of engineering for mission assurance has been extended to ensuring the ability to\neffectively operate at the “tactical edge” in an environment with limited, austere, or intermit­\ntent communications, by selecting from a variety of communications options in the event\nthat primary means become inoperable. Designing communications systems for survivability\nmeant redundant communications links and factoring in potential adversary actions such as\nelectronic warfare. Although all these are still needed in the Internet era, engineering sys­\ntems for mission assurance has been further expanded to include engineering for information\nassurance and cyber security.\n\nIn recent years, cyber threats have become the predominant focus of mission assurance.\nSome worry that such intense focus on “all things cyber” risks losing sight of other dimen­\nsions of mission assurance. Others see a tension or conflict between mission assurance’s “get\nthe operational job done” ideal of achieving 100 percent mission success every time and the\nsecurity-focused aims of information assurance, which could at times constrain aspects of\noperations in order to protect data and systems. Yet others are concerned that the acquisition\ncommunity does not have a sufficiently mature mission assurance culture or mindset and\nthat we are not yet sufficiently attuned to mission assurance as an “implicit requirement” that\nneeds to be considered for all systems, whether or not it is explicitly demanded.\n\n\n-----\n\nWhen we engineer for mission assurance, what essential attribute are we seeking to\n“engineer in”? Is it robustness, resilience, dependability, risk management, security, agility,\nflexibility, or adaptability? Is it one of them, some of them, or all of them? What are the tradeoffs and how are they determined? Who is the decision maker—the operator, the overseer, or\nthe accreditor—and what role should each play in the decision-making process? What does\n“systems engineering for mission assurance” look like? The reality is that we don’t yet have\na complete answer. But we do have partial answers, and we are continuously evolving our\nunderstanding and practice of it every day. What we do know is that, taken together, the vari­\nous dimensions of mission assurance pose some of the most difficult challenges in engineer­\ning systems today.\n\nThe working definition of “systems engineering for mission assurance” in this guide is\nrooted in the insight that operational users of military systems are almost always willing to\n_accept some level of risk in accomplishing their missions. It is in the nature of their profession,_\nbut to do that, they need the tools to understand the risks they are accepting and the ability to\nassess and balance available options and alternatives. This suggests that “systems engineer­\ning for mission assurance” is the art of engineering systems with options and alternatives\nto accomplish a mission under different circumstances and the capability to assess, under­\nstand, and balance the associated risks. Options and alternatives will likely take the form of\na blend of technical and operational elements, which requires systems engineer to have an\nintimate understanding of the technical details and limitations of the system, the doctrine\nand operations of the user, and the environmental conditions and threats that will or may be\nencountered.\n\n###### Articles Under This Topic\n\nThe articles under this topic are focused on what we know today about systems engineering\nfor mission assurance. It is a rapidly evolving field, so check back often for updates and addi­\ntional material.\n\n“Cyber Mission Assurance” structures the cyber response discussion around the notion of\na system architecture that is resilient in the face of different levels of cyber threat. The article\nfocuses on near-term actions, rooted in actual experience, to begin evolving architectures\nthat reduce their attack surface and are more secure, resilient, understandable, agile, and\nmanageable.\n\nThe next three articles step through the elements of the mission assurance engineering\n(MAE) process. “Crown Jewels Analysis (CJA)” is a methodology that helps identify the cyber\nassets most critical to mission accomplishment—the “crown jewels” of a crown jewel analy­\nsis—and that begins during system development and continues through deployment. “Cyber\nThreat Susceptibility Assessment” helps understand the threats and associated risks to those\n\n\n-----\n\nassets. “Cyber Risk Remediation Analysis (RRA)” is used to identify and select mitigation\nmeasures to prevent or fight through cyber-attacks.\n\n“Secure Code Review” provides an overview of the specialized task of automatically or\nmanually reviewing security-related weaknesses of an application’s source code to understand\nwhat classes of security issues are present. The goal of a secure code review is to arm the\nsystems engineer and code developer with information to make an application’s source code\nmore sound and secure.\n\n“Supply Chain Risk Management” discusses the threats to and vulnerabilities of commer­\ncially acquired information and communications technologies that government information\nand weapon systems use. It discusses how to minimize the risk to systems and their com­\nponents from sources that are not trusted or identifiable, or that provide inferior materials or\nparts.\n\n###### References and Resources\n\n[MITRE Digest, May 2010, “Mission Not Impossible,” The MITRE Corporation.](http://www.mitre.org/news/digest/defense_intelligence/05_10/itdefense.html)\n\n[Guerro, S. B., and W. F. Toseny, eds., The Aerospace Corporation, 2007, Mission Assurance](https://aeroweb.aero.org/m_dir/maddl.nsf/02380404B01E96368825757E00751050/$file/TOR-2007(8546)-6018_REV_A.pdf)\n[Guide, TOR-2007(8546)-6018 Rev. A.](https://aeroweb.aero.org/m_dir/maddl.nsf/02380404B01E96368825757E00751050/$file/TOR-2007(8546)-6018_REV_A.pdf)\n\n[Gupta, Rahul, 2006, The Need for Mission Assurance, PRTM.](http://www.prtm.com/uploadedFiles/Strategic_Viewpoint/Articles/Article_Content/PRTM_The_Need.pdf)\n\nNational Defense Industrial Association (NDIA) System Assurance Committee, 2008,\n[Engineering for System Assurance, Arlington, VA.](http://www.acq.osd.mil/se/docs/SA-Guidebook-v1-Oct2008.pdf)\n\n\n-----\n\nDefinition: Mission assurance\n\n_is “a process to ensure that_\n\n_assigned tasks or duties can_\n\n_be performed in accordance_\n\n_with the intended purpose or_\n\n_plan ... to sustain . . . operations_\n\n_throughout the continuum of_\n\n_operations” [1]. It is executed_\n\n_through a “risk management_\n\n_program that seeks to ensure_\n\n_the availability of networked_\n\n_assets critical to department_\n\n_or agency missions. Risk_\n\n_management activities include_\n\n_the identification, assessment,_\n\n_and security enhancement of_\n\n_assets essential for executing_\n\n_... national ... strategy” [1]. Cyber_\n\n_mission assurance focuses_\n\n_on threats resulting from our_\n\n_nation’s extreme reliance on_\n\n_information technology (IT)._\n\nKeywords: architecture, cyber,\n\n_cyber threat, mission assur­_\n\n_ance, resilience_\n\n\nSYSTEMS ENGINEERING FOR MISSION\nASSURANCE\n###### Cyber Mission Assurance\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be\n\nable to help customers acquire robust systems\n\nthat can successfully execute their mission even\n\nwhen under attack through cyberspace. To do\n\nthis, MITRE SEs are expected to be conversant\n\nin mission operations, the various types of cyber\n\nthreats to IT systems, and system malfunctions\n\nthat can cause missions to fail. They are expected\n\nto be familiar with best security practices and the\n\nbasic principles for building and operating systems\n\nthat can sufficiently fight or operate through these\n\nobstacles, including architecture resilience against\n\nthe upper end of the cyber threat spectrum. Given\n\nthe complexity, variety, and constant change of\n\ncyber threats, MITRE SEs should seek out MITRE\n\ncyber security experts to support these activities.\n\n\n-----\n\nThey are expected to recommend requirements, architectures, strategies, and solutions for\ncyber protection and mission assurance capabilities, including consideration of technical and\noperational dimensions across all phases of the system life cycle, from concept development\nthrough deployment and operations. MITRE SEs are also expected to keep abreast of the\nevolving discipline of engineering for cyber mission assurance.\n\n###### Background and Introduction\n\nToday’s information technology (IT) environments are increasingly subject to escalating cyber\nattacks. Cyber threats vary widely in sophistication, intent, and consequences to the targeted\nsystems and networks. The range of attackers extends from users who unintentionally dam­\nage systems to hackers, cyber criminals, and full-scale cyber spies and cyber warriors; their\nintentions span from annoying vandalism to economic threats to taking out the electric grid\nor defeating armed forces. Similarly, the target of the attacks can vary from a single computer\nor router to an entire online banking system, business enterprise, or global supply chain. At\nthe same time, missions and businesses fall along a spectrum of criticality—from desirable\nto necessary, essential, and mission- or safety-critical. Given the broad spectrums of threat,\nintent, and consequence to mission-critical functions, determining exactly where mission sys­\ntems lie in this continuum of dimensions is vital to determine the appropriate level of invest­\nment and response.\n\nThe notion that 100 percent cyber protection can be achieved is not only unrealistic but\nalso results in a false sense of security that puts our missions and businesses at serious risk.\nConsequently, the inability to achieve full protection must be compensated by ensuring that\nmissions can be accomplished despite cyber attacks.\n\nWhen engineering systems for cyber mission assurance, the focus needs to build upon\nengineering defensive capabilities via protection technologies and engineering both offen­\nsive and defensive capabilities within a comprehensive framework of risk management.\nEngineering for cyber mission assurance requires a mindset akin to the air traffic control\n(ATC) system concept of “graceful degradation.” Weather cannot be controlled, so the air\ntraffic controllers plan how to gracefully degrade the flow of air traffic during bad weather,\nand the ATC systems are engineered to enable the execution of those plans. It also calls for\naddressing the unexpected and the undetectable cyber attack in ways that make the adver­\nsary’s exploit harder and more costly, less likely to succeed, and more likely to cause minimal\nimpact on mission operations.\n\nCyber defenses generally available today help address low-end threats but alone are often\nineffective against more capable forms of cyber attacks that may target our most missioncritical systems. It is at the high end of the continuum that resilience of the system architec­\nture will matter most—to enable continuity of mission-critical operations and support rapid\n\n\n-----\n\nreconstitution of existing or minimal essential capabilities or the deployment of alternative\nmeans of accomplishing the mission.\n\nThus, although this article presents ideas along the full spectrum of cyber security, it\nconcentrates on architectural resilience against the upper end of the threat spectrum, where\nthe stakes are high, the mission or business is critical, and the adversary is sophisticated,\nmotivated, and persistent.\n\nNevertheless, many of the same techniques are valuable at the low to medium levels of\nthreats and consequences because they can significantly reduce the operational impact and\ncost of cleanup after an attack. Even if the intentions and consequences of a threat are cur­\nrently not very serious, it must be kept in mind that today’s massive data thefts or passive\nreconnaissance can quickly escalate into data and system modification, surreptitious com­\nmandeering of control, or denial of essential services with far more dire mission impact in the\nfuture.\n\nSome of the recommendations in this article are clearly in the domain of systems engi­\nneers or designers, while others may fall more naturally to program managers, cyber security\nexperts, or operational users and their leadership. Cyber mission assurance recommendations\nare most effective when used in combinations to achieve an overall security strategy. The\nrecommendations are therefore presented as a whole instead of attempting to parse out those\nthat fall under the particular purview of systems engineering.\n\nLast, the specific recommendations that follow will not be practical for all systems. Some\ncan be done in the short term for certain systems, like those now in design, but would take\nlonger or might never be implemented for legacy systems with a large installed base. Some\nrecommendations address a common concern but in different ways, with the expectation that\npractitioners will find something that makes cost-effective sense for their particular situa­\ntion. Following any of these recommendations will decrease risk, but the recommendations\nare best followed in combinations derived from following a security engineering strategy and\npractice based on modeling the threats the user is trying to protect against.\n\n###### Three Levels of Cyber Threat\n\nLow-end threats are often known as hackers or script kiddies, and their techniques typi­\ncally involve email phishing and hacking. They often take advantage of widely known,\nstill-unpatched vulnerabilities in today’s operating systems, applications, and hardware. The\nmotive can be mischief or the bragging rights that come with success. Yet, the same vulner­\nabilities used by the low-end threat can be used by any threat, including high-end.\n\nMid-range threats are often state-sponsored and will use low-end techniques to target\nwell-known vulnerabilities where effective. They may also use zero-day attacks (that take\nadvantage of the delay between when vulnerabilities are discovered and when they are\n\n\n-----\n\nreported and corrected); perform reconnaissance and probing to gain knowledge of infrastruc­\nture, controls, and configuration weaknesses; and use social engineering to manipulate online\nusers into revealing personal information and other exploits to exfiltrate and/or manipulate\ninformation. Mid-range threats can remotely implant malware (viruses, worms, adware, or\nspyware that can threaten a network) and back doors, cause denial-of-service attacks, and\nintroduce undetectable software modifications that can hide in a system once penetrated and\nmaintain presence across many types of system changes. The cyber espionage documented in\na report to the U.S.-China Economic and Security Review Commission is an example of this\ntype of threat [2].\n\nHigh-end threats use all of the above techniques and add the ability to circumvent physi­\ncal security measures; create undetectable hardware and software modifications and insert\nthem via the supply chain; plant or turn insiders in the target organization; and use full-spec­\ntrum intelligence to identify targets and vulnerabilities.\n\n###### Responding to Low-End Threats\n\nSome suggestions and resources for managing low-end threats are provided in the paragraph\nbelow. More details are at the references cited.\n\nResponding to low-end threats is tedious and costly, but a necessary part of using IT.\nUsing Security Content Automation Protocol (SCAP)-compliant tools will make dealing with\nlow-end threats a more automated process. Even when faced with a mid-range or high-end\nthreat, it makes sense to first deal with the low-end threat, rather than get distracted and let\ndefenses down. Dealing with the low-end threat involves the application of end-to-end solu­\ntions incorporating commonly understood components such as firewalls, anti-virus protec­\ntion, anti-spyware protection, anti-spam protection, intrusion detection, vulnerability patching\ntools, and scans for wireless access points. Note: The Security Content Automation Protocol\n(SCAP) is a method for using specific standards to enable automated vulnerability manage­\nment, measurement, and policy compliance evaluation (e.g., FISMA compliance). The National\nVulnerability Database (NVD) is the U.S. government content repository for SCAP\n\nThe SANS Institute maintains a list of the “Top 20 Internet Security Problems, Threats,\nand Risks” and what to do about them [3]. MITRE and SANS also produced a list of the “Top\n25 Programming Errors” [4]. The SANS website also hosts the Consensus Audit Guidelines\n(CAG) Twenty Critical Controls for Effective Cyber Defense [5]. However, although commonly\nunderstood, these defenses are often either not employed, incompletely employed, misconfig­\nured, or not maintained. Note: The SANS Institute, founded in 1989, provides computer secu­\nrity training, professional certification through Global Information Assurance Certification\n(GIAC), and a research archive—the SANS Reading Room. It also operates the Internet Storm\n\n\n-----\n\nCenter, an Internet monitoring system staffed by a global community of security practitioners.\nSANS is an acronym for SysAdmin, Audit, Network, and Security.\n\n###### Responding to Higher Level Threats\n\nFor obvious reasons, most detailed suggestions and resources for managing mid- to high-end\nthreats are sensitive, closely held, and often rapidly evolving to keep pace with ever-changing\nthreats. Recently, cyber mission assurance thought leaders have started structuring the cyber\nresponse discussion around the notion of a system architecture that is resilient in the face of\ndifferent levels of cyber threat. The MITRE report Building Secure, Resilient Architectures\nfor Cyber Mission Assurance [6] surveys the emerging thinking on building secure, resilient\narchitectures for cyber mission assurance. It motivates the need, lays out the goals and objec­\ntives of a resilient cyber architecture, defines its key characteristics, and provides an overview\nof key resilience techniques and mechanisms.\n\n###### Defining Resilient Architectures\n\nThe term resilience has many definitions depending on the context and application. For a\ncomputing paradigm, the simple definition from the University of Kansas’s ResiliNets Project\nproves most useful: “Resilience is the ability to provide and maintain an acceptable level\nof service in the face of faults and challenges to normal operation.” Resilience is related to\nsurvivability, which builds on the disciplines of security, fault tolerance, safety, reliability, and\nperformance.\n\nGovernment departments and agencies are increasing their attention to resilience. Though\nthis increased attention clearly indicates an understanding of the importance of resilience, the\ncommunity is just beginning to understand what it means to turn the concept into practice.\nMuch work is needed to define and validate resilience: techniques and strategies; policies to\npromote operational and system resilience; risk decision methodologies, analytic processes,\nand acquisition guidance; and metrics for measuring resilience improvements and evaluating\nprogress. Moreover, funding must be aligned to budget cycles to reflect these needs and build\nmomentum.\n\nGame-changing technologies, techniques, and strategies can make transformational\nimprovements in the resilience of our critical systems. A number of the detailed ideas on\nbuilding secure, resilient architectures for cyber mission assurance in [6] are future-looking\nand suggest the art of the possible from which to begin evaluating the viability of promising\nstrategies and techniques for resilience, singly and in combination, to determine which are the\nmost cost-effective to pursue.\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nThe following four items are among the most\n\nmature practices of engineering for cyber mission\n\nassurance. They are rooted in actual experience.\n\nThe references point to more details.\n\nTo begin evolving our architectures to be more\n\nsecure and resilient, the first step is to reduce\n\ntheir attack surface and make them more under­\n\nstandable, agile, and manageable. Near-term re\narchitecting actions can begin now by addressing\n\nthese four principles:\n\nVirtualization: Leverage or introduce virtualiza­\n\ntion as a foundation to implement techniques\n\nfor isolation, non-persistence, replication,\n\nreconstitution, and scaling. Doing so will support\n\ncapabilities to constrain attacks and damage\n\npropagation, improve availability, and provide agil­\n\nity to create, deploy, and move critical capabilities\n\nat will (moving target defense) if the system is\n\nunder attack. ([6], pp. 8, 10, 11–14)\n\nNon-persistence: Non-persistence techniques\n\ncan be applied for access to data, applications,\n\nand connectivity when continuous access is\n\nnonessential. They can be used to reduce the\n\nexposure of the data and applications, as well as\n\nthe opportunity for the adversary to analyze our\n\nvulnerabilities or gain a stronghold and maintain\n\na persistent presence. Non-persistence can also\n\nprovide operational provisioning and management\n\nbenefits by pushing a new gold image when a\n\nuser connects each day or at some fixed interval,\n\nthus reducing the period of vulnerability. The goal\n\nis to set the frequency so that refreshes occur\n\noften enough to prevent the spread or intended\n\nimpact of an attack, but not so often that it makes\n\n\nthe system unstable. Refreshing aperiodically to\n\na known good image can provide the additional\n\nadvantage of hindering an attacker’s ability to pre­\n\ndict the window of opportunity in which to launch\n\nan attack, thus increasing the risk that the attack\n\nwill fail or be detected, and reducing the likeli­\n\nhood of gaining a persistent stronghold. Aperiodic\n\nrefreshing may require additional coordination.\n\n([6], pp. 8–9, 12, 14–15)\n\nPartition/isolate: Segregate components of\n\ndubious pedigree from trusted ones to reduce\n\nthe attack surface, simplify systems and\n\ninterfaces, and limit the damage and spread of\n\nexploits, when they occur. Separation require­\n\nments should implement the principle of least\n\nprivilege and separate critical from non-critical\n\nmission functions and data. Partitioning supports\n\nthe distribution and placement of highly special­\n\nized sensors that can improve situational aware­\n\nness and better detect behavioral anomalies ([6],\n\npp. 3–6, 8–9, 11–13). Examples include:\n\n###### � [Separation at the network level (e.g., Inter­]\n\nnet from Intranet and demilitarized zone\n\nsegmentation) and at the application and\n\ndata levels (e.g., non-sensitive from sensi­\n\ntive) segregates risky traffic and process­\n\ning from critical traffic, processing, and\n\ndata. These tactics help reduce complexity\n\nby removing extraneous data and transac­\n\ntions and promote more effective intru­\n\nsion detection, packet capture analytics,\n\nand other anomaly detection capabilities\n\nbecause anomalous behavior cannot eas­\n\nily “hide in the noise.”\n\n\n-----\n\n###### � [Segmentation at the network level should ]\n\nimplement controlled interfaces between\n\nsegments, as needed, both within an\n\nenterprise and at its boundaries. This will\n\nhelp limit local area network contamina­\n\ntion and flow-through, and with proper\n\nsensor deployment can provide better\n\nsituational awareness (SA) and more pre­\n\ncisely targeted computer network defense\n\n(CND).\n###### � [Isolating the CND network from critical ]\n\nprocessing networks can help prevent\n\nthe adversary from learning our intrusion\n\nanalysis and forensic capabilities.\n###### � [Isolating asynchronous communica­]\n\ntions, analyzing and correlating request\nresponse traffic, and isolating different\n\nprotocols support detecting anomalous\n\ntraffic.\n###### � [Implementing white lists will constrain ]\n\napplication pathways. A white list or\n\napproved list is a list or register of entities\n\nthat, for one reason or another, are being\n\nprovided a particular privilege, service,\n\nmobility, access, or recognition.\n###### � [Using secure browsers, thin clients, ]\n\nand virtualized clients to sandbox risky\n\n###### References and Resources\n\n\nprocessing from critical processing. In\n\ncomputer security, a sandbox is a security\n\nmechanism for separating running pro­\n\ngrams. It is often used to execute untested\n\ncode, or untrusted programs from unveri­\n\nfied third parties, suppliers, and untrusted\n\nusers. The sandbox typically provides\n\na tightly controlled set of resources for\n\nguest programs to run in, such as scratch\n\nspace on disk and memory. Network\n\naccess and the ability to inspect the host\n\nsystem or read from input devices are\n\nusually disallowed or heavily restricted. In\n\nthis sense, sandboxes are a specific exam­\n\nple of virtualization.\n\nSituational awareness: Beef up detection,\n\nanalysis, correlation, and forensics tools and\n\nprocesses. Improve integrated SA understanding\n\nby improving sensor data collection, analytics for\n\nsecurity and mission-critical capabilities’ health\n\n(i.e., better detect degradations, faults, intrusions,\n\netc.), and visualization techniques. Baseline normal\n\ncritical processing and user behavior, and focus\n\non anomaly detection within this context. Use\n\nforensics to drive evolution of CND and opera­\n\ntions. ([6], pp. 4, 6, 16)\n\n\n[1. Department of Defense, July 1, 2010, DoD Directive 3020.40 Change 1 “DoD Policy and](http://www.fas.org/irp/doddir/dod/d3020_40.pdf)\n\n[Responsibilities for Critical Infrastructure.”](http://www.fas.org/irp/doddir/dod/d3020_40.pdf)\n\n[2. Krekel, B., October 9, 2009, Capability of the People’s Republic of China to Conduct Cyber](http://www.uscc.gov/researchpapers/2009/NorthropGrumman_PRC_Cyber_Paper_FINAL_Approved%20Report_16Oct2009.pdf)\n\n[Warfare and Computer Network Exploitation,” prepared for The US-China Economic and](http://www.uscc.gov/researchpapers/2009/NorthropGrumman_PRC_Cyber_Paper_FINAL_Approved%20Report_16Oct2009.pdf)\nSecurity Review Commission, Northrop Grumman Corporation.\n\n[3. SANS, September 2009, Top Cyber Security Risks.](http://www.sans.org/top-cyber-security-risks/?ref=top20)\n\n\n-----\n\n[4. MITRE/SANS, February 6, 2010, Top 25 Programming Errors.](http://www.sans.org/top25-programming-errors/)\n\n[5. SANS, November 13, 2009, “Twenty Critical Controls for Effective Cyber Defense:](http://www.sans.org/critical-security-controls/guidelines.php)\n\n[Consensus Audit,” 20 Critical Security Controls, Ver. 2.3.](http://www.sans.org/critical-security-controls/guidelines.php)\n\n[6. Goldman, Harriet G., November 2010, Building Secure, Resilient Architectures for Cyber](http://www.mitre.org/work/tech_papers/2010/10_3301/10_3301.pdf)\n\n[Mission Assurance, The MITRE Corporation.](http://www.mitre.org/work/tech_papers/2010/10_3301/10_3301.pdf)\n\n###### Additional References and Resources\n\n[Defense Science Board, April 2007, 2006 Summer Study on Information Management for Net-](http://www.acq.osd.mil/dsb/reports/ADA467538.pdf)\n[Centric Operations, Vol. 1.](http://www.acq.osd.mil/dsb/reports/ADA467538.pdf)\n\n[MITRE authors, May 5, 2009, Selected MITRE Cyber-Related Research and Initiatives. The](http://www.mitre.org/work/tech_papers/tech_papers_09/09_1535/09_1535.pdf)\nMITRE Corporation.\n\n\n-----\n\nDefinition: Crown Jewels\n\n_Analysis (CJA) is a process for_\n\n_identifying those cyber assets_\n\n_that are most critical to the_\n\n_accomplishment of an orga­_\n\n_nization’s mission. CJA is also_\n\n_an informal name for Mission-_\n\n_Based Critical Information_\n\n_Technology (IT) Asset_\n\n_Identification. It is a subset of_\n\n_broader analyses that identify_\n\n_all types of mission-critical_\n\n_assets._\n\nKeywords: Advanced Cyber\n\n_Threat (ACT), Advanced_\n\n_Persistent Threat (APT),_\n\n_criticality analysis, cyber, fight_\n\n_through, information assurance,_\n\n_mission assurance, mission_\n\n_critical, resilience_\n\n\nSYSTEMS ENGINEERING FOR MISSION\nASSURANCE\n###### Crown Jewels Analysis (CJA)\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to help\n\ncustomers acquire robust systems that can\n\nsuccessfully execute their mission even when\n\nunder attack through cyberspace. To do this,\n\nMITRE SEs are expected to be conversant in\n\nbest security practices and the basic prin­\n\nciples for analyzing and identifying IT assets\n\nthat are critical to the accomplishment of an\n\norganization’s mission. They are expected to\n\nkeep abreast of new and evolving techniques\n\nfor identifying mission-critical assets.\n\n\n-----\n\n###### Background\n\nIn a large and complex enterprise, it is difficult to know how problems with a portion of an IT\ninfrastructure may affect the broader operational mission. CJA provides a methodology to help\nunderstand what is most critical—beginning during systems development and continuing\nthrough system deployment. CJA is often the first step in a Mission Assurance Engineering\n(MAE) process (see Figure 1), which provides a rigorous analytical approach to:\n\n###### �Identify the cyber assets most critical to mission accomplishment—the “crown jewels”\n\nof CJA.\n###### �Understand the threats and associated risks to those assets—via a subsequent cyber\n\nThreat Susceptibility Assessment (TSA) [1].\n###### �Select mitigation measures to prevent and/or fight through attacks—via Cyber Risk\n\nRemediation Analysis (RRA) [2], which identifies recommended mitigation measures.\nMAE offers a common, repeatable risk management process that is part of building\nsecure and resilient systems [3]. The underlying premise for performing a CJA is that protec­\ntion strategies focused entirely on “keeping the adversary out” are usually doomed to fail\n(recall the Maginot Line). The Advanced Cyber Threat (ACT) has sophisticated capabilities\n\nWhat is most\nimportant?\n\nCONOPS Establish mission priorities\nUse cases\nEnd-to-end flows\n\nIdentify mission\ndependencies\n\nCrown Jewels\n\nMission impact analysis\n\nAnalysis (CJA)\n\nWhat are the\nrisks?\n\nCyber threats &\n\nCyber threat susceptibility\n\nintelligence\n\nassessment\n\nHow to mitigate\nthe risks\n\nMitigation Cyber risk remediation\ntechniques analysis\n\nThreat Assessment & Remediation Analysis (TARA)\n\nFigure 1. The Mission Assurance Engineering (MAE) Process\n\n\n-----\n\nMission Objectives\n\n\nCyber Assets\n\nCyber & information\ndependencies come\nfrom tech inputs\n\nFigure 2. CJA Model Using Dependency Maps\n\nthat adversaries can use to gain and maintain a persistent presence in the hardware and\nsoftware that make up mission systems—providing the opportunity to “attack” (i.e., deny,\ndegrade, deceive) these assets at a time and place of their choosing. As cyber threats con­\ntinue to escalate, it is prudent to assume that adversaries will aim to successfully penetrate\nand then deny and/or degrade our cyber assets, necessitating that we must maintain our\nability to “fight through” such attacks. Because it would be prohibitively difficult and costly\nto design every component of a system to fight through all conceivable attacks, a CJA is\nused to identify the most important cyber assets to an organization’s mission—allowing\nsystems engineers, designers, and operators to focus on ensuring that these critical compo­\nnents can effectively endure an attack.\n\nOrganizations (especially operational units) often have limited resources to use in finding\ntheir mission-critical cyber assets, and yet they need a reasonably accurate idea of what those\nare. One technique for performing a CJA makes use of “dependency maps” [4]. This technique\nstemmed from a need for a convenient but moderately rigorous approach—with more detail\nand structure than is possible from using a questionnaire. A CJA dependency map uses fea­\ntures adapted from MITRE’s Risk-to-Mission Assessment Process (RiskMAP) [4, 5]. As a result,\nthis particular CJA methodology and RiskMAP are often taken as one and the same, but they\n\n\n-----\n\nare not. A more rigorous approach would involve Cyber Mission Impact Assessment (CMIA)\n\n[6], which uses a more detailed description of the user’s system.\n\nThe dependency map method uses facilitated discussions with system subject matter\nexperts (SMEs) to assemble a model, from the top down, as shown in Figure 2 [7].\n\nThese dependencies are qualitatively expressed in terms of “If <child> fails or is\ndegraded (as defined by the SMEs), the impact on <parent> is <failure, degrade, work_around, nominal>.” Provisions are included to minimize subjectivity. Once the model is_\ncomplete, it is possible to predict the impact of a cyber asset failure/degradation as the realiza­\ntion of each IF...THEN statement, tracing the potential impact back “up” to key Tasks and\nObjectives, as shown in Figure 3.\n\n###### Government Interest and Use\n\nGovernment interest in identifying, prioritizing, and protecting critical elements of the\nnational infrastructure crosses all departments and stems from Homeland Security\nPresidential Directive 7 (HSPD-7) [8]. Preserving the government’s ability to perform essen­\ntial missions and deliver essential services is among the key policy tenets in HSPD-7. These\ntenets were carried into the National Infrastructure Protection Plan (NIPP) developed by the\n\nDependency Legend Impact Legend\nFail/Deg. > Failure Failure\nFail/Deg. > Degrade Degrade\n\nMission Objective O1 Fail/Deg. > Work    around NominalWork around\n\nTasks\n\nT1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13\n\nI1 I2 I3 I4 I5 I6 I7 I8 I9 I10 I11 I12 I13\n\nInformation\nassets\n\nCyber\n\nC1 C2 C3 C4 C5 C6 C7 C8 C9 C10\n\nassets\n\nFigure 3. Predicted Impact of Cyber Asset Failure\n\n\n-----\n\nDepartment of Homeland Security (DHS) [9]. The NIPP outlines a risk management strategy\nthat allows for choices of risk management methodologies so long as they meet certain crite­\nria. Among the Core Criteria for making consequence or impact assessments is “mission\ndisruption.” The special importance of mission disruption is clear in the NIPP, which states,\n“Mission disruption is an area of strong NIPP partner interest for collaborative development\nof the appropriate metrics to help quantify and compare different types of losses. While\ndevelopment is ongoing, qualitative descriptions of the consequences [of loss] are a suffi­\ncient goal.”\n\nWithin the DoD, the Defense Critical Infrastructure Protection (DCIP) Program called for\nin DODD 3020.40 [10] and described in DODI 3020.45 [11] requires a mission-based critical­\nity assessment of assets supporting DoD missions. The nine-step Critical Asset Identification\nProcess (CAIP) set forth in DoDM 3020.45 V1 [12] requires Mission Owners and Asset Owners\nto jointly determine what is mission critical, based on performance criteria for missions\nand mission-essential tasks. CJA provides a useful framework for conducting this analysis,\nusing Information Assets as the link between Tasks and the Cyber Assets that support them.\nFollowing the CJA with a TSA to determine risks and a RRA to identify possible risk mitiga­\ntions is consistent with the guidance in these DoD documents.\n\n###### Best Practices and Lessons Learned\n\n\nThe overarching goal. The general goal of a CJA\n\nis to make the adversary’s job both more “difficult”\n\n(more costly and more time-consuming—hence\n\nmore “expensive”) and more risky.\n\nTiming is critical. Generally the more enlighten­\n\ning insights come from using the dependency\n\nmap methodology to evaluate system designs\n\n(and any alternatives or “design trades”) after\n\nthe system design starts to take shape. Some\n\nefforts have been made to perform a CJA early\n\nin an acquisition effort to identify mission-critical\n\nfunctions and mission-critical data. This can\n\nhelp identify information to protect at rest and in\n\ntransit, where this information might be either a\n\ncritical input or a computational “product” of the\n\ndesigned system.\n\n\nInclude all tasks needed to achieve the mission\n\nobjectives. This means looking beyond those\n\ntasks that are on the critical path in a mission\n\nthread. What are the security-related tasks? What\n\nare the logistics-related tasks, or other support\nrelated tasks? Excluding such tasks overlooks\n\nwhat must be done to ensure continued mis­\n\nsion capability. An honest assessment of mis­\n\nsion dependency on security-related, and other\n\nsupporting tasks will ensure that the components\n\nsupporting those tasks receive due attention.\n\nRemember the supporting actors. Cyber assets\n\nthat perform mission-critical functions are not\n\nthe only crown jewels in a system. Any system\n\ncomponents that have unmediated access to\n\ncrown jewels, or provide protection for crown\n\njewels, or enable crown jewels to perform their\n\n\n-----\n\nMS A MS B MS C Full RateProduction\nDecision Review\n\nMateriel Engineering & Production\n\nTechnology\n\nSolutions Manufacturing and O&S\n\nDevelopment\n\nAnalysis CDD Development CPD Deployment\n\nAoA ASR SRR SFR PDR CDR\n\nCJA CJA CJA\n\n\nICD\n\n\nCDD\n\n\nAoA\n\n\nBased on baseline system\ndesign\n\nProvides opportunities to\nconduct end-to-end\nresiliency testing\n\nFocus shifts from design to\noperational mitigations\n\n\nCBA\n\n\nI\n\nI\n\nI\n\n\nCPD\n\n\nFacilitates coordination between\nmission operators and network\noperators\n\nAllows development and exercise\nof tailored operate-thru TTPs\n\nFocuses exclusively on\noperational mitigations\n\n\nI\n\nI\n\nI\n\n\nCan help with articulation of\nsystem requirements\n\nFocus on design mitigations\n\nBased on limited or\npreliminary system\ndocumentation\n\n\nI\n\nI\n\nI\n\n\nFigure 4. Alternate Time Frames for Performing CJA During a System Life Cycle\n\n\ncritical functions must themselves be considered\n\nfor crown jewel status. Identifying them requires\n\nan understanding of the system architecture and\n\nthe overall system functions, and the analysis will\n\nnot be complete unless this step is performed.\n\nForthcoming guidance in the Defense Acquisition\n\nGuidebook will address this point in the section\n\non Criticality Analyses for Program Protection\n\nPlanning.\n\nThe devil’s in the details. System design details\n\ninfluence “criticality” in ways that developers—not\n\noperators—will more readily understand, so identi­\n\nfying key system accounts, critical files, and other\n\ncritical assets will require technical insights from\n\nthe development team (as depicted in the bottom\n\ntwo rows of Figures 2 and 3). Deciding which cyber\n\nassets are most important to “protect” (by close\n\nmonitoring, using redundancy or other measures\n\nto more quickly restore these critical assets) is\n\nbased on the insights provided by the depen­\n\ndency map “linkage” to the Tasks and Mission\n\n\nObjective. CJA can provide insight into which\n\nnodes to protect, where to apply intrusion detec­\n\ntion, whether anti-tamper software and hardware\n\nare needed, and where and how to apply them.\n\nRemember to factor in changing priorities.\n\nWhen circumstances require operators to “fight\n\nthrough” a cyber-attack, other considerations will\n\nalso shape the CJA—such as putting priority into\n\nmaintaining scenario-specific, “minimum essen­\n\ntial” functions, or allowing for secure reconstitu­\n\ntion from a pristine, protected software image\n\nthat can be quickly patched to a current state and\n\nloaded with the latest data.\n\nA life-cycle view of CJA. Figure 4 illustrates\n\nwhere CJAs might be performed at differ­\n\nent points along a System Life Cycle (SLC) and\n\ndescribes the purposes they would serve. A CJA\n\ncan be initiated at Milestone B and updated\n\nthroughout the program, but it could certainly be\n\nstarted at a later phase if this analysis was not\n\nperformed sooner.\n\n\n-----\n\nNeeds and resources—The big drivers. The\n\nchoice of CJA method depends on the needs\n\nand resources of the requesting organization. If\n\nan organization’s mission is clearly defined, with\n\na modest number of tasks to accomplish their\n\nbroader mission objectives, their crown jewels\n\nmay be readily apparent. In some cases, they\n\nwill have defined specific use cases; or in user\n\nparlance, “mission threads.” For organizations that\n\n###### References and Resources\n\n\nsupport many mission objectives and/or have\n\ncomplex and overlapping mission dependen­\n\ncies on IT, it is useful to employ the dependency\n\nmap techniques of CJA. Where an even more\n\ndetailed examination is necessary—based on\n\ncomplex ripple effects from IT failures or temporal\n\neffects during overlapping operations—a CMIA\n\n[13] approach offers the necessary capabilities to\n\naddress these challenges.\n\n\n1. Cyber Threat Susceptibility Assessment, Systems Engineering Guide.\n\n2. Cyber Risk Remediation Analysis, Systems Engineering Guide.\n\n[3. Goldman, H., October 2010, Building Secure, Resilient Architectures for Cyber Mission](http://www.mitre.org/work/tech_papers/2010/10_3301/)\n\n[Assurance, The MITRE Corporation.](http://www.mitre.org/work/tech_papers/2010/10_3301/)\n\n[4. Watters, J., S. Morrissey, D. Bodeau, and A. Powers, October 2009, The Risk-to-Mission](http://www.mitre.org/work/tech_papers/tech_papers_09/09_2994/)\n\n[Assessment Process (RiskMAP): A Sensitivity Analysis and an Extension to Treat](http://www.mitre.org/work/tech_papers/tech_papers_09/09_2994/)\n[Confidentiality Issues, The MITRE Corporation.](http://www.mitre.org/work/tech_papers/tech_papers_09/09_2994/)\n\n5. RiskMAP, MITREpedia article.\n\n[6. Musman, S., A. Temin, M. Tanner, D. Fox, B. Pridemore, July 2009, Evaluating the Impact](http://www.mitre.org/work/tech_papers/2010/09_4577/)\n\n[of Cyber Attacks on Missions, The MITRE Corporation.](http://www.mitre.org/work/tech_papers/2010/09_4577/)\n\n7. MITRE Institute Course TSV-418, Crown Jewels Analysis Using Dependency Maps.\n\n[8. Bush, President George W., December 2003, Critical Infrastructure Identification,](http://www.ndu.edu/uchs/hspd-7.pdf)\n\n[Prioritization, and Protection. Homeland Security Presidential Directive/HSPD-7. The](http://www.ndu.edu/uchs/hspd-7.pdf)\nWhite House.\n\n[9. National Infrastructure Protection Plan, 2009, Department of Homeland Security.](http://www.dhs.gov/xlibrary/assets/NIPP_Plan.pdf)\n\n[10. DoD Policy and Responsibilities for Critical Infrastructure, DoD Directive 3020.40, Change](http://www.fas.org/irp/doddir/dod/d3020_40.pdf)\n\n[1. July 2010, Department of Defense.](http://www.fas.org/irp/doddir/dod/d3020_40.pdf)\n\n[11. Defense Critical Infrastructure Program (DCIP) Management, DoD Instruction 3020.45.](http://www.fas.org/irp/doddir/dod/i3020_45.pdf)\n\nApril 2008, Department of Defense.\n\n[12. Defense Critical Infrastructure Program (DCIP): DoD Mission-Based Critical Asset](http://www.fas.org/irp/doddir/dod/m3020_45_v1.pdf)\n\n[Identification Process (CAIP), DoD Manual 3020.45 V1. October 2008, Department of](http://www.fas.org/irp/doddir/dod/m3020_45_v1.pdf)\nDefense.\n\n\n-----\n\n[13. Musman, S., M. Tanner, A. Temin, E. Elsaesser, L. Loren, 2011, “A Systems Engineering](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5949403)\n\n[Approach for Crown Jewels Estimation and Mission Assurance Decision Making,”](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5949403)\n_Proceedings of the IEEE Symposium on Computational Intelligence in Cyber Security, Paris,_\nFrance, April 11–15, 2011.\n\n###### Additional References and Resources\n\nCyber Operations Center, MITREpedia.\n\nHastings, G., L. Montella, and J. Watters, MITRE Crown Jewels Analysis Process, April 8,\n2009.\n\nMission Assurance Against Cyber Attacks SharePoint site.\n\n\n-----\n\nDefinition: Cyber Threat\n\n_Susceptibility Assessment_\n\n_(TSA) is a methodology for_\n\n_evaluating the susceptibility_\n\n_of a system to cyber-attack._\n\n_TSA quantitatively assesses_\n\n_a system’s [in]ability to resist_\n\n_cyber-attack over a range_\n\n_of cataloged attack Tactics,_\n\n_Techniques, and Procedures_\n\n_(TTPs) associated with the_\n\n_Advanced Persistent Threat_\n\n_(APT). In the Mission Assurance_\n\n_Engineering (MAE) methodol­_\n\n_ogy, cyber TSA is a follow-on_\n\n_to Crown Jewel Analysis (CJA),_\n\n_and a prerequisite to cyber Risk_\n\n_Remediation Analysis (RRA)._\n\nKeywords: advanced persistent\n\n_threat, APT, cyber attack, MAE,_\n\n_mission assurance engineer­_\n\n_ing, risk remediation, Threat_\n\n_Susceptibility Matrix, TTP_\n\n\nSYSTEMS ENGINEERING FOR MISSION\nASSURANCE\n###### Cyber Threat Susceptibility Assessment\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the purpose and role of Mission\n\nAssurance Engineering (MAE) in the systems\n\nacquisition life cycle and its constituent activities,\n\nincluding cyber Threat Susceptibility Analysis\n\n(TSA). The MAE methodology has application\n\nthroughout the system life cycle. MITRE SEs\n\nare expected to know the context(s) in which\n\nthis methodology can be applied. MITRE SEs\n\nare also expected to help establish the scope\n\nof the assessment and set sponsor expecta­\n\ntions regarding deliverables and schedules.\n\n\n-----\n\n###### Introduction and Background\n\nThe MAE process framework (Figure 1 in the preceding article, “Crown Jewels Analysis”)\nprovides an analytical approach to:\n\n###### �Identify the cyber assets most critical to mission accomplishment (the “crown jewels” of\n\na Crown Jewels Analysis).\n###### �Understand the threats and associated risks to those assets (accomplished via a subse­\n\nquent cyber Threat Susceptibility Analysis [TSA]).\n###### �Select mitigation measures to prevent and/or fight through attacks (cyber Risk\n\nRemediation Analysis [RRA] is used to identify recommended mitigation measures).\n###### �The MAE process framework provides a common repeatable risk management process\n\nthat is part of building secure and resilient systems [1].\nCyber threat susceptibility analysis (TSA) is an MAE activity that quantitatively assesses\na system’s [in]ability to resist cyber-attack over a range of adversary Tactics, Techniques, and\nProcedures (TTPs). A TSA assessment produces a Threat Susceptibility Matrix, which provides\na ranked list of TTPs that cyber assets are susceptible to. This matrix is used in a follow-on\nMAE activity called cyber Risk Remediation Analysis, which develops recommendations for\nhow to mitigate cyber TTP risk.\n\nThis article focuses on what we know today about cyber TSA, a fast-moving discipline.\nThe concepts and methods described are evolving and will continue to mature as more expe­\nrience is gained in applying this discipline.\n\nThe first step in a cyber TSA assessment is to establish the scope of the evaluation.\nAssessment scope is characterized in terms of:\n\n###### �The set of assets being evaluated �The range of attack TTPs being considered �The types of adversaries\nWhen TSA is conducted as a follow-on to a Crown Jewel Analysis (CJA), the set of system\nassets within the scope of the assessment may include identified crown jewel cyber assets\n(i.e., cyber assets whose compromise would seriously impair mission capability or readiness.)\nIf the TSA is being conducted independently or in the absence of the CJA, the list of cyber\nassets may be arbitrarily selected or may include a presumptive list of crown jewel cyber\nassets.\n\nThe range of attacks considered in a TSA assessment may include but is not limited to\ncyber, electronic warfare (EW), and supply chain. A cyber attack targets an enterprise’s use\nof cyberspace for the purpose of disrupting, disabling, destroying, or maliciously control­\nling a computing environment/infrastructure; destroying the integrity of the data; or steal­\ning controlled information. Electronic warfare refers to military action involving the use of\nelectromagnetic and directed energy to control the electromagnetic spectrum or to attack\n\n\n-----\n\nthe enemy. Supply chain attacks allow the adversary to use implants or other vulnerabilities\ninserted prior to installation in order to infiltrate or exfiltrate data or to manipulate informa­\ntion technology hardware, software, operating systems, peripherals (information technology\nproducts), or services at any point during the life cycle. The Advanced Persistent Threat (APT)\nrefers to adversaries, typically nation-states, capable of mounting sophisticated attacks in each\nof these areas.\n\nTypes of adversaries considered in a TSA assessment may include external adversar­\nies, insiders, and trusted insiders. The distinctions among the types are fuzzy, but relate to\nthe adversary’s proximity to the targeted system. A security perimeter separates an external\nadversary from an internal adversary (i.e., an insider). This perimeter can take the form of\na firewall, a DMZ, a locked door, and so on. Once the security perimeter is breached, how­\never, the external adversary has gained insider access. Similarly, an insider is distinguished\nfrom a trusted insider by the level of access granted (i.e., a trusted insider may have physical\nor administrative access that an unprivileged user does not). Enforcing the principle of least\nprivilege separates insiders from trusted insiders, who may have opportunities to apply a\nwider range of attack TTPs than insiders or external adversaries. The scope of a TSA assess­\nment may include or exclude each of these types of adversaries.\n\nOnce the scope of the TSA assessment is established, the next step is to evaluate the\ncyber asset’s architecture, technology, and security capabilities against a cataloged set of\nTTPs. Unclassified, open source TTP catalogs used in TSA assessments include MITRE-hosted\nresources such as:\n\n###### �Common Attack Pattern Enumeration and Classification (CAPEC)—A compilation of\n\ncyber-attack patterns that describe common methods for exploiting software derived\nfrom specific real-world incidents [2]. In this context, the terms “attack TTP” and “attack\npattern” are synonymous.\n###### �Common Weakness Enumeration (CWE)—A catalog of defined software weaknesses\n\nthat attack TTPs may exploit [3].\n###### �Common Vulnerabilities and Exposures (CVE)—An open-source dictionary of publicly\n\nknown information security vulnerabilities and exposures [4].\nThis initial set of TTPs undergoes a narrowing process to eliminate TTPs considered\nimplausible. Several factors can make a TTP an implausible method of cyber-attack. Many\nTTPs have prerequisites or conditions that must hold true in order for that TTP to be effective.\nA prerequisite for a structured query language (SQL) injection attack, for example, is that the\nsystem must include a SQL database. Weak passwords is one condition that must hold true in\norder for an adversary to successfully conduct brute force password attacks. Many candidate\nattack TTPs may be eliminated because of missing prerequisites.\n\n\n-----\n\nIt is also possible to eliminate candidate attack TTPs by making assumptions about\nthe system’s security posture. For example, DoD systems undergo the Defense Information\nAssurance Certification and Accreditation (DIACAP) process to verify that all required secu­\nrity controls are implemented. One set of security controls requires that the system’s configu­\nration be hardened using Defense Information Systems Agency published Security Technical\nImplementation Guides (STIGs). Certain attack TTPs may not be plausible for systems that\nhave been hardened in accordance with these STIGs.\n\nCandidate attack TTPs that cannot be eliminated may be ranked using a scoring model\nthat assesses the risk associated with each TTP relative to other plausible TTPs considered\nin the assessment. This ranking helps set priorities on where to apply security measures to\nreduce the system’s susceptibility to cyber-attack. The default TTP scoring model spreadsheet\nis illustrated in Table 1.\n\nTable 1. Default TTP Risk Scoring Model\n\n|Factor range|1|2|3|4|5|\n|---|---|---|---|---|---|\n|Proximity What proximity would an adversary need in order to apply this TTP?|No physical or network access required|Proto­ col access through DMZ and firewall|User account to target system (no admin access)|Admin access to target system|Physical access to tar­ get system|\n|Locality How localized are the effects posed by this TTP?|Isolated to single unit|Single unit and sup­ porting network|External networks potentially impacted|All units in theater or region|All units globally and associated infrastructure|\n|Recovery Time How long would it take to recover from this TT P once the attack was detected?|< 10 hours|20 hours|30 hours|40 hours|>50 hours|\n|Restoration Costs What is the estimated cost to restore or replace affected cyber asset?|< $10K|$25K|$50K|$75K|>$100K|\n|Impact How serious an impact is loss of data conf­i dentiality resulting from suc­ cessful application of this TTP?|No impact from TTP|Minimal impact|Limited impact requir­ ing some remediation|Remedia­ tion activi­ ties detailed in COOP|COOP reme­ diation activi­ ties routinely exercised|\n\n\n-----\n\n|Impact How serious in impact is loss of data integ­ rity resulting from successful application of this TTP?|No impact from TTP|Minimal impact|Limited impact requir­ ing some remediation|Remedia­ tion activi­ ties detailed in COOP|COOP reme­ diation activi­ ties routinely exercised|\n|---|---|---|---|---|---|\n|Impact How serious an impact is loss of system availability resulting from suc­ cessful application of this TTP?|No impact from TTP|Minimal impact|Limited impact requir­ ing some remediation|Remedia­ tion activi­ ties detailed in COOP|COOP reme­ diation activi­ ties routinely exercised|\n|Prior Use Is there evidence of this TTP in the MITRE Threat DB?|No evi­ dence of TTP use in MTDB|Evidence of TTP use possible|Confirmed evidence of TTP use in MTDB|Frequent use of TTP reported in MTDB|Widespread use of TTP reported in MTDB|\n|Required Skills What level of skill or specific knowledge is required by the adversary to apply this TTP?|No spe­ cific skills required|Generic technical skills|Some knowledge of targeted system|Detailed knowledge of targeted system|Knowledge of both mission and targeted system|\n|Required Resources Would resources be required or consumed in order to apply this TTP?|No resources required|Minimal resources required|Some resources required|Significant resources required|Resources required and consumed|\n|Stealth How detectable is the TTP when it is applied?|Not detectable|Detec­ tion pos­ sible with specialized monitoring|Detection likely with specialized monitoring|Detec­ tion likely with routine monitoring|TTP obvi­ ous without monitoring|\n|Attribution Would residual evidence left behind by this TTP lead to attribution?|No residual evidence|Some residual evidence, attribution unlikely|Attribution possible from char­ acteristics of the TTP|Some or similar TTPs previously attributed|Signature attack TTP used by adversary|\n\n\nThe default TTP scoring model assesses TTP risk based on twelve criteria, including impact,\nrestoration costs, down time, level of sophistication, likelihood for attribution, and so on. This\nlist of criteria, which has evolved over time, may be tailored for use in a given assessment. Use\nof the same scoring model provides a common context for comparing and ranking TTPs based\non relative risk. TTP risk scores derived using different scoring models are not comparable.\n\nA uniform range of values [1–5] is assigned to each criterion. For criteria such as impact,\na higher value results in a higher TTP risk score. These criteria appear in blue in the scoring\nmodel spreadsheet. For adversary level of sophistication criteria, such as required skills and\nrequired resources, a higher value results in a lower TTP risk score. These criteria appear in red\n\n\n-----\n\nTable 2. Sample Threat Susceptibility Matrix\n\n**Cyber Asset #1** **Cyber Asset #2**\n\n**Risk**\n**TTP ID**\n**Score** **Trusted** **Trusted**\n\n**External** **Insider** **External** **Insider**\n\n**Insider** **Insider**\n\nT000017 4.4 4.4 4.4 4.3 4.3\nT000030 4.2 4.1 4.1 4.1 4.1\nT000039 3.6 3.6 3.6 3.6\nT000041 3.2 3.2 3.2 3.2 3.2\nT000053 3.0 3.0\nT000064 2.9 2.9\nT000086 2.6 2.6 2.6 2.6\nT000127 2.6 2.6\nT000018 2.3 2.3 2.3\nT000022 2.3 2.3 2.3 2.3 2.3 2.3 2.3\nT000023 2.3 2.3 2.3 2.3 2.3\nT000029 2.2 2.2 2.2 2.2 2.2\nT000048 2.0 2.0\nT000054 1.9 1.9 1.9\nT000063 1.6 1.6 1.6\nT000065 1.3 1.3\n\nAggregate 14.9 22.1 12.8 21.6 30.4 18.6\nSusceptibility 49.8 70.6\n\nin the scoring model spreadsheet. In the threat model from which this scoring model derives, it\nis assumed that a high degree of adversary sophistication required to successfully execute a TTP\nreduces the overall likelihood of occurrence, leading to a lower overall risk score.\n\nIn the default TTP scoring model, different criteria can have different weightings. Some\ncriteria may be more significant to the overall risk score than others. For a system that pro­\ncesses classified data, for example, a higher weighting is assigned to loss of confidentiality\nthan for a system that processes unclassified data. TTP risk scores are calculated based on the\ncriteria value assignments and associated criteria weightings. In the default scoring model,\nthis calculation yields a TTP risk score in the range [1–5], with the value 5 signifying a TTP\nthat poses the greatest risk.\n\nA TSA assessment produces a Threat Susceptibility Matrix, which lists plausible attack\nTTPs ranked by decreasing risk score, and their mapping to cyber assets as a function of\nadversary type. The Threat Susceptibility Matrix also tabulates TTP risk scores to provide an\noverall assessment of aggregate susceptibility to cyber-attack for each cyber asset considered\n\n|TTP ID|Risk Score|Cyber Asset #1|Col4|Col5|Cyber Asset #2|Col7|Col8|\n|---|---|---|---|---|---|---|---|\n|||External|Insider|Trusted Insider|External|Insider|Trusted Insider|\n|T000017|4.4||4.4|4.4||4.3|4.3|\n|T000030|4.2||4.1|4.1||4.1|4.1|\n|T000039|3.6|3.6|3.6|||3.6||\n|T000041|3.2|3.2|3.2||3.2|3.2||\n|T000053|3.0||||||3.0|\n|T000064|2.9||||2.9|||\n|T000086|2.6||||2.6|2.6|2.6|\n|T000127|2.6||||2.6|||\n|T000018|2.3|||||2.3|2.3|\n|T000022|2.3|2.3|2.3|2.3|2.3|2.3|2.3|\n|T000023|2.3|2.3|2.3||2.3|2.3||\n|T000029|2.2|2.2|2.2||2.2|2.2||\n|T000048|2.0|||2.0||||\n|T000054|1.9||||1.9|1.9||\n|T000063|1.6||||1.6|1.6||\n|T000065|1.3|1.3||||||\n|Aggregate Susceptibility||14.9|22.1|12.8|21.6|30.4|18.6|\n|||49.8|||70.6|||\n\n\n-----\n\nin the assessment. This matrix is used in the follow-on cyber risk remediation analysis (RRA)\nto identify countermeasures that effectively mitigate TTP susceptibilities. For further infor­\nmation on cyber RRA, see the companion article under this same topic. A sample Threat\nSusceptibility Matrix is illustrated in Table 2.\n\nThe sample Threat Susceptibility Matrix in Table 2 evaluates two cyber assets over a\nrange of sixteen attack TTPs that are scored using the default TTP scoring model from Table\n1. If a cyber asset is susceptible to a TTP, its risk score is transferred to that cyber asset. At the\nbottom of the matrix, aggregate susceptibility is tabulated for each cyber asset and adversary\ntype. In this example, Cyber Asset #2 is more susceptible than Cyber Asset #1.\n\nTTPs are “binned” into risk categories based on risk score, as follows:\n\n###### �TTPs in the range [4.0–5.0] pose serious risk and appear in red. �TTPs in the range [2.5–3.9] pose moderate risk and appear in yellow. �TTPs in the range [1.0–2.4] pose minimal risk and appear in blue.\n\n Government Interest and Use\n\nTSA has been applied to sponsor systems in various forms for a number of years. Before 2010,\nTSA assessments used a loosely defined, non-rigorous, and undocumented methodology. In\n2010, a formal methodology for conducting TSA assessments was developed by MITRE, which\nhas subsequently been applied to Army, Navy, and Air Force programs [5]. The methodology\noutlined above reflects this TSA approach.\n\n###### Best Practices and Lessons Learned\n\n\nTiming is critical. TSA may not be well suited to\n\nall phases of acquisition programs. For example,\n\nthe Threat Susceptibility Matrix cannot be\n\nconstructed without knowledge of the cyber\n\nassets that make up the system. The identifica­\n\ntion of cyber assets is derived from the system’s\n\nallocated baseline, which may not be fully defined\n\nprior to PDR.\n\nAssume the adversary can gain access. Mission\n\nAssurance Engineering (MAE) is based on the\n\nassumption that APT adversaries are able to suc­\n\ncessfully penetrate a system’s security perimeter\n\nand gain persistent access. TSA’s focus on the\n\nInsider or Trusted Insider relates to adversary\n\n\nproximity and in no way reflects on IT staff loyalty\n\nor ability.\n\nTSA of non–crown jewel assets—the value\n\nproposition. Although a Crown Jewel Analysis\n\n(CJA) identifies cyber assets of greatest impor­\n\ntance to mission success, it does not identify the\n\ncyber assets that are most susceptible to attack.\n\nThere is value in scoping a TSA assessment to\n\nevaluate non–crown jewel cyber assets, especially\n\nthose whose compromise would give an attacker\n\na path to any crown jewel assets.\n\nImportance of documented rationale as con­\n\ntext for future efforts. It is important to record\n\n\n-----\n\nthe rationale for eliminating candidate attack\n\nTTPs from consideration, including assumptions\n\nmade regarding the system’s security posture\n\nor Security Technical Implementation Guide\n\n(STIG) compliance. The rationale provides con­\n\ntext in follow-on MAE activities such as Threat\n\nRemediation Engineering (TRE).\n\nMore than one cyber risk assessment meth­\n\nodology. Several methodologies similar to TSA\n\nassess risk to cyber assets, including Microsoft’s\n\nDREAD [6] and the National Security Agency’s\n\nMORDA [7]. Each methodology functions by\n\nassessing cyber assets using a defined set of\n\nevaluation criteria. The criteria used in this article’s\n\ndefault TTP scoring model are representative of\n\ncriteria used by these other methodologies and\n\ncan be tailored to meet the needs of the program.\n\nPathological scores and what to do about them.\n\nCertain “pathological” TTP scoring modes may\n\nreflect situations where more information about\n\na cyber asset is required, evaluation criteria need\n\nto be revised, or the assigned range of values is\n\n###### References and Resources\n\n\neither too narrow or too wide. When tailoring the\n\nscoring model to address this, it is necessary to\n\ngo back and rescore all TTPs using the updated\n\nmodel. Otherwise, a single scoring model is not\n\nbeing used and there is no basis for comparing\n\nTTP risk scores in the assessment.\n\nVariation on the TTP risk scoring theme. One\n\nvariation on the TTP risk score calculation is to\n\ncompute low and high TTP risk scores based on\n\na range of values for each evaluation criteria. This\n\napproach produces risk scoring that reflects best\n\ncase and worst case assumptions.\n\nThe need for remediation. A cyber TSA provides\n\nno recommendations on how to respond to cyber\n\nrisk. Cyber Risk Remediation Analysis (RRA) is the\n\ncore MAE portfolio activity used to identify risk\n\nmitigation alternatives. Threat Assessment and\n\nRemediation Analysis (TARA) [8] is the MAE port­\n\nfolio practice that combines Cyber TSA with RRA.\n\nSponsors seeking to identify, assess, and mitigate\n\ncyber risks are encouraged to consider a TARA\n\nassessment.\n\n\n[1. Goldman, H., October 2010, Building Secure, Resilient Architectures for Cyber Mission](http://www.mitre.org/work/tech_papers/2010/10_3301/)\n\n[Assurance, The MITRE Corporation.](http://www.mitre.org/work/tech_papers/2010/10_3301/)\n\n[2. Common Attack Pattern Enumeration and Classification (CAPEC).](http://capec.mitre.org/)\n\n[3. Common Weakness Enumeration (CWE).](http://cwe.mitre.org/)\n\n[4. Common Vulnerabilities and Exposures (CVE).](http://cve.mitre.org/)\n\n5. Wynn, J., and L. Montella, October 2010, “Cyber Threat Susceptibility Analysis (TSA)\n\nMethodology,” Ver. 2.0, MITRE Technical Report 100379.\n\n[6. Meier, J. D., A. Mackman, et al., June 2003, Microsoft Patterns and Practices, Chapter 3,](http://msdn.microsoft.com/en-us/library/ff648644.aspx)\n\n[Threat Modeling.](http://msdn.microsoft.com/en-us/library/ff648644.aspx)\n\n\n-----\n\n7. [Buckshaw, D., G. Parnell, et al., 2005, “Mission Oriented Risk and Design Analysis of Critical](http://www.innovativedecisions.com/documents/Buckshaw-Parnelletal.pdf)\n[Information Systems (MORDA),” Military Operations Research, Vol. 10, No. 2, pp. 19–38.](http://www.innovativedecisions.com/documents/Buckshaw-Parnelletal.pdf)\n\n8. Wynn, J., et al., October 2011, “Threat Assessment and Remediation Analysis (TARA)”,\n\nVer. 1.4, MITRE Technical Report 110176, The MITRE Corporation.\n\n\n-----\n\nDefinition: Cyber Risk\n\n_Remediation Analysis (RRA) is_\n\n_a methodology for selecting_\n\n_countermeasures to reduce_\n\n_a cyber-asset’s susceptibility_\n\n_to cyber-attack over a range_\n\n_of attack Tactics, Techniques,_\n\n_and Procedures (TTPs) asso­_\n\n_ciated with the Advanced_\n\n_Persistent Threat (APT). In the_\n\n_Mission Assurance Engineering_\n\n_(MAE) methodology, RRA is_\n\n_a follow-on to cyber Threat_\n\n_Susceptibility Analysis (TSA)_\n\n_and provides recommendations_\n\n_to sponsors seeking to reduce_\n\n_susceptibility to cyber-attack._\n\nKeywords: advanced persistent\n\n_threat, APT, CM, counter­_\n\n_measure, cyber-attack, MAE,_\n\n_mission assurance engineering,_\n\n_risk Remediation, RRA, threat_\n\n_susceptibility, TSA, TTP, recom­_\n\n_mendation, utility-cost ratio,_\n\n_U/C ratio_\n\n\nSYSTEMS ENGINEERING FOR MISSION\nASSURANCE\n###### Cyber Risk Remediation Analysis\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the purpose and role of Mission\n\nAssurance Engineering (MAE) in the systems\n\nacquisition life cycle and its constituent activi­\n\nties, including cyber Risk Remediation Analysis\n\n(RRA). The MAE methodology has application\n\nthroughout the system life cycle. MITRE SEs\n\nare expected to know the context(s) in which\n\nthis methodology can be applied. They are\n\nalso expected to help establish the scope of\n\nthe assessment and set sponsor expecta­\n\ntions regarding deliverables and schedules.\n\n\n-----\n\n###### Introduction and Background\n\nThe MAE process framework (Figure 1 in the “Crown Jewels Analysis” article) provides an\nanalytical approach to:\n\n###### �Identify the cyber-assets most critical to mission accomplishment (the “crown jewels” of\n\na Crown Jewels Analysis).\n###### �Understand the threats and associated risks to those assets (accomplished via a subse­\n\nquent cyber Threat Susceptibility Analysis [TSA]).\n###### �Select mitigation measures to prevent and/or fight through attacks (cyber Risk\n\nRemediation Analysis [RRA] is used to identify recommended mitigation measures).\nThe MAE process framework provides a common repeatable risk management process\nthat is part of building secure and resilient systems [1].\n\nCyber risk remediation analysis (RRA) is the final step in the MAE process framework. It\nis a methodology for selecting countermeasures (CMs) to reduce a cyber-asset’s susceptibility\nto cyber-attack over a range of tactics, techniques, and procedures (TTPs) associated with the\nAPT. A CM is defined as an action, device, procedure, or technique that opposes or coun­\nters a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the\nharm it can cause, or by detecting and reporting it so that corrective action can be taken [2].\nThe selection of CMs is governed by the system life cycle of the cyber-asset being evaluated.\nRecommended CMs are those judged to be effective at mitigating TTPs to which a cyber-asset\nmay be susceptible. CMs cover a broad spectrum, including changes to requirements, system\ndesign, testing, deployment configuration, and/or operating procedures.\n\nThis article focuses on what we know today about cyber RRA, a fast-moving branch of\nsystems engineering. The concepts and methods described are evolving and will continue to\nmature as more experience is gained in applying this discipline. Please revisit this article for\nadditional insights as the community’s collective knowledge builds.\n\nThe Advanced Persistent Threat (APT) refers to an adversary with sophisticated levels of\nexpertise and significant resources that can apply multiple, different attack vectors to achieve\nits objectives, which include the establishment of footholds within the information technology\ninfrastructure of an organization to continually exfiltrate information and/or undermine or\nimpede critical aspects of a mission, program, or organization, or to place itself in a position to\ndo so in the future [3]. The APT pursues its objectives over an extended period of time, adapts\nto a defender’s efforts to resist it, and maintains the level of interaction needed to execute its\nobjectives.\n\nCyber RRA is a follow-on to a cyber Threat Susceptibility Analysis (TSA), which produces\na Threat Susceptibility Matrix that ranks TTPs and maps them to cyber-assets. In a TSA\nassessment, a scoring model spreadsheet may be used to rank TTPs on a risk scale of [1–5],\nwith 1 representing very low risk and 5 representing very high risk. Factors used in the TTP\n\n\n-----\n\nrisk scoring spreadsheet can vary from one assessment to the next, but must be uniformly\napplied across all TTPs evaluated in an assessment to ensure consistent ranking. This scoring\ntool can be tailored, (e.g., add or remove criteria, modify weightings) or even replaced to meet\na program’s needs. The interested reader is referred to [4, 5] for details on TSA and the default\nTTP risk scoring model.\n\nThe first step in cyber RRA is to use the Threat Susceptibility Matrix to identify which\nTTPs to mitigate for each cyber-asset. There are several strategies for performing this selec­\ntion. One strategy is to focus only on the highest risk TTPs of each cyber-asset. Another strat­\negy is to focus on the cyber-asset(s) that have the highest aggregate susceptibility. Aggregate\nsusceptibility is calculated for each cyber-asset and category of threat actor by summing the\nrisk scores of the mapped TTPs. Note that these calculations use rounded risk scores and will\nbe subject to rounding errors. A third strategy is for RRA to focus exclusively on crown jewel\ncyber-assets. A hybrid approach might select high-risk TTPs for the crown jewel cyber-assets\nwith the highest aggregate susceptibility. Whatever strategy is used, the result will be a list of\nTTPs for each cyber-asset assessed.\n\nTable 1 in the preceding article, “Cyber Threat Susceptibility Assessment,” provided a\nnotional example of a Threat Susceptibility Matrix for two cyber-assets: cyber-asset #1 and\ncyber-asset #2. In this example, both assets are judged to be essentially equally susceptible to\nhigh-risk TTPs T000017 and T000030. Overall, cyber-asset #2 is more susceptible than cyberasset #1 to a range of TTPs, as reflected by its higher aggregate susceptible scores. The color\ncoding indicates the relative severity of the threat.\n\nBecause different cyber-assets are susceptible to different TTPs, cyber RRAs are con­\nducted separately for each cyber-asset. The RRA uses a mapping table that associates TTPs\nwith CMs. A sample TTP/CM mapping table is illustrated in Table 2.\n\nEach row in a TTP/CM mapping table corresponds to a countermeasure and each column\ncorresponds to a TTP. A CM to TTP mapping is characterized by the mitigation effectiveness\nof the CM over a range of criteria: detect, neutralize, limit, and recover. Detect CMs serve to\nidentify or uncover the action or presence of a TTP. Neutralize CMs stop or prevent execu­\ntion of a TTP. Limit CMs serve to reduce or constrain the risk associated with a TTP, either\nby lessening the severity or likelihood. Recovery CMs facilitate recovery from attack. A given\nCM may be highly effective at detecting a certain TTP, moderately effective at neutralizing or\nlimiting its impact, but provide no mitigation value in recovering from its effects. A 2-char­\nacter notation is used to denote mitigation effectiveness within the mapping table, where the\nfirst character signifies the type of mitigation from the list: (N)eutralize, (D)etect, (L)imit, and\n(R)ecover, and the second character represents the degree of effectiveness from the list: (L) ow,\n(M)edium, (H)igh, and (V)ery high. The value NH represents Neutralize-High mitigation\neffectiveness, while the value DM represents Detect-Medium mitigation effectiveness.\n\n\n-----\n\nTable 2. TTP/CM Mapping Table\n\n**Mitigation Effectiveness (by TTP ID)**\n\n**CM ID**\n\n**T000017** **T000030** **T000039** **T000041** **T000053** **T000064** **T000086** **T000127**\n\nC000039 NM\n\nC000045 NH NH\n\nC000047 NH\n\nC000058 NH\n\nC000067 DL, NM\n\nC000073 LM\n\nC000083 LH,NH\n\nC000086 LM\n\nC000096 NM\n\nC000097 DM, NM\n\nC000110 LM, NL\n\nC000113 NM\n\nC000121 DM, NM\n\nC000122 NM\n\nC000124 LM\n\nC000126 LM\n\nC000129 NM\n\nC000133 NM\n\nC000144 NH\n\nC000145 NH NH\n\nC000147 NM\n\nC000159 NM\n\nC000164 NM\n\nC000165 LH\n\nC000173 NM\n\nC000187 LM\n\nC000188 NM\n\nThe RRA seeks to identify a list of CMs that mitigate a list of TTPs based on this map­\nping table. This list is optimal if it identifies CMs that are highly effective at mitigating most\nor all of the listed TTPs at a minimal cost. One key assumption made with this approach is\nthat CMs can be applied in combination to achieve greater mitigation effectiveness than they\nwould individually, which is the basis for the onion model of security.\n\nTo identify an optimal list of CMs, it is first necessary to assess the relative merit of each\nCM. One approach, detailed below, is to calculate the utility/cost (U/C) ratio for each CM.\nA U/C ratio is a “bang-for-buck” valuation of a CM derived from its estimated utility and cost.\n\n|CM ID|Mitigation Effectiveness (by TTP ID)|Col3|Col4|Col5|Col6|Col7|Col8|Col9|\n|---|---|---|---|---|---|---|---|---|\n||T000017|T000030|T000039|T000041|T000053|T000064|T000086|T000127|\n|C000039||||||NM|||\n|C000045||NH|NH||||||\n|C000047|||NH||||||\n|C000058|||NH||||||\n|C000067|DL, NM||||||||\n|C000073||||LM|||||\n|C000083|LH,NH||||||||\n|C000086||||LM|||||\n|C000096||||NM|||||\n|C000097||||DM, NM|||||\n|C000110||||LM, NL|||||\n|C000113||||NM|||||\n|C000121|||||||DM, NM||\n|C000122||||NM|||||\n|C000124|||LM||||||\n|C000126|||||LM||||\n|C000129|||||||NM||\n|C000133||||||NM|||\n|C000144|||NH||||||\n|C000145|NH||NH||||||\n|C000147||NM|||||||\n|C000159||||||NM|||\n|C000164||NM|||||||\n|C000165||||LH|||||\n|C000173||NM|||||||\n|C000187||||||||LM|\n|C000188||NM|||||||\n\n\n-----\n\nWith the default scoring model, CM utility is estimated by assigning a numeric score to each\nmitigation effectiveness value and multiplying by the number of mappings containing that\nmitigation effectiveness value across the list of TTPs being assessed. Once U/C ratios are cal­\nculated for each CM, the CM ranking table is sorted by descending U/C ratios. This approach\nfor calculating U/C rations is depicted in Table 3.\n\nThe next step is to walk through the ranking table to identify sets of CMs that mitigate\nthe list of TTPs, starting at the top and working down. Ordering CMs in the ranking table by\ndescending U/C ratio facilitates early identification of optimal solutions. When a combination\n\nTable 3. CM Ranking Table Example\n\n|CM ID|Neutralize|Col3|Col4|Limit|Col6|Detect|Col8|CM Merit Scoring|Col10|Col11|\n|---|---|---|---|---|---|---|---|---|---|---|\n||NH = 9|NM = 7|NL = 5|LH = 7|LM = 5|DM = 3|DL = 1|Utility|Cost|U/C Ratio|\n|C00159||T000064||||||7|1|7.0|\n|C00164||T00030||||||7|1|7.0|\n|C00165||||T000041||||7|1|7.0|\n|C00173||T000030||||||7|1|7.0|\n|C00188||T000030||||||7|1|7.0|\n|C00045|T000030, T000039|||||||18|3|6.0|\n|C00145|T000039, T000017|||||||18|3|6.0|\n|C00083|T000017|||T000017||||16|3|5.3|\n|C00073|||||T000041|||5|1|5.0|\n|C00067||T000017|||||T000017|8|2|4.0|\n|C00096||T000041||||||7|2|3.5|\n|C00113||T000041||||||7|2|3.5|\n|C00133||T000064||||||7|2|3.5|\n|C00097||T000041||||T000041||10|3|3.3|\n|C00110|||T000041||T000041|||10|3|3.3|\n|C00047|T000039|||||||9|3|3.0|\n|C00058|T000039|||||||9|3|3.0|\n|C00144|T000039|||||||9|3|3.0|\n|C00086|||||T000041|||5|2|2.5|\n|C00121||T000086||||T000086||10|4|2.5|\n|C00124|||||T000039|||5|2|2.5|\n|C00126|||||T000053|||5|2|2.5|\n|C00187|||||T000127|||5|2|2.5|\n|C00039||T000064||||||7|3|2.3|\n|C00122||T000041||||||7|3|2.3|\n|C00129||T000086||||||7|3|2.3|\n|C00147||T000030||||||7|3|2.3|\n\n\n-----\n\nof CMs is identified that provides mitigation value over the range of TTPs, a solution effective­\nness table is constructed to illustrate the coverage provided and to calculate the relative cost\nfor that solution. These solution effectiveness tables are used to compare alternative solutions.\nTables 4 and 5 represent two alternative solutions that provide roughly equivalent mitigation\nover the same list of TTPs but at different costs.\n\nIn addition to providing a basis for comparing alternative solutions, solution effectiveness\ntables document the mapping between each TTP and the CMs that provide mitigation value.\nThey can be used to identify cases where mitigation redundancies or coverage gaps exist.\nFor example, additional countermeasures may be advisable for T000053 and T000127 in both\nsolution alternatives above.\n\nTable 4. Solution Effectiveness Table Example #1\n\n**Mitigation Effectiveness (by TTP ID)**\n\n**CM ID** **Cost**\n\n**T000017** **T000030** **T000039** **T000041** **T000053** **T000064** **T000086** **T000127**\n\nC000045 3 NH NH\n\nC000083 3 LH,NH\n\nC000126 2 LM\n\nC000129 3 NM\n\nC000165 1 LH\n\nC000187 2 LM\n\nC000159 1 NM\n\nTotal 15 NH NH NH LH LM NM NM LM\n\nTable 5. Solution Effectiveness Table Example #2\n\n|CM ID|Cost|Mitigation Effectiveness (by TTP ID)|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|||T000017|T000030|T000039|T000041|T000053|T000064|T000086|T000127|\n|C000045|3||NH|NH||||||\n|C000083|3|LH,NH||||||||\n|C000126|2|||||LM||||\n|C000129|3|||||||NM||\n|C000165|1||||LH|||||\n|C000187|2||||||||LM|\n|C000159|1||||||NM|||\n|Total|15|NH|NH|NH|LH|LM|NM|NM|LM|\n\n|CM ID|Cost|Mitigation Effectiveness (by TTP ID)|Col4|Col5|Col6|Col7|Col8|Col9|Col10|\n|---|---|---|---|---|---|---|---|---|---|\n|||T000017|T000030|T000039|T000041|T000053|T000064|T000086|T000127|\n|C000058|3||NH|NH||||||\n|C000067|2|DL,NM||||||||\n|C000096|2||||NM|||||\n|C000133|2||||||NM|||\n|C000121|4|||||||DM,NM||\n|C000126|2|||||LM||||\n|C000147|2||NM|||||||\n|C000187|2||||||||LM|\n|Total|20|NM|NH|NH|NM|LM|NM|NM|LM|\n\n\n-----\n\nThe final step is to translate the list of CMs reflected by the lowest cost solution into\nwell-formed recommendations. A well-formed recommendation includes three pieces of\ninformation:\n\n###### �The action, device, procedure, or technique recommended (i.e., which CM to apply) �The reason why the CM is required (i.e., the TTPs that it mitigates) �The implication or effect if the CM is not applied (i.e., the potential impact to mission\n\ncapability resulting from compromise of the cyber asset)\nA cyber RRA conducted as follow-on to a cyber TSA addresses the first two items above.\nTo detail all three elements, however, a crown jewel analysis may be necessary in order to\nidentify the range of mission impact(s) that result from compromise of a cyber asset.\n\n###### Best Practices and Lessons Learned\n\n\nAdapt RRA to satisfy program needs. The objec­\n\ntive of an RRA assessment may not be to identify\n\nan optimal solution but instead to understand\n\nthe range of mitigation alternatives and/or areas\n\nwhere gaps exist. In this context, the RRA deliver­\n\nable would consist of TTP/CM mapping table data\n\nfor a specified set of TTPs.\n\nUse TARA to evaluate cyber risks and mitiga­\n\ntions. Early assessments demonstrated that\n\na TSA conducted without a follow-on RRA\n\nprovides limited value to sponsors who seek\n\nto reduce susceptibility to cyber-attack. The\n\nMAE portfolio now combines cyber TSA and\n\nRRA into a single engineering practice called\n\nThreat Assessment and Remediation Analysis\n\n(TARA) [5].\n\nConsider alternative scoring approaches. A\n\nvariety of more sophisticated scoring models and\n\napproaches can be considered prior to conduct­\n\ning an assessment. The RRA approach does not\n\nmandate using U/C ratios or the default RRA\n\nscoring model; any approach for estimating CM\n\n\nmerit may be used provided all CMs are assessed\n\nconsistently.\n\nAutomate to manage large amounts of data.\n\nLarge catalogs of TTPs and CMs produce very\n\nlarge TTP/CM mapping tables, which require\n\nautomation to be processed effectively.\n\nEvaluate security measures for operational sys­\n\ntems. For deployed and operational systems, one\n\noptional step not discussed above is the evalu­\n\nation of existing security measures to determine\n\nwhether effective TTP mitigations have already\n\nbeen applied. In cases where such security mea­\n\nsures are judged to be highly effective, it may be\n\nexpedient to remove the TTP from the list of TTPs\n\nbeing evaluated for that cyber-asset.\n\nReduce the CM search space. The process used\n\nto enumerate the solution set of CMs can benefit\n\nfrom the application of heuristics that reduce the\n\nsearch space. The heuristic outlined previously is\n\nto rank CMs by U/C ratio in order to facilitate early\n\nidentification of optimal (i.e., lowest cost) solu­\n\ntions. Other heuristics may also apply.\n\n\n-----\n\nCrown jewel analysis is essential. A well-formed\n\nrecommendation details risk-to-mission impact,\n\nwhich is needed to make informed decisions.\n\n###### References and Resources\n\n\nA crown jewel analysis or other form of mission\n\nimpact analysis is essential for that determination.\n\n\n[1. Goldman, H., October 2010, Building Secure, Resilient Architectures for Cyber Mission](http://www.mitre.org/work/tech_papers/2010/10_3301/)\n\n[Assurance, The MITRE Corporation.](http://www.mitre.org/work/tech_papers/2010/10_3301/)\n\n2. NIST Special Publication 800-39, March 2011, Integrated Enterprise-Wide Risk\n\nManagement.\n\n[3. CNSS Instruction 4009, National Information Assurance (IA) Glossary, April 2010, Cyber](http://www.mitre.org/work/systems_engineering/guide/enterprise_engineering/se_for_mission_assurance/cyberthreat_susceptibility.html)\n\n[Threat Susceptibility Analysis (TSA), Systems Engineering Guide.](http://www.mitre.org/work/systems_engineering/guide/enterprise_engineering/se_for_mission_assurance/cyberthreat_susceptibility.html)\n\n4. Wynn, J., and L. Montella, October 2010, “Cyber Threat Susceptibility Analysis (TSA)\n\nMethodology,” Ver. 2.0, MTR 100379, The MITRE Corporation.\n\n5. Wynn, J., et al., October 2011, “Threat Assessment and Remediation Analysis (TARA),”\n\nVer. 1.4, MTR 110176, The MITRE Corporation.\n\n\n-----\n\nDefinition: A secure code review\n\n_is a specialized task involving_\n\n_manual and/or automated_\n\n_review of an application’s_\n\n_source code in an attempt_\n\n_to identify security-related_\n\n_weaknesses (flaws) in the code._\n\n_A secure code review does not_\n\n_attempt to identify every issue_\n\n_in the code, but instead looks to_\n\n_provide insight into what types_\n\n_of problems exist and to help_\n\n_the developers of the applica­_\n\n_tion understand what classes_\n\n_of issues are present. The goal_\n\n_is to arm the developers with_\n\n_information to help them make_\n\n_the application’s source code_\n\n_more sound and secure._\n\nKeywords: code review, evalua­\n\n_tion, secure code, secure code_\n\n_review, secure development,_\n\n_security, test, vulnerable_\n\n_software_\n\n\nSYSTEMS ENGINEERING FOR MISSION\nASSURANCE\n###### Secure Code Review\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystem engineers (SEs) often help our sponsors\n\nand customers formulate plans and policies for\n\ndeveloping applications through all stages of\n\nthe software development life cycle. Security\n\nhas become a major point of emphasis and a\n\nkey component within the larger area of mission\n\nassurance. Writing source code that is sound\n\nand secure is key in creating applications that\n\nwithstand attack and function as intended in\n\nthe face of a malicious adversary. As a conse­\n\nquence, MITRE SEs are expected to understand\n\nthe rationale behind a secure code review and\n\nwhen such a review is appropriate. They are\n\nexpected to understand where a secure code\n\nreview fits into the software development life\n\ncycle and how it can be used most effectively to\n\n\n-----\n\nidentify potential issues within the code. Finally, MITRE SEs are expected to understand how\na secure code review is performed and what its strengths and limitations are.\n\n###### Background\n\nApplication-level security is increasingly coming under fire. The Stuxnet worm [1] in 2010\nwas a high-profile example of how a malicious user can leverage an application vulnerabil­\nity to subvert protection mechanisms and damage an end system. Verifying that applica­\ntions correctly implement security mechanisms and do not contain vulnerabilities is critical\nto achieving mission assurance goals.\n\nCompounding the problem are the facts that applications are becoming more intercon­\nnected and that flaws in one application often lead to exploitation of other applications. There\nis no unimportant application from a security point of view. Malicious users are eager to take\nadvantage of any flaw in any application that enables them to achieve their goal.\n\nAlmost all software development life cycles include testing and validation, which is\noften accomplished as a code review by either a peer or an external entity. The review veri­\nfies that the application functions as expected and that required features have been imple­\nmented correctly. Code reviews are important and should still occur. However, an additional\nreview with a focus solely on security should also be conducted.\n\nA secure code review is a specialized task involving manual and/or automated review\nof an application’s source code in an attempt to identify security-related weaknesses (flaws)\nin the code. It does not attempt to identify every issue in the code, but instead looks to\nprovide insight into what types of security-related problems exist and help the application\ndeveloper understand what classes of issues are present. A secure code review will not\nnecessarily find every security flaw in an application, but it should arm developers with\ninformation to help make the application’s source code more sound and secure.\n\nThe goal of a secure code review is to find and identify specific security-related flaws\nwithin the code that a malicious user could leverage to compromise confidentiality, integrity,\nand availability of the application. For example, an unchecked input to a buffer may enable\na write past the end of the buffer and allow a malicious user to execute arbitrary code with\nescalated privileges. A secure code review looks to find these types of issues, notify develop­\nment teams of them, and eventually result in fewer vulnerabilities in the application.\n\nA secure code review is not a silver bullet, but it is a strong part of an overall risk miti­\ngation program to protect an application.\n\n###### Government Interest and Use\n\nOn October 29, 2010, The Defense Information Systems Agency (DISA) issued Version 3,\nRelease 2, of the Application Security and Development Security Technical Implementation\n\n\n-----\n\nGuide (STIG) [2]. This document “provides the guidance needed to promote the development,\nintegration, and updating of secure applications” throughout their life cycles.\n\nDepartment of Defense (DoD) Directive (DoDD) 8500.01E requires that all information assur­\nance (IA) and IA-enabled information technology (IT) products incorporated into DoD informa­\ntion systems be configured in accordance with DoD-approved security configuration guidelines,\nand it tasks DISA to develop and provide security configuration guidance for IA and IA-enabled\nIT products in coordination with the Director, National Security Agency. The Application Security\nand Development STIG is provided under the authority of DoDD 8500.01E.\n\nAPP5080 within the Application Security and Development STIG mandates a secure code\nreview before an application is released.\n\n###### Focus of a Secure Code Review\n\nA secure code review focuses on seven security mechanisms, or areas. An application that is\nweak in any area makes itself a target for a malicious user and increases the likelihood that the\napplication will be used in an attack. A secure code review should inform the developers of the\nsoundness of the source code in each of these areas:\n\n###### �Authentication �Authorization �Session management �Data validation �Error handling �Logging �Encryption\nSeveral weaknesses (flaws) can affect each of the preceding security mechanisms. Flaws\nin the handling of passwords often affect authentication. Flaws related to the type of informa­\ntion included in a message often affect error handling. Flaws in regular expressions often affect\ndata validation.\n\nThe Common Weakness Enumeration [3] is a listing of the specific types of flaws that a\nsecure code review looks for. “It serves as a common language for describing software security\nweaknesses, as a standard measuring stick for software security tools targeting these vulnerabili­\nties, and as a baseline standard for weakness identification, mitigation, and prevention efforts.”\n\n###### Manual vs. Automated Review\n\nA secure code review can be a manual or automated review, each with advantages and dis­\nadvantages. In a manual review, an analyst reviews the code line by line, looking for defects\nand security-related flaws. An automated review uses a tool to scan the code and report\npotential flaws.\n\n\n-----\n\nManual review is time-consuming and requires significant domain expertise to be done\ncorrectly. Often it takes years of experience to become efficient at manual code review. Even\nwith experienced human analysis, errors in the review (missed and incorrect findings) are\nunavoidable. A proficient reviewer can get through about 3,000 lines of code a day, based on\nthe experiences of the MITRE Secure Code Review Practice.\n\nAutomated review helps solve the problems associated with manual review. However,\ngood automated review tools are expensive. Additionally, the technology behind auto­\nmated tools is only effective at finding certain types of flaws. A single automated tool may\nbe good at finding some issues but unable to detect others. Employing multiple auto­\nmated tools can mitigate this problem but will still not uncover every issue. Automated\ntools also tend to produce false positives (reported findings that are not actually issues).\nAdjudicating false positives requires human intervention and takes time away from the\ndevelopment team.\n\nThe best approach for a secure code review is to understand the advantages and disad­\nvantages of each method and to incorporate both as appropriate.\n\n###### When to Perform a Secure Code Review\n\nSecurity should be a focus throughout the entire development life cycle. Creating threat mod­\nels during the design phase, educating developers on secure coding practices, and perform­\ning frequent peer reviews of code with security personnel involved will all help increase the\noverall quality of the code and reduce the number of issues reported (and hence that need to\nbe fixed) by the secure code review.\n\nHowever, a secure code review is best used toward the end of the source code develop­\nment, when most or all functionality has been implemented. The reason for waiting until\nlate in the development phase is that a secure code review is expensive and time-consuming.\nPerforming it once toward the end of the development process helps mitigate cost.\n\n###### Best Practices and Lessons Learned\n\n\nUnderstand the developers’ approach. Before\n\nstarting a secure code review, talk to the develop­\n\ners and understand their approaches to mecha­\n\nnisms like authentication and data validation.\n\nInformation gathered during this discussion\n\ncan help jump-start the review and significantly\n\ndecrease the time a reviewer spends trying to\n\nunderstand the code.\n\n\nUse multiple techniques. If possible, use both\n\nmanual and automated techniques for the\n\nreview because each method will find things\n\nthat the other doesn’t. In addition, try to use\n\nmore than one automated tool because the\n\nstrengths of each differ and complement the\n\nothers.\n\n\n-----\n\nDo not assess level of risk. A secure code\n\nreview should not attempt to make judgments\n\nabout what is acceptable risk. The review team\n\nshould report what it finds. The customer uses\n\nthe program’s approved risk assessment plan to\n\nassess risk and decide whether to accept it or\n\nnot.\n\nFocus on the big picture. When performing a\n\nmanual review, resist trying to understand the\n\ndetails of every line of code. Instead, gain an\n\nunderstanding of what the code as a whole is\n\ndoing and then focus the review on important\n\n###### Limitations of a Secure Code Review\n\n\nareas, such as functions that handle login or\n\ninteractions with a database. Leverage auto­\n\nmated tools to get details on specific flaws.\n\nFollow up on review points. After a review, hold\n\na follow-up discussion with the development\n\nteam to help them understand what the findings\n\nmean and how to address them.\n\nStick to the intent of the review. Secure code\n\nreview is not penetration testing. Review teams\n\nshould not be allowed to “pen-test” a running\n\nversion of the code because it can bias the\n\nresults by giving a false sense of completeness.\n\n\nA secure code review is not a silver bullet, and performing such a review does not mean that\nevery instance of a security flaw will be discovered. Rather it is one of many different types of\nactivities that can help increase the quality of an application and reduce the number vulner­\nabilities in software, making it more difficult for a malicious user to exploit.\n\n###### References and Resources\n\n[1. Wikipedia contributors, Wikipedia, The Free Encyclopedia, “Stuxnet,” accessed February](http://en.wikipedia.org/wiki/Stuxnet)\n\n14, 2011.\n\n[2. Application Security and Development Security Technical Implementation Guide, Defense](http://iase.disa.mil/stigs/downloads/zip/u_application_security_and_development_stig_v3r3_20110429.zip)\n\nInformation Systems Agency, Ver. 3, Rel. 3.\n\n[3. The MITRE Corporation, Common Weakness Enumeration, cwe.mitre.org/.](http://cwe.mitre.org/)\n\n###### Additional References and Resources\n\n[Department of Homeland Security, 2010, Software Assurance Pocket Guide Series: Software](https://buildsecurityin.us-cert.gov/swa/downloads/software_security_testing.pdf)\n[Security Testing.](https://buildsecurityin.us-cert.gov/swa/downloads/software_security_testing.pdf)\n\nDowd, M., J. McDonald, and J. Schuh, 2007, The Art of Software Security Assessment:\n_Identifying and Preventing Software Vulnerabilities, Addison-Wesley, ISBN 0-321-44442-6._\n\nViega, J., and G. McGraw, 2002, Building Secure Software: How to Avoid Security Problems the\n_Right Way, Addison-Wesley, ISBN 0-201-72152-X._\n\n\n-----\n\nDefinition: Supply Chain Risk\n\n_Management (SCRM) is a_\n\n_discipline that addresses the_\n\n_threats and vulnerabilities of_\n\n_commercially acquired infor­_\n\n_mation and communications_\n\n_technologies within and used_\n\n_by government information_\n\n_and weapon systems. Through_\n\n_SCRM, systems engineers can_\n\n_minimize the risk to systems_\n\n_and their components obtained_\n\n_from sources that are not_\n\n_trusted or identifiable as well_\n\n_as those that provide inferior_\n\n_material or parts._\n\nKeywords: advanced cyber\n\n_threat, configuration manage­_\n\n_ment, emerging threat, materiel,_\n\n_program protection plan, risk_\n\n_management, supply chain,_\n\n_systems engineering process_\n\n\nSYSTEMS ENGINEERING FOR MISSION\nASSURANCE\n###### Supply Chain Risk Management (SCRM)\n\n**MITRE SE Roles and Expectations: The expan­**\n\nsion of the global economy, increased use of\n\noutsourcing, and development of open standards\n\nare some of the modern-day factors that present\n\nnew challenges to the security of government\n\nsystems. These factors have resulted in emerging\n\nthreats and have made protection of the supply\n\nchain increasingly difficult [1]. All MITRE systems\n\nengineers (SEs) must understand these emerging\n\nthreats and why SCRM is necessary to ensure the\n\nprotection and viability of all government systems.\n\n\n-----\n\n###### Why SCRM Is Important\n\nThe National Security Presidential Directive 54, Homeland Security Presidential Directive\n23, and Defense Authorization Act 254 have made SCRM a national priority [2, 3]. Thus the\nDepartment of Defense (DoD), Department of Homeland Security, and other departments\nhave begun to review and refine their SCRM practices and procedures. The goal of one of the\nDoD Comprehensive National Cyber Initiatives (CNCI) is to provide the U.S. government with\na robust toolset of supply chain assurance defense-in-breadth and defense-in-depth methods\nand techniques. The CNCI effort conducted a pilot program and produced a Key Practices\nGuide to provide SEs with key practices that can help manage supply chain risk. All SEs\nshould become familiar with ongoing efforts within their sponsor’s organization and materi­\nals like the Key Practices Guide. A summary of best practices follows.\n\n###### Best Practices and Lessons Learned\n\n\nSupply chain analysis. To determine the appli­\n\ncability of SCRM to a MITRE systems engineering\n\nproject or initiative, MITRE SEs must comprehend\n\nor become educated on supply chain materiel\n\nmanagement processes, the emerging threat, and\n\nthe current supply chain challenges. This back­\n\nground will assist SEs in assessing which systems,\n\ncomponents, software, organizational processes,\n\nand workforce issues have vulnerabilities or weak­\n\nnesses that can be exploited.\n\nThe term “supply chain” has different meanings\n\nto commercial, government, and commercial\n\nentities. The military has extensive processes for\n\nstructuring supplies (materiel management) to\n\ntheir units and organizations (see DoD 4140.1-R)\n\n[4]. Historically, the DoD has assessed the logisti­\n\ncal tail of supply chain by focusing on the distribu­\n\ntion and shipment of equipment, but this does\n\nnot address the complete “chain.” To address\n\nthe emerging threat, the “supply chain” analysis\n\nmust address all parts and components of a\n\nsystem early in the program, including firmware\n\n\nand software. It must also analyze the impact of\n\npeople, purchase of substitute parts, and auto­\n\nmated processes (e.g., software patching) on the\n\nsupply chain processes.\n\nTherefore, an accurate SCRM assessment\n\nincludes an evaluation of the origin of the mate­\n\nriel, how it is distributed, and the government\n\ndecision-making process in the selection of the\n\nproduct. The MITRE SE role is to ensure that\n\nthe systems engineering process is applied to all\n\ncomponents and parts of a system throughout\n\ntheir life cycle.\n\nLife-cycle applicability. MITRE SEs should be\n\nprepared to apply SCRM at any point of a system’s\n\nlife; it is never too late nor too early in a system\n\nlife to incorporate the SCRM process. SCRM is\n\ncurrently being applied to materiel supply during\n\nthe logistic phases, but a more effective systems\n\nengineering process should include addressing\n\nSCRM as early in the program as possible.\n\nThe DoD CNCI SCRM pilot program produced\n\nan implementation guide that offers detailed\n\n\n-----\n\nsuggestions on how and when SCRM should be\n\nintegrated into the life cycle of a system. This\n\nguide was developed to assist SEs and explains\n\nhow they can incorporate SCRM prior to design\n\nand throughout its life. A summary of some key\n\nsteps identified in the guide that a MITRE SE\n\nshould understand includes:\n\n###### � [Determine system criticality. ] � [Determine the supply chain threat. ] � [Select build versus buy. ] � [Select SCRM key practices and determine ]\n\nsufficiency.\n###### � [Understand the Risk Management Plan ]\n\nadopted by the government efforts they\n\nsupport.\n###### � [Understand the likelihood and the conse­]\n\nquence of insufficient SCRM practices.\n\nSystems engineering and SCRM. The core\n\nsystems engineering process used to pro­\n\ntect the supply chain is risk management (see\n\nRisk Management in the Acquisition Systems\n\nEngineering section of the SEG). Pilot programs\n\nselected by the DoD to help refine SCRM policy\n\nare using the Information and Communication\n\nTechnology Supplier Risk Management Process.\n\nThe concept of operations for the DoD\n\nComprehensive National Cybersecurity Initiative\n\nSupply Chain Risk Management Pilot Program\n\ndescribes this process [5].\n\nThough risk management establishes the core\n\nfor an effective SCRM process, SEs should\n\nalso understand the relationship of other sys­\n\ntems engineering disciplines and processes to\n\nSCRM [6]. Standard program documentation\n\n\naddressing software engineering practices and\n\nprocedures should include applicability to their\n\nSCRM process. Another process that supports\n\nthe protection of the supply chain is configuration\n\nmanagement. Through configuration control and\n\nmanagement, SEs can ensure that the system’s\n\nbaseline is tightly controlled and any changes to\n\nit are in accordance with appropriate systems\n\nengineering rigor and review (see Configuration\n\nManagement in the Acquisition Systems\n\nEngineering section of the SEG).\n\nSEs should ensure that acquisition, sustainment,\n\ndisposal, and other program documentation are\n\nproperly updated to include SCRM. At a minimum,\n\nthe following kinds of documents should incor­\n\nporate the SCRM process and findings: Program\n\nProtection Plan, Systems Engineering Plans/\n\nProcedures, and Life Cycle Management Plans. In\n\naddition, SEs should work closely with contracts\n\nand legal staff to verify that SCRM is included as\n\npart of the acquisition documentation, source\n\nselection criteria, and contractual clauses. They\n\nshould also ensure that the SCRM practices are\n\nincluded as part of the sustainment documen­\n\ntation, supplier selection criteria, purchasing\n\nclauses, incoming inspection, quality verification\n\ntesting, acceptance for inventory, and disposal\n\nprocesses.\n\n\n-----\n\n###### References and Resources\n\n1. Mirsky, A., May 4, 2009, Supply Chain Risk Management (SCRM).\n\n2. National Security Presidential Directive 54/Homeland Security Presidential Directive 23,\n\nJanuary 8, 2008, National Cyber Security Initiative, paragraph 45.\n\n[3. Extract from Public Law 110-417, October 14, 1008, “Duncan Hunter NDAA for Fiscal Year](http://www.dod.mil/dodgc/olc/docs/2009NDAA_PL110-417.pdf)\n\n[2009.”](http://www.dod.mil/dodgc/olc/docs/2009NDAA_PL110-417.pdf)\n\n4. Office of the Deputy Under Secretary of Defense for Logistics and Materiel Readiness, May\n\n[23, 2003, DoD 4140.1-R DoD Supply Chain Materiel Management Regulation.](http://www.dtic.mil/whs/directives/corres/pdf/414001r.pdf)\n\n5. SCRM PMO Globalization Task Force OASSA(NII)-CIO/ODASD(IIA), April 24, 2009,\n\n_Concept of Operations for the DoD Comprehensive National Cybersecurity Initiative Supply_\n_Chain Risk Management Pilot Program, Ver. 2.0._\n\n6. National Defense Industrial Association System Assurance Committee, October 2008,\n\n[Engineering for System Assurance, Ver. 1.0.](http://www.acq.osd.mil/se/docs/SA-Guidebook-v1-Oct2008.pdf)\n\n\n-----\n\n##### Transformation Planning and\n Organizational Change\n\nDefinition: Transformation planning is a process that develops a plan to modify an\n\n_enterprise’s business processes by modifying policies, procedures, and processes_\n\n_to move from an “as is” to a “to be” state. Change management is a process for_\n\n_gaining business intelligence to perform transformation planning by assessing an_\n\n_organization’s people and cultures to determine how changes (e.g., to strategy,_\n\n_structure, process, or technology) will impact the enterprise._\n\nKeywords: business transformation, complex systems, life-cycle development,\n\n_organizational change management, organizational development, organizational_\n\n_strategy, organizational transformation, psychology, social sciences, stakeholder_\n\n_management, systems thinking, trust_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to be able to assist in\n\nformulating the strategy and the plans for transforming a customer’s\n\nengineering/technical organization, structure, and processes, including\n\nthe MITRE support to that organization. MITRE SEs are expected to\n\nrecommend interfaces and interactions with other organizations, lead\n\nchange, collaborate, build consensus across the MITRE support and\n\nother stakeholders for the transformation, and to assist in communicat­\n\ning the changes. To execute these roles and meet these expectations,\n\nMITRE SEs are expected to understand the complex, open-systems\n\n\n-----\n\nnature of how organizations change, and the importance of developing the workforce trans­\nformation strategies as a critical, fundamental, and essential activity in framing a project plan.\nThey must understand the social processes and other factors (e.g., leadership, culture, struc­\nture, strategy, competencies, and psychological contracts) that affect the successful transfor­\nmation of a complex organizational system.\n\n###### Context\n\nThe objective of organizational change management is to enable organization members and\nstakeholders to adapt to a sponsor’s new vision, mission, and systems, as well as to identify\nsources of resistance to the changes and minimize resistance to them. Organizations are\nalmost always in a state of change, whether the change is continuous or episodic. Change\ncreates tension and strain in a sponsor’s social system that the sponsor must adapt to so that it\ncan evolve. Transformational planning and organizational change is the coordinated manage­\nment of change activities affecting users, as imposed by new or altered business processes,\npolicies, or procedures and related systems implemented by the sponsor. The objectives are to\neffectively transfer knowledge and skills that enable users to adopt the sponsor’s new vision,\nmission, and systems and to identify and minimize sources of resistance to the sponsor’s\nchanges.\n\n###### Best Practices and Lessons Learned\n\n\nImplementation of a large-scale information\n\ntechnology (IT) transformation project affects the\n\nentire organization. In a technology-based trans­\n\nformation project, an organization often focuses\n\nsolely on acquiring and installing the right hard­\n\nware and software. But the people who are going\n\nto use the new technologies—and the processes\n\nthat will guide their use—are even more impor­\n\ntant. As critical as the new technologies may be,\n\nthey are only tools for people to use in carrying\n\nout the agency’s work.\n\nIntegrate organizational change manage­\n\nment principles into your program. As Figure\n\n1 shows, the discipline of organizational change\n\nmanagement (OCM) is intended to help move an\n\norganization’s people, processes, and technology\n\n\nfrom the current “as is” state to a desired future\n\n“to be” state. To ensure effective, long-term, and\n\nsustainable results, there must be a transition\n\nduring which the required changes are introduced,\n\ntested, understood, and accepted. People have\n\nto let go of existing behaviors and attitudes and\n\nmove to new behaviors and attitudes that achieve\n\nPresent Future\nTransition\nstate state\n\nFigure 1. Organizational Transition Model\n\n\n-----\n\nand sustain the desired business outcomes.\n\nThat is why OCM is a critical component of any\n\nenterprise transformation program: It provides\n\na systematic approach that supports both the\n\norganization and the individuals within it as they\n\nplan, accept, implement, and transition from the\n\npresent state to the future state.\n\nStudies have found that the lack of effective\n\nOCM in an IT modernization project leads to a\n\nhigher percentage of failure. According to a 2005\n\nGartner survey on “The User’s View of Why IT\n\nProjects Fail,” the findings pinned the failure in 31\n\npercent of the cases on an OCM deficiency. This\n\ndemonstrates the importance of integrating OCM\n\nprinciples into every aspect of an IT moderniza­\n\ntion or business transformation program.\n\nCommit to completing the change process.\n\nMITRE SEs need to assess change as a process\n\nand work in partnership with our sponsors to\n\ndevelop appraisals and recommendations to\n\n\nidentify and resolve complex organizational issues.\n\nThe change process shown in Figure 2 is designed\n\nto help assess where an organization is in the\n\nchange process and to determine what it needs\n\nto do as it moves through the process.\n\nBy defining and completing a change process,\n\nan organization can better define and document\n\nthe activities that must be managed during the\n\ntransition phase. Moving through these stages\n\nhelps ensure effective, long-term, and sustainable\n\nresults. These stages unfold as an organization\n\nmoves through the transition phase in which the\n\nrequired transformational changes are introduced,\n\ntested, understood, and accepted in a manner\n\nthat enables individuals to let go of their existing\n\nbehaviors and attitudes and develop any new skills\n\nneeded to sustain desired business outcomes.\n\nIt is very common for organizations to lose focus\n\nor create new initiatives without ever complet­\n\ning the change process for a specific program or\n\n\nMeasure progress,\ndemonstrate value,\ncommunicate\nsuccess, take\ncorrective action\nif needed\n\n\nEngage\nworkforce in\nplanning the\nchange, validate\ncosts and\nbenefits\n\n\nIntroduce new\ntools,\ntechnology,\nreward systems,\ntraining\n\n\nIdentify\nstakeholders,\ncosts, and\nbenefits of the\nchange\n\n\nEngage\nleadership:\ncommunicate\nthe compelling\nneed for change\n\n\nFigure 2. An Organizational Change Process\n\n\n-----\n\nproject. It is critical to the success of a transfor­\n\nmation program for the organization to recognize\n\nthis fact and be prepared to continue through the\n\nprocess and not lose focus as the organizational\n\nchange initiative is implemented. Commitment\n\nto completing the change process is vital to a\n\nsuccessful outcome. For more information, see\n\nthe SEG article “Formulation of Organizational\n\nTransformation Strategies.”\n\nEstablish a framework for change. In any enter­\n\nprise transformation effort, a number of variables\n\nexist simultaneously and affect the acceptance of\n\nchange by an organization. These variables range\n\nfrom Congressional mandates to the organiza­\n\ntion’s culture and leadership to the attitude and\n\nbehavior of the lowest ranking employee. At\n\nMITRE, social scientists use the Burke-Litwin\n\nModel of Organizational Performance and\n\nChange, or other approaches in line with the\n\nsponsor’s environment and culture, to assess\n\nreadiness and plan to implement change. The\n\nBurke-Litwin Model identifies critical transforma­\n\ntional and transactional factors that may impact\n\nthe successful adoption of the planned change.\n\nIn most government transformation efforts, the\n\nexternal environment (such as Congressional\n\nmandates), strategy, leadership, and culture\n\ncan be the most powerful drivers for creating\n\norganizational change. For more information,\n\nsee the SEG article “Performing Organizational\n\nAssessments.”\n\nAlign the transformation strategy with the\n\norganization’s culture. Most organizations\n\nultimately follow one of three approaches to\n\ntransformation. The type of approach relates to\n\nthe culture and type of organization (e.g., loosely\n\n\ncoupled [relaxed bureaucratic organizational\n\ncultures] or tightly coupled [strong bureaucratic\n\norganizational cultures]):\n\n###### � [Data-driven][ change strategies emphasize ]\n\nreasoning as a tactic for bringing about a\n\nchange in a social system. Experts, either\n\ninternal or external to the sponsor, are\n\ncontracted to analyze the system with the\n\ngoal of making it more efficient (level­\n\ning costs vs. benefits). Systems science\n\ntheories are employed to view the social\n\nsystem from a wide-angle perspective and\n\nto account for inputs, outputs, and trans­\n\nformation processes.\n\nThe effectiveness of a sponsor’s data\ndriven change strategy depends on (a) a\n\nwell-researched analysis that the trans­\n\nformation is feasible, (b) a demonstration\n\nthat illustrates how the transformation\n\nhas been successful in similar situations,\n\nand (c) a clear description of the results\n\nof the transformation. People will adopt\n\nthe transform when they understand the\n\nresults of the transformation and the\n\nrationale behind it.\n###### � [Participative][ change strategies assume ]\n\nthat change will occur if impacted units\n\nand individuals modify their perspective\n\nfrom old behavior patterns in favor of new\n\nbehaviors and business/work practices.\n\nParticipative change typically involves not\n\njust changes in rationales for action, but\n\nchanges in the attitudes, values, skills, and\n\npercepts of the organization.\n\nTo be successful, this change strategy\n\ndepends on all impacted organizational\n\n\n-----\n\nunits and individuals participating both\n\nin the change (including system design,\n\ndevelopment, and implementation of the\n\nchange) and their change “re-education.”\n\nThe degree of success depends on the\n\nextent to which the organizational units,\n\nimpacted users, and stakeholders are\n\ninvolved in the participative change transi­\n\ntion plan.\n###### � [Compliance-based][ change strategies are ]\n\nbased on the “leveraging” of power com­\n\ning from the sponsor’s position within the\n\norganization to implement the change.\n\nThe sponsor assumes that the units or\n\nindividuals will change because they are\n\ndependent on those with authority. Typi­\n\ncally the change agent does not attempt\n\nto gain insight into possible resistance\n\nto the change and does not consult with\n\nimpacted units or individuals. Change\n\nagents simply announce the change and\n\nspecify what organizational units and\n\nimpacted personnel must do to implement\n\nthe change.\n\nThe effectiveness of a sponsor’s compli­\n\nance-based change strategy depends on\n\nthe discipline within the sponsor’s chain\n\nof command, processes, and culture and\n\nthe capability of directly and indirectly\n\nimpacted stakeholders to impact sponsor\n\nexecutives. Research demonstrates that\n\ncompliance-based strategies are the least\n\neffective.\n\nRegardless of the extent of the organizational\n\nchange, it is critical that organizational impact and\n\nrisk assessments be performed to allow sponsor\n\n\nexecutives to identify the resources necessary to\n\nsuccessfully implement the change effort and to\n\ndetermine the impact of the change on the orga­\n\nnization. For more information, see the SEG article\n\n“Performing Organizational Assessments.”\n\nDistinguish leadership and stakeholders. MITRE\n\nSEs need to be cognizant of the distinction\n\nbetween sponsor executives, change agents/\n\nleaders, and stakeholders:\n\n###### � [Sponsor executives:][ Typically sponsor ]\n\nexecutives are the individuals within an\n\norganization who are accountable to the\n\ngovernment. Sponsor executives may or\n\nmay not be change leaders.\n###### � [Change leaders:][ Typically the change ]\n\nleader is the sponsor’s executive or com­\n\nmittee of executives assigned to manage\n\nand implement the prescribed change.\n\nChange leaders must be empowered to\n\nmake sponsor business process change\n\ndecisions, to formulate and transmit the\n\nvision for the change, and to resolve resis­\n\ntance issues and concerns.\n###### � [Stakeholders:][ Typically stakeholders are ]\n\ninternal and external entities that may be\n\ndirectly (such as participants) or indirectly\n\nimpacted by the change. A business unit’s\n\ndependence on a technology application\n\nto meet critical mission requirements is\n\nan example of a directly impacted stake­\n\nholder. An external (public/private, civil,\n\nor federal) entity’s dependence on a data\n\ninterface without direct participation in\n\nthe change is an example of an indirect\n\nstakeholder.\n\n\n-----\n\nBoth directly and indirectly impacted stakehold­\n\ners can be sources of resistance to a sponsor’s\n\ntransformation plan. For more information, see\n\nthe SEG articles “Stakeholder Assessment and\n\nManagement” and “Effective Communication and\n\nInfluence.”\n\nView resistance as a positive and integrative\n\nforce. Resistance is a critical element of orga­\n\nnizational change activities. Resistance may be\n\na unifying organizational force that resolves the\n\ntension between conflicts that are occurring as\n\nthe result of organizational change. Resistance\n\nfeedback occurs in three dimensions:\n\n###### � [Cognitive resistance occurs as the unit ]\n\nor individual perceives how the change\n\nwill affect its likelihood of voicing ideas\n\nabout organizational change. Signals of\n\ncognitive resistance may include limited\n\nor no willingness to communicate about\n\nor participate in change activities (such\n\nas those involving planning, resources, or\n\nimplementation).\n###### � [Emotional resistance occurs as the unit ]\n\nor individuals balance emotions dur­\n\ning change. Emotions about change are\n\nentrenched in an organization’s values,\n\nbeliefs, and symbols of culture. Emotional\n\nhistories hinder change. Signals of emo­\n\ntional resistance include a low emotional\n\ncommitment to change leading to inertia\n\nor a high emotional commitment leading\n\nto chaos.\n###### � [Behavior resistance is an integration of ]\n\ncognitive and emotional resistance that is\n\nmanifested by less visible and more covert\n\nactions toward the organizational change.\n\n\nSignals of behavioral resistance are the\n\ndevelopment of rumors and other informal\n\nor routine forms of resistance by units or\n\nindividuals.\n\nResistance is often seen as a negative force dur­\n\ning transformation projects. However, properly\n\nunderstood, it is a positive and integrative force\n\nto be leveraged. It is the catalyst for resolving\n\nthe converging and diverging currents between\n\nchange leaders and respondents and creates\n\nagreement within an organizational system.\n\nFor more information, see the SEG articles\n\n“Performing Organizational Assessments” and\n\n“Effective Communication and Influence.”\n\nCreate an organizational transition plan. As\n\ndiscussed earlier (Figure 1), successful support\n\nof individuals and organizations through a major\n\ntransformation effort requires a transition from\n\nthe current to the future state. Conducting an\n\norganizational assessment based on the Burke\nLitwin Model provides strategic insights into the\n\ncomplexity of the impact of the change on the\n\norganization. Once the nature and the impact of\n\nthe organizational transformation are understood,\n\nthe transformation owner or champion will have\n\nthe critical data needed to create an organiza­\n\ntional transition plan.\n\nTypically the content or focus of the transition\n\nplan comes from the insights gained by conduct­\n\ning a “gap” analysis between the current state\n\nof the organization (based on the Burke-Litwin\n\nassessment) and the future state (defined through\n\nthe strategy and vision for the transformation\n\nprogram). The transition plan should define how\n\nthe organization will close the transformational\n\nand transactional gaps that are bound to occur\n\n\n-----\n\nFigure 3. The Strategic Organizational Communications Process\n\n\nduring implementation of a transformation proj­\n\nect. Change does not occur in a linear manner,\n\nbut in a series of dynamic but predictable phases\n\nthat require preparation and planning if they are\n\nto be navigated successfully. The transition plan\n\nprovides the information and activities that allow\n\nthe organization to manage this “nonlinearity” in a\n\ntimely manner.\n\nLarge organizational change programs, which\n\naffect not only the headquarters location but also\n\ngeographically dispersed sites, require site-level\n\ntransition plans. These plans take into account\n\nthe specific needs and requirements of each\n\nsite. Most important, they will help “mobilize” the\n\norganizational change team at the site and engage\n\n###### References and Resources\n\n\nthe local workforce and leaders in planning for the\n\nupcoming transition.\n\nOpen and frequent communication is essen­\n\ntial to effective change management. A key\n\ncomponent of the transition plan should address\n\nthe strategic communications (Figure 3) required\n\nto support the implementation of the transfor­\n\nmation. When impacted individuals receive the\n\ninformation (directly and indirectly) they need\n\nabout the benefits and impact of the change,\n\nthey will more readily accept and support it. The\n\napproach to communication planning needs to\n\nbe integrated, multi-layered, and iterative. For\n\nmore information, see the SEG article “Effective\n\nCommunication and Influence.”\n\n\nBurke, W., 2008, Organizational Change: Theory and Practice. Sage Publications, 2nd Ed.\n\nBurke, W., and G. Litwin, 1992. “A Causal Model of Organizational Performance and Change,”\n_Journal of Management, Vol. 18, No. 3._\n\nFlint, D., 2005, “The User’s View of Why IT Projects Fail,” Gartner Report.\n\nKotter, John P., 1998, “Winning at Change,” Leader to Leader, 10 (Fall 1998), 27–33.\n\nLawson, E., and C. Price, 2003, “The Psychology of Change Management,” McKinsey\n_Quarterly._\n\n\n-----\n\nKelman, S., 2005, Unleashing Change: A Study of Organizational Renewal in Government, The\nBrookings Institute.\n\n_[Mergers and Transformations: Lessons Learned from DHS and Other Federal Agencies,](http://www.gao.gov/assets/240/236371.pdf)_\nNovember 2002, GAO-03-293SP.\n\nOstroff, F., May 2006, “Change Management in Government,” Harvard Business Review.\n\n\n-----\n\nDefinition: Organizational\n\n_assessments follow a systems_\n\n_science approach to analyze_\n\n_a proposed transformation,_\n\n_determine the impacts of the_\n\n_transformation on the organi­_\n\n_zation, assess the prepared­_\n\n_ness of the organizational_\n\n_entities to adopt the transfor­_\n\n_mation, and assess the “people_\n\n_and organizational” risks asso­_\n\n_ciated with the transformation._\n\nKeywords: business intelligence,\n\n_direct and indirect stakehold­_\n\n_ers, organizational impacts,_\n\n_organizational risk, strategic_\n\n_alignment_\n\n\nTRANSFORMATION PLANNING AND\nORGANIZATIONAL CHANGE\n###### Performing Organizational Assessments\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be\n\ncognizant of the behavioral complexities of\n\ntransformation on organizations, the neces­\n\nsity to analyze the organization’s capability,\n\nto understand organizational drivers that will\n\nimpact the transformation, and how to align the\n\norganization to successfully adopt the change.\n\nMITRE uses organizational assessments to\n\nprovide sponsor executives and managers the\n\nbusiness intelligence to successfully lead the\n\ntransformation. MITRE SEs are expected to\n\ndevelop and recommend organizational strate­\n\ngies that will facilitate the successful adoption of\n\nthe change, and to monitor and evaluate spon­\n\nsor organizational change management (OCM)\n\nefforts recommending changes as warranted.\n\n\n-----\n\nConduct\nbackground\nreview, project\nplanning, and\nkickoff\n\n\nInterview\nmanagement\nteam and select\nstaff to identify\nkey issue areas\n\n\nI\n\nI\n\nI\n\nI\n\n\nI\n\nI\n\nI\n\n\nI\n\nI\n\nI\n\nI\n\nI\n\n\nI\n\nI\n\nI\n\nI\n\nI\n\n\nReview background\nmaterial, including\nother stakeholder\nassessments\n\nDevelop work plan\n\nConduct kickoff\nmeeting\n\n\nWork Plan I Preliminary I Detailed Findings I Analysis &\nKickoff Briefing Findings – Key Alternatives\nIssues I Change Priorities\n\n\nDevelop interview\nprotocols (based on\norganizational system\nmodel)\n\nConduct Phase 1\ninterviews\n\nAnalyze data\n\nIdentify key issues\n\nMeet with project\nleader to agree to\nfocus for Phase 2\nData Collection\n\n\nI\n\nI\n\n\nCollect data on\nkey issues to\nguide the\ndevelopment of\nalternatives/\nsolutions\n\nDevelop data\ncollection strategy to\nassess key issues in\nmore depth\n\nConduct Phase 2 Data\nCollection (additional\ninterviews and/or\nfocus groups)\n\n\nEngage management team\nto identify\nstrategic\nchanges\n\nAnalyze data\n\nDevelop key findings\n\nConduct offsite(s)\nand/or working\nsessions with\nmanagement/staff to\npresent and discuss\nfindings and identify\nstrategic priorities\n\nCollaboratively develop\nchange recommendations\n\n\nDevelop action\nplans to address\nchange\npriorities\n\nDevelop action plans\nto address change\npriorities\n\nConduct briefing(s)\nto review plans\n\nAction Plans\n\n\nFigure 1. MITRE’s Organizational Assessment Approach\n\n###### Background\n\nOrganizational assessments follow a system science approach to assess the dynamics at work\nin the sponsor’s organization. The approach is to collect data and analyze factors that impact\norganizational performance to identify areas of strength as well as opportunity. There are a\nnumber of excellent models for understanding and analyzing data during an organizational\nchange assessment, including the Burke-Litwin Model of Organizational Performance and\nChange shown in Figure 2. This model has a number of interdependent factors, both external\nand internal, that exist simultaneously and affect the performance of an organization. These\ninterdependent variables range from external budget pressures, to the organization’s culture\nand leadership, to the skills and behavior of the lowest level employee. The Burke-Litwin\nmodel provides a framework to effectively analyze, interpret, develop recommendations, com­\nmunicate, and manage change within an organization.\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nMITRE’s organizational assessment approach.\n\nOne organizational assessment approach that\n\nMITRE uses is shown in Figure 1. The assess­\n\nment is a repeatable process that applies social\n\nbehavioral best practices developed and proven\n\neffective in the public and private sectors. This\n\nprocess is designed to help leaders assess\n\nwhere their organization is in the change process,\n\nidentify organizational gaps, transformation risks/\n\nissues, and to determine what they need to do as\n\nthey move through the process.\n\nThe following five-step approach is suggested:\n\n\n1. Project Mobilization—During the start-up\n\nphase of the project, review background\n\nmaterial, conduct project planning, and\n\nconduct initial meetings with the client\n\nto gain insights and discuss the project\n\n­approach.\n\nDeliverables: Work plan and kickoff\n\nbriefing\n\n2. Phase 1 Data Collection (Big Picture)—\n\nThe first phase of data collection will\n\nprovide a holistic, big picture assess­\n\nment of the organization. Working with\n\nan organization’s leadership, identify\n\n\nFigure 2. Burke-Litwin Model of Organizational Performance and Change\n\n\n-----\n\nkey stakeholders to interview. Develop\n\ninterview protocols based on an organiza­\n\ntional systems model and investigate such\n\nareas as External Environment, Mission\n\nand Strategy, Leadership, Organizational\n\nCulture, Organizational Structure, Man­\n\nagement Practices/Processes, and any\n\nspecific areas of interest and need to the\n\norganization. Collect and analyze data, and\n\nidentify key issue areas.\n\nDeliverable: Preliminary findings—key\n\nissues\n\n3. Phase 2 Data Collection (Targeted)—\n\nAfter discussion and agreement with\n\norganizational leaders, conduct a second\n\nphase of data collection to gather more\n\nin-depth understanding around key issue\n\nareas to guide the development of alter­\n\nnatives and solutions.\n\nDeliverable: Detailed findings\n\n4. Analysis and Identification of Strate­\n\ngic Changes—After analyzing the more\n\ndetailed data, engage the organization’s\n\nmanagement team in a process to identify\n\nstrategic changes (through offsites and/or\n\n\nworking sessions).\n\nDeliverable: Analysis and alternatives\n\n5. Action Planning—If desired, collaborate\n\nwith an organization’s management team\n\nto develop action plans to address change\n\npriorities.\n\nDeliverable: Action plans\n\n_Note: All organizational assessments require_\n\n_sponsor participation and direction on the goals_\n\n_and objectives of the transformation prior to_\n\n_performing the analysis of workforce._\n\nBurke-Litwin Model of organizational perfor­\n\nmance and change (Figure 2). A system science\n\nmodel that describes the linkages among the\n\nkey factors that affect performance, and deter­\n\nmines how change occurs in an organization. SEs\n\nuse this model system to obtain data on what\n\norganizational factors to change and why. Higher\n\nlevel factors (blue boxes) have greater weight in\n\neffecting organizational change; a change in any\n\nvariable ultimately affects every other variable.\n\nTable 1 provides key sample questions SEs should\n\nask regarding the 12 variables, or dimensions, of\n\nthe Burke-Litwin Model.\n\n\nTable 1. Dimensions of Burke-Litwin Model\n\n|Dimensions of Model|Key Questions|\n|---|---|\n|1. External Environment|What are the key external drivers? How are these likely to impact on the organization? Does the organization recognize these?|\n|2. Mission and Strategy|What does top management see as the organization’s mission and strategy? Is there a clear vision and mission statement? What are employees’ perceptions of these?|\n\n\n-----\n\n|3. Leadership|Who provides overall direction for the organization? Who are the role models? What is the style of leadership? What are the perspectives of employees?|\n|---|---|\n|4. Organizational Culture|What are the overt and covert rules, values, customs, and principles that guide organizational behavior?|\n|5. Structure|How are functions and people arranged in specific areas and levels of responsibility? What are the key decision-making, communication, and control relationships?|\n|6. Systems|What are the organization’s policies and procedures, including sys­ tems for reward and performance appraisal, management information, human resources, and resource planning?|\n|7. Management Practices|How do managers use human and material resources to carry out the organization’s strategy? What is their style of management, and how do they relate to subordinates?|\n|8. Work Unit Climate|What are the collective impressions, expectations, and feelings of staff? What is the nature of relationship with work unit colleagues and those in other work units?|\n|9. Task and Individual Skills|What are the task requirements and individual skills/abilities/knowl­ edge needed for task effectiveness? How appropriate is the organization’s “job-person” match?|\n|10. Individual Needs and Values|What do staff members value in their work? What are the psychological factors that would enrich their jobs and increase job satisfaction?|\n|11. Motivation|Do staff feel motivated to take the action necessary to achieve the organization’s strategy? Of factors 1 through 10, which seem to be impacting motivation the most?|\n|12. Individual and Organizational Performance|What is the level of performance in terms of productivity, customer satisfaction, quality, and so on? Which factors are critical for motivation and therefore performance?|\n\n\n###### Organizational Assessment Products\n\nPrimary outputs from the organizational assessments include:\n\n**Organizational Impact Assessment (OIA). Provides information on the status of the orga­**\nnizational entities and personnel to adopt the transformation. The OIA will identify direct and\nindirect impacts on the workforce, direct and indirect stakeholders and how the transforma­\ntion will impact the accomplishment of the sponsor’s mission.\n\n\n-----\n\n**Organizational Risk Assessment (ORA). Provides sponsor executives with business intel­**\nligence on the type and severity of transformation risks and issues and potential mitigation\nsolutions. The ORA may be integrated into one overall organizational impact assessment.\n_Note: The organizational change strategy output from the OIA and ORA provide sponsor executives_\n_with the business intelligence to develop the organizational change management (OCM) direction._\n\n**[optional] Deliverable-Workforce Transformation Strategy and Plan. Explains the**\ntransformation plan ensuring integration with the sponsor’s technical and deployment teams,\nintegrates organization preparation, communication, and training activities into one transfor­\nmation plan, and explains how the transformation program management team will manage\ndaily OCM activities and briefings.\n\nSEs also need to be cognizant that a system science approach includes communications\nplanning and outreach strategies to initiate and sustain communications to affected organi­\nzational entities and key transformation participants (e.g., internal and external stakehold­\ners). Communications planning requires the development of near-term communications and\nsubsequent implementation of the plans. For more information, see the SEG article “Effective\nCommunication and Influence.”\n\n###### References and Resources\n\nBurke, W., 2008. Organizational Change: Theory and Practice, 2nd Ed., Sage Publications.\n\nBurke, W., and G. Litwin, 1992, “A Casual Model of Organizational Performance and Change,”\n_Journal of Management, Vol. 18, No. 3._\n\nFlint, D., 2005, “The User’s View of Why IT Projects Fail,” Gartner Report.\n\nKelman, S., 2005, Unleashing Change: A Study of Organizational Renewal in Government, The\nBrookings Institute.\n\nKotter, J. P., 1998, “Winning at Change,” Leader to Leader, 10 (Fall 1998), 27–33.\n\nLawson E. and C. Price, 2003, “The Psychology of Change Management,” McKinsey Quarterly.\n\n_Mergers and Transformations: Lessons Learned from DHS and Other Federal Agencies,_\nNovember 2002, GAO-03-293SP.\n\nOstroff, F., May 2006, “Change Management in Government,” Harvard Business Review.\n\n\n-----\n\nDefinition: The formulation of\n\n_an organizational transforma­_\n\n_tion strategy documents and_\n\n_institutionalizes the sponsor’s_\n\n_commitment and the strategic_\n\n_approach to the transformation._\n\n_The formulation of the trans­_\n\n_formation strategy provides the_\n\n_foundation on which the spon­_\n\n_sor’s change agents will assist_\n\n_affected organizational units_\n\n_and users to align and adapt to_\n\n_the transformation._\n\nKeywords: change manage­\n\n_ment, organizational alignment,_\n\n_organizational change_\n\n\nTRANSFORMATION PLANNING AND\nORGANIZATIONAL CHANGE\n###### Formulation of Organizational Transformation Strategies\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be\n\ncognizant of the complexities of organizational\n\ntransformation. They are expected to be able\n\nto formulate an organizational transformation\n\nstrategy that considers the human dimen­\n\nsion of a technology modernization effort.\n\n\n-----\n\n###### Background: Why Projects Fail\n\nAccording to a 2005 Gartner survey of failed information technology projects, in 31 percent of\ncases, failure was due to a deficiency in organizational change management. In addition, in 44\npercent of failed projects, organizational change problems were identified as part of the reason\nfor project failure. Essentially, the degree to which an organization’s management is able to\nmanage change, develop consensus, and sustain commitment will determine the success or\nfailure of any large enterprise modernization effort.\n\nThe formulation of an organizational transformation management strategy is a critical\ncomponent of any modernization program and involves a systematic approach that supports\nboth the organization and the individuals in it to plan for the change and then accept, imple­\nment, and benefit from the change.\n\nAs government agencies expand and improve their services, they may undergo a funda­\nmental transformation of mission, strategy, operations, and technology. If managed effectively,\nthese changes can increase the quality of government services and reduce taxpayer costs. For\nmost large government modernization programs, an organization’s predominant focus is often\non the technology. If a program’s success depended solely on installing the right hardware and\nsoftware, however, many more modernization programs would be successful. It is the people\nwho are going to use the new technologies who add an unpredictable, complex dimension.\nThe following best practices suggest approaches for the development of organizational trans­\nformation strategies.\n\n###### Best Practices and Lessons Learned\n\n\nFramework for Formulating Organizational\n\nTransformation: Burke-Litwin Model of\n\nOrganizational Performance and Change. During\n\nan enterprise modernization effort, a number\n\nof variables exist simultaneously that affect the\n\nacceptance of change within an organization.\n\nThe Burke-Litwin model (B-L) is a framework\n\nto assess the scope and complexity of these\n\nvariables within an organization. As a model of\n\norganizational change and performance, B-L pro­\n\nvides a link between an assessment of the wider\n\ninstitutional context and the nature and process\n\nof change within an organization. The B-L model\n\n\nidentifies these key factors to consider during\n\norganizational change:\n\n###### � [The external environment is the most ]\n\npowerful driver of organizational change.\n###### � [The changes that occur in the external ]\n\nenvironment lead to “transformational”\n\nfactors within an organization—mission\n\nand strategy, organizational culture, and\n\nleadership.\n###### � [The changes in transformational factors ]\n\nlead to changes in the “transactional”\n\nfactors within an organization—structure,\n\n\n-----\n\nsystems, management practices, and\n\norganizational climate.\n###### � [Together, changes in transformational and ]\n\ntransactional factors affect motivation,\n\nwhich in turn affects individual and organi­\n\nzational performance.\n\nFor an enterprise modernization effort to be\n\neffective and sustainable, changes in transforma­\n\ntional and transactional factors need to be inte­\n\ngrated and consistent. Experience and practice\n\nsuggest that the variables highlighted in the model\n\nand the relationships between them provide a\n\nuseful tool for communicating not only how orga­\n\nnizations perform, but how to effectively imple­\n\nment change. For more information, see the SEG\n\narticle “Performing Organizational Assessments.”\n\nElements of Organizational Transformation\n\nStrategy. Organizational transformation relies\n\non five key elements— leadership, communica­\n\ntions and stakeholder engagement, enterprise\n\norganizational alignment, education and training,\n\nand site-level workforce transition—that provide\n\nan overall framework for change. Each of these\n\nelements is considered a “work stream” in the\n\ntransformation strategy, which are addressed in\n\nlater sections of this article. The fifth work stream,\n\nsite-level workforce transition, incorporates the\n\nfirst four elements and applies them at the level of\n\nthe affected site or geographic region to prepare\n\nand manage users in the field through the imple­\n\nmentation effort.\n\nFigure 1 shows the assessment approach used to\n\nformulate the organizational transformation strat­\n\negy. This approach has been found to enhance\n\nthe formulation of organizational transformation\n\nstrategies. When used in concert, the elements\n\n\ncreate a powerful, mutually reinforcing field for\n\nthe support of organizational change and improve\n\nthe chances that the transformation will meet its\n\nobjectives. These elements of change leverage\n\nthe resources within the sponsor’s organization\n\nto reduce the risks and address the challenges\n\noutlined above. For more information, see the SEG\n\narticle “Performing Organizational Assessments.”\n\nMITRE SEs must understand that the develop­\n\nment of organizational transformation strategies\n\ninvolves the following assessments:\n\n###### � [Leadership:][ Assess the sponsor’s lead­]\n\nership. Mobilizing leaders is critical to\n\nspearheading a successful effort. Leaders\n\nplay a vital role throughout the life cycle in\n\npromoting the initiative, ensuring resources\n\nare available and able to support the effort,\n\nand resolving critical implementation issues\n\nas they arise. Leaders must be aware of\n\noutcomes across the organization and be\n\nable to make decisions accordingly.\n###### � [Communications and Stakeholder ]\n\nEngagement: Identify key stakeholders\n\n(those who will be impacted), determine\n\nhow best to communicate with them, and\n\nkeep them involved. Effective communica­\n\ntions allow for two-way dialogue, so issues\n\ncan be understood, and changes can be\n\nmade appropriately. Assess access to\n\nstakeholder information. Access to stake­\n\nholder information is critical to the training\n\nteam, which must determine which groups\n\nneed to be trained and how. For more\n\ninformation, see the SEG article “Effective\n\nCommunication and Influence.”\n\n\n-----\n\nFigure 1. Organizational Change Management Framework\n\n\n###### � [Knowledge Management: ][Assess directly ]\n\nand indirectly affected users to determine\n\nif they are prepared to adopt the transfor­\n\nmation. While training is delivered just prior\n\nto “going live,” education needs to occur\n\nmuch sooner. End users must understand\n\nwhat is changing and why, before they are\n\ntrained on “how.” This assessment is tightly\n\nlinked with leadership and communication\n\nassessments. For more information, see\n\nthe SEG articles “Performing Organiza­\n\ntional Assessments,” “Stakeholder Assess­\n\nment and Management,” and “Effective\n\nCommunication and Influence.”\n\n\n###### � [Enterprise Organizational Alignment: ]\n\nAssess the sponsor’s organization to\n\ndetermine how the transformation will\n\nspecifically affect the organization and any\n\nexternal stakeholders. The transforma­\n\ntion may be creating new organizational\n\nunits or user roles to be filled by current\n\nemployees. The Burke-Litwin analysis\n\nwill identify current organizational gaps.\n\nUnderstanding the gap between pres­\n\nent and future roles and responsibilities\n\nis critical to prepare the organization to\n\nsuccessfully adopt the change. For more\n\ninformation, see the SEG article “Perform­\n\ning Organizational Assessments.”\n\n\n-----\n\n###### � [Site-Level Workforce Transition:][ The ]\n\nrelationship between headquarters and\n\nfield offices adds complexity to the orga­\n\nnizational assessment. Systems engineers\n\nmust be cognizant of the need to assess\n\nfield offices as part of the overall orga­\n\nnizational assessment. The success of\n\norganizational changes to each site will\n\ndepend on the degree of involvement by\n\nits local team. Each site likely has its own\n\nprocesses, issues, constraints, and num­\n\n###### References and Resources\n\n\nbers of people affected. Therefore, they\n\nmust each be accountable for developing\n\na transition plan that is tailored to meet\n\ntheir needs. For more information, see the\n\nSEG articles “Stakeholder Assessment and\n\nManagement” and “Performing Organiza­\n\ntional Assessments.”\n\nThese work stream assessments create a com­\n\nprehensive blueprint for the formulation of an\n\norganizational transformation strategy to increase\n\nthe likelihood of transformation success.\n\n\nBaba, M., March 6, 2005, The Defense Logistics Enterprise: Transforming Organizations in the\n_Information Era, Prepared for Enterprise Integration Group._\n\nBurke, W., and Litwin, G., 1992, “A Causal of Organizational Performance and Change,\n_Journal of Management, 18(3), pp. 523–545._\n\nGartner, 2005, “The User’s View of Why IT Projects Fail,” Research Notes, 2, 4.\n\n\n-----\n\nDefinition: Stakeholder man­\n\n_agement is the process of_\n\n_identifying stakeholder groups,_\n\n_the interests they represent,_\n\n_the amount of power they_\n\n_possess, and determining if_\n\n_they represent inhibiting or_\n\n_supporting factors toward the_\n\n_transformation. The objective of_\n\n_stakeholder planning and man­_\n\n_agement is determining who_\n\n_the stakeholders are and how_\n\n_they should be dealt with [1]._\n\nKeywords: communications,\n\n_interfaces, monitoring, power,_\n\n_risk development, stakeholders_\n\n\nTRANSFORMATION PLANNING AND\nORGANIZATIONAL CHANGE\n###### Stakeholder Assessment and Management\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) must understand the impor­\n\ntance of identifying both directly and indirectly\n\nimpacted stakeholders in transformation planning\n\nand organizational change. Systems engineers\n\nmust assess the impact of transformation on\n\npeople and the organization to identify all stake­\n\nholders, identify transformation risks and issues,\n\nrank the risks associated with the transformation,\n\nand recommend mitigation strategies to sponsor\n\nexecutives. MITRE SEs should work closely with\n\nthe sponsor’s communications team to promote\n\ntransformation awareness, understanding, and\n\nacceptance across key stakeholder groups.\n\nGaining the support of key stakeholders is criti­\n\ncal to creating successful organizational change\n\nefforts [2].\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nBuild trust with stakeholders. The Organizational\n\nChange Management Practice (OCM) in MITRE’s\n\nCenter for Connected Government (CCG) has\n\nconducted several stakeholder analyses on behalf\n\nof our sponsors. Lessons learned show that the\n\nfollowing characteristics are common among\n\neffective stakeholder relationships:\n\n###### � [A deep level of trust with the change ]\n\nsponsor and the stakeholder groups\n\naffected by the change initiative\n###### � [Effective communication with the stake­]\n\nholders allowing them to gain a new\n\nunderstanding of the benefits and costs of\n\nthe change\n###### � [Close change sponsor and change agents’ ]\n\nrelationships that allow them to become\n\npersonally engaged in and committed\n\nto initiatives based on the findings and\n\nrecommendations.\n\nIdentify stakeholders. MITRE SEs must ensure\n\nthat representatives from all key stakehold­\n\ners are included in the organizational impact\n\nassessments and that the assessment informa­\n\ntion collected is representative of the affected\n\npopulation. It is important to take into account\n\ngeographic distribution of stakeholder groups to\n\nobtain the full range of perspectives.\n\nDuring transformation planning, identify all key\n\nstakeholder groups, including:\n\n###### � [Decision makers involved in the decisions ]\n\nregarding the change\n###### � [Change sponsors and agents responsible ]\n\nfor executing the change\n\n\n###### � [Employees [and contractors] directly ]\n\nimpacted by the change\n###### � [“Customers” of the change agents ]\n\naffected by the change\n###### � [Representatives of all groups in headquar­]\n\nters and in field offices across the country\n\n(as appropriate).\n\nStarting from a list of individuals and organizations\n\nprovided by the sponsor, the stakeholder assess­\n\nment team should ask interviewees and oth­\n\ners within the sponsor’s organization to provide\n\nsuggestions for additional names and organiza­\n\ntions to be included. The stakeholder assessment\n\nshould seek out and integrate input from sup­\n\nporters, skeptics, and rivals of the transformation\n\ninitiative.\n\nCollect and analyze data. MITRE SEs must build\n\nrelationships with the stakeholders throughout\n\nthe transformation process [3]. They should\n\nemploy a combination of one-on-one interviews,\n\nfocus groups, and surveys to rapidly establish rap­\n\nport and create an environment that contributes\n\nto the stakeholders being open and honest while\n\ndescribing challenging situations.\n\nRather than just fire off one question after the\n\nnext, it is important to engage stakeholders in\n\ndialogue and exhibit interest in their opinions and\n\nperspectives. Ask follow-up questions to solicit\n\nspecific examples and understand how stake­\n\nholders developed their opinions and percep­\n\ntions. The interview protocol should include\n\nopen-ended and Likert-scaled questions. Likert\n\nscales are a type of survey response format\n\nwhere survey respondents are asked to indicate\n\n\n-----\n\ntheir level of agreement/interest on a continuum\n\n(e.g., from strongly agree to strongly disagree or a\n\nnumerical scale). This method provides a way to\n\nassign a quantitative value to qualitative informa­\n\ntion. Although there is a certain amount of vari­\n\nance inherent in Likert responses, these questions\n\nhelp bring a quantitative measure to enhancing\n\nunderstanding of stakeholders. In addition to ask­\n\ning probing questions on a variety of topics, solicit\n\nsuggestions for addressing concerns.\n\nMaintain detailed notes and analyze the\n\ninformation for key themes. Analyze the data to\n\ndevelop stakeholder maps (Figure 1) to graphically\n\ndisplay the relative influence and support that\n\nstakeholder groups have for the transformation.\n\nOverlay the quantitative (Likert data) to identify\n\nsimilarities and differences across the stakeholder\n\nnetworks.\n\n\nPresent findings. Best practices for developing\n\nand presenting stakeholder findings include:\n\n###### � [Include history and context up front: This ]\n\napproach establishes a common under­\n\nstanding of the sponsor’s challenging\n\nenvironment.\n###### � [Provide “good news”: Sharing success ]\n\nstories acknowledges what is going well\n\nand contributes to a balanced message.\n\nChange sponsors are not trying to make\n\nlife difficult for their stakeholders; they are\n\nfocused on achieving specific business\n\nobjectives that may have been initiated by\n\nCongress or other external factors.\n###### � [Present key themes and ground findings ]\n\nwith specific examples: Identify overarch­\n\ning themes based on data analysis and\n\n\nTo be successful, stakeholders must be moved from         to X\n\nLow Neutral High\nStakeholder Group Awareness Understanding Buy-in Commitment\n\n\n###### X X\n\nAffected employees **X**\n\nLevel of current support (represents range within stakeholder group) Level of support required\n\n###### X\n\nFigure 1. Stakeholder Findings\n\n\n###### X\n\n\n-----\n\ninclude supporting evidence (including\n\n\nLikert data), examples, and quotes [4, 5].\n###### � [Highlight differences across stakeholder ]\n\ngroups (see Figure 1): Avoid making gen­\n\neralizations and note when opinions are\n\n\n###### � [Share recommendations and/or next ]\n\nsteps: Findings are only useful in the\n\n\ncontext of how they can address issues\n\nand concerns. Sometimes, sponsors need\n\n\nconsistent or divergent across stakeholder\n\n\nto ponder the findings before engaging in\n\nrecommendations. In that case, identifying\n\n\ngroups.\n###### � [Provide general information about the ]\n\nchange process: Sponsors are more open\n\n\nnext steps is helpful.\n###### � [Present the findings by telling stories using ]\n\nthe key themes and supporting data: This\n\n\nto receiving challenging feedback when\n\nthey understand that their situation is not\n\nunique. Be careful not to overwhelm spon­\n\n\napproach will enable the data to come\n\nalive for different stakeholder groups.\n\nIdentify a few common themes when\n\n\nsors with too much theory.\n\nSenior mgt.\neast\n\nHigh\n\n\nreporting findings to sponsors.\n\nActions must be\n\nNetwork taken to move\nCIOs stakeholders with\n\n_High influence to_\nthe right.\n\nRegional\ndirectors\n\n_High influencers_\n# }\n_must be com-_\n_mitted advocates._\n\n\nMed.\n\nLow\n\n\nLow\n(Awareness)\n\n\nSample size varies\n\nHigh\n(Commitment)\n\n\nLevel of support\n\nFigure 2. Current vs. Required Level of Support\n\n\n-----\n\n_Note: Before the stakeholder assessment, the_\n\n_executive change sponsors may perceive the_\n\n_employees as resistant. As a result of the stake­_\n\n_holder assessment findings, they may realize_\n\n_that the perceived resistance was really a lack_\n\n_of understanding about the rationale for change_\n\n_and how to operationalize it._\n\n###### Summary\n\n\nWhen MITRE SEs are asked to present the\n\nresults from the stakeholder analysis to stake­\n\nholder groups, MITRE’s approach should help\n\nbuild credibility. Stakeholders appreciate having\n\ntheir perspectives accurately presented to senior\n\nmanagement and the value transparency that\n\nresults from seeing the findings gathered across\n\nthe stakeholder groups.\n\n\nStakeholders are a critical asset that may have a significant impact on transformation initia­\ntives. Stakeholder analysis is an effective method for enabling different stakeholder groups to\nunderstand each other’s perspectives and concerns. Establishing trust and credibility through­\nout the process—from planning and gathering to analyzing and presenting data—is critical to\nensuring that the findings are valued and acted on.\n\n###### References and Resources\n\n1. Gardner, J., R. Rachlin, and H. Sweeny, 1986, Handbook of Strategic Planning, John Wiley.\n\n2. Ostroff, F., 2006, “Change Management in Government,” Harvard Business Review, 84(5),\n\n141–147.\n\n3. Maister, D., C. Green, and R. Galford, 2000, The Trusted Advisor, Simon & Schuster, New\n\nYork, NY.\n\n4. Kassinis, G., and N. Vafeas, 2006, “Stakeholder Pressures and Environmental\n\nPerformance,” Academy of Management Journal, 49(1), 145–159.\n\n5. Mitchell, R., B. Agle, and D. Wood, 1997, “Toward a theory of stakeholder identifica­\n\ntion and salience: Defining the principle of who and what really counts,” Academy of\n_Management Review, 22, 853–886._\n\n\n-----\n\nDefinition: Communication is a\n\n_two-way process in which there_\n\n_is an exchange of thoughts,_\n\n_opinions, or information by_\n\n_speech, writing, or symbols_\n\n_toward a mutually accepted_\n\n_goal or outcome [1]._\n\nKeywords: behavior, behavior\n\n_change, communication, elabo­_\n\n_ration, elaboration likelihood_\n\n_model (ELM), influence, mes­_\n\n_sage, persuasion, processing,_\n\n_social_\n\n\nTRANSFORMATION PLANNING AND\nORGANIZATIONAL CHANGE\n###### Effective Communication and Influence\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) need to understand\n\nthat communication is vital for transforma­\n\ntion success. They are expected to assist in\n\ndeveloping communication strategies as part of\n\norganizational transformations. SEs are expected\n\nto be able to communicate and interpret the\n\n“big picture” across multiple disciplines, teams,\n\nand environments. They translate the visions of\n\nleaders into the engineering work performed by\n\ntechnical staff, and communicate information\n\nbroadly and accurately to achieve project suc­\n\ncess. In addition to being able to express ideas in\n\na clear and compelling manner, they must adjust\n\ntheir language and communication vehicles to\n\ncapture the attention of diverse audiences. Within\n\ntheir project teams, MITRE SEs are expected to\n\n\n-----\n\nhelp establish an environment of trust by communicating openly and behaving consistently\nin words and actions. In addition, they are expected to communicate effectively to persuade\nor influence others outside their formal authority to accept a point of view, adopt a specific\nagenda, or take a course of action that is in the best interests of the sponsor and the wider\nstakeholder community. They are expected to be proficient in analyzing audiences, organiz­\ning ideas effectively, choosing appropriate media, and knowing how to promote ideas to a\nwide range of audiences. Proper use of a systems science approach to communication will\nhelp build good relationships with team members, sponsors, and other key stakeholders, to\nincrease the likelihood of project success.\n\n###### Elements of Communication\n\nIn supporting program management, a key “purpose of effective communication is sustain­\n_ing the on-going work with maximum efficiency [1].” This goes beyond just sending progress_\nreports and providing periodic briefings. It also includes communicating high-quality stan­\ndards for the project team, clearly communicating the financial impact of the work, and\ndefining the outcome and benefits that stakeholders can expect [1, p. 1]. It involves facilitating\nbroad participation in the decision-making process, thereby increasing support and commit­\nment to the project. SEs who carry out these activities effectively may better accomplish the\nproject objectives and lead to a more collaborative relationship with the customer [1, p. 2].\n\n###### Strategic Communication Planning in Government Agencies\n\nAs government agencies expand and improve their services, they often undergo a fundamen­\ntal transformation of mission, strategy, operations, and technology. If managed effectively,\nthese changes can increase the quality of government services and reduce taxpayer costs.\nOften there is strong resistance to change within an organization because it requires that\npeople change not only the way they work, but their attitudes and beliefs. This is challenging,\nbut such transformation is essential if government agencies are to provide the high quality of\nservice expected by citizens and mandated by legislators.\n\nFor most large government modernization programs, an organization’s predominant\nfocus is often on the enabling technology—the definition, acquisition, and implementation of\ninformation technology systems. If a program’s success depended solely on installing the right\nhardware and software, however, many more modernization programs would be successful.\nIt is the people who are going to use the new technologies who add an unpredictable, complex\ndimension.\n\n\n-----\n\nThe Role of Communication in Transformation Projects\n\nPeople don’t like change—they fear the unknown, fear losing control, worry that their jobs\nmay change or go away. Affected individuals may oppose a transformation program by refus­\ning to implement the necessary changes in their daily operations or by speaking against it and\ninfluencing others to oppose it. If the benefits of the transformation are not communicated\nbroadly and consistently, departments and business units may refuse to support it. They lose\nsight of the overall mission of the agency, compete with each other for funds and resources,\nand refuse to see the value of collaborating with other units.\n\nOpen and frequent communication is an essential factor in successful transformation.\nGive people the information they need about the benefits and impact of the transformation,\nand they will more readily accept and support the effort. Leaders of transformation programs\nneed a strategy that incorporates the communication needs of key stakeholders, the resources\nand channels required to reach these audiences, and the processes that support an under­\nstanding of the goals and benefits of the transformation program.\n\n###### Best Practices and Lessons Learned\n\n\nFigure 1 shows an approach to developing effec­\n\ntive communications and influencing sponsor\n\ninteractions. It incorporates industry best prac­\n\ntices as well as MITRE lessons learned and proven\n\nmethodologies in supporting government agen­\n\ncies and private sector organizations. It depicts\n\na four-step systems approach to developing a\n\ncommunications strategy, building an action plan,\n\nexecuting the action plan, measuring feedback\n\nto assess the effectiveness of communication\n\nactivities, and integrating feedback into revisions\n\nof the communication activities to improve their\n\neffectiveness.\n\nDeveloping the communication strategy. The\n\nrequirement for a communication strategy may be\n\ntriggered by a variety of events: a new administra­\n\ntion or agency leadership, a significant change\n\nin an agency’s mission, or a legislative mandate\n\nthat requires significant reorganization or mod­\n\nernization of an agency’s operations or systems.\n\n\nIn each case, the failure to consider the “human\n\ndimension” in a transformation leads to a higher\n\npercentage of failure.\n\nThe application of a systems science approach\n\nis effective in helping agencies understand\n\nthe human dimensions of their transformation\n\nand in understanding how to effectively inte­\n\ngrate communications into the transformation\n\nprogram. This approach requires that systems\n\nengineers listen carefully to the sponsor’s needs\n\nand concerns, and then collaborate with them to\n\ndevelop and validate an effective communication\n\nstrategy by:\n\n###### � [Assessing and analyzing both the commu­]\n\nnication needs of the agency’s key audi­\n\nences (internal and external stakeholders)\n\nand their concerns regarding the proposed\n\ntransformation effort.\n\n\n-----\n\n###### � [Developing clarity about the goals and ]\n\nobjectives that the communication effort\n\nis intended to accomplish.\n###### � [Establishing governance, with clear roles ]\n\nand responsibilities for those involved in\n\nthe communication effort.\n###### � [Conducting an audit to determine exist­]\n\ning internal and external communication\n\nresources and channels, and identify\n\nopportunities for new resources and\n\nchannels.\n###### � [Identifying a measurement process and ]\n\nfeedback mechanisms to ensure that the\n\nstrategy is achieving its goals.\n\nDeveloping the communication plan. After the\n\ncommunication strategy has been validated and\n\naccepted, the next step is to develop and imple­\n\nment a communication action plan to deliver key\n\nmessages to each audience—in the language\n\nand through the channels that are most effective\n\nfor each group. Development of the plan could\n\ninclude the following steps:\n\n1. Determine the activities needed.\n\n2. Develop key messages targeted to specific\n\naudiences and establish the process\n\nthrough which these messages will be\n\nreviewed and approved; this concurrence\n\nprocess may vary according to the audi­\n\nence (e.g., internal vs. external, legislative\n\nvs. media, etc.).\n\n3. Identify the resources and/or commu­\n\nnication channels that may be required\n\nand that would be most effective for\n\neach audience (e.g., electronic vs. print vs.\n\nin-person meetings; mailings and phone\n\n\ncalls; website and social media such as\n\nFacebook).\n\n4. Establish a detailed timeline for delivery\n\nof the messages, with sequenced deliv­\n\nery and multiple impressions for greatest\n\neffect.\n\nExecuting communication activities. The execu­\n\ntion of communication activities begins when\n\ntriggering events occur (i.e., program milestones\n\nthat will affect users/stakeholders, system\n\ndeployment, or significant unplanned events). The\n\nexecution of communication activities should\n\ninclude the following steps:\n\n###### � [Review planned communication activities ]\n\nwith the sponsor and revise the plan to\n\naddress the specific triggering event.\n###### � [Obtain the needed media/channel ]\n\nresources.\n###### � [Draft the material. ] � [Perform internal edits and reviews. ] � [Execute concurrence process. ] � [Assign delivery dates. ] � [Execute communication activity. ]\n\nMeasuring effectiveness. Measuring the effec­\n\ntiveness of communication activities is a critical\n\ncomponent in the development of an effective\n\ncommunication strategy. The measurement pro­\n\ncess should include the following steps:\n\n1. Define measures of effectiveness and\n\na measurement process to assess the\n\nresults of various messages, channels, and\n\ndelivery schedules.\n\n\n-----\n\n|/|Col2|\n|---|---|\n|ss ess/ re||\n|Asse succ failu||\n\n|? Yes|Col2|\n|---|---|\n\n\n-----\n\n2. Review measures of effectiveness and\n\nclarify, if needed.\n\n3. Implement measurement process.\n\n4. Collect and analyze feedback data.\n\n5. Assess success of communication activity.\n\n6. Identify need for additional communica­\n\ntion activities or improvements.\n\n7. Incorporate feedback from measurements\n\nto continuously improve the communica­\n\ntion process and activities throughout the\n\ntransformation.\n\n_Most management failures result from a failure_\n\n_to communicate somewhere along the line._\n\n_Recognition of this need to communicate ought_\n\n_to be written into the job specifications of every_\n\n_chief executive and senior manager._\n\n_– Jacques Maisonrouge, former chairman,_\n\n_IBM World Trade Corporation_\n\nThe exact form of communication needed dur­\n\ning a transformation project is driven by a vari­\n\nety of factors: the sponsor’s culture, the nature\n\nof the transformation, the communication\n\nchannels available, and the time and resources\n\navailable for communication activities. Consider\n\nthe following key discussion points for develop­\n\ning sponsor buy-in for communication planning:\n\nExplain the importance. A failed program is a\n\nwaste of valuable funds, time, and reputation.\n\n###### � [Three out of four large IT programs fail to ]\n\nachieve their objectives.\n\nyy Poor communication is a primary factor\n\nin one-third of failed programs [PwC\n_Mori Survey 1997]._\n\n\nyy Good communication is a critical factor\n\nin 70 percent of successful programs.\n###### � [Communication planning reduces risk of ]\n\nproject failure.\n\nyy Accurate, truthful, and timely informa­\n\ntion replaces gossip and rumor and\neases anxiety.\n\nyy Key leaders will become champions if\n\nthey understand fully the impact and\nbenefits.\n\nyy Employees who trust the communica­\n\ntion process are more secure, focused,\nand productive, providing better service\nto constituents.\n\nExplain the value. People are afraid of the\n\nunknown, but they’ll support a project if they can\n\nsee its value.\n\n###### � [Resistance to change is normal. In govern­]\n\nment agencies, resistance is caused in\n\npart by:\n\nyy The graying workforce—nearly half of\n\ngovernment employees are approach­\ning retirement in the next five years.\nSome may lack the time to learn new\nprocesses or skills that a major longterm change may require.\n\nyy Change fatigue—employees are\n\nexhausted by multiple (and often con­\nflicting) initiatives launched by shortterm appointees.\n\nyy “Wait them out”—change is often\n\nimposed from above with little input\nfrom actual users, who may delay the\nchange by simply waiting until the lead­\nership changes.\n###### � [Focus on addressing “What’s in it for me?”]\n\nyy Identify key stakeholders and their par­\n\nticular concerns and needs.\n\n\n-----\n\nyy Determine the specific benefits (and\n\npain points) of the project for each\nstakeholder group.\n\nyy Communicate early, often, and clearly.\n\nTell stakeholders what is going on, tell\n\n###### References and Resources\n\n\nthem why, tell them what they need to\ndo, and specify the benefits for them.\n\nyy Set up feedback mechanisms and\n\nsolicit stakeholder input to continuously\nreview and improve the project.\n\n\n1. Amin, A., November 2008, The Communication Key of Program Management,\n\n_PMWorldToday, X (XI)._\n\n###### Additional References and Resources\n\nCampbell, G. M., 2009, Communications Skills for Project Managers, AMACOM.\n\nCialdini, R., 2008, Influence: Science and Practice, 5th Ed., Boston, MA: Allyn & Bacon.\n\nDow, W., and B. Taylor, B., 2008, Project Management Communications Bible, Wiley\nPublishing.\n\nHarvard Business School Press, 2003, Business Communication, Harvard Business School\nPublishing.\n\nHirsch, H. L., Essential Communication Strategies for Scientists, Engineers, and Technology\n_Professionals, 2nd Ed., John Wiley & Sons._\n\nKendrick, T., 2006, Results Without Authority: Controlling a Project When the Team Doesn’t\n_Report to You—A Project Manager’s Guide. American Management Association, AMACOM._\n\nKerzner, H., 2009, Project Management Case Studies, John Wiley & Sons, Inc.\n\nKliem, R. L., 2008, Effective Communications for Project Management, Auerbach Publications.\n\nPerloff, R., 2008, The Dynamics of Persuasion: Communication and Attitudes in the 21st\n_Century, 2nd Ed., Mahwah, NJ: Lawrence Erlbaum Associates._\n\nPhillips, J. J., and W. F. Tush, W. F., 2008, Communication and Implementation: Sustaining\n_the Practice, Pfeiffer._\n\nRuben, B. D., and L. P. Steward, May 2005, Communication and Human Behavior, 5th Ed.,\nBoston, MA: Allyn & Bacon.\n\n\n-----\n\nDefinition: Deriving from usabil­\n\n_ity engineering and organiza­_\n\n_tional change management,_\n\n_capability and technology tran­_\n\n_sition strategies aim to increase_\n\n_the likelihood that users will_\n\n_adopt an application._\n\n_Usability measures how intui­_\n\n_tive, efficient, and task-enabling_\n\n_users think an application is._\n\n_Usability engineering refers to_\n\n_structured methods applied_\n\n_to achieve usability in user_\n\n_interface design during the_\n\n_entire application development_\n\n_life cycle._\n\n_Organizational change man­_\n\n_agement is the art and science_\n\n_of moving an organization from_\n\n_its current state to a desired_\n\n_future state._\n\nKeywords: organizational\n\n_change, transition strategies,_\n\n_usability engineering, user_\n\n_adoption_\n\n\nTRANSFORMATION PLANNING AND\nORGANIZATIONAL CHANGE\n###### Planning for Successful User Adoption\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\ndevelop and recommend strategies to eliminate,\n\nreduce, and manage end-user reluctance to\n\nuse newly deployed capabilities or technology\n\nor outright reject them. They are expected to\n\ndefine requirements for capability and tech­\n\nnology transitions in requests for proposals\n\n(RFPs) and related acquisition documents and\n\nassess the quality of bidder responses as a\n\nkey element of the source selection process.\n\nMITRE SEs are also expected to monitor and\n\nevaluate contractor capability and technology\n\nadoption efforts and the acquisition program’s\n\noverall processes and recommend changes\n\nwhen warranted. Throughout the application\n\ndevelopment life cycle, MITRE SEs serving in\n\n\n-----\n\nproject management, development, and oversight roles should emphasize the importance of\ntransition plans in achieving successful end-user adoption.\n\n###### Background\n\nEveryone agrees that users and developers need to work together to build systems. The ques­\ntion is “how? [1]” In developing mission-critical applications that involve significant invest­\nment of time and resources, how can we ensure that the application will actually be used\nonce deployed? Two primary factors impede user adoption of mission applications: (1) the lack\nof acquisition program and user mission management support, and (2) the failure to develop\na tool as “seen through the user’s eyes [2].” While there is ample evidence that some applica­\ntions developed and deployed by the government are never used or require massive redesign\nafter roll-out, there is a dearth of baseline metrics for understanding the extent of the problem\nor developing “best practice” solutions. Thus, we still rely on intuition and anecdotal evidence\nto guide our understanding of the problems and solutions to improve outcomes.\n\nToday’s standard practice for achieving user adoption of a new application is to follow the\nengineering development life cycle, define requirements, and establish user groups as sound­\ning boards. But even with enthusiastic early adopters providing inputs, the number of times\nthat users reject sponsor applications at the point of deployment does not inspire confidence\nin this approach. Many of these applications require years of subsequent releases to get them\nright, while others simply die a slow death as the funding runs out, having been deployed but\nnever widely adopted.\n\nThis article describes strategies for stimulating user adoption of mission applications,\nincluding usability engineering to align the application with user tasks and organizational\nchange management strategies to ensure the organizational readiness of the stakeholders to\npromote and accept change.\n\n###### Strategies for Stimulating User Adoption\n\nUsability Engineering\n\nUsability measures how intuitive, efficient, and task-enabling an application is. Usability\nengineering refers to the structured methods applied to achieve usability in user interface\ndesign and process enablement. MITRE experience suggests that relying exclusively on user\ngroups to derive requirements and engage users in an application development effort is not\na promising path. User groups are necessary but insufficient for engaging users. They attract\nearly adopters and technical enthusiasts, and while they provide valuable input, their limited\nrepresentation quickly compromises the engagement process.\n\n\n-----\n\nUsability engineering can fill the representation gap. It provides formal methods for\nidentifying the different types of users, modeling the tasks they perform, and deriving usage\nconventions for the new application from those used in the existing software environment [3].\nIt employs structured methods such as field observations, task assessments, heuristic evalu­\nations, and cognitive walkthroughs to obtain actionable feedback from representative users.\nUsability engineering provides a method for determining which users to engage and the kind\nof information needed to create usable designs, the optimum time to gather different types\nof user inputs in the application development life cycle, the most promising approaches to\nsoliciting user inputs, and how to resolve disagreements among users. Further, it provides an\nobjective means of measuring the progress toward establishing the usability of an application\nby developing and periodically assessing quantifiable usability requirements. Ideally, there­\nfore, usability engineering begins early in the acquisition process and operates throughout the\nlife cycle of application development.\n\nOrganizational Change Management\n\nOrganizational change management is a discipline that moves an organization from its cur­\nrent state to a desired future state. It derives its methodologies from the social and behavioral\nsciences, particularly organizational psychology, communication theory, and leadership devel­\nopment. The deployment of enterprise mission applications typically involves some type of\nchange, such as the imposition of new workflows, business processes, quality standards, and/\nor metrics for measuring effectiveness. Addressing the organizational dimensions of deploying\nnew technology is critical to establishing manager buy-in and engendering user adoption. For\nmore information, see the other articles in the Transformation Planning and Organizational\nChange topic.\n\nSome acquisition stakeholders interpret application transition narrowly as hardware and\nsoftware transition. The focus of transition should be extended to include the broader organi­\nzational issues relevant to user adoption. This includes: (1) leadership engagement to involve\nbusiness managers in promoting new technology and ensuring organizational readiness, (2)\nrelease strategies designed to optimize end user adoption, (3) communication strategies that\nprepare stakeholders for the coming change, and (4) training that equips end users to perform\ntasks with the new technology. The fifth element is usability engineering, as described above,\nwhich provides a quantitative means of ensuring the usability of an application as it evolves.\nIn combination, these strategies address the factors that affect organization and site readiness\n\n[4] to embrace change in both processes and technology.\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nBuild the right multi-disciplinary systems\n\nengineering team. Recognize that MITRE’s\n\ncontribution to acquisition, oversight, or develop­\n\nment activities may call for usability engineers and\n\norganizational change specialists to be embed­\n\nded in the acquisition and or systems engineering\n\nteam. This is a form of the wisdom that says, “build\n\nteams with the skill sets needed to achieve a suc­\n\ncessful outcome.”\n\nInclude a transition strategy in the RFP. In addi­\n\ntion to detailing requirements, the RFP should\n\ninclude a transition strategy that delineates the\n\nrole of government, contractors, and if applicable,\n\nfederally funded research and development cen­\n\nters (FFRDCs), in stimulating user adoption of a\n\nnew application. This will assure that the strategy\n\nhas been discussed and agreed to by users and\n\nother stakeholders before issuing the RFP. Bidders\n\nshould be asked to detail the methods and per­\n\nsonnel they will employ in executing this strategy.\n\nThe contractor strategy should be a major source\n\nselection criterion.\n\nConvene a transition team on day one. Consider\n\nincluding the following team members: (1) con­\n\ntractor transition lead, (2) government transition\n\nlead, (3) mission-side transition lead, (4) usability\n\nengineering lead on development side, (5) inde­\n\npendent assessor usability lead, (6) organizational\n\nchange management lead, and (7) training lead.\n\nConsideration should be given to establishing a\n\nrole for an independent usability assessor, in an\n\norganizational change capacity, or as the lead or\n\nco-lead of the transition team (in concert with\n\nor in lieu of a government team member). The\n\n\nteam should monitor risk, measure progress, and\n\nsteer the program toward developing an applica­\n\ntion that is quantifiably verified as usable. The\n\ntransition team and program manager should\n\ndevelop and agree on a process for verifying\n\ntransition-readiness.\n\nIdentify and prioritize user types. Define the dif­\n\nferent user types and rank their needs. If conten­\n\ntion surfaces among requirements, schedule, and\n\nbudget, this will ensure that necessary trade-offs\n\nare informed by mission priorities.\n\nContinue usability assessment after an applica­\n\ntion is deployed. Continue to survey the rate of\n\nuser adoption, assess ease of use, determine the\n\neffectiveness of training, etc. Usability engineering\n\nstarts at program inception and continues after\n\ndeployment to identify operational or field-spe­\n\ncific problems [5].\n\nRecognize “red flags” and address them early.\n\nDevelop early indicators to alert you that the orga­\n\nnization’s change approach is likely to founder.\n\nThese indicators may appear at the very beginning\n\nof an effort and continue through deployment.\n\nRecognizing the red flags and addressing them\n\nearly can get you back on track. Here are some\n\nexamples:\n\n###### � [IT expects to lead transformational ]\n\nchange for the mission: IT can partner, IT\n\ncan support, but IT cannot lead an orga­\n\nnizational change initiative for the mission\n\nside. Without mission leadership, the effort\n\nto introduce transformational technology\n\nwill fail.\n\n\n-----\n\n###### � [Transition team is buried or marginal­]\n\nized: We have all seen it. Program man­\n\nagement pays lip service to user adoption\n\nwhen necessary to get through control\n\ngates or milestones. If management and\n\nthe mission are not aligned and commit­\n\nted to supporting transition strategies, the\n\nultimate outlook for application adoption\n\nis grim.\n###### � [Expectation that transition will be han­]\n\ndled by the developers: Developers play\n\na key role in building user-centric applica­\n\ntions, but expertise in usability engineering\n\nand organizational change is fundamental\n\nto a successful process.\n###### � [Increasingly greater reliance on train­]\n\ning as the “cure-all”: A growing need\n\nfor training is often an early indicator of\n\na decline in usability and design quality.\n\nAvoid the temptation to rely on “training”\n\nas a workaround in lieu of building usable,\n\nintuitive systems.\n\nConsider convening an enterprise-wide devel­\n\noper group. Involve developers throughout the\n\napplication life cycle. In one program for a single\n\nsponsor, a monthly meeting was introduced where\n\ndevelopers come together to harmonize interface\n\n###### Bottom Line\n\n\ndesigns and usage conventions across major\n\nenterprise applications under development. This is\n\nan initial effort in the early stages of implementa­\n\ntion, but the expectation is that it will lead to more\n\nusable systems. While the early results appear\n\npromising, the ultimate impact of this strategy is\n\nstill being assessed.\n\nExpect resistance. Do not assume that proj­\n\nect managers will be rewarded for advocating\n\nusability. Many organizations still reward project\n\nmanagers on the basis of their adherence to\n\nschedule and budget, regardless of adoption\n\noutcomes. In fact, the norm in many organizations\n\nis initial user rejection of applications, followed by\n\nyears of subsequent releases to “get it usable.”\n\nThese dysfunctional practices are difficult to turn\n\naround, particularly when they are entrenched in\n\norganizational culture and reward systems.\n\nExpect formidable challenges when asked to\n\nintroduce transition planning at the end of the\n\ndevelopment life cycle. You may be called in after\n\nthe fact when initial application transition has\n\nfailed. With enough time, money, and expertise, it\n\nmay be possible to determine what went wrong\n\nand how to correct it, but turning around a failed\n\ndeployment is a daunting task.\n\n\nTo stimulate user adoption of transformational technology, the systems engineering effort\nrequires an interdisciplinary team that includes embedded experts in organizational change\nand usability engineering who can leverage relevant social and behavioral methods.\n\nAdoption of new technologies requires a transition team that can develop an overarching\nplan, manage the activities as a coherent whole, and measure progress toward the develop­\nment of a usable system. The team should begin these activities at program inception and\ncontinue throughout the acquisition phase and after deployment.\n\n\n-----\n\n###### References and Resources\n\n1. For a fundamental introduction to technology adoption, see Rogers, E.M., 1995, Diffusion\n\n_of Innovations, 4th Ed., New York: The Free Press._\n\n2. Adelman, L., P. Lehner, and A. Michelson, September 27, 2009, Why Professionals Are\n\n_Reluctant to Use Judgment and Decision Aids: Review of the Literature and Implications_\n_for the Development Process, The MITRE Corporation._\n\n3. MITRE has significant experience pairing developers with analysts in “small cells” to\n\nelicit requirements and build tools in real time for a discrete set of users. Organizations\ndeveloping technology for a small group of users should consider adopting the “small cell”\napproach. See Games, R., and L. Costa, April 2002, Better Intelligence Analysis Through\n_Information Technology—Lessons Learned from the Analysis Cell Initiative._\n\n4. For more on usability engineering, see Bradley R., T. Sienknecht, M. Kerchner, and\n\nA. Michelson, October 2005, Incorporating Usability Engineering into Application\nDevelopment, draft briefing; May 2007, Incorporating Usability Engineering into\nApplication Development, draft briefing; and Bradley, R., E. Darling, J. Doughty, and\nT. Sienknecht, September 4, 2007, Findings from the Agile Development and Usability\nEngineering TEM, MITRE briefing.\n\n5. For an excellent example of post-deployment usability engineering, see Kerchner, M.,\n\nJune 2006, “A Dynamic Methodology for Improving the Search Experience,” Information\n_Technology and Libraries, Vol. 25, No. 2, p. 78–87._\n\n\n-----\n\n##### Enterprise Governance\n\nDefinition: MITRE systems engineers are expected to develop a broad under­\n\n_standing of the policy making, capability management, planning, and other_\n\n_business functions of their sponsor or customer enterprise that either influence_\n\n_or are influenced by systems engineering. They are expected to recommend and_\n\n_apply systems engineering approaches that support these business enterprise_\n\n_functions._\n\nKeywords: COI, customer focus, governance, outcomes, policy, standards,\n\n_strategy_\n\n###### MITRE SE Roles and Expectations\n\nTaken together, the different levels of customer governance define what\n\nthe government is attempting to achieve, how they intend to go about\n\nit, and why they are attempting to do so. MITRE systems engineers\n\n(SEs) are expected to understand the governance of the programs\n\nthey support as well as the larger enterprise in which the programs are\n\nembedded. This understanding is important even if MITRE staff are not\n\ndirectly contributing to or influencing the day-to-day workings of that\n\ngovernance. When MITRE SEs understand how their program fits into its\n\ngovernance context, they can factor that knowledge into making techni­\n\ncal recommendations. This directly affects MITRE’s ability to provide\n\nsystems engineering value and impact.\n\n\n-----\n\nThe expectations for MITRE SEs depend on the position and role in the sponsor’s gover­\nnance organization:\n\nFor MITRE staff executing tasks to support specific programs, our influence is usually\nfocused on our customer’s program outcomes. This requires an understanding of our immedi­\nate customer’s objectives and desired outcomes so we can help the customer achieve them. In\ndoing so, we should always provide independent, unbiased recommendations. In formulating\nrecommendations, it is critically important that MITRE SEs take into account the other levels\nof the customer’s governance organization, whether or not the immediate customer does so.\nOn occasion, this may mean that we do not always agree with the immediate customer’s\ndirection. But it does require that we explain how consideration of the other levels of gover­\nnance factored into our recommendations.\n\n###### �MITRE staff also play a role in helping the customer define or refine business pro­\n\ncesses, such as technical or systems engineering aspects of portfolio management. For\nmid- and senior-level MITRE staff, our role often involves recommending how to apply\nengineering analysis, advice, processes, and resources to achieve desired portfolio\noutcomes. Understanding the interconnections and dynamics across the different levels\nof the customer’s governance structure is important to providing thoughtful, balanced\nrecommendations.\n###### �Most MITRE staff probably have little influence in directly shaping enterprise behavior\n\nand standards on a continuing basis. However, for staff that participate in enterpriselevel activities, such as representing their programs in communities of interest (COIs),\nparticipating on standards boards, helping define policy at high levels within the gov­\nernment, etc., the influence can be broad and far reaching. When in such a role, MITRE\nstaff are expected to coordinate extensively across the corporation, other FFRDCs,\nacademia, and industry to ensure that all affected programs and other stakeholders are\naware of the activity and can provide input to shape products or decisions. MITRE con­\ntributions should consider all aspects of the problem, provide an enterprise perspective,\nbe product and vendor-neutral, and anticipate future missions and technologies.\n\n###### Context\n\nMITRE’s systems engineering support exists in a complex political, organizational, and pro­\ncess-driven environment. Many of the actions and behaviors of our customers are motivated\nby this environment. Although MITRE’s work is focused on technical aspects of the systems\nand enterprises, it is essential that our systems engineers be aware of, understand the effects\nof, and navigate effectively within the governance structure of how systems and enterprises\nare acquired and managed. Whereas the other topics in the Enterprise Engineering section\nof the SEG focus on the more technical aspects of our systems engineering support in the\n\n\n-----\n\nproject’s enterprise, the Enterprise Governance topic addresses the business and policy envi­\nronment of our projects.\n\nGovernance of programs, projects, and processes can be envisioned as operating at three\ndifferent, interconnected levels of our clients’ organization:\n\n###### �Program level: In general terms, success at this level is defined by meeting the goal of\n\ndelivering a system that meets specified, contracted-for performance, price, and sched­\nule parameters. Program-level decisions, directions, and actions align with that view of\nsuccess and influence the expectations of systems engineering provided at that level.\n###### �Portfolio level: At his level, the focus shifts to making trades among a collection of\n\nprograms to achieve capability-level outcomes. The trade-offs balance various criteria,\nincluding the importance of capabilities to be delivered, likelihood of program success,\nand expected delivery schedule within constraints, like availability of funding and\ncapability operational need dates. Portfolio-level decisions can result in programs being\nadded and accelerated, cut back and slowed, deferred, or even cancelled.\n###### �Enterprise level: Interventions at his level shape and change the environment (or rules\n\nof the game) in which programs and portfolios play out their roles and responsibilities\nto achieve enterprise-wide outcomes, like joint interoperability or net-centricity. Often\nthis is achieved through department or agency-wide policies or regulations that rarely\ndirectly control, manage, or decide the fate of specific programs or portfolios. Instead,\nthey indirectly influence programs and portfolios to stimulate variety and exploration of\ntechnologies, standards, and solutions or reward, incentivize, or demand uniformity to\nconverge on common enterprise-wide approaches.\nThese levels of governance and their interactions are not unique to government depart­\nments and agencies. An example of a similar construct is the U.S. economy. Commercial com­\npanies produce consumer goods, like automobiles, to gain market share by offering products\nwith competitive price-performance potential. Large commercial enterprises (e.g., General\nMotors [GM]) maintain portfolios of these products and expand or contract them based on\nmarket analyses and economic forecasts. Examples include GM closing production of its\nSaturn line of automobiles and selling off Saab. Of course, GM’s goals are quite different from\na government organization managing a capability portfolio to support government operations,\nbut the essential governance considerations, analyses, and decision making are remarkably\nsimilar.\n\n###### Best Practices and Lessons Learned\n\n\nView governance as scaffolding, not prison\n\nbars. Think of governance as a mechanism for\n\n\nnavigating the solution space for the problems\n\nwe are attempting to solve for our customers,\n\n\n-----\n\nrather than as a set of restrictive constraints that\n\ninhibit our freedom. Use the governance principles\n\nas a context for guiding your recommendations.\n\nLeverage and use the governance concepts to\n\nsupport your recommendations when they align\n\n(as they usually will).\n\nFinding the right balance point. As you conduct\n\nyour tasks, ask yourself whether the direction of\n\nyour results (products, recommendations, etc.)\n\nis consistent with the enterprise’s governance\n\nstructure, including the customer’s business\n\nprocesses, broader COI, and top-level policies\n\nand standards. If not, consider whether the results\n\nor the current position defined by the governance\n\nis more important to the program and enterprise\n\nyou are supporting.\n\nBe willing to challenge governance if you decide\n\nit is not valid or is detrimental to achieving\n\ndesired results. Although high-level governance\n\npractices, such as policies or standards, are cre­\n\nated with the intent of being applicable across\n\nthe enterprise, generally those practices can­\n\nnot account for every possible situation. Strike a\n\n###### Articles Under This Topic\n\n\nbalance between the governance practices that\n\nwork for most parties and situations with those\n\nappropriate for your program and enterprise. Use\n\nyour engineering judgment, combined with your\n\nbroad understanding of the governance practices\n\n(and how they are applied and why they are ben­\n\neficial), to determine that balance.\n\nIf governance practices need to be changed,\n\nconsider how to augment, adjust, or refine\n\nexisting guidance while satisfying local objec­\n\ntives, rather than recommending dramatic\n\nchanges that may be almost impossible to\n\nachieve. When recommending changes, ensure\n\nthat the intent of the governance concepts is\n\nhonored and that the proposed revisions are\n\napplicable to and suitable for programs and enter­\n\nprises beyond your local environment or situation.\n\nEnlist support from peers and management—first\n\nwithin MITRE, and then with the sponsor—to\n\neffect the changes desired. Develop a strategy\n\nearly on about how to accomplish the governance\n\nchanges, accounting for stakeholders, their moti­\n\nvations, and how to achieve win-win results.\n\n\n“Communities of Interest and/or Community of Practice” provides advice on working in\ngroups that collectively define items like interoperability concepts.\n\n“Standards Boards and Bodies” provides best practices and lessons learned for MITRE\nstaff participating on technical standards committees shaping technical compliance in pro­\ngrams and systems.\n\n“Policy Analysis” discusses MITRE support to decision making in a multi-stakeholder,\nmulti-objective environment.\n\n\n-----\n\nDefinition: A Community\n\n_of Interest (CoI) and/or_\n\n_Community of Practice (CoP)_\n\n_is a group of people operating_\n\n_within or in association with a_\n\n_client, customer, sponsor, or_\n\n_user in MITRE’s business realm_\n\n_or operating sphere of influence_\n\n_for the purpose of furthering a_\n\n_common cause by sharing wis­_\n\n_dom, knowledge, information, or_\n\n_data, and interactively pursuing_\n\n_informed courses of action._\n\nKeywords: community of inter­\n\n_est, community of practice,_\n\n_group dynamics, information_\n\n_exchange, mission success,_\n\n_mutual trust, shared goals,_\n\n_systems integration, terminol­_\n\n_ogy, user involvement_\n\n\nENTERPRISE GOVERNANCE\n###### Communities of Interest (COI) and/or Community of Practice (COP)\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers are expected to participate in\n\nCoIs or CoPs associated with their projects. This\n\nwill assist in harmonizing domain terminology,\n\nexchanging pertinent information, and looking\n\nfor and acting on issues and opportunities in\n\ntheir project’s enterprise. MITRE staff in CoP or\n\nCoI settings are expected to bring the corpora­\n\ntion to bear by providing the greatest value to\n\nour clients, customers, sponsors, or users, in\n\nconjunction with other government contractors.\n\n\n-----\n\n###### Characteristics of CoIs and CoPs\n\nThe terms CoI and CoP are sometimes invoked interchangeably, but there are distinctions:\n\n###### �Communities of Practice are “groups of people who share a concern, a set of problems,\n\nor a passion about a topic, and who deepen their knowledge and expertise in the topic\nby interacting on an ongoing basis.” They operate as “learning systems” or “action sys­\ntems” where practitioners connect to solve problems, share ideas, set standards, build\ntools, and develop relationships with peers and stakeholders. A CoP is typically broader\nin scope and tends to focus on a common purpose, follow-on actions, and information\nexchanges. CoPs can be both internal and external to MITRE, including various govern­\nment, industry, academia, and MITRE participants.\n###### �Communities of Interest are typically narrower in scope and tend to have a specific\n\nfocus, such as information exchange. COIs typically tend to be a government organi­\nzation approach (particularly in the Department of Defense [DoD]) to bring together\nindividuals with common interests/references who need to share information internal\nto their community. They also need to provide an external interface to share with other\ncommunities (e.g., allowing a community-based loose coupling and federation as high­\nlighted in other Enterprise Engineering section articles).\nThe MITRE business realm, program, or project context may determine which term is\nused more prevalently according to these characteristics.\n\nThe characteristics of CoIs and CoPs are discussed below in terms of social interactions,\noperations, longevity, and commitment.\n\n###### Operations\n\nA CoP may operate with any of the following attributes:\n\n###### �Some sponsorship �A vision and/or mission statement �Goals and/or objectives �A core team and/or general membership �Expected outcomes and/or impacts �Measures of success �Description of operating processes �Assumptions and/or dependencies �Review and/or reflection\nOften CoIs span similar organizations (e.g., DoD, particularly when there is a common\ninterest in an outcome).\n\nIndividual members may be expected to:\n\n###### �Support the CoP through participation and review/validation of products\n\n\n-----\n\n###### �Attempt to wear the “one hat” associated with the CoP while maintaining the integrity\n\nand autonomy of their individual organizations.\n###### �Participate voluntarily with the blessing of their organizations that determine their level\n\nof participation and investment.\nSponsoring organizations might provide a nominal budget needed to participate in CoIs/\nCoPs, make presentations at external organizations, or support meetings of the core team.\nThus MITRE staff participating in CoPs must be mindful of the time and effort they contrib­\nute, and ensure that their participation is an appropriate and justifiable investment of project\nresources.\n\n###### Longevity\n\nThe “practice” part of CoP relates to the work the community does. This includes solving com­\nmon problems, sharing ideas, setting standards, building tools, and developing relationships\nwith peers and stakeholders. Collective learning takes place in the context of the common\nwork. These groups learn to work not so much by individual study, lectures, etc., but by the\nosmosis derived from everyone working together—from experts to newcomers—and by “talk­\ning” about the work. This provides value to all organizations represented. MITRE participants\nshould attempt to contribute their best ideas to the CoP and bring back good practices to share\nwith their project teams and colleagues.\n\nLike many other communities, a CoP grows based on the increasing benefits individuals\nor organizations accrue from participating in the activity. Sometimes these rewards include\npersonal satisfaction in contributing and being recognized for adding value. A CoP that has a\npositive impact by helping to solve important problems not only retains a substantial percent­\nage of its members, but attracts new ones.\n\n###### Social Interactions\n\nA CoI or CoP can operate in various interpersonal modes, including face-to-face or via video/\naudio teleconference, telephone, email, and website access devices. MITRE has been operating\nin all these modes for some time; it is becoming increasingly immersed in the virtual envi­\nronments. It behooves us to become familiar and adept with the newer, more pervasive and\neffective methods.\n\n###### Important Topics Relevant to CoIs and CoPs\n\nTerminology\n\nOne important goal of any community is to achieve a shared understanding of terminology,\nparticularly community-specific terms of art. It is not unusual for different stakeholders in a\n\n\n-----\n\nCoI/CoP to start with different meanings for a given word, or different words for something\nwith a common meaning. Because MITRE is frequently in the position of providing technical\nsupport to different constituents of a CoI/CoP, we are in a position to assess whether a com­\nmon understanding of a term is important to achieve and, if so, how to achieve that harmo­\nnization. For example, it is critical that the term “identification” has a commonly understood\nmeaning between the military tactical situation awareness community and the intelligence\nanalysis community when the two communities are operating together.\n\nInformation Sharing\n\nOne of the primary CoI/CoP functions is to share information that is important to the com­\nmon purpose of the CoI/CoP. There are forces at work in organizations that may discourage\ninformation sharing, either explicitly or implicitly. The reasons for this are many. Some are\nlegitimate (e.g., sharing has the potential to compromise classified information or threaten\nnetwork security) while others are an artifact of organizational cultures that see the retention\nof information as a form of power or self-protection.\n\nTrust\n\nGood relationships are built on interpersonal trust. In the context of CoIs/CoPs, trust assumes\ntwo forms. First, information provided by an individual must be true and valid (i.e., the\nindividual is viewed as a competent source of information). Second, a trustworthy person is\ndedicated to the goals of the CoI/CoP and treats others with respect. Trust is an important\ningredient in the facilitation of information sharing.\n\nGroup Dynamics\n\nEffective participation and operation within a CoI/CoP is highly correlated with good inter­\npersonal skills in group settings. This requires an awareness and understanding of human\nmotivation and behavior.\n\n###### CoI Lessons Learned\n\n\nValue and Focus\n\n###### � [Purpose:][ Start with a clear purpose and ]\n\ninformed understanding of the require­\n\nments. Lack of clearly defined require­\n\nments can cause restarts. Define the\n\nscope early, work closely with the forma­\n\n\ntion teams to ensure all necessary infor­\n\nmation is included, and prevent “require­\n\nments creep.”\n###### � [Passion:][. Known consumers with known ]\n\nneeds are important for CoI success. Pro­\n\ngrams of record that have an imperative to\n\n\n-----\n\ndeliver capability to the warfighter and are\n\ndependent on CoI products to do so can\n\nbe used to drive toward results.\n\nStrategy\n\n###### � [Objectives:][ Define the terminology, goals, ]\n\nand constraints, and understand the prob­\n\nlem and objectives so people are willing to\n\nparticipate. Ensure there is a well-defined\n\npurpose addressing a scoped problem\n\nrelevant to the participants and tackled in\n\nachievable increments/spirals to adapt to\n\nchanging needs. Select a scope permit­\n\nting delivery of useful results within 9 to\n\n12 months. Try to adopt a common model\n\nand avoid generating unique vocabularies\n\nand schema across domains to prevent\n\nan “N-squared problem” of communica­\n\ntion among participants. System builders\n\nare usually important contributors to CoI\n\nvocabulary work and should be encour­\n\naged to drive the common vocabulary\n\nactivities. Most CoI efforts do not have\n\ntime to create or learn large new vocabu­\n\nlaries, so leverage past efforts.\n###### � [Stakeholders:][. Address cross-organi­]\n\nzational and cultural challenges through\n\nstructure, partnership, and transparency.\n\nAny issues and competing agendas need\n\nto be addressed directly, seeking common\n\nground and win-win solutions. Institu­\n\ntionalize the CoI through creative friction\n\nand an adaptable system of rewards/\n\nincentives.\n###### � [Invite key stakeholders to help ensure ]\n\nbroader acceptance of results: Identify\n\norganizations willing to contribute early\n\n\nin the process and those with a vested\n\ninterest in the outcomes. It is better to\n\nget diverse inputs to surface showstop­\n\nper issues early, so they can be dealt with\n\nappropriately as the work progresses.\n\nGovernance\n\n###### � [Leadership:][ Ensure that there is strong ]\n\nleadership and commitment to success.\n\nBoth attributes are important to keep the\n\nteam engaged and moving in a com­\n\nmon direction. There is no substitute for\n\ngovernance, self-policing, and personal\n\nrelationships.\n###### � [Commitment:][ Prepare for long-term ]\n\ncommitment. CoIs are nontrivial and\n\nrequire significant levels of participa­\n\ntion over time. This has the potential for\n\nsignificant unfunded costs to support\n\nand implement. Assess and continually\n\nre-evaluate the return on investment of\n\nMITRE’s participation in the CoI.\n###### � [Procedures:][ Each CoI must establish its ]\n\nown operating procedures. Though other\n\nCoI procedures could be used as a basis,\n\neach CoI needs to tailor its norms and\n\nprocedures to the organization (and cul­\n\nture) and objectives.\n###### � [Have a set of exit criteria:][ Develop a set ]\n\nof criteria and exit strategy for disbanding\n\nthe CoI (or for MITRE to cease participa­\n\ntion), using the CoI objectives and our\n\nintended role.\n###### � [Limit attendance to one or two repre­]\n\nsentatives per organization/program:\n\nTry to limit attendance to key players (e.g.,\n\n\n-----\n\nan authoritative manager and a technical\n\nexpert).\n###### � [Limit teleconferences to preparing for ]\n\nmeetings or reviewing status: Face-to\nface meetings are required to get the work\n\ndone. Teleconferences have limited benefit\n\nfor working through complex issues.\n###### � [Have important tasks and announce­]\n\nments distributed by a high-ranking\n\nleader to those with authority: This tends\n\nto get people’s attention and increases the\n\nlevel of cooperation. For example, official\n\n“taskers” need to be sent by a government\n\nrepresentative to other government rep­\n\n###### Conclusion\n\n\nresentatives when many of the CoI players\n\nare contractors.\n###### � [Have fewer but longer meetings:][ This ]\n\nimproves the chance of retaining the same\n\nplayers and helps eliminate the problem of\n\nrestarting and retracing steps and agree­\n\nments made at previous meetings for the\n\nbenefit of new players.\n###### � [Take real-time minutes to ensure agree­]\n\nment on issues, results, and action\n\nitems: Take minutes and document\n\nsignificant happenings as they occur. This\n\nprovides a tangible track record that helps\n\nprevent disagreements later.\n\n\nAs you participate in the CoI/CoP process, leverage the lessons learned in this article and\nidentify additional ways to enhance the CoI/CoP efficiency and effectiveness. Equally impor­\ntant, share the lessons learned and best systems engineering practices that you experienced\nthrough participating in CoIs/CoPs.\n\n###### References and Resources\n\nAdvanced Distributed Learning, “Simulation and Training Community of Practice,” accessed\nJanuary 22, 2010.\n\n[Generation YES Blog—Thoughts About Empowering Students with Technology, accessed](http://blog.genyes.org/index.php/2008/11/05/community-of-interest-or-community-of-practice/)\nJanuary 22, 2010.\n\n[“Knowledge Management,” accessed January 22, 2010.](http://www.knowledge-management-online.com/KnowledgeNetworks.html)\n\n[Land Grant University CoP, “Applying to Become a Professional Development Community of](http://collaborate.extension.org/wiki/Applying_to_Become_a_Professional_Development_Community_of_Practice)\n[Practice,” accessed January 22, 2010.](http://collaborate.extension.org/wiki/Applying_to_Become_a_Professional_Development_Community_of_Practice)\n\n“Leveraging the Corporation,” MITRE Project Leadership Handbook, The MITRE Corporation.\n\nMedric, L., February 9, 2007, “Communities of Practice (CoP)—An Overview and Primer,”\nCEM Forum.\n\n\n-----\n\nDefinition: In many instances,\n\n_MITRE’s systems engineering_\n\n_and subject matter expertise_\n\n_is brought to bear in helping_\n\n_committees produce industry_\n\n_standards. Industry standards_\n\n_typically require the consensus_\n\n_of the committee’s members,_\n\n_which may include representa­_\n\n_tives from government or indus­_\n\n_try or both. MITRE’s contribu­_\n\n_tions include direct technical_\n\n_contributions, managing_\n\n_committees and their docu­_\n\n_ments, and helping to moderate_\n\n_negotiations between commit­_\n\n_tee members to bring about_\n\n_consensus._\n\nKeywords: AIEE, ASE, consen­\n\n_sus documents, IEEE, negotia­_\n\n_tions, RTCA, standards_\n\n\nENTERPRISE GOVERNANCE\n###### Standards Boards and Bodies\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) need to understand\n\nthe objectives of the standards body that is\n\nproducing the standard, typically articulated in\n\nTerms of Reference for the committee. They\n\nshould ensure that the goals of the standards\n\ncommittee, the MITRE work program, and\n\nsponsor are in alignment. SEs are expected to\n\nbring expert technical analyses and discipline\n\nto the standards process by providing objective\n\ndata relevant to the topic of standardization.\n\n\n-----\n\n###### MITRE Interest\n\nMITRE’s interest in standards board participation is in establishing the best standards to\nfurther technology and industry implementation to improve interoperability across govern­\nment capabilities. Participation in standards bodies provides opportunities to bring the\nworld to bear on our customers’ problems, and collaborate with other FFRDCs, industry,\nand academia.\n\n###### Background\n\nStandards committees have historically provided standards that allow for compatibility of\nequipment produced by different vendors, or that provide for minimum safety of equipment\nor devices. For example, the Radio Technical Commission for Aeronautics (RTCA) develops\nstandards for aircraft equipment that allow multiple vendors’ equipment to be cross compat­\nible within the aircraft, from aircraft to ground systems, and between neighboring aircraft.\nAnother example is minimum requirements for electrical equipment.\n\nStandards bodies are typically attended voluntarily by participating government and\nindustry organizations that send experts on standardization. Government employees may\nwork directly with industry in developing the standards. Usually standards bodies’ meetings\nare open to participation and to the public.\n\nMost standards are agreed upon by consensus; that is, all participating organizations\nagree to the requirements represented in the standard. Arriving at a consensus can be chal­\nlenging and time-consuming, which is a principal reason why standards sometimes take\nsubstantial time to produce. MITRE’s expertise and objectivity can be key to brokering\nconsensus.\n\n###### Government Interest and Use\n\nMany U.S. federal, state, and local government agencies depend on voluntary consensus\nstandards. In many cases, the U.S. government relies on standards bodies to provide input\nto potential government rules and regulations. RTCA, for example, functions as an advisory\ncommittee to the Federal Aviation Administration (FAA) under the U.S. Federal Advisory\nCommittee Act of 1972 [1]. RTCA standards, when accepted by the FAA, may become the basis\nfor FAA Technical Standard Orders, which govern the requirements for equipment manufac­\nture, or FAA advisory circulars, which provide advice on equipment installation, usage, etc.\n\nAn agency may adopt a voluntary standard without change by incorporating the stan­\ndard in an agency’s regulations or rules. Depending on the relationship of the standard body\nto the government, the government may adopt the standard with certain exceptions. In other\ngenerally exceptional cases, the government may ignore the standard outright. Under the U.S.\nFederal Advisory Committee Act, for example, a voluntary consensus standard is submitted\n\n\n-----\n\nto the government as advice, and the government is under no obligation to accept that advice.\nMITRE systems engineers can play an important role in brokering agreements between gov­\nernment and industry to ensure that the standards are accepted and utilized by government\nagencies [2].\n\nIn some cases, standards become the basis for rulemaking by the government. In this\nsituation, the government will first propose the rule in the Federal Register as a notice to the\npublic, known as a Notice of Public Rulemaking. At this stage, the public is invited to com­\nment on the proposed rule. All comments must be considered by the government in advance\nof issuing a final rule. The government’s response to the comments becomes a matter of\npublic record. The relevant standards may form a substantial basis for the final rule; in some\ncases, a standard may be one accepted means of compliance, may be the basis for guidelines\nfor compliance, or voluntary compliance with a given standard may be accepted in lieu of\nformal rulemaking [3].\n\n[The U.S. federal government updates the Code of Federal Regulations (CFR) once per year.](http://www.gpoaccess.gov/cfr/)\nThe CFR contains all the rules published by the U.S. federal government. The CFR is divided\ninto various areas of regulation [4].\n\n###### Best Practices and Lessons Learned\n\n\nObjectivity is paramount. MITRE must act, and\n\nbe viewed, as an objective participant, since\n\nour goal is to be able to moderate negotiations.\n\nCommittee participants should make sure that all\n\nperspectives are considered fairly, even though\n\nsome perspectives may conflict with our spon­\n\nsor’s point of view. MITRE’s role is to bring objec­\n\ntive analysis to the table for all parties to consider.\n\nIt is highly desirable to bring analytical results to\n\nthe conversation to inform the discussions. In lieu\n\nof analytical data, objective expert opinion should\n\nbe clearly articulated.\n\nBring the best expertise to the table.\n\nCommittees are usually public forums in which\n\nMITRE’s reputation and credibility are at stake.\n\nMost organizations tend to staff committees\n\nwith their most senior and knowledgeable staff;\n\nMITRE should do no less. Specific subject matter\n\n\nexpertise should be brought into conversations\n\nwhenever appropriate; key staff should be on call\n\nto serve in these roles.\n\nInvolvement in committee leadership is an\n\nasset. One way to demonstrate MITRE’s influ­\n\nence and objectivity more effectively is for MITRE\n\nparticipants to be involved in the committee\n\nleadership. MITRE roles have varied from high\nlevel leadership positions in the overall organiza­\n\ntion (e.g., leading the RTCA Program Management\n\nCommittee), leading special committees, leading\n\nworking groups under the larger committees, and\n\ntaking on the role of committee or working group\n\nsecretary. All of these types of leadership roles\n\nreflect well on the company and put MITRE in a\n\nposition of regard and influence.\n\n\n-----\n\nHold the pen. It might seem like a tedious job, but\n\nvolunteering to manage the standards docu­\n\nment puts MITRE in an effective position to help\n\nassume a key responsibility for the develop­\n\nment of the standard. The book manager may be\n\npersonally responsible for significant textual input\n\nto the standard. In addition, the book manager is\n\nresponsible for coordinating inputs from various\n\nauthors, managing configuration control, incor­\n\nporating updates to the document, and ensuring\n\nthat all comment resolutions are implemented as\n\nagreed.\n\nStandards development should be a disci­\n\nplined process. There should be clear, agreed\n\nprocedures for running meetings with a leader\n\nwho can moderate the conversation such that\n\nall voices are heard while progress and decisions\n\nare made. Documents should be developed with\n\na clear configuration management plan. After a\n\n###### References and Resources\n\n\nrough draft, documents should be reviewed and\n\na record of comments and their dispositions,\n\nusually in the form of a comment matrix, should\n\nbe maintained. A written record of proceedings is\n\nessential so that issues that have been discussed\n\nand dispositioned are not reopened.\n\nWork difficult issues or specific subtasks in\n\nsmaller subgroups. A key manner in which to\n\naccelerate the standards development process is\n\nto assign small groups to work out and agree on\n\nkey technical points. In some instances, a small,\n\nad hoc group may be formed to make recom­\n\nmendations on a specific issue. In other cases,\n\nmore formal, long-term subgroups may be formed\n\nto draft whole sections of a standard. In any case,\n\na “divide and conquer” approach is usually more\n\neffective in bringing acceptable proposals back\n\nto a larger group, rather than having a large group\n\ndebate a given topic.\n\n\n[1. The Federal Advisory Committee Act, October 6, 1972, and the Federal Advisory](http://www.gsa.gov/portal/content/104514)\n\n[Committee Act Amendments of 1997, December 17, 1997.](http://www.gsa.gov/portal/content/104514)\n\n[2. ANSI, “U.S. Government Use of Voluntary Consensus Standards,” http://www.standard­](http://www.standardsportal.org)\n\n[sportal.org/, accessed February 22, 2010.](http://www.standardsportal.org)\n\n[3. Office of Management and Budget, February 10, 1998, “OMB Circular No. A-119: Federal](http://www.whitehouse.gov/omb/circulars_a119/)\n\n[Participation in the Development and Use of Voluntary Consensus Standards and in](http://www.whitehouse.gov/omb/circulars_a119/)\n[Conformity Assessment Activities.”](http://www.whitehouse.gov/omb/circulars_a119/)\n\n[4. GPO Access, “U.S. Code of Federal Regulations,” accessed February 22, 2010.](http://www.gpoaccess.gov/cfr/index.html)\n\n\n-----\n\nDefinition: Policy analysis is\n\n_a disciplined process to help_\n\n_people make decisions in situa­_\n\n_tions of multiple objectives and_\n\n_multiple perspectives._\n\nKeywords: decision making,\n\n_policy, policy analysis_\n\n\nENTERPRISE GOVERNANCE\n###### Policy Analysis\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the role and implication of policy\n\nin our customers’ activities and how sys­\n\ntems engineering relates to it. MITRE SEs are\n\nexpected to know the basic characteristics of\n\ngood policy analysis, so they can constructively\n\ncollaborate with policy analysts on ques­\n\ntions and issues arising at the boundary of\n\nsystems engineering and government policy.\n\n\n-----\n\n###### Government Interest and Use\n\nWithin the United States government, very few important decisions are made by a single\nindividual. Congress and the Supreme Court make decisions by voting. While most execu­\ntive branch decisions are made by an individual senior official, the decision is normally the\nresult of a deliberative process in which numerous people with diverse expertise or diverse\ninterests offer advice that it is imprudent to ignore. The output of a good policy analysis is a\nset of questions regarding priorities or a set of options among which to choose, along with the\nmajor arguments for each competing priority or the major pros and cons of each option. Either\noption will help organize the interactions that lead to choosing a course of action.\n\nSystems engineering can be viewed as a process for arriving at a solution that represents\nan acceptable balance among multiple objectives. Traditionally, systems engineering typically\npresumed that the objectives and relevant operational constraints can be defined, and that\nthe extent to which any given outcome meets a given objective can be quantified. When these\nconditions exist, systems engineering can usually arrive at a “best,” “correct,” or “optimal”\ndesign solution. Systems engineering also can derive the requirements for various subsystems\non the basis of the overall system design, including the requirements that each subsystem\nmust meet to interact properly with other subsystems. In contrast, policy analysis, when done\nwell, leads to courses of action that may not be the “best” from any one perspective, but are\n“good enough” for enough players to win the necessary political support to move ahead. New\nforms of systems engineering are adopting this “good enough” solution perspective, particu­\nlarly in large-scale enterprise settings. Indeed, a good policy analysis may be used by individ­\nuals who disagree with the government’s objectives; in this case, one individual may conclude\nthat the policy analysis shows option B is best while the other concludes that the same policy\nanalysis shows option D is best, and they both agree that either option is acceptable.\n\nSystems engineering and policy analysis must account for costs and affordability. An\nelegant engineering solution that the customer cannot afford is useless; so too is a policy\noption that would make many people happy, but at a prohibitive cost. Therefore, careful\nefforts to estimate the cost of a particular option and the risk that the actual cost may exceed\nthe estimate are necessary for systems engineering and policy analysis. Engineers who design\nproducts for commercial sale are familiar with the concept of “price points,” and a manufac­\nturer may wish to produce several products with similar purposes, each of which is optimal\nfor its own selling price. In the case of systems engineering for the government, it may be\nnecessary to conduct a policy analysis to determine how much the government is willing to\nspend, before conducting a systems engineering analysis to arrive at the technically “best”\nsolution at that cost level.\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nEspecially rigorous quality assurance. Policy\n\nanalysis at MITRE poses a special concern. The\n\nmissions of MITRE’s federally funded research and\n\ndevelopment centers (FFRDCs) are systems engi­\n\nneering and research, while policy analysis is the\n\nmission of other FFRDCs. At times, it is completely\n\nappropriate for MITRE to conduct policy analysis.\n\nMITRE has excellent policy analysts on its staff,\n\nbut it falls outside the mainstream of our work.\n\nThus, it important that all MITRE policy analysis\n\ndelivered to the government is of high quality. If\n\na MITRE policy analysis is substandard, we have\n\nfew resources to fix the problem and are vulner­\n\nable to the accusation of taking on work that is\n\noutside our sphere of competency. Therefore, any\n\nMITRE policy analysis intended for delivery to the\n\ngovernment typically requires a degree of quality\n\nassurance beyond our routine practices.\n\nThe technical-policy boundary—know and\n\nrespect it. Some MITRE work requires policy\n\nanalysis as a deliverable to our government spon­\n\nsors (i.e., the sponsors ask us to provide analytical\n\nsupport for government policy-making). At times,\n\nMITRE conducts policy analysis for internal con­\n\nsumption only. This helps MITRE understand the\n\nmultiple perspectives and objectives of our spon­\n\nsors so that our technical work can be responsive\n\nto “real” needs that they may be precluded from\n\nexpressing in official documents. Finally, MITRE\n\nis sometimes asked to support a government\n\npolicy process by providing technical analysis that\n\nnarrows the scope of the government’s disagree­\n\nments; the task of “taking the technical issues off\n\nthe policy table” requires that MITRE staff suf­\n\nficiently understand policy analysis to assure our\n\n\ntechnical analysis stops where true policy analysis\n\nbegins.\n\nPolicy analysis basics for systems engineers.\n\nThere are a few basics that characterize good\n\npolicy analysis. MITRE SEs should be familiar with\n\nthem, so they can constructively collaborate with\n\npolicy analysts on questions and issues arising at\n\nthe boundary of systems engineering and govern­\n\nment policy. These are summarized in the order\n\nin which they appear during the course of a policy\n\nanalysis:\n\n###### � [Transform a situation into one or more ]\n\nissues: The analysis must identify the pol­\n\nicy decisions that are most appropriate for\n\nthe situation. Figuring out what questions\n\nto ask is the most critical, and often the\n\nmost difficult, part of the analysis. Asking\n\nthe right questions is what transforms a\n\n“messy situation” into an “issue” or a “set of\n\nissues.” When policy analysis is being per­\n\nformed for an identifiable customer, it is of\n\nlittle use unless the analysis is framed in\n\nterms of decisions that the customer has\n\nthe authority to make—or perhaps deci­\n\nsions that the customer’s boss or boss’s\n\nboss has the authority to make, provided\n\nthat the customer has a charter to go to\n\nthe boss and say, “I can’t do my job until\n\nyou make this decision.”\n###### � [Create executable options:][ The analy­]\n\nsis must identify options for each deci­\n\nsion. This is where policy analysis can be\n\ngenuinely creative, even while remaining\n\nrigorous. There are many options in a typi­\n\n\n-----\n\ncal government policy dilemma; however,\n\nthe number of options that the policy\nmakers can seriously consider is small.\n\nA senior government official looks for an\n\noption that will meet the most important\n\nobjectives, can be implemented with the\n\nresources available, and will attract sup­\n\nport from enough other perspectives to\n\ncommand a majority vote or support from\n\na preponderance of advisers. A good set\n\nof options: (a) are responsive to the issues\n\nposed (see the previous bullet): (b) could\n\nbe implemented, if chosen; and (c) none\n\nof the important players in the decision\n\nprocess will react by saying “none of the\n\nabove.”\n###### � [Options have advantages, disadvan­]\n\ntages, and uncertainties: The analysis\n\nmust identify the advantages, disadvan­\n\ntages, and uncertainties associated with\n\neach option. This is a straightforward\n\nprocess. However, if the analysis is to be\n\ncredible, one must carefully state the pros\n\nand cons in ways that are recognized as\n\naccurate by those whose views they por­\n\ntray. For example, an analysis of an option\n\nfor sharing extremely sensitive intelligence\n\nwith an ally should state the pros in lan­\n\nguage that might be used by a proponent\n\nof this option, and the cons in language\n\nthat might be used by an opponent.\n\nOtherwise, the product will be viewed as\n\nadvocacy, not an analysis.\n###### � [Strategies for reducing uncertainty:]\n\nSometimes an analysis, having identi­\n\nfied uncertainties that make it difficult to\n\n\nchoose an option, may propose a strategy\n\nfor reducing the uncertainties. Of course\n\ntime reduces some uncertainties, and a\n\nserious effort to gather additional informa­\n\ntion will require time. Delaying a decision\n\noften permits a bad situation to become\n\nworse. Much of the art of the statesman\n\nis sensing the moment to make a difficult\n\ndecision. When a policy analyst chooses\n\nto propose a strategy for reducing uncer­\n\ntainty, the analyst is helping the decision\nmaker understand how much time would\n\nbe required to obtain additional informa­\n\ntion or understanding, and thus make a\n\ngood judgment about when to decide.\n###### � [Identifying additional options, if needed:]\n\nSometimes if an analysis failed to iden­\n\ntify an acceptable set of options, it may\n\npropose a strategy for identifying addi­\n\ntional options. Such a strategy could be a\n\ntargeted research program or consultation\n\nwith other organizations that have not\n\nparticipated in the process.\n###### � [Decision-making strategies:][ Finally, the ]\n\nanalysis may identify a strategy for arriving\n\nat a decision. In some circumstances, this\n\nis not necessary if the strategy is obvious;\n\nin other cases, some or all of the options\n\nmay require concurrence of others or a\n\nprocess that is unusual in some way.\n\nAs is the case with many MITRE services and\n\nproducts, a policy analysis may contain extensive\n\ndata and argumentation that the actual decision\nmaker will never read or hear. The executive\n\nsummary of a paper and the first few slides of a\n\nbriefing must clearly convey the information that\n\n\n-----\n\nthe decision-maker should learn and understand,\n\nwhile the body of the paper and the extensive\n\nback-up slides in the briefing provide credibility\n\nto the message and a means for staff to check\n\nthe validity of summary statements they find\n\n###### References and Resources\n\n\nsurprising. Therefore, it is highly desirable that the\n\nexecutive summary or the summary slides be well\n\nwritten. In contrast, the segments providing detail\n\nmust be checked carefully for clarity and accu­\n\nracy, but need not be models of graceful prose.\n\n\nAllison, G. T., and P. Zelikow, 1999, Essence of Decision: Explaining the Cuban Missile Crisis,\n2nd Edition, New York: Longman.\n\n[Conklin, J., Winter 2009, “Building Shared Understanding of Wicked Problems,” Rotman](http://www.cognexus.org/Rotman-interview_SharedUnderstanding.pdf)\n_Magazine._\n\n[Rittel, H., and M. Webber, 1973, “Dilemmas in a General Theory of Planning,” Policy Sciences,](http://www.uctc.net/mwebber/Rittel+Webber+Dilemmas+General_Theory_of_Planning.pdf)\nVol. 4, Elsevier Scientific Publishing Company, Inc., Amsterdam, pp. 155–169.\n\n“Trade-off and Engineering Analysis,” MITRE Project Leadership Handbook, The MITRE\nCorporation.\n\nWildavsky, A., 1979, Speaking Truth to Power: The Art and Craft of Policy Analysis, Little,\nBrown.\n\nWildavsky, A., 1988, The New Politics of the Budgetary Process, Scott, Foresman & Co.\n\n\n-----\n\n##### MITRE FFRDC Independent\n Assessments\n\nDefinition: An independent assessment is a tool that can be used at any point in a\n\n_program life cycle to provide insight into progress and risks._\n\nKeywords: accident investigation, audit, baseline assessment, independent\n\n_expert review, independent technical assessment, red team, SCAMPI appraisal,_\n\n_Tiger Team_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to be able to lead or par­\n\nticipate in independent assessment teams, particularly when program\n\nprocesses are being evaluated or there are concerns about program\n\nprogress or contractor performance [1]. MITRE SEs are expected to\n\napply strong domain and technical expertise and experience and\n\nperform with objectivity consistent with the FFRDC role.\n\n###### Context\n\nAn Independent Assessment is a team activity in which the team\n\nleader and team members are not members of the organization being\n\nassessed. This allows the team to more readily fulfill its charter objec­\n\ntively and without conflicts of interest. Ideally, assessments are proactive\n\nand intended to provide an early look into what potential problems may\n\nbe on the horizon in time to take action and avoid adverse impact to the\n\nprogram. For example, an assessment may be used to take an early\n\n\n-----\n\nlook at the challenges associated with technologies and provide feedback to the technology\ndevelopment strategy. In other cases, an assessment might be intended to assess progress with\na process improvement framework, such as CMMI (Capability Maturity Model Integration) [2],\nor may be intended to examine causes of concerns with program performance.\n\nIn MITRE’s work, independent assessments are known by several names, including:\n\n###### �Independent Reviews �Red Teams �Blue or Green Teams �Technical Assessments—Tiger Teams �Appraisals (against a model or standard) �Independent Expert Reviews (IER) �Audits and Compliance Assessments �Accident Investigations\nFor related information, see the article, “Planning and Managing Independent\nAssessments” in this topic, and “Data-Driven Contractor Evaluations and Milestone Reviews”\nand “Earned Value Management” in the topic Contractor Evaluation in the Acquisition\nSystems Engineering section.\n\nMITRE SEs are frequently asked to lead and participate in independent assessments\nbecause the characteristics of an FFRDC, as chartered under the Federal Acquisition\nRegulation (FAR) 35.017 [3], promote independence, objectivity, freedom from conflicts of\ninterest, and technical expertise. These characteristics support the management goals of the\nassessment team.\n\nFor example, the FAR describes the special relationship between a sponsor and its\nFFRDC [3]. The FFRDC:\n\n“... meets some special long-term research or development need which cannot be met as\neffectively by in-house or contractor resources ... to accomplish tasks that are integral to the\nmission and operation of the sponsoring agency.”\n\n“...has access, beyond that which is common to the normal contractual relationship, to\ngovernment and supplier data, including sensitive and proprietary data, and to employees and\nfacilities.”\n\n“...conduct[s] its business ... to operate in the public interest with objectivity and indepen­\ndence, to be free from organizational conflicts of interest, and to have full disclosure of its\naffairs to the sponsoring agency.”\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nMITRE’s assessment teams have experienced\n\nsome challenges, including [4]:\n\n###### � [Sponsor oversight has been delegated, ]\n\nleading to a lack of clarity about what\n\nneeds to be accomplished.\n###### � [The review is additional work and seen as ]\n\na lower priority.\n###### � [Members of the organization being ]\n\nreviewed are not able to participate as\n\nplanned.\n\n###### References and Resources\n\n1. Independent Technical Assessments.\n\n\n###### � [Subjects of the review are not prepared for ]\n\nthe review.\n###### � [Objective evidence is difficult to locate. ] � [Appraisal team working space is rarely ]\n\navailable.\n\nMITRE practitioners have found that these and\n\nother challenges can be mitigated through com­\n\nmunication and following the process described\n\nin the “Planning and Managing Independent\n\nAssessments” article in the SEG.\n\n\n[2. The MITRE Institute, September 1, 2007, MITRE Systems Engineering (SE) Competency](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n[Model, Ver. 1, p. 39.](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n3. Standard CMMI® Appraisal Method for Process Improvement (SCAMPI [SM]) Methodology.\n\n[4. Federal Acquisition Regulation (FAR).](https://www.acquisition.gov/far/)\n\n###### Additional References and Resources\n\n“Assessment and Adaptation,” MITRE Project Leadership Handbook, The MITRE Corporation.\n\n[Clapp, J. A., and P. G. Funch, March 5, 2003, A Guide to Conducting Independent Technical](http://www.amazon.com/Results-Without-Authority-Controlling-Managers/dp/0814473431/ref=pd_bxgy_b_text_c)\n[Assessments, MITRE Center for Air Force C2 Systems.](http://www.amazon.com/Results-Without-Authority-Controlling-Managers/dp/0814473431/ref=pd_bxgy_b_text_c)\n\n“Customer and Contractor Interaction,” MITRE Project Leadership Handbook, The MITRE\nCorporation.\n\n“FFRDC Role and Public Interest,” MITRE Project Leadership Handbook, The MITRE\nCorporation.\n\n[The Project Management Institute, 2008, A Guide to the Project Management Body of](http://www.pmi.org/Movies/4StandardsVideos/PMBOKonly.html)\n_[Knowledge, (PMBOK Guide), 4th Ed.](http://www.pmi.org/Movies/4StandardsVideos/PMBOKonly.html)_\n\n\n-----\n\nDefinition: An independent\n\n_assessment is a tool that_\n\n_can be used at any point in a_\n\n_program life cycle to provide_\n\n_insight into progress and risks._\n\nKeywords: audit, baseline\n\n_assessment, independent_\n\n_expert review, independent_\n\n_technical assessment, red_\n\n_team, SCAMPI appraisal, Tiger_\n\n_Team_\n\n\nMITRE FFRDC INDEPENDENT ASSESSMENTS\n###### Planning and Managing Independent Assessments\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be able\n\nto plan, lead, or be team members or subject\n\nmatter experts of independent review teams.\n\n\n-----\n\n###### Introduction\n\nIndividual skills and experience are a solid foundation for participation in independent\nreviews, but completing the review on schedule with quality findings also depends on a\ndisciplined process. This article describes the three phases of a formal review, the essential\nactivities within each phase, why each activity is essential, the risk assumed when an activity\nis not performed, and lessons learned from actual appraisals and independent reviews.\n\nAn independent assessment is a team activity in which the team leader and team mem­\nbers are not members of the organization being assessed. This allows the team to more read­\nily fulfill its charter objectively and without conflicts of interest. The methodology described\nin this article is based on established appraisal and assessment methodologies and is tailor­\nable to most types of independent assessments.\n\nAn independent review can be planned and managed as a project with a three-phase life\ncycle:\n\n**Planning and preparing for an independent review. Paraphrasing Yogi Berra, 90 percent**\nof a review is in the planning; the other 50 percent is executing the plan and delivering the\nfindings. In this phase, we emphasize the critical importance of working with the sponsor to\nidentify and document all relevant details of the review from start to finish. The overarching\nproduct of this phase is a review plan, a contract signed by sponsor and the team leader.\n\n**Executing the plan. In this phase, the review team interacts more broadly with the orga­**\nnization under review and executes the review plan as approved by the sponsor.\n\n**Preparing and delivering final findings. This section describes how the review team devel­**\nops and presents defensible findings from evidence they have collected, or from their analyses.\n\n###### Planning and Preparation\n\nGive me six hours to chop down a tree, and I will spend the first four sharpening\nthe axe. _—Abraham Lincoln_\n\nThe first phase of an independent review is the most important. The sponsor (whoever is\npaying for the review) and review team leader meet face-to-face, and develop a review plan\nthat includes a charter for the review team, a clear specification of objectives and issues to be\nresolved, a schedule and related dependencies, communications requirements, and resource\nrequirements such as funding. After the initial meeting, the team leader prepares a draft plan\nfor the sponsor’s review and approval.\n\nAs illustrated in the standard independent review life cycle in Figure 1, reviews must\nbe planned and managed as projects. That is, each should have a beginning and an end,\nsufficient resources, and a schedule that divides the review into phases with entry and exit\n\n\n-----\n\ncriteria. During the review, the team leader is responsible for monitoring and controlling the\nreview against the plan and for all communications with relevant stakeholders.\n\nThe team leader is also responsible for:\n\n###### �Re-planning as needed �Team selection, team-building, and team performance �Resolution of conflicts and impasses (must be able to motivate, adjudicate, and cajole) �Reporting results or findings to the sponsor and other stakeholders as required\nPlanning must be thoroughly documented because every planning element is a potential\nelement of risk. Elements that occasionally get overlooked include names of review partici­\npants; the ability of personnel to participate; security and travel requirements; commitment\nto schedules, particularly during the execution phase and format/content of the final report\n(PowerPoint? hard copy, Word file, or both?) so the sponsor knows what to expect.\n\n**Plan Content. When developing your review plan, include the following sections:**\n\n###### �Cover page (include date and current version numbered) and revision page �Context information (Organization size and other data, how the review came about) �Purpose and objectives (Why and what the sponsor expects to gain from the review)\n\nPlanning and Preparation Perform the Review Integrate and Report Results\nand Complete the Review\n\n\nI\n\nI\n\nI\n\nI\n\n\nFormat\n\nContent\n\nAudience\n\nTeam wrap-up\n\n\nMonitoring, Controlling, and Communications\n\nFigure 1. Standard Independent Review Life Cycle\n\n\n-----\n\n###### �Review team charter (To establish the independent review and the authority of the\n\nreview team, include a charter signed by the sponsor and the team leader that details\npreliminary requirements/scope, dependencies, and constraints of the review, and con­\nveys purpose and scope of the review to the subjects of the review)\n###### �Key participants (Sponsor, review team, interviewees, evidence providers, presenters) �Scope (Sponsor’s needs and expectations; what the team will do and deliver. Leave as\n\nlittle as possible to interpretation, and update the plan whenever scope changes.)\n###### �Schedule (Top-level at first, then add details as the review unfolds; “extreme detail” for\n\nthe execution phase)\n###### �Reference standards or models (Standards, models, etc., used for analysis by the team) �Dependencies (e.g., among participants, schedule, other activities) �Constraints (Availability of personnel, time, staffing, funding, tools, facilities, security) �Resource Requirements (Funding, people, time, facilities, tools) �Logistics (On-site support, escorts, working hours, Internet access, printers, copiers,\n\nphones)\n###### �Risk management (What might go wrong, probability, consequences, mitigation steps) �Roles/responsibilities (Who does what) �Training (What may be required to perform the review) �Security (Facility/information access, on-site escorting, visit requests, and clearances) �Communications plan (Who talks to whom, when, why, what must be documented,\n\netc.)\n###### �Management reviews (Status report, issues, triggers for ad hoc meetings) �Deliverables/format (Program or project-specific, tabletop, or formal presentation, level\n\nof detail)\n###### �Ownership of the results (Usually the agency or organization that paid for the review) �Signature page (Sponsor and review team)\n**Team Selection. After establishing the scope of the review, the team leader selects a team.**\nCandidates must have an appropriate level of engineering and management experience (rook­\nies are not recommended), and they may also need specific technical domain experience.\nTeam members must know how work is done in the context of the project or program being\nappraised. Is it a Department of Defense (DoD) acquisition program? Homeland Security?\nDefense Information Systems Agency (DISA)? Internal Revenue Service (IRS)? Find team\nmembers who have worked in those arenas. Previous review experience is recommended;\nhowever, depending on the appraisal and team size, including one or two inexperienced team\nmembers should be con sidered as a means to grow the organization’s independent assess­\nment bench strength. It is strongly recommended that candidates be able to commit full-time\n\n\n-----\n\nto the schedule. Part-time team members are a significant risk, so qualified alternates are\nrecommended.\n\nAfter the team is selected and before the execution phase begins, a full day should be\nreserved for team building where the team meets, reviews the plan (especially the schedule),\ndevelops analysis strategies, and allocates workload. The team may be asked to sign non-attri­\nbution statements and non-disclosure agreements, which guarantee that nothing observed,\nread, or heard during the review will be attributed to a person or project, and that all intellec­\ntual property rights of the organization(s) being appraised will be respected.\n\n**Initial Findings and Readiness Review. At this point the team should assess the avail­**\nability of information or evidence needed and make a preliminary feasibility assessment (is\nthere enough time/information/etc., to conduct the review as planned?), then deliver a readi­\nness assessment to the sponsor. Preliminary recommendations can range from “it appears\nthat enough information is available to complete the review—we recommend proceeding with\nthe review as planned” to “we have questions” to “we recommend changing scope” to “we\nrecommend delaying the review—you need more time to prepare.”\n\nTable 1. Risks Assumed When Planning Activities Are Not Performed\n\n|Activity|Risk|\n|---|---|\n|Team leader and sponsor meeting to develop preliminary inputs|Weak basis for further planning: Scope poorly defined Schedule ambiguous No authority to proceed|\n|Establish a written charter|No formal contract between the sponsor and the review team Without authority from the sponsor, the review becomes a lower priority in the organization|\n|Obtain and review initial program information|Too many assumptions Reduced objectivity|\n|Select and build a team|Inappropriate knowledge and skill sets Inconsistent review/analysis methods among sub-teams Team member absenteeism|\n|Develop initial issues|Time lost trying to focus the review at a later stage|\n|Develop the review team’s methodology|Inconsistent findings Challenges to findings|\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\n � [Meet with the sponsor, not a delegate. ] � [Don’t start the review without a signed ]\n\ncharter and a signed review plan.\n###### � [Expect the review to be seen an intrusion ]\n\nor new impediment to progress by the\n\nsubjects of an independent review. They\n\nwill, of course, want to be fully engaged in\n\nthe day-to-day activities of their project.\n\nAsk the sponsor to send a copy of the\n\ncharter to those who will be involved in\n\nthe review. This will establish the team’s\n\nauthority and its level of access.\n###### � [Keep scope as narrow as possible in ]\n\norder to produce supportable and usable\n\nfindings.\n###### � [Activities in scope must be achievable. ] � [Establish an understanding with the spon­]\n\nsor about the constraints that are placed\n\non the review team and its activities.\n\n###### Executing the Plan\n\n\n###### � [Schedule interviews well in advance. Ask ]\n\nfor early notification of cancellations. Be\n\nefficient with the time of those being\n\ninterviewed. They may already be stressed\n\nor behind schedule.\n###### � [Update the review plan whenever some­]\n\nthing changes, and publish revisions.\n###### � [Review team composition and interper­]\n\nsonal skills of team members are key.\n###### � [Team building really pays off. ] � [Use mini-teams wherever and whenever ]\n\npossible.\n###### � [Don’t wait until the execution phase to ]\n\nbegin planning how the team will locate or\n\nidentify the evidence they need.\n###### � [Start to finish for an assessment should ]\n\nbe 30-60 days, depending on scope and\n\nteam size.\n\n\nDuring this phase, the appraisal team interacts with the organization and its personnel. The\nteam leader briefs the appraisal plan and the organization presents contextual information\nabout itself.\n\nThe team then collects and analyzes evidence by comparing work products, observations\n(e.g., demonstrations), and oral evidence against the standard or model agreed upon for the\nreview. The team leader monitors team progress, redistributes workload as needed to main­\ntain schedule, and meets daily with the entire team to assess progress. Midway through this\nphase, the team should conduct a more detailed progress review.\n\nAfter this internal review the team leader meets with the sponsor, describes issues that\nwarrant attention, and presents recommendations, for example, to expand or reduce the\nappraisal scope and to continue or terminate the appraisal.\n\nIf the sponsor says to continue the appraisal, the team completes the preliminary findings\nusing a consensus decision-making process, which requires any negative vote to be resolved\n\n\n-----\n\nbefore there is a finding. Preliminary findings should be presented to and validated by organi­\nzation personnel involved in the appraisal. They alone are allowed to attend the presentation\nof preliminary findings because findings, if based on incomplete evidence or misinterpreted\ninterview statements, could be incorrect. After the presentation, give the organization person­\nnel a few hours to present new evidence that might change a preliminary finding. Evidence\nreview is then closed, and the team develops its final findings report.\n\n###### Best Practices and Lessons Learned:\n\n\n###### � [Have a detailed schedule and stick to it. ] � [Maintain objectivity and fairness–avoid ]\n\n“witch hunts.”\n\n\n###### � [Find ground truth and evidence to support ]\n\nconclusions.\n###### � [Report to the sponsor when independent ]\n\nassessment risks are happening (e.g., par­\n\nticipants are not participating).\n\n\nTable 2. Risks Assumed When Execution Phase Activities Are Not Performed\n\n**Activity** **Risk**\n\nLost opportunity to present evidence\n\nConfusion about who is supposed to participate in events\n\nOpening briefs\n\nTiming and location of events\n\nSchedule\n\nWasted time\n\nDetailed schedule Cancellations\n\nScramble to find rooms\n\nDiluted findings\nConsensus\n\nCustomer confusion\n\nValidation of preliminary Quality of final findings\nfindings Customer satisfaction\n\n###### Final Findings: Preparation and Delivery\n\nFinal findings, the ultimate deliverable of the review, should address all issues and questions\nidentified in the scope statement of the plan. They must be supportable (i.e., developed by\nthe review team from evidence or the results of analyses using a consensus method). The\nteam should review/polish the final findings before delivering them to the sponsor, and then\npresent them as required by the review plan. After the presentation, a record of the appraisal\n\n|Activity|Risk|\n|---|---|\n|Opening briefs|Lost opportunity to present evidence Confusion about who is supposed to participate in events Timing and location of events Schedule|\n|Detailed schedule|Wasted time Cancellations Scramble to find rooms|\n|Consensus|Diluted findings Customer confusion|\n|Validation of preliminary findings|Quality of final findings Customer satisfaction|\n\n\n-----\n\nis given to the sponsor and to others authorized by the sponsor. The sponsor alone owns the\ninformation presented in the briefing and controls its distribution.\n\nFinally, the independent review team should conduct a wrap-up session to record lessons\nlearned and to discuss possible next steps.\n\n###### Best Practices and Lessons Learned\n\n\n###### � [Stay focused until the review is over. ] � [Tiny details that are missed can spoil sev­]\n\neral weeks of excellent work.\n\n\n###### � [The team leader is not necessarily the ]\n\nbest presenter for every element of scope.\n###### � [Record lessons learned before team ]\n\nmembers return to their regular jobs.\n\n\nTable 3. Risks Assumed If Final Phase Activities Are Not Performed\n\n**Activity** **Risk**\n\nItems not covered or completely understood\nTeam review of final findings\n\nPresentation assignments not finalized\n\nAn uncoordinated presentation\n\nEstablish delivery method “Winging it” in front of a senior audience\n\nBest person not presenting a finding\n\nObvious things not covered:\n\nTime of presentation not advertised\n\nCoordinate the final findings brief Poor availability of attendees\n\nRoom reservation not made\n\nNo audio-visual setup\n\nLessons learned not tabulated\nTeam wrap-up\n\nNo coordination of potential next steps\n\n###### References and Resources\n\n“Assessment and Adaptation,” MITRE Project Leadership Handbook, The MITRE Corporation.\n\n[Clapp, J.A., and P.G. Funch, March 5, 2003, A Guide to Conducting Independent Technical](http://www.mitre.org/work/sepo/toolkits/assessment_guide.pdf)\n[Assessments, MITRE Center for Air Force C2 Systems.](http://www.mitre.org/work/sepo/toolkits/assessment_guide.pdf)\n\n[SEPO Program Assessment Toolkit, viewed February 4, 2010.](http://www.mitre.org/work/sepo/toolkits/assessment_guide.pdf)\n\n[Standard CMMI® Appraisal Method for Process Improvement (SCAMPI) Methodology.](http://www.sei.cmu.edu/reports/11hb001.pdf)\n\n[The Project Management Institute, 2008, A Guide to the Project Management Body of](http://marketplace.pmi.org/Pages/ProductDetail.aspx?GMProduct=00101095501)\n_[Knowledge (PMBoK Guide), 4th Ed.](http://marketplace.pmi.org/Pages/ProductDetail.aspx?GMProduct=00101095501)_\n\n|Activity|Risk|\n|---|---|\n|Team review of final findings|Items not covered or completely understood Presentation assignments not finalized|\n|Establish delivery method|An uncoordinated presentation “Winging it” in front of a senior audience Best person not presenting a finding|\n|Coordinate the final findings brief|Obvious things not covered: Time of presentation not advertised Poor availability of attendees Room reservation not made No audio-visual setup|\n|Team wrap-up|Lessons learned not tabulated No coordination of potential next steps|\n\n\n-----\n\n-----\n\n### SE Life‑Cycle Building Blocks\n\n##### MITRE\n\n\n### SE Life‑Cycle Building Blocks\n\n##### MITRE\n\n\n-----\n\n###### Introduction\n\nMITRE systems engineers (SEs) orchestrate the complete development of a system, from a\nneed through operations to retirement, by applying a set of life-cycle building blocks. SEs\nare expected to understand and work with fundamental building blocks for engineering\nsystems, regardless of the specific life-cycle methodology used. They are expected to define\nsystems conceptually, transform user needs into system requirements, and develop and assess\narchitectures. They are expected to compose and assess alternative design and development\napproaches; develop test and certification strategies; monitor and assess contractor efforts in\ndesign, development, integration, and test; and assist with field deployment, operations, and\nmaintenance.\n\n###### Background\n\nAll systems engineering models and processes are organized around the concept of a life\ncycle. Although the detailed views, implementations, and terminology used to articulate\nthe SE life cycle differ across MITRE’s sponsors and customers, they all share fundamental\nelements.\n\nFor example, Department of Defense (DoD) Instruction 5000.02 [1] uses the following\nphases: materiel solution analysis, technology development, engineering and manufacturing\ndevelopment, production and deployment, and operations and support; however, this concep­\ntualization of the system life cycle is by no means unique.\n\nISO/IEC 15288 [2] is an international systems engineering standard covering processes\nand life-cycle stages. It defines a set of processes divided into four categories: technical, proj­\nect, agreement, and enterprise. Example life-cycle stages described in the document are: con­\ncept, development, production, utilization, support, and retirement. The International Council\non Systems Engineering (INCOSE) uses a consistent approach in its Systems Engineering\nHandbook, version 3.1 [3].\n\nA V-model [4] is a common graphical representation of the systems engineering life cycle\n(Figure 1). The left side of the V represents concept development and the decomposition\nof requirements into function and physical entities that can be architected, designed, and\ndeveloped. The right side of the V represents integration of these entities (including appropri­\nate testing to verify that they satisfy the requirements) and their ultimate transition into the\nfield, where they are operated and maintained. The model we use in this guide is based on\nthis representation. For each phase, we have written articles that succinctly describe the major\nactivities in each cycle. They are summarized in below.\n\n\n-----\n\nFigure 1. V-model\n\n###### Concept Development\n\nThis first phase is concerned with transforming a user’s expression of an operational need\ninto a well-defined concept of operations, a high-level conceptual definition, and a set of initial\noperational requirements. Articles in this topic area include “Operational Needs Assessment,”\n“Concept of Operations,” “Operational Requirements,” and “High-Level Conceptual\nDefinition.”\n\n###### Requirements Engineering\n\nIn this phase, detailed system requirements are elicited from the user and other stakeholders,\nthe requirements are further analyzed and refined, and plans and processes for managing the\nrequirements throughout the rest of the system life cycle are developed. With today’s com­\nplex systems, there is always a degree of instability and uncertainty with the requirements,\n\n\n-----\n\nso methods to accommodate this are included as well during this phase. Articles in this topic\narea include “Eliciting, Collecting, and Developing Requirements,” “Analyzing and Defining\nRequirements,” and “Special Considerations for Conditions of Uncertainty: Prototyping and\nExperimentation.”\n\n###### System Architecture\n\nOnce the requirements are expressed and folded into a management process, a system\narchitecture can be described. The architecture will be the foundation for further develop­\nment, integration, testing, operation, interfacing, and improvement of the system as time\ngoes on. In the system architecture articles, we discuss various architecture patterns (e.g.,\nservice-oriented architecture), architectural frameworks (e.g., DoDAF [architectural frame­\nwork]), and formal processes for developing architectures. Articles in this topic area include\n“Architectural Frameworks, Models, and Views,” “Approaches to Architecture Development,”\nand “Architectural Patterns.”\n\n###### System Design and Development\n\nAt this point in the system life cycle, a complete and comprehensive description of what\nand how the system is expected to perform has been developed along with an architectural\nrepresentation to guide the actual design and development of the hardware, software, and\ninterfaces. Articles in this topic area include “Develop System-Level Technical Requirements,”\n“Develop Top-Level System Design,” and “Assess the Design’s Ability to Meet the System\nRequirements.”\n\n###### Systems Integration\n\nDuring the design and development phase, all of the system’s subsystems are complete. In this\nnext system integration phase, the system’s components and its interfaces with other systems\nare integrated into an operational whole. Articles in this topic area include “Identify and\nAssess Integration and Interoperability (I&I) Challenges,” “Develop and Evaluate Integration\nand Interoperability (I&I) Solution Strategies,” “Assess Integration Testing Approaches,” and\n“Interface Management.”\n\n###### Test and Evaluation\n\nBecause the system is completely designed at this point, it is now necessary to test the system\nto see if it fulfills the users’ needs (verification) and all of the defined requirements (valida­\ntion). Testing at this phase also involves properties such as reliability, security, and interoper­\nability. Articles in this topic area include “Create and Assess Test and Evaluation Strategies,”\n\n\n-----\n\n“Assess Test and Evaluation Plans and Procedures,” and “Create and Assess Certification and\nAccreditation Strategies.”\n\n###### Implementation, Operations and Maintenance, and Transition\n\nFinally, to ensure a successful transition of the system into the field, plans and procedures\nmust be developed for operations and maintenance. Because the technological underpinnings\nof a system are constantly changing, product improvements—including the insertion of new\ntechnologies—must be planned for.\n\n###### Other SE Life-Cycle Building Blocks Articles\n\nThis topic is a staging area for articles on subjects of relevance to SE Life-Cycle Building\nBlocks but that don’t neatly fit under one of its other topics. In most cases, this is because the\nsubject matter is at the edge of our understanding of systems engineering, represents some of\nthe most difficult problems MITRE SEs work on, and has not yet formed a sufficient critical\nmass to constitute a separate topic.\n\nThe system life cycle just described is rarely, if ever, as linear as this discussion might\nimply. There are often iterative cycles, missing phases, overlapping elements, etc. Additionally,\nprocesses and activities may apply to more than one phase in a system life cycle, which are\nbetter envisioned as threading through or overarching the other building blocks. Articles\nin this topic area include “Spanning the Operational Space—How to Select Use Cases and\nMission Threads,” “Acquiring and Incorporating Post-Fielding Operational Feedback into\nFuture Developments: The Post-Implementation Review, Test and Evaluation of Systems of\nSystems,” and two articles on modeling and simulation —“Verification and Validation of\nSimulation Models” and “Affordability, Efficiency, and Effectiveness (AEE).”\n\n###### References and Resources\n\n1. Department of Defense, December 8, 2008, “Operation of the Defense Acquisition\n\nSystem,” Instruction Number 5000.02.\n\n2. ISO/IEC 15288, 2002, Systems Engineering—System Life Cycle Processes.\n\n3. International Council on Systems Engineering (INCOSE), January 2010, INCOSE Systems\n\nEngineering Handbook, Ver. 3.2, INCOSE-TP-2003-002-03.2.\n\n4. Wikipedia contributors, “V-Model,” Wikipedia, accessed January 13, 2010.\n\n\n-----\n\n###### SE Life‑Cycle Building Blocks Contents\n\nConcept Development 275\nOperational Needs Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279\nConcept of Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284\nOperational Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290\nHigh-Level Conceptual Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296\n\nRequirements Engineering 301\nEliciting, Collecting, and Developing Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304\nAnalyzing and Defining Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314\nSpecial Considerations for Conditions of Uncertainty:\nPrototyping and Experimentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\n\nSystem Architecture 324\nArchitectural Frameworks, Models, and Views . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327\nApproaches to Architecture Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\nArchitectural Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n\nSystem Design and Development 347\nDevelop System-Level Technical Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\nDevelop Top-Level System Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361\nAssess the Design’s Ability to Meet the System Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\n\nSystems Integration 378\nIdentify and Assess Integration and Interoperability (I&I)\nChallenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381\nDevelop and Evaluate Integration and Interoperability (I&I)\nSolution Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386\nAssess Integration Testing Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390\nInterface Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 396\n\nTest and Evaluation 402\nCreate and Assess Test and Evaluation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\nAssess Test and Evaluation Plans and Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413\nVerification and Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419\nCreate and Assess Certification and Accreditation Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425\n\nImplementation, O&M, and Transition 433\n\nOther SE Life-Cycle Building Blocks Articles 436\nSpanning the Operational Space—How to Select Use Cases and\nMission Threads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438\nAcquiring and Incorporating Post-Fielding Operational Feedback\ninto Future Developments: The Post-Implementation Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\nTest and Evaluation of Systems of Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451\nVerification and Validation of Simulation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461\nAffordability, Efficiency, and Effectiveness (AEE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\n\n\n-----\n\n##### Concept Development\n\nDefinition: Concept development is a set of activities that are carried out early\n\n_in the systems engineering life cycle to collect and prioritize operational needs_\n\n_and challenges, develop alternative concepts to meet the needs, and select a_\n\n_preferred one as the basis for subsequent system or capability development and_\n\n_implementation._\n\nKeywords: analysis, concept, definition, development, exploration, requirements,\n\n_systems engineering_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to develop and agree\n\nupon a working description and view of how the systems or capabilities\n\nto be developed will be used, how they will function in their expected\n\nenvironments, what top-level requirements they will satisfy, and their\n\nhigh-level conceptual design. MITRE SEs are expected to be able to\n\nuse a variety of approaches to elicit user needs and explore and assess\n\nalternative concepts to meet them, including prototypes (see the article\n\n“Competitive Prototyping” in the SEG’s Acquisition Systems Engineering\n\nsection) and experiments that involve users, developers, and integrators.\n\n###### Context\n\nConcept development takes place early in the systems engineering life\n\ncycle. The success of the subsequent development of a system or\n\n\n-----\n\ncapability can be critically dependent on the soundness of the foundation that is laid during\nthe concept development stage. In their definitions of concept development, Kossiakoff and\nSweet [1] highlight phases of needs analysis (valid need and practical approach), concept\nexploration (performance to meet the need, feasible cost-effective approach), and concept defi­\nnition (key characteristics that balance capability, operational life, and cost).\n\nIn this guide, concept development is described as four activities that identify and charac­\nterize user needs:\n\n**1. Operational Needs Assessment: The application of operational experience to iden­**\n\ntify and characterize gaps in existing capabilities that are significant impediments to\nachieving the mission-area objectives.\n**2. Concept of Operations: A description of a proposed system’s characteristics in terms of**\n\nthe needs it will fulfill from a user’s perspective.\n**3. Operational Requirements: Statements that formally, unambiguously, and as com­**\n\npletely as possible, identify the essential capabilities and associated performance\nmeasures.\n**4. High-Level Conceptual Definition: A clear description or model of the characteristics**\n\nor attributes needed to address a specific set of requirements or capabilities.\nMITRE systems engineers (SEs) should understand that, like the environment, opera­\ntional needs and requirements cannot be viewed as static. User needs change, their priorities\nchange, and the technology to enable them changes. This means that requirements cannot be\nviewed as cast in stone with subsequent systems engineering aligned to an inflexible baseline.\nTrade-off analyses may be required more or less continuously to ensure effective capabilities\nare delivered to meet users’ immediate and evolving needs.\n\nMany processes, methods, and tools are available for conducting concept development.\nOf critical importance are the initial questions that must be answered early to get the require­\nments elicitation done right. These questions apply whether developing a product (system,\ncapability, or service) or an operational structure to employ the product. MITRE SEs must ask\nmore than “who, what, where, when, why, and how.” They must develop specific questions to\naddress broader issues, such as:\n\n###### �What are the current deficiencies and gaps? �What are the external constraints? �What are the real-world performance drivers? �What are the operational, security, and support concepts? �Is it feasible technically, economically, and in a timely manner? �What are the associated, external, and interfacing activities? �What happens if any of the questions above cannot be answered?\n\n\n-----\n\nThe insight obtained from such questions will likely lead to a preferred concept to satisfy\nthe end users’ needs and provide a sound basis for development. MITRE SEs should seek\nout guidance on customer-specific elements and approaches (i.e., Air Force [AF] Concept\nDevelopment) [2].\n\n###### Best Practices and Lessons Learned\n\n\nQuestions to Ask (adapted from [1]):\n\nTo meet the need, have at least two alternative\n\nconcepts been developed and evaluated? The\n\npurpose of alternatives is to stimulate thinking to\n\nfind simpler, faster, or cheaper solutions.\n\nWhat technologies does each concept depend\n\non? Have they been critically assessed for matu­\n\nrity? Are there more mature technologies that can\n\nsupport the concepts? A number of high-level\n\ngovernment studies concluded that the develop­\n\nment of risky new technology to support a major\n\nacquisition program is a leading contributor to\n\ncost and schedule overruns and failure to deliver.\n\nIncreasingly, government departments and agen­\n\ncies are requiring mature technologies before\n\nallowing an acquisition program to go on contract.\n\nIs the proposed solution right-sized economi­\n\ncally? Would delivering 80 percent of the solution\n\ndelivered early be of greater value to accomplish­\n\ning mission success? Beware attempts to satisfy\n\nthat last, lone requirement before getting capabili­\n\nties to the users.\n\n###### References and Resources\n\n\nHave external interface concepts, require­\n\nments, and complexities, including depen­\n\ndencies on other programs, been identified\n\nand addressed? Are there one or more specific\n\n“in-hand” alternatives that will enable the concept\n\nto be negotiated or realized, particularly when the\n\nconcept relies on capabilities to be delivered by\n\nthird-party programs over which your program\n\nhas no control or little influence? Complex, ill\ndefined external requirements and interfaces can\n\nbe a major source of requirements instability dur­\n\ning the development phase. This can be important\n\nwhen a system must operate in a system-of\nsystems environment.\n\nAre concepts and requirements firmly tied to\n\noperational and mission success versus indi­\n\nvidual user/organization preferences? Achieving\n\ncapabilities or demonstrating critical subsystems\n\nthat transcend individual perspectives while\n\nmeeting operational timelines is important for\n\nachieving service quickly and cost effectively, and\n\nto begin the process of incremental improve­\n\nments based on operational experience, needs,\n\nand capability evolutions.\n\n\n1. Kossiakoff, A., and W. Sweet, 2003, Systems Engineering: Principles and Practice, John\n\nWiley & Sons, Inc.\n\n2. Air Force Policy Directive 10-28, September 15, 2003, AF Concept Development.\n\n\n-----\n\n###### Additional References and Resources\n\nAir Force Studies Board Division on Engineering and Physical Sciences, 2008, Pre-Milestone\nA and Early-Phase Systems Engineering: A Retrospective Review and Benefits for Future Air\nForce Systems Acquisition.\n\nInternational Council on Systems Engineering website.\n\nInternational Organization for Standardization, 2008, “ISO/IEC Standard 15288, Systems and\nSoftware Engineering—System Life Cycle Processes.”\n\nRechtin, E., 1991, Systems Architecting: Creating and Building Complex Systems, Prentice Hall.\n\nStevens, R., P. Brook, K. Jackson, and S. Arnold, 1998, Systems Engineering: Coping with\n_Complexity, Prentice Hall._\n\nThe MITRE Corporation, “Concept Definition,” MITRE Systems Engineering Competency\nModel, section 2.1, accessed February 22, 2010.\n\nWikipedia contributors, “Systems Engineering,” accessed February 22, 2010.\n\n\n-----\n\nDefinition: An operational needs\n\n_assessment identifies and_\n\n_characterizes gaps in existing_\n\n_capabilities that are significant_\n\n_impediments to achieving the_\n\n_mission-area objectives. It does_\n\n_so through the application of_\n\n_operational experience in a_\n\n_particular mission/business_\n\n_area, knowledge of related pro­_\n\n_cesses and elements involved_\n\n_in conduct of the mission, and_\n\n_knowledge of the mission’s_\n\n_objectives and measures of_\n\n_success/effectiveness._\n\nKeywords: acquisition devel­\n\n_opment program, capability-_\n\n_based assessment, CBA,_\n\n_operational needs assessment,_\n\n_program management_\n\n\nCONCEPT DEVELOPMENT\n###### Operational Needs Assessment\n\n**MITRE SE Roles and Expectations: The opera­**\n\ntional or capability-based needs assessment\n\nis typically the responsibility of the operational\n\nrequirements organization of the system’s or\n\ncapability’s end user. MITRE systems engineers\n\n(SEs) are often requested to support such\n\nassessments, or even develop complete assess­\n\nments for review and approval. Key roles for\n\nMITRE SEs in this process may include ensuring\n\nthat operational needs statements are clear and\n\ncomplete; understanding and conveying areas\n\nof uncertainty or flexibility; ensuring analyses\n\ncontain appropriate attributes and associated\n\nmetrics supported by analytical or operational\n\nevidence, and are clearly tied to operational\n\ngoals; modeling/prototyping/experiment­\n\ning on needs/gaps for clarity, feasibility, and\n\n\n-----\n\nintegration; assessing technical readiness/risk/feasibility of technology-driven capability\nneeds; and identifying risk/cost drivers in capability needs.\n\nMITRE SEs should have a sound understanding of the principles of needs assessment, the\nneeds assessment process of the supported government element, and the political, business,\noperational, and technical (enterprise) context of the capability area of interest, as well as\nhow the government customer/sponsor intends to continue evolution and sustainment of the\nproduct following product delivery.\n\n###### Background\n\nOperational needs assessments are frequently the initial step toward a new development or\nprocess improvement program. A needs assessment is conducted by the user community to\ndetermine the best capabilities that will help users accomplish their operational tasks. These\nassessments are accomplished through operational experiments, exercises, modeling and\nsimulation of user tasks/operations, etc. From these, users formulate their needs and require­\nments. Assessment products are the basis for applicable technology assessments, solution\nalternatives analyses, operational requirements definitions, and ultimately the acquisition pro­\ngram (or programs). An operational needs assessment defines the business and mission need\nfor providing systems, services, capabilities, or platforms to end users and other stakeholders,\nand develops a business case that justifies the return on investment in order to obtain funding\nfor a system or multiple systems [1, 2].\n\nNew needs can arise for a number of reasons: new goals (e.g., manned mission to Mars),\nnew conditions (new or improved threats, deterioration/discontinuation of existing capabili­\nties, hardware/software end of life), changing processes/regulations (new laws, new organiza­\ntional responsibilities, new relationships), introduction of new technologies that enable previ­\nously unattainable capabilities or enable improved or more cost-effective solutions to existing\ncapabilities, etc.\n\nWhy do we perform operational needs assessments? First, we are typically required to\ndo so by Department of Defense (DoD), Federal Aviation Administration, Internal Revenue\nService, and other formal organizationally run programs. But even if not required, an\nassessment of operational needs and lessons learned provides greater understanding by\nthe user, acquisition, development, and support communities so that needs can be satisfied\nwith capabilities, products, or improved processes valuable to mission success. The govern­\nment has limited resources to obtain its goals. Operational needs must be described and\nquantified in the context of operational user needs and goals for decision makers to assess\ntheir validity, importance, and urgency in the context of competing needs, and determine\nthe risk of not obtaining the capability. If new needs can be most cost-effectively met by\nchanges to DOTLPF (Doctrine, Organization, Training, Logistics, Personnel, Facilities), new\n\n\n-----\n\nmateriel solutions may not be necessary. In any case, the operational needs must be defined\nand quantified in a manner that allows for assessment of the most cost-effective solution\nalternatives for the need.\n\n###### Process\n\nIn the context of a systems engineering life cycle, an operational needs assessment forms the\nbasis for defining requirements for a program and a system. It occurs as an initial step in the\nlife cycle but also must be continuously addressed as operational environments, evolutionary\nstrategies, priorities, and funding change. As part of initial program planning, MITRE is fre­\nquently involved in the establishment of systems engineering processes, of which operational\nneeds assessment is an important one. The process elements are:\n\n###### �Determine the specific requirements of the needs assessment process that apply. �Identify specific stakeholders in this particular needs assessment process, including\n\ntheir responsibility, goals, and roles/relationships.\n###### �Identify and obtain support from operational or capability domain experts. �Develop a needs assessment plan and schedule, which can include scenario-driven\n\nexperiments, gap analysis, trade-off studies, etc.\n###### �Identify and put in place any analytical tools necessary to define and quantify needs. �Implement and conduct needs assessment.\n\n Operational Needs Considerations\n\nAs an example, the DoD Joint Capability Integration and Development System (JCIDS) pro­\ncess [3, 4] includes several steps leading to an operational requirements document (capabil­\nity development document [CDD]) for acquisition of a system. Although other government\ndepartments and agencies may have different specifics, the basic approach has general appli­\ncability. It begins with a capabilities-based assessment that identifies the mission, the capabili­\nties required, and their associated operational characteristics and attributes, capability gaps,\npotential solutions (e.g., processes, systems, technologies), and associated operational risks. If\na DOTLPF assessment determines that a new system (materiel) capability is required, an ini­\ntial capability document (ICD) is developed. The ICD contains the definition of the capabilities\nneeded along with their important attributes and associated metrics. This is used as the basis\nfor an analysis of alternatives (AoA), which provides quantitative cost/effectiveness trades for\nalternative approaches to providing the capability. The results of this AoA are then used to\ndevelop the solution-approach specific CDD.\n\nThe ICD is the repository of the capability needs assessment results. The needs statements\nshould have the following attributes:\n\n\n-----\n\n###### �Enterprise and Operational Context: It is important that needs be considered in an\n\nenterprise context. If related enterprise capabilities can address part of the need, define\nthe unique characteristics of the new need in this context.\n###### �Complete (End-to-End) Need Defined: Ensure that the need is defined as completely\n\nas possible (e.g., detect, identify, and defeat incoming cruise missiles vs. detect incom­\ning cruise missiles). Recognize where areas of uncertainty remain or areas of flexibility\nexist.\n###### �Conditions/Scenario: Define the conditions/scenario under which the capability/need\n\nwill exist (e.g., indications and warning vs. major combat operations, jamming vs. clear,\ncommunications/power outage).\n###### �Attributes/Metrics: Consider quantifiable metrics for the capability that define how\n\nmuch, how well, how often, and how quickly the capability must perform. These met­\nrics should be directly related to mission goals. Again, recognize where areas of uncer­\ntainty remain or flexibility exist.\n###### �Growth/Extensibility: If current needs are expected to increase or expand in the future,\n\nstate those expectations so that expandability/extendibility of solution approaches can\nbe properly taken into account and hooks are put in place to enable those extensions.\nNote that making design choices that favor enhanced adaptability is always prudent.\n###### �Independent of Solution Approach: If needs are stated as a particular solution\n\napproach, they can eliminate consideration of more effective approaches to meeting\nthe actual need. Thus, needs should be articulated in terms that describe a successful\noperation or mission instead of a proposed solution.\n\n###### Lessons Learned\n\n\nBeware solutions masquerading as needs.\n\nOperational or capability needs are often rep­\n\nresented by users in terms of specific solution\n\napproaches. This can result from marketing or\n\ntechnology demonstrations, familiarity with a\n\nspecific solution approach, or preference for a\n\nspecific solution/approach due to unstated (or\n\nunrecognized) aspects of the need (political, eco­\n\nnomic, etc.). The challenge is to extract from the\n\nusers the full definition of the underlying capabil­\n\nity needed, and obtain stakeholder concurrence\n\nthat the focus needs to be on those identified\n\n\ncapabilities, not a solution-based approach. The\n\nbest approach for understanding the needs is by\n\nobserving and talking with the actual end users. It\n\nmay be a challenge to get their time and access.\n\nIf so, consider a user “surrogate,” including MITRE\n\nemployees with recent operational experience.\n\nState needs unambiguously. Key attributes\n\nand metrics are frequently missing, stated in\n\nambiguous terms, or stated with no corroborat­\n\ning analysis or evidence basis. The challenge is to\n\nclarify needs in unambiguous terms, with attri­\n\nbutes and metrics directly related to mission goals\n\n\n-----\n\n(measures of effectiveness), and supported by\n\nanalysis or operational evidence.\n\nGet all relevant views. Operational needs can\n\nbe driven by a subset of the key stakeholders\n\n(e.g., system operators vs. supported operational\n\nelements), and thereby miss key capability needs.\n\nThe challenge is to ensure that all key stakehold­\n\ners’ needs are taken into consideration.\n\nOne size may (or may not) fit all. The union of\n\na set of needs may lead to a solution that is too\n\ncumbersome to implement cost-effectively.\n\nRemember that multiple solutions to subsets of\n\nneeds, or satisfying additional needs by iterative\n\nsolution augmentations, may sometimes be the\n\nmost practical approach, assuming operational\n\nneeds can be adequately met. Methods such\n\nas modeling and simulation and prototyping/\n\nexperimentation allow an examination of the\n\n###### References and Resources\n\n\nneeds-satisfaction approaches and evolution\n\nand help plan the augmentations that best satisfy\n\noperational needs and missions over time.\n\nThe educated consumer is the best customer.\n\nParticularly in the case of new technology-driven\n\nneeds, operational requirements contributors can\n\nbe unfamiliar with potential capabilities, the user’s\n\nconcept of operations or concept of use, organi­\n\nzational, and political implications. The challenge\n\nis to educate users on capabilities, limitations,\n\ncost drivers, and operational implications of the\n\nnew technologies so that the capability delivered\n\nprovides the best cost/performance balance for\n\nthe customer. Prototyping and experimentation,\n\nparticularly with heavy user involvement, can help\n\neducate not only the end user, but the SEs as\n\nwell. For best practices and lessons learned, see\n\nthe article “Competitive Prototyping” in the SEG’s\n\nAcquisition Systems Engineering section.\n\n\n1. The MITRE Corporation, “Concept Development,” MITRE Systems Engineering\n\nCompetency Model, section 2.1, accessed February 22, 2010.\n\n2. An example of a document that contains operational needs is “U.S. Citizenship and\n\nImmigration Service Concept of Operations,” October 22, 2009, USCIS Transformation\nProgram.\n\n3. Joint Capabilities Integration and Development System (JCIDS), March 1, 2009, Chairman\n\nof the Joint Chiefs of Staff Instruction 3170.01.\n\n4. Manual for the Operation of the Joint Capabilities Integration and Development System,\n\nupdated July 31, 2009.\n\n\n-----\n\nDefinition: A Concept of\n\n_Operations (CONOPS) is a_\n\n_user-oriented document that_\n\n_“describes systems character­_\n\n_istics for a proposed system_\n\n_from a user’s perspective. A_\n\n_CONOPS also describes the_\n\n_user organization, mission, and_\n\n_objectives from an integrated_\n\n_systems point of view and is_\n\n_used to communicate overall_\n\n_quantitative and qualitative_\n\n_system characteristics to_\n\n_stakeholders [1].”_\n\nKeywords: concepts, CONOPS,\n\n_operational concept descrip­_\n\n_tion, operational concepts,_\n\n_operational scenarios, system_\n\n_concepts, use cases, user_\n\n_needs, user/system roles,_\n\n_viewpoints_\n\n\nCONCEPT DEVELOPMENT\n###### Concept of Operations\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to understand\n\nand recommend the development and use of a\n\nCONOPS as a tool throughout the systems engi­\n\nneering life cycle to communicate user needs and\n\nsystem characteristics to developers, integrators,\n\nsponsors, funding decision makers, and other\n\nstakeholders. In some cases MITRE SEs may be\n\nasked to support the development of a CONOPS.\n\nMITRE SEs should be able to apply systems engi­\n\nneering methods to map user (operational) needs\n\nto system requirements, functions, and conceptual\n\nsystem designs. They should also be able to develop\n\ntest requirements that are traceable to system\n\nrequirements and user needs. In addition, they\n\nshould test operational concepts (concept valida­\n\ntion) and user utility as described in the CONOPS.\n\n\n-----\n\n###### Background\n\nThe Office of Management and Budget defines a CONOPS as describing “the proposed system\nin terms of the user needs it will fulfill, its relationship to existing systems or procedures,\nand the ways it will be used. CONOPS can be tailored for many purposes, for example, to\nobtain consensus among the acquirer, developers, supporters, and user agencies on the\noperational concept of a proposed system. Additionally, a CONOPS may focus on communi­\ncating the user’s needs to the developer or the developer’s ideas to the user and other inter­\nested parties [2].”\n\nThe purpose of a CONOPS is to describe the operational needs, desires, visions, and\nexpectations of the user without being overly technical or formal. The user, developer, or\nboth may write CONOPS, often with help from MITRE SEs. The CONOPS written by a user\nrepresentative communicates the overall vision for the operational system to the organizations\n(e.g., buyer, developer) that have a role in the system acquisition and/or development effort. A\nCONOPS can also be written by the buyer, developer, or acquirer to communicate their under­\nstanding of the user needs and how a system will fulfill them. In both cases, the CONOPS is\nintended to facilitate a common understanding of ideas, challenges, and issues on possible\nsolution strategies without addressing the technical solution or implementation; it is often a\nfirst step for developing system requirements.\n\nAs systems continue to evolve in complexity, SEs and mission owners can use a CONOPS\nto develop and sustain a common vision of the system for all stakeholders over the sys­\ntem’s life cycle. The original CONOPS written at the beginning of system acquisition should\nbe updated after developmental and operational testing, to convey how the system being\nacquired will actually be used. This update is needed since many final systems include some\nadditional capabilities not originally envisioned at program start, and may not include some\ncapabilities that were omitted during trade-off analysis. The CONOPS should include the full\nrange of factors that are needed to support the mission (i.e., doctrine, organization, training,\nleadership, materiel, personnel, facilities, and resources). Post-fielding life cycle costs often\ndwarf those of the development effort. Therefore, it is critical that the CONOPS provide suf­\nficient information to determine long-term life cycle needs such as training, sustainment, and\nsupport throughout capability fielding and use.\n\nA CONOPS should contain a conceptual view of the system (i.e., a preliminary func­\ntional flow block diagram or operational architecture) that illustrates the top-level functional\nthreads in the proposed system or situation. A CONOPS should define any critical, top-level\nperformance requirements or objectives stated either qualitatively or quantitatively (includ­\ning system rationale for these objectives). The SE should consider the CONOPS as a functional\nconcept definition and rationale from the user and customer perspectives.\n\n\n-----\n\nMultiple CONOPS guidelines, models, and methodologies are available that can be tai­\nlored as needed for particular environments or situations. A MITRE SE should be able to deter­\nmine which CONOPS format, model, or methodology is appropriate for the specific situation,\nand if (or how) it should be tailored for that system/environment. Johns Hopkins University’s\nWhiting School of Engineering provides an approach to making this decision based on SE\nanalysis of criteria:\n\n###### �Program risks �Customer desires, requirements �Funding constraints �Market considerations �Technology considerations �Nature of the system to be developed.\n\n Sample Methodology\n\nThe Institute of Electrical and Electronics Engineers (IEEE) Standard 1362-1998 (IEEE Std\n1362-1998), IEEE Guide for Information Technology—System Definition—Concept of Operations\n_(ConOps), is an example of a well-developed and commonly used SE CONOPS guideline._\nSeveral SE organizations, including the International Council on Systems Engineering\n(INCOSE), currently use the IEEE CONOPS guidelines, which state:\n\nThis guide does not specify the exact techniques to be used in developing the ConOps\ndocument, but it does provide approaches that might be used. Each organization that\nuses this guide should develop a set of practices and procedures to provide detailed\nguidance for preparing and updating ConOps documents. These detailed practices and\nprocedures should take into account the environmental, organizational, and political\nfactors that influence application of the guide [1].\n\n###### CONOPS Objectives\n\nIn the situation where the operational user has not developed a CONOPS, MITRE SEs should\nselect or recommend a CONOPS guideline or model, and the objectives for developing a\nCONOPS. They should also consider any guidelines that have been put in place by the organi­\nzation. The main objective of a CONOPS is to “communicate with the end user of the system\nduring the early specification stages to assure the operational needs are clearly understood\nand incorporated into the design decisions for later inclusion in the system and segment speci­\nfications [1].”\n\nRegardless of who develops the CONOPS, frequent interaction is needed among the\nend users, MITRE SEs, acquisition organizations, and development, test, and security\n\n\n-----\n\nstakeholders. It may also be the case that the operational user does not understand or cannot\nenvision how new capabilities will operate in their environment, particularly if it is a new\ntype of system or operation. In these cases, experiments and prototypes can be of value in\nilluminating these issues. Additional CONOPS objectives include:\n\n###### �Provide end-to-end traceability between operational needs and captured source\n\nrequirements.\n###### �Establish a high-level basis for requirements that supports the system over its life cycle. �Establish a high-level basis for test planning and system-level test requirements. �Support the generation of operational analysis models (use cases) to test the interfaces. �Provide the basis for computation of system capacity. �Validate and discover implicit requirements.\n\n Critical CONOPS Components\n\nWhen tailoring IEEE Standard 1362-1998 CONOPS for a specific purpose, noncritical com­\nponents can be deleted or minimized. However, any CONOPS should always include critical\ncomponents. These components are contained in IEEE Standard 1362-1998 (discussed below):\n\n###### �The existing system (manual or automated) the user wants to replace. �Justification for a new or modified system (including restrictions on that system). �A description of the proposed system. �Scenarios highlighting use of the system in the user’s environment, including internal\n\nand external factors.\nFor a software-intensive capability, the CONOPS might have a greater emphasis on the\ninformation system perspective of the users’ needs and developers’ products, concentrating on\nsoftware feasibility and software requirements.\n\n###### Systems Engineering Applications for a CONOPS\n\nMITRE SEs should be able to use various iterations of a CONOPS as a tool throughout the\nsystems engineering life cycle to communicate user needs and system characteristics to\ndevelopers, integrators, sponsors, funding decision makers, and stakeholders. IEEE Standard\n1362-1998 guidance on the application of a CONOPS provides additional clarification. “The\nConOps approach provides an analysis activity and a document that bridges the gap between\nthe user’s needs and visions and the developer’s technical specifications.” The ConOps docu­\nment also provides the following information:\n\n###### �A means of describing a user’s operational needs without becoming bogged down in\n\ndetailed technical issues that shall be addressed during the systems analysis activity.\n\n\n-----\n\n###### �A mechanism for documenting a system’s characteristics and the user’s operational\n\nneeds in a manner that can be verified by the user without requiring any technical\nknowledge beyond that required to perform normal job functions.\n###### �A place for users to state their desires, visions, and expectations without requiring the\n\nprovision of quantified, testable specifications. For example, the users could express\ntheir need for a “highly reliable” system, and their reasons for that need, without having\nto produce a testable reliability requirement. [In this case, the user’s need for “high reli­\nability” might be stated in quantitative terms by the buyer prior to issuing a request for\nproposal (RFP), or it might be quantified by the developer during requirements analysis.\nIn any case, it is the job of the buyer and/or the developer to quantify users’ needs.]\n###### �A mechanism for users and buyer(s) to express thoughts and concerns on possible solu­\n\ntion strategies. In some cases, design constraints dictate particular approaches. In other\ncases, there may be a variety of acceptable solution strategies. The CONOPS document\nallows users and buyer(s) to record design constraints, the rationale for those con­\nstraints, and to indicate the range of acceptable solution strategies [1].\n\n###### Best Practices and Lessons Learned\n\n\nUser’s perspective. Use tools and/or techniques\n\nthat best describe the proposed system from the\n\nusers’ perspective and how it should operate.\n\nSimple and clear. Describe the system simply\n\nand clearly so that all intended readers can fully\n\nunderstand it.\n\nUser’s language. Write the CONOPS in the user’s\n\nlanguage. Avoid technical jargon. If user jargon is\n\nemployed, provide a glossary that translates it for\n\nnonusers.\n\nGraphics. Use graphics and pictorial tools as\n\nmuch as possible because a CONOPS should be\n\nunderstandable to different types of stakeholders.\n\n(Useful graphical tools include, but are not limited\n\nto, node-to-node charts, use cases, sequence\n\nor activity charts, functional flow block diagrams,\n\nstructure charts, allocation charts, data flow\n\n\ndiagrams, object diagrams, storyboards, and entity\n\nrelationship diagrams.)\n\nOperational environment. Describe the opera­\n\ntional environment in detail to give the readers an\n\nunderstanding of the assumptions, constraints,\n\nnumbers, versions, capacity, etc., of the opera­\n\ntional capability to be used.\n\nPhysical environment, safety, security, and\n\nprivacy. Describe those aspects of the physical\n\nenvironment, safety, security, and privacy that\n\nexert influence on the operation or operational\n\nenvironment of the proposed system.\n\nVoluminous descriptions. Include voluminous\n\ndescriptions, such as a data dictionary, in an\n\nappendix, or incorporate them by reference.\n\n\n-----\n\n###### References and Resources\n\n1. IEEE Computer Society, March 19, 1998, IEEE Guide for Information Technology—System\n\n_Definition—Concept of Operations (ConOps) (IEEE Std 1362-1998)._\n\n2. Office of Management and Budget, December 5, 1994, Operational Concept Description\n\n(OCD), Data Item Description DI-IPSC-81430.\n\n###### Additional References and Resources\n\nFairley, R. E., R. H. Thayer, and P. Bjorke, April 18–22, 1994, Proceedings of the First\n_International Conference on Requirements Engineering, pp. 40–47._\n\n\n-----\n\nDefinition: Operational require­\n\n_ments, the basis for system_\n\n_requirements, “identify the_\n\n_essential capabilities, associ­_\n\n_ated requirements, perfor­_\n\n_mance measures, and the_\n\n_process or series of actions to_\n\n_be taken in effecting the results_\n\n_that are desired in order to_\n\n_address mission area deficien­_\n\n_cies, evolving applications or_\n\n_threats, emerging technologies,_\n\n_or system cost improvements_\n\n_[1].” The operational require­_\n\n_ments assessment starts with_\n\n_the Concept of Operations_\n\n_(CONOPS) and goes to a_\n\n_greater level of detail._\n\nKeywords: concept definition,\n\n_concept development, opera­_\n\n_tional requirements, require­_\n\n_ments attributes, stakeholders,_\n\n_user needs, user requirements,_\n\n_users_\n\n\nCONCEPT DEVELOPMENT\n###### Operational Requirements\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be\n\nable to understand the users’ needs based\n\non the operational needs assessment (i.e.,\n\nwhat mission-area capability gaps need to be\n\naddressed). They must be able to analyze the\n\nneeds identified by the capability gaps and\n\ndevelop or assist in defining the operational and\n\ntop-level characteristics or requirements of\n\nthe system. They also should use the concept\n\nof operations (CONOPS) to understand the\n\noperational needs, desires, visions, expectations,\n\nperformance requirements, and challenges of\n\nthe system. MITRE SEs, together with the users,\n\ndevelopers, and integrators, assist in defining the\n\nsystem operational requirements, ensuring the\n\nrequirements map to the operational needs\n\n\n-----\n\nassessment and CONOPS. They work closely with the users to define and develop operational\nrequirements that are reasonable and testable.\n\nMITRE SEs are expected to be able to lay out an evolutionary strategy for the require­\nments that identifies and prioritizes initial capabilities and subsequent capability increments\nto be implemented over time. This approach allows for rapid delivery of initial capabilities\nand enables agility in delivering future capabilities that are responsive to changes in the\noperational environment. MITRE SEs are responsible for identifying and assessing condi­\ntions, constraints, conflicting requirements, and organizational issues, including safety and\nsecurity factors, and reaching a resolution. They will typically work to gain user agreement\non the operational requirements, including refining and changing requirements, throughout\nthe system development process. For more information on CONOPS, see the SEG’s Concept\nDevelopment topic.\n\n###### Background\n\nA key process in the concept development phase is analysis to define the operational require­\nments of the system. Operational requirements are typically prepared by a team of users, user\nrepresentatives, developers, integrators, and MITRE SEs and are based on the identified user\nneed or capability gaps (see the article “Operational Needs Assessment”). Establishing opera­\ntional requirements forms the basis for subsequent system requirements and system design in\nthe system design and development phase.\n\nThe operational requirements focus on how the system will be operated by the users,\nincluding interfaces and interoperability with other systems. The requirements establish how\nwell and under what conditions the system must perform. The operational requirements\nshould answer:\n\n###### �Who is asking for this requirement? Who needs the requirements? Who will be operat­\n\ning the system?\n###### �What functions/capabilities must the system perform? What decisions will be made\n\nwith the system? What data/information is needed by the system? What are the perfor­\nmance needs that must be met? What are the constraints?\n###### �Where will the system be used? �When will the system be required to perform its intended function and for how long? �How will the system accomplish its objective? How will the requirements be verified?\n\n Process\n\nThe operational requirement definition process includes the following activities:\n\n###### �Identify stakeholders who will or should have an interest in the system throughout its\n\nentire life cycle.\n\n\n-----\n\n###### �Elicit requirements for what the system must accomplish and how well. Doing this in\n\nthe form of operational scenarios and/or use cases can be particularly helpful in discus­\nsions with end users.\n###### �Define constraints imposed by agreements or interfaces with legacy or co-evolving\n\nenabling systems.\n###### �Establish critical and desired user performance: thresholds and objectives for opera­\n\ntional performance parameters that are critical for system success and those that are\ndesired but may be subject to compromise in order to meet the critical parameters. To\nassess the feasibility of meeting performance, consider, suggest (if appropriate), and\nhelp formulate prototypes and experiments to determine whether near-term capabilities\ncan satisfy users’ operational performance needs. Results of the prototypes can help\ndetermine an evolutionary strategy to meet critical performance. Additionally, technol­\nogy assessments can help gauge when desired performance might be met in the future.\n###### �Establish measures of effectiveness and suitability: measures that reflect overall cus­\n\ntomer/user satisfaction (e.g., performance, safety, reliability, availability, maintainabil­\nity, and workload requirements) [2]. Many of these measures will be used for the test\nand evaluation life-cycle building phase.\nThis process is consistent with the standard process for determining any level of require­\nments. See the SEG’s Requirements Engineering topic for a discussion on eliciting, analyz­\ning, defining, and managing requirements and discussions on the characteristics of “good”\nrequirements (e.g., concise, necessary, attainable, verifiable, traceable, implementation free,\nevolvable). It is also important to establish a requirements baseline that is kept under configu­\nration control (see the SEG’s Configuration Management topic). Together with the rationale,\nthis provides an established and complete audit trail of decisions and changes that were made.\nThe configuration baseline will also identify and manage the trade-offs of satisfying near-term\nrequirements versus allocating requirements over the evolution of a system.\n\n###### Challenges\n\nAs you work to determine user needs, capabilities, and requirements, there are likely to be\nchallenges and complications, including:\n\n###### �It’s not clear who the user is. �Needs are not well stated or understood by the user or customer and therefore not\n\nunderstood by the developer and integrator.\n###### �What is stated may not be what is really needed. �Needs are too detailed and focus on a solution. �Implicit or unreasonable expectations may not be achievable. �Customer or user changes occur during the system development process.\n\n\n-----\n\n###### �Needs often evolve or change. Sometimes this is necessary, but “requirements creep”\n\nshould always be critically assessed. Does it contribute to an immediate, impor­\ntant need? Is it technically feasible? Is it likely to work in the targeted operational\nenvironment?\n###### �The concept may not solve the problem. �Users don’t know about current technology.\nMITRE SEs should work to overcome these challenges by getting close to the users to\nunderstand their needs and environment; help them understand the realm of the possible\nwith current technical capabilities; and create demonstrations for the users illustrating what is\npossible to meet their immediate and future needs.\n\n###### Documentation\n\nThe operational requirements are captured in a document, model, or specification (e.g., user\nrequirements document, operational requirements document, or capabilities development\ndocument). The type of document is dependent on the type of acquisition and the customer\norganization, (e.g., Department of Defense, Federal Aviation Administration, Department of\nHomeland Security, Internal Revenue Service, or other government agency). Whatever name\nand form these documents take, they provide a basic framework for the articulation and\ndocumentation of operational requirements to be used by all stakeholders. The complexity of\nthe intended system and its operational context will govern the required level of detail in the\noperational requirements document. Examples and formats of these documents are found in\nthe references.\n\n###### Best Practices and Lessons Learned\n\n\nThe following tips from active systems engineer­\n\ning practitioners may help your work through the\n\nconcept development phase and the operational\n\nrequirements development process:\n\nWork with the end users early and often. Be\n\nsure to fully understand their mission, operational\n\ndomain, and most important, their constraints. It\n\nis helpful to talk their “language.” This allows for an\n\neasier exchange of ideas, resolution of conflicts,\n\netc. Participate in training and exercises that the\n\nuser community is involved in to get a firsthand\n\nperspective of the operational environment.\n\n\nCreate mutually beneficial interactions.\n\nDetermine the users’ needs by using mutually\n\nbeneficial studies or analyses, including modeling\n\nand simulation, prototypes, and demonstrations\n\nwhere appropriate. These help the users justify\n\nand defend capability needs while providing the\n\nacquisition organization with requirements and\n\nCONOPS to start system development, testing,\n\nand fielding.\n\nOrganize your thinking before engaging users. It\n\nis often difficult for users to develop requirements\n\nfrom scratch. Draft your understanding of their\n\n\n-----\n\nrequirements prior to engaging with them and\n\ncreate a straw man for discussion. This provides\n\na good starting point for discussion. They will tell\n\nyou if it is wrong. Demonstrations of your under­\n\nstanding using executable models or prototypes\n\nof capabilities will help both you and the users to\n\nengage on the operational needs and realm of the\n\npossible.\n\nHelp users understand new technology. Provide\n\nusers with suggestions on how they might employ\n\na new technology. Often, users cannot see past\n\nhow they do business today. Introducing them\n\nto technology might help them break loose from\n\ntheir thought processes, develop new processes,\n\nand possibly rethink or refine some requirements.\n\nConsider the use of prototypes to help demon­\n\nstrate possibilities and show users the technical\n\naspects of a potential solution as they identify\n\ntheir operational needs and consider gives-and\ntakes based on solution feasibility and constraints.\n\nExplain technology limitations clearly and\n\nsimply. Clearly and simply explain the limitations\n\nof the technology to users, including maturity and\n\nassociated risk. This helps ensure that require­\n\nments are achievable, secures their buy-in on\n\nwhat is possible, and stimulates them to think\n\nabout how to use what they can get. Again,\n\nconsider using prototypes or experiments of\n\n###### Summary\n\n\ncapabilities that can help bring technology issues\n\nto the forefront with users.\n\nEngage users throughout the process. It is\n\nimportant to stay engaged with the user com­\n\nmunity through the system development process.\n\nBreak down barriers and overcome incorrect and\n\nbad perceptions. Ensure that users are involved\n\nin the decision making process. Ensure that they\n\nare involved in subsequent decision making that\n\nconcerns trade-offs affecting operational utility\n\nor performance. Keep users apprised of sched­\n\nule and capability impacts. This builds trust and\n\ncooperation, facilitates quick turn times on ques­\n\ntions, and helps ensure that the users’ needs and\n\nobjectives are met.\n\nMake user satisfaction a priority. Make cus­\n\ntomer/user satisfaction a key metric for your\n\nprogram.\n\nMake delivery to the users a primary driver.\n\nGetting capabilities to users early and often is the\n\nbest strategy. The users have a mission to satisfy,\n\nand every capability that can help them is needed\n\nas soon as feasible. Evolve the deliveries over time\n\nbased on priorities, associated capability, and\n\nfeasibility of implementation.\n\nBuild a foundation of trust. The greatest likeli­\n\nhood of making good decisions occurs when the\n\nusers and acquisition communities trust each\n\nother.\n\n\nThe following summary points can help the development of operational requirements:\n\n###### �Requirements define problems while specifications define solutions. �Make sure your operational requirements are product, service, and solution agnostic\n\n(i.e., they do not assume or target a certain solution).\n\n\n-----\n\n###### �Make the solution space broad. �Keep it simple; make it easy for a reader to understand the problem and requirements\n\nthat address it [3].\nProject success is rooted in understanding operational requirements. This requires the\nuser and acquisition communities and other stakeholders to invest the time and effort both\nearly in the concept development process and throughout the development cycle. Skillfully\ndone, this should result in a greater likelihood of fielding a capable initial system and subse­\nquent evolutions that meet user needs within schedule and cost.\n\n###### References and Resources\n\n1. Kossiakoff, A., and N. Sweet, 2003, Systems Engineering Principles and Practices, Wiley &\n\nSons.\n\n2. International Council on Systems Engineering (INCOSE), January 2010, INCOSE Systems\n\n_Engineering Handbook, Version 3.2, INCOSE-TP-2003-002-03.2, p. 58._\n\n3. Celluci, T., November 2008, Developing Operational Requirements: A Guide to the Cost\n_Effective and Efficient Communication of Needs, version 2.0, Department of Homeland_\nSecurity.\n\n###### Additional References and Resources\n\nBahil, T., and F. Dean, 1997, “The Requirements Discovery Process,” SAND—96-2901C, Sandia\nNational Laboratories, Albuquerque, NM.\n\nESC/EN Requirements Process Toolkit, ENwebWiki, The MITRE Corporation.\n\n“Requirements Management,” ENwebWiki, The MITRE Corporation.\n\nThe SEPO Requirements Process Toolkit, The MITRE Corporation.\n\n\n-----\n\nDefinition: High-level concep­\n\n_tual definition (HLCD) is the_\n\n_explicit construction of the_\n\n_ideas or concepts needed to_\n\n_understand what a system,_\n\n_product, or component is,_\n\n_what it does, and how it is best_\n\n_used. An HLCD is used by the_\n\n_operational users or, more_\n\n_generally, the stakeholder_\n\n_community. The HLCD may_\n\n_also address what a product is_\n\n_not, what it doesn’t do, and how_\n\n_it is not well used. The HLCD_\n\n_reflects a shared point of view,_\n\n_conveying a clear description or_\n\n_model of the characteristic or_\n\n_attributes needed to address a_\n\n_specific set of requirements or_\n\n_capabilities._\n\nKeywords: acquisition program,\n\n_concept definition, concept_\n\n_development, early systems_\n\n_engineering_\n\n\nCONCEPT DEVELOPMENT\n###### High-Level Conceptual Definition\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to develop\n\nor help develop a high-level conceptual definition\n\nduring the concept development phase of system\n\ndevelopment. They are expected to assess\n\nthe full breadth of the solution space (trade\n\nspace) and to consider, refine, discard, or adopt\n\nalternative concepts. This assessment is useful\n\nas the life cycle continues into acquisition and\n\ndevelopment. It is a key input to performing the\n\nanalysis of alternatives to support the acquisition\n\napproach (see the article “Performing Analyses\n\nof Alternatives”). MITRE SEs are also expected\n\nto take operational requirements and translate\n\nthem into a concept that provides the stake­\n\nholder community with a clear and unambiguous\n\ndefinition of the capability, system, product, or\n\n\n-----\n\ncomponent. They are expected to use this concept definition to guide users in refining\nrequirements, reconsidering the concept of operations or employment, and exploring fun­\ndamental implementation approaches. Techniques such as prototyping and experimentation\nwith user community involvement can help highlight aspects of the trade space and illumi­\nnate alternatives for addressing user operational concepts and needs.\n\nIn developing an HLCD, MITRE SEs are expected to create a user-centered view that\nfacilitates user/stakeholder discussion focused on further system requirements development\nand specification.\n\n###### Background\n\nThe HLCD process, especially the concept definition, is a useful tool for establishing a com­\nmon framework or construct early in the systems engineering and product development\ncycle. (Note: Don’t exclude elements in the initial concept that may be beyond the scope of the\nsystem or product eventually specified. They may help stimulate thinking.) Though seem­\ningly an obvious initial step in the solution development process and basic systems engineer­\ning, frequently the clear articulation of a high-level concept definition is omitted because it is\nbelieved that such a definition is implicit knowledge among the group (engineers, acquisition\nprofessionals, developers, integrators, users, etc.) or because a detailed design solution is “in\nhand,” thus obviating the need for the higher level composition.\n\nGiven the diverse experiences of a typical team, the assumption that even a small num­\nber of engineering, acquisition, development, and integration professionals share a common\nunderstanding of a complex system is likely to yield disappointing results. More often, engi­\nneering, acquisition, and user perspectives diverge at some point, and failure to tie solution\ndevelopment to a common view (the conceptual definition) may allow these differing ideas\nto go unchallenged and lead to significant disagreements and capability, schedule, and cost\nimpacts later in the design cycle.\n\nProceeding directly to a more detailed design solution and bypassing an analysis of the\ntrade space performed as an important step in developing the HLCD can lead to an expedi­\ntious, but inefficient solution. More effort and resources may eventually be expended to adapt\nthe proposed solution to the user needs, due to discoveries late in the systems engineering\nprocess.\n\nIn either case, the result can be a solution requiring extensive rework to meet the basic\nuser expectations—often at considerable cost and delivery delay.\n\n###### Conceptual Definition Process\n\nAs part of the early life-cycle systems engineering process, HLCD uses the operational needs\nassessment, concept of operations (CONOPS), operational requirements, initial capability\n\n\n-----\n\nstatements, articulated high-level stakeholder requirements, and an understanding of the\ndomain to lay the foundation for a definition of user expectations and a further understand­\ning of the solution space. The process of HLCD involves a set of steps for translating capability\nstatements or operational requirements into a recognizable concept or model. The process\nbegins by identifying the central capabilities or main objectives of the effort, and proceeds\nby organizing a set of descriptors aimed at helping illustrate critical attributes of the central\nobjectives. Throughout this process, a deeper understanding of the users and their require­\nments is developed and captured in the outline or model that characterizes the concept defini­\ntion; this will later support further system design.\n\nThe form this outline or model captures can vary, and depends on many factors, includ­\ning the complexity of the concept and the breadth of the stakeholder community. One form of\nthis outline or model is a conceptual definition map, shown in Figure 1. This map helps the\nSE explore a spectrum of factors that must be considered to fully chart user expectations and\ntranslate them into a concise definition.\n\nStep-by-step considerations for completing this map (see Figure 1) include:\n\n1. Begin by capturing the main objective(s) or necessary capabilities (green box). These\n\nconcise statements explain the need, which is clearly defined from the user point of\nview. They are written for the broader stakeholder and acquisition (including systems\nengineering) communities.\n2. Proceed to identify stakeholders and their roles and objectives (blue box). The MITRE\n\nSE will need to know or ascertain who will be involved in the various aspects of the\nwhole solution across development, use, modification, and sustainment of the sys­\ntems and capabilities supporting the objectives in Step 1. This step also identifies the\nstakeholder community that will use the concept definition as the basis for exploration\nof the solution space, and eventual decisions on program direction, acquisition, and\nfurther solution development.\n3. Describe the key properties of the concept (orange box). These meaningful statements\n\ndescribe the basic properties of the concept so that the full stakeholder community can\neasily and uniformly understand the needs and objectives of the users.\n4. Identify the products, information, or consumables required to meet user requirements\n\n(aqua box). These items should tie into the needs of the user and stakeholder commu­\nnity and support the CONOPS and concept of employment.\n5. Describe major technical, operational, and organizational interfaces (e.g., to other\n\nproducts, systems, domains, data/information, or communities) (yellow box). This\nportion of the map describes how the concept and user fit into the large enterprise or\ninteract with other elements of the domain or mission area.\n\n\n-----\n\nWho will use it?\n\n\n###### 2\n\n\n###### 4\n\n\nWhat does it\nprovide?\n\n\nCapabilities\n\nProducts or\ninformation\n\nG\n\nG\n\nG\n\nEtc.\n\n\n###### 1\n\n\nStakeholder\n\nStakeholder\n\nG\n\nG\n\nG\n\nStakeholder\n\n\nWhat does it\ninterface with? What is it like?\n\n###### 5 3\n\n\nSystem\n\nData source\n\nGG\n\nGG\n\n\nKey property\n\nKey property\n\nG\n\nG\n\n|on Main Ob or Cap s it with? How m nee rce When is it|S jectives abilities S W K uch is ded? K needed? G|\n|---|---|\n\n\nGG\n\nEtc.\n\n\n###### 6\n\n\nG\nG\n\nG\n\nG\n\nKey property\n\nEtc.\n\nWhat are the\nconstraints?\n\n\nFigure 1. Conceptual Definition Map\n\n6. Articulate constraints (gray box). Describe all constraints, especially those that may\n\nhelp bound the solution space during later design evolution. In particular, cost or\nschedule constraints may be driving factors in defining the extent of possible future\nsolutions. It is not the intent during this portion of the systems engineering process\nto eliminate specific solutions, but to express those constraints that may be critical in\nshaping the future solution.\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nAs with any element of systems engineering,\n\npotential hazards must be negotiated when\n\napplied to complex integrated systems. These\n\ninclude:\n\nNarrowing the solution space too quickly.\n\nBecoming too focused on a single approach,\n\ntechnology, or solution during concept definition\n\nand eliminating viable (and possibly better) alter­\n\nnatives early in the process.\n\nNarrowing the solution space too slowly. Too\n\nmuch exploration of alternative approaches,\n\nexcessive waiting for technologies to mature, or\n\nworking to an expectation of finding a solution\n\nthat addresses all the consideration of all the\n\nstakeholders can lead to analysis paralysis and\n\nfailure to deliver in a reasonable time period.\n\nInsufficient stakeholder engagement. Failing to\n\nfully engage the end-user/stakeholder commu­\n\nnity, and missing critical perspectives and inputs\n\nthat might shape the final concept definition. In\n\n###### References and Resources\n\n\nparticular, be sure to engage those who are not\n\nimmediate stakeholders (e.g., certification and\n\naccreditation authorities) whose considerations\n\ncan be showstoppers if they are engaged late in\n\nthe process.\n\nExcluding non-materiel solutions. Beware of\n\nthe inclination to narrow the focus of the high\nlevel concept to materiel options, intentionally\n\nor unintentionally avoiding other elements of\n\nthe possible solution including doctrine, training,\n\noperations, etc.\n\nFinally, each concept definition phase provides a\n\nnew opportunity to ensure a clear, understandable\n\nrepresentation of the users’ objectives and needs\n\nthat are developed and vetted by the stakeholder\n\ncommunity. A successful concept definition activ­\n\nity helps anchor the future design and engineer­\n\ning efforts, so that customer expectations and\n\nacquisition commitments are well managed from\n\nthe beginning.\n\n\nKossiakoff, A., and W. Sweet, 2003, Systems Engineering: Principles and Practice, Hoboken,\nNJ: John Wiley & Sons, Inc.\n\nMurch, R., 2001, Project Management: Best Practices for IT Professionals, Upper Saddle River,\nNJ: Prentice Hall PTR.\n\n\n-----\n\n##### Requirements Engineering\n\nDefinition: A requirement is a singular documented need—what a particular prod­\n\n_uct or service should be or how it should perform. It is a statement that identifies a_\n\n_necessary attribute, capability, characteristic, or quality of a system in order for it_\n\n_to have value and utility to a user. Requirements engineering is the discipline con­_\n\n_cerned with establishing and managing requirements. It consists of requirements_\n\n_elicitation, analysis, specification, verification, and management._\n\nKeywords: analysis, definition, development, elicitation, management, require­\n\n_ments, systems engineering, verification_\n\n###### Context\n\nRequirements are derived from operational needs and concepts and are\n\nused as inputs to design and development. Requirements are also an\n\nimportant input to verification, since tests must trace back to specific\n\nrequirements to determine if the system performs as intended. Require­\n\nments indicate what elements and functions are necessary for the\n\nparticular project. The typical phases of requirements development are\n\neliciting, collecting and developing, analyzing and defining, and com­\n\nmunicating and managing requirements. Because of the rapid changes\n\nin operational requirements and the pace of technology, increasingly\n\nSEs are faced with unprecedented levels of uncertainty in developing\n\nrequirements.\n\n\n-----\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to be able to integrate business, mission, and\noperational needs and transform these needs into system requirements. They elicit, develop,\nanalyze, communicate, and manage requirements as well as facilitate stakeholder engage­\nment and agreement on system requirements. They are expected to be able to decompose\noperational needs and requirements and flow them down to operational capabilities, technical\nrequirements, technical implementation, and verification of the requirements. MITRE SEs are\nexpected to ensure operational value and traceability from the operational need all the way to\nthe system verification and ultimately to the fielding and sustainment of the system. They are\nexpected to actively mitigate uncertainty in requirements through prototyping and experi­\nmentation activities.\n\n###### Discussion\n\nThe articles in this topic address the major phases of requirements engineering.\n\nRequirements engineering starts early in concept development by eliciting and collecting\noperational needs from the relevant user community, and developing requirements from the\nneeds. It involves more than talking to the user or reading their concept of operations, and\nasking them to review the requirements you created. It is a disciplined approach that includes\ncollecting, validating, prioritizing, and documenting requirements. The article “Eliciting,\nCollecting, and Developing Requirements” describes a disciplined approach that can be used\nfor different types of strategies, from classic large-scale Department of Defense block acquisi­\ntions to agile incremental acquisitions.\n\nToward the end of the eliciting and collecting phase, SEs analyze the requirements to\nensure they are sound and can form a stable basis for the duration of the planned develop­\nment and testing period. The article “Analyzing and Defining Requirements” describes\nattributes of a well-crafted requirement to minimize design missteps, confusion, and re-work\ndownstream. It also references tools that exist within MITRE to support and manage this\nphase of the requirements engineering effort.\n\nDespite best efforts, sometimes the requirements management techniques described in\nthe previously mentioned articles are insufficient. This occurs most often when the user is\nunsure of their needs or leading-edge technology is needed to meet requirements. In this\nenvironment, key tools in the MITRE SE’s toolbox are prototyping and experimentation. These\nare particularly useful for gauging whether a requirement is achievable or assessing feasibility\nand maturity of a technology to meet a requirement. The article “Special Considerations for\nConditions of Uncertainty: Prototyping and Experimentation” discusses when and how these\ntools should be applied, the different approaches, “weight” or fidelity available (and which\n\n\n-----\n\nlevel makes sense for what situations), and ideas for how to evolve the prototyping and experi­\nmentation efforts over time to reduce the risk of requirements uncertainty.\n\n###### References and Resources\n\nInternational Council on Systems Engineering (INCOSE) website, http://www.incose.org/.\n\nKossiakoff, A., W. N. Sweet, S. Seymour, and S. M. Biemer, 2011, Systems Engineering:\n_Principles and Practice, 2nd Ed., Wiley._\n\nStevens, R., P. Brook, K. Jackson, and S. Arnold, 1998, Systems Engineering: Coping with\n_Complexity, Prentice Hall._\n\n[Sutcliffe, A., 1996, “A Conceptual Framework for Requirements Engineering,” Requirements](http://www.springerlink.com/content/h743l3m48l7x78r5/?p=085a0980eeb240b595620be6947b64c6&pi=3)\n_Engineering Journal, Vol. 1, No. 3, Springer, London._\n\n“Systems Engineering,” Wikipedia, accessed March 11, 2010.\n\nThe MITRE Corporation, “Requirements Engineering,” Systems Engineering Competency\nModel, section 2.2, accessed February 4, 2010.\n\nU.S. Department of Homeland Security, Requirements Engineering, accessed March 11, 2010.\n\n\n-----\n\nDefinition: Requirements\n\n_define the capabilities that a_\n\n_system must have (functional)_\n\n_or properties of that system_\n\n_(non-functional) that meet_\n\n_the users’ needs to perform a_\n\n_specific set of tasks (within a_\n\n_defined scope)._\n\nKeywords: agile, elicitation,\n\n_elicitation techniques, project_\n\n_scope, requirements, require­_\n\n_ments attributes, requirements_\n\n_elicitation, root cause, scope,_\n\n_spiral, stakeholders, user_\n\n_requirements, users, waterfall_\n\n\nREQUIREMENTS ENGINEERING\n###### Eliciting, Collecting, and Developing Requirements\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be\n\nable to elicit business, mission, and operational\n\nneeds from operational users and other stake­\n\nholders. They are also expected to be able to\n\nanalyze, integrate, and transform these needs\n\ninto system requirements as well as facilitate\n\nstakeholder engagement on and resolution\n\nof requirements. MITRE SEs are expected\n\nto be able to tailor the principles of require­\n\nments elicitation to different development\n\nmethodologies (waterfall, spiral, agile, etc.).\n\n\n-----\n\n###### Overview\n\nAfter operational needs are assessed and the concept of operations (CONOPS) and high-level\nconcept definition are completed, the next step—and typically the first task on development\nprojects—is to discover, elicit, collect, define, and analyze requirements. Requirements will\ncover various aspects of a capability or system—user needs, behavioral, quality, implementa­\ntion, etc. Given these, the SE will analyze, transform, and integrate users’ needs into system\nrequirements. For more information on the first steps in development projects, see the SEG’s\nConcept Development topic.\n\nFigure 1 highlights a typical process for collecting and evaluating requirements.\nAllocating sufficient time and effort to the requirements process to build a strong foundation\nfor the effort has proven to be cost-effective in the long run.\n\nFigure 1 represents typical sequencing of many of the activities and milestones that are\npart of the requirements collection and management processes. Activities may be added,\nmodified, deleted, and their sequences changed, depending on the scope and type of project\n\nRequirements Requirements Requirements Requirements\nbroadly covered details validated prioritized documented\n\nIdentify Real\nRoot CausePerform Problemand/or requirementsmodify, anddeleteAdd, No requirementsdetailedRevise No prioritizingContinue No requirementsDocumentas needed& revise\nAnalysis\n\nYes\n\nObtain Discover Stake- Details Stake- Are\n\nstakeholders,and locateContactSMEs summaryapprovalprojectscope high-levelrequire-& elicitments concur?holdersStake- Yes require-detailedholdersprovidements validated?clarified& Yes prioritizerequire-holdersments require-priorit-mentsized? requirementsare correct &Usersagree\nrelevant complete, &\ndocuments & customer\nresourcesother Add, modify, delete, and track requirements and maintain documentation approvesspec?\n\nIncorporate changes Proposed changes\n\nPre-launch\nInvestigate Yes\n\nrequirementsStart Yes Solutions?options?\ncollection COTS?\n(CONOPsneeded)process changesWere Are Cost-benefits?Prototypes? Set up a formatrequirements“Freeze”\n\napproved usingchange Yes requirementschanges LaunchPlan process forchanging\nmanagement in scope? Implements requirements\nprocess? using\n\nReiterate, go back waterfall,\n\nspiral, or\nagile\n\nNext step, forward No No model\n\nQuestion, evaluate, decide Review change\n\nrequest\n\nActivities after requirements Design, develop, test, etc. until project (or part 1 of a\n\nphased implementation) ends\n\ncollection ends\n\nFigure 1. Overview of Requirements Collection and Change Processes\n\n\n-----\n\nor task. Generally, subtasks within a larger project focus on fewer activities and may have dif­\nferent stakeholders and finer grained criteria for success than the project itself.\n\nThe process the SE follows depends on the project’s complexity and implementation\nmethodology: waterfall, spiral, agile, etc. Studies have shown that accurate, well-defined, and\nclearly stated requirements reduce development time and effort and are essential to the qual­\nity and success of the final product. Users provide functional and nonfunctional requirements,\nwhich form the substrate on which the project is built. Functional requirements are associated\nwith the capability/application need to directly support the users’ accomplishment of their\nmission/tasks (features, components, etc.). Performance requirements are those that are typi­\ncally implicit and technical in nature that emerge as system requirements to satisfy the users’\nfunctional needs (e.g., quality of service, availability, timeliness, accuracy). SEs work closely\nwith users to observe, discuss, and understand the user requirements.\n\n###### �Waterfall model: Projects using the waterfall model progress through a series of\n\nphases/milestones in a linear fashion, with the first phase dedicated to the require­\nments task. The first milestone occurs when a complete set of functional, performance,\nand other requirements has been documented, validated, and approved by the user.\nStabilizing requirements early in the project’s life cycle facilitates subsequent project\nwork and significantly reduces risk. This type of model can be feasible in the increas­\ningly rare situations when the customer mission or business is fairly static, the need is\nfocused, and the user environment is stable.\n###### �Spiral model: Each cycle or level in the spiral model includes several activities found in\n\nvarious phases of the waterfall model. This model is used to reduce project risk incre­\nmentally. At the end of each cycle, stakeholders analyze risks and develop appropri­\nate risk reduction strategies for use at the next level. The SE collects, documents, and\nupdates requirements before the project starts and after each cycle. The requirements\nmay be known up front, but spirals are used to get capabilities to the users quicker,\nor the requirements may not be completely known up front, but the basic operational\nneeds and concepts are known, so projects can begin and allow the future evolution’s\nrequirements to be determined over time. The first milestone usually occurs early in the\nspiral under these conditions: requirements for the spiral are complete and agreed to by\nthe user concurrently with an operational concept, plans, design, and code.\n###### �Agile model: The agile software development model does not require detailed docu­\n\nmentation and design at start-up but does require flexible systems engineering support\nduring the project. Typically small efforts are performed and the set of requirements is\nfocused on small, specific capabilities with the users and developers teaming to work\nthe interplay of requirements and capabilities together. “Agile” emphasizes very short\ncycles, substantial user collaboration from start to finish, close teamwork, constant\n\n\n-----\n\ncommunication among participants, the ability to adapt to change, and incremental\ndevelopment. The goal is to quickly develop working functional software that meets\nthe users’ needs, not produce detailed requirements or documentation. The SE may\nwear several hats in an agile environment by providing support as needed, for example:\nidentifying emerging requirements that may violate standards and regulations; analyz­\ning, then documenting requirements as they evolve; calculating metrics; and writing\nfunctional specifications, test cases, meeting minutes, and progress reports.\n###### �Using multiple models: More than one model can be used during a project’s develop­\n\nment. Regardless of the particular model, all approaches should include requirements\nelicitation in some form. The activities in the Best Practices (below) are often associated\nwith the waterfall model, but many are modified for use with other models as well. SEs\nmay change, reorder, repeat, or omit activities on the list, depending on the project type,\ncomplexity, methodology, and environment. A structured approach can help guide the\nrequirements collection process from the first (i.e., “kickoff”) meeting between the SE\nand stakeholders until requirements are baselined and approved. These guidelines are\napplicable and adaptable for requirements collection on large and small systems, new\nsystems, and existing systems that are being updated or replaced. Requirements may\nalso evolve over time due to mission changes, business environment changes, etc. The\nrequirements must be managed throughout the life cycle to ensure the needed capabili­\nties are being created and delivered to accommodate changes.\nChallenges exist today with the requirements engineering process—frequently, sufficient\ntime is not allocated to understand operational concepts and thus the requirements associated\nwith them; requirements are specified, not managed to accommodate changes; requirements\nare not revisited often enough to further assess trade-offs that users would consider in order\nto manage schedule and costs. However, a good requirements process can provide a strong\nfoundation for satisfying user needs.\n\n###### Best Practices\n\n\nApply good interpersonal skills. Such skills are\n\nalways an asset, but they are a necessity when\n\neliciting requirements. When SEs are objec­\n\ntive and open-minded and have good listening\n\nskills, their relationships with users and other\n\nteam members are productive. Their ability to\n\neffectively communicate project status and\n\n\nresolve issues and conflicts among stakeholders\n\nincreases the likelihood of the project’s success.\n\nThink broadly. SEs with broad knowledge of\n\nthe enterprise in which requirements are being\n\ndeveloped (whether for a system, service, or the\n\nenterprise) add value and may be able to iden­\n\ntify solutions (e.g., process changes) that are\n\ncost-effective.\n\n\n-----\n\nBe prepared. Collect data and documents that\n\nprovide context for the project. Review data\n\ngenerated during enterprise and concept analysis,\n\nand review any business case and decision brief­\n\nings for the project. Become familiar with histori­\n\ncal information, organizational policies, standards,\n\nand regulations that may affect requirements\n\nand impose constraints. Gather information on\n\nprevious projects, successful or not, that share\n\ncharacteristics with the new project. Review their\n\nsystem specifications and other technical docu­\n\nments, if they exist. The SE may derive “explicit” or\n\n“implicit” lessons learned and requirements from\n\ndata on the previous project. Find out whether\n\nthere are descriptions of current operations, pref­\n\nerably an approved concept of operations (see\n\nthe SEG’s Concept Development topic), and any\n\ndocumented issues. Some of this material may\n\nidentify potential stakeholder types and sub­\n\nject matter experts (SMEs) that may be needed.\n\nDraft a requirements collection plan, estimate\n\nresources needed, and consider the types of tools\n\nthat would be appropriate on a project using this\n\nparticular methodology. Identify potential risks\n\nthat might arise during the requirements collec­\n\ntion process (e.g., key stakeholders are unavailable\n\ndue to time constraints) and plan risk mitigation\n\nstrategies.\n\nIdentify and manage stakeholders. A single\n\nindividual or organization often initiates a project.\n\nInevitably, the new project will affect other individ­\n\nuals, organizations, and systems, either directly or\n\nindirectly, thereby expanding the list of stakehold­\n\ners. Stakeholders’ “roles” are: the executive spon­\n\nsor funding the project and possibly a contributor\n\nto requirements; primary stakeholders and others\n\n\nproviding functional and performance require­\n\nments; stakeholders affected by the project indi­\n\nrectly (e.g., interfacing businesses and operations)\n\nwho may contribute requirements; SMEs (e.g.,\n\nmanagers, system architects and designers, secu­\n\nrity staff, and technical and financial experts); and\n\nstakeholders who must be kept in the loop (e.g.,\n\nbusiness analysts, legal and financial experts). As\n\nneeded, stakeholders should be asked to review,\n\ncomment on, and approve requirements for which\n\nthey are responsible. Set up a process for com­\n\nmunicating with stakeholders (e.g., meetings of all\n\ntypes, formal presentations, biweekly reports, and\n\nemail).\n\nDetermine the root cause of the problem.\n\nBefore requirements collection starts, it is critical\n\nthat the SE answer the question: what is the real\n\nneed that the project and its product are intended\n\nto address? The SE must tread carefully but\n\nresolutely in the user’s environment to uncover\n\nthe real vs. perceived needs. Examining some of\n\nthe concept and operational needs information\n\ncan help with the analysis. The next vital ques­\n\ntion is: have all stakeholders agreed on a clear and\n\nunambiguous need statement that is consistent\n\nwith the business case? The SE’s ability to state\n\nthe problem in an implementation-independent\n\nmanner is extremely important. Customers may\n\nfind it difficult to accept the fact that their solution\n\nto their perceived problem is not viable, or that\n\nother options should be explored.\n\nDefine capability scope. The SE generates a\n\ncapability scope that provides a framework for the\n\nproject and guides the requirements collection\n\nprocess. The capability scope is usually the first\n\nsource of information about the project available\n\n\n-----\n\nto all stakeholders before the project gets under\n\nway. It is reviewed by stakeholders and approved\n\nby the customers. The SE’s goal is to elicit and\n\ndiscover all requirements and ensure that each\n\nis within the boundaries described in the scope.\n\nThis criterion is used to evaluate requirements’\n\nchanges throughout the life cycle. Scope assess­\n\nments are not limited to the requirements phase.\n\nThey are often used to cover project activities\n\nfrom launch to completion, specific activities\n\n(e.g., pilots and testing), and for small tasks within\n\nlarger projects. Capability scopes are generated\n\nas needed, for example: before or after require­\n\nments collection, or for inclusion in a request for\n\nproposal, work breakdown structure, or statement\n\nof work. Other documents, such as PDDs (project\n\ndefinition documents) and SOOs (statements\n\nof objectives) often serve the same purpose as\n\ncapability scopes.\n\nCapability scope documents describe the\n\n“who, what, when, and why” of the project and\n\ninclude information needed for project planning.\n\nCapability scope documents cover most of the\n\ntopics below. Some top-level information for the\n\nscope can be found in the operational needs and\n\nconcepts information from the user community.\n\n###### � [Purpose:][ What problem is the customer ]\n\ntrying to solve? What does the customer\n\nneed and want? What will this project\n\nachieve?\n###### � [Value Proposition:][ Why is this capability ]\n\njustified?\n###### � [Objectives/Goals:][ High-level goals that ]\n\ncan be measured\n###### � [Sponsor:][ Who is paying for the capability? ]\n\n\n###### � [Customers:][ Who will use the results of the ]\n\nproject?\n###### � [Scope of Project:][ Activities and deliver­]\n\nables included in this project\n###### � [Out-of-Scope:][ Activities and deliverables ]\n\nnot included in this project\n###### � [Interfacing:][ What are the interfacing ]\n\ncapabilities, systems, or user communities\n\nthat will touch this project?\n###### � [Major Milestones:][ Events denoting prog­]\n\nress in the project life cycle (e.g., comple­\n\ntion of key deliverables or important\n\nactivities)\n###### � [Dates:][ When are deliverables due? What ]\n\nare the planned milestone dates?\n###### � [Critical Assumptions:][ Assumptions ]\n\nunderlying plans for conducting and com­\n\npleting the project\n###### � [Risks:][ Potential changes in the project’s ]\n\nenvironment or external events that may\n\nadversely affect the project\n###### � [Issues:][ Issues that have already been ]\n\nidentified for this project\n###### � [Constraints:][ Rules and limitations (e.g., ]\n\ntime, resource, funding) that may dictate\n\nhow the project is carried out\n###### � [Success Criteria:][ Outcomes that meet ]\n\nrequirements, targets, and goals and sat­\n\nisfy the customer.\n\nDiscover and elicit requirements from all rele­\n\nvant sources. The SE collects requirements from\n\nmany sources including, but not limited to: expe­\n\nrienced and new users, other stakeholders, SMEs,\n\nmanagers, and, if necessary, the users’ customers.\n\nOperational users are key contributors because\n\n\n-----\n\nthey provide some or all requirements for the\n\nsystem’s functional and performance capabilities\n\nand user interface. Their inputs are essential to\n\ndelivering a product, system, or service that helps\n\nimprove their efficiency by enabling them to easily\n\naccess the data they need when they need it. The\n\nSE elicits requirements directly or indirectly based\n\non users’ informal narratives, observing the user\n\nenvironment, or capturing their responses to tar­\n\ngeted questions. The SE wants to learn about the\n\noperational users’ environments and needs a lot\n\nof information for that purpose, such as: detailed\n\ndescriptions of users’ daily, weekly, monthly, and\n\nother periodic tasks; documentation (e.g., training\n\nmanuals); reporting requirements and examples of\n\nwritten reports; preconditions and/or triggers for\n\ntaking various actions; workflows and sequenc­\n\ning of specific tasks they perform; external and\n\ninternal rules they must follow including security\n\nrequirements; interactions with other operational\n\nusers, staff members, systems, and customers;\n\ntype and frequency of problems they encounter;\n\nand, overall, what does and does not work for\n\nthem currently. Users’ responses to questions\n\nsuch as “describe your ideal system” may open up\n\nareas not previously considered. The SE can con­\n\nfirm requirements collected and possibly uncover\n\nnew ones if given the opportunity to directly\n\nobserve users doing their jobs. Passive observa­\n\ntion is often time well spent.\n\nThe SE consults with SMEs to ensure that sys­\n\ntem, security, and operational requirements are\n\ncomplete and feasible; the SE also brings atten­\n\ntion to and incorporates into the requirements,\n\ngovernment and other regulations that must be\n\ntaken into account during the program. Project\n\n\nsize and type, complexity, schedule, number of\n\ninterviewees, and locations are factors that will\n\ndetermine techniques best suited to eliciting\n\nrequirements for this project. Techniques include\n\ndirect observation, one-on-one and/or group\n\ninterviews, brainstorming sessions, focus groups,\n\nsurveys and targeted questions, and prototyp­\n\ning. Joint (users, developers, integrators, systems\n\nengineering) requirements gathering sessions are\n\nfrequently one of the most powerful techniques\n\nfor eliciting requirements. When SEs analyze\n\nrelated documents, system interfaces, and data,\n\nthey are likely to discover new requirements.\n\nReverse engineering may be needed to uncover\n\nrequirements for legacy systems that are poorly\n\nor not documented. Collection activities proceed\n\nto the next life-cycle step (e.g., beginning system\n\ndesign) when users confirm that implementation\n\nof the current set of requirements will meet their\n\nneeds, and project staff agrees that they can build\n\na viable product based on these requirements.\n\nHowever, with many changes in the stakeholders’\n\noperations, a continuous requirements collection\n\nand refinement effort is needed to ensure initial\n\nrequirements are captured and future capabil­\n\nity assessments are started by examining the\n\nnext evolution of requirements due to change,\n\nincreased certainty in need, or a phased imple­\n\nmentation approach.\n\nDocument requirements’ types and attributes.\n\nCategorizing and organizing many require­\n\nments can be daunting. As the process matures,\n\nrequirements’ attributes must be documented\n\nand kept up to date to remain traceable during\n\ntesting, validation, and verification. This process\n\nhelps SEs and others identify duplicate, missing,\n\n\n-----\n\nand contradictory requirements. Attributes\n\nmost often tracked are these requirements: ID\n\n(number), description, type (e.g., functional, non\nfunctional, performance, explicit, derived, system,\n\noperational), priority (e.g., mandatory, desirable),\n\nphase (threshold or objective), level of risk, busi­\n\nness value (e.g., high, medium, or low), source\n\n(e.g., stakeholder, regulation, interface specifica­\n\ntion), rationale for including the requirement (e.g.,\n\nimproves performance), name of implementer,\n\nlevel of effort, status (e.g., proposed, approved,\n\nverified, closed), and, later, release number/release\n\ndate.\n\nModel requirements for validation. Stakeholders\n\nare frequently asked to review documents that\n\ninclude those requirements for which they are\n\nresponsible. Stakeholders sometimes need help\n\ninterpreting requirements; they also expect and\n\nare entitled to receive clear explanations of out­\n\ncomes when they are implemented. Explanations\n\ncan be facilitated by creating “as is” and “to be”\n\nprocess flows, activity diagrams, use cases, entity\n\nrelationship diagrams, workflow models, and\n\nflowcharts. Models can also be in the form of\n\nprototypes or experiments to provide a limited\n\nfunctioning context where users can try out vari­\n\nous alternatives, and together the user and SE can\n\nassess the requirements (see the article “Special\n\nConsiderations for Conditions of Uncertainty:\n\nPrototyping and Experimentation”). Visual aids\n\nthat are focused on these areas tend to engage\n\nthe stakeholders’ interest. Models show stake­\n\nholders how the requirements they contributed\n\nrepresent their statements and goals, and are\n\ncomplete and consistent. Agreeing on the mean­\n\ning of each requirement and its effect on the final\n\n\nproduct may call for several iterations of discus­\n\nsions, modifications, and reviews by different\n\ngroups of stakeholders. Putting everyone on the\n\nsame page takes time. The SE updates require­\n\nments when changes are made at reviews and\n\nmeetings and tracks issues (e.g., action items) and\n\nconflicts. When conflicts cannot be resolved, the\n\nSE brings them to the customer’s or sponsor’s\n\nattention, using other levels of MITRE manage­\n\nment, as appropriate.\n\nPrioritize requirements. As the collection\n\nprocess winds down, stakeholders are asked to\n\nassign a priority to each requirement. There may\n\nbe differences of opinion about which ones are\n\nmandatory, critical, desirable, or optional. It is up\n\nto the SE to define each priority (e.g., needs vs.\n\nwants), point out inappropriate priorities, and sug­\n\ngest changes based on knowledge of this particu­\n\nlar project and past experience. Prioritization ends\n\nwhen stakeholders reach agreement. Getting\n\nstakeholders to reach agreement can be difficult.\n\nA best practice is to develop and put in place a\n\nstakeholder contention adjudication protocol\n\nearly in the requirements elicitation process.\n\nIdentifying critical requirements is particularly\n\nimportant when evaluating competing systems\n\nand commercial-off-the-shelf products. They are\n\nalso used to evaluate deliverables at various mile­\n\nstones or evolutions, and, in projects developed\n\nincrementally, help determine which requirements\n\nare included in each phase.\n\nWork toward getting final agreement from con­\n\ntributing stakeholders. At the end of the require­\n\nments collection process, plan to hold a face\nto-face requirements review meeting attended\n\nby stakeholders who contributed requirements.\n\n\n-----\n\nInclude project team members if possible. Often,\n\nlast-minute requirements changes are needed to\n\nreach consensus that they are complete and cor­\n\nrect. At that point, requirements are considered\n\n“baselined,” “locked,” or “frozen.” But be care­\n\nful—flexibility is needed throughout the life cycle\n\nto ensure that system development and imple­\n\nmentation does not try to meet the exhaustive set\n\nof requirements when earlier delivery or perhaps\n\nreduced costs could be achieved (e.g., why the\n\nprioritization step is very important).\n\nDocument requirements for final approval.\n\nThe requirements document or specification will\n\ndictate much of the project’s future work. Before\n\nthe specification is approved, ask reviewers who\n\nknow the characteristics of “good” requirements\n\nto review it. Good requirements are unique and\n\nuniquely identified, necessary, consistent, com­\n\nplete, traceable, testable, implementation-free,\n\nattainable, unambiguous, and verifiable. It may be\n\nnecessary to modify or delete “bad” requirements\n\nand their dependents. Dependent requirements\n\nare associated requirements, many times implicit\n\nor emerging from a functional requirement (e.g.,\n\nneed for data would drive a technical need for\n\nsome form of database/repository). Final approval\n\nis made by the executive sponsors or one of the\n\ncustomers. Inevitably, some requirements in the\n\nspecification will be misinterpreted by implement­\n\ners, many of whom may be seeing them for the\n\nfirst time. To avoid or minimize misinterpretations,\n\nthe SE, optimally together with the user commu­\n\nnity, must be given time to go over the approved\n\nspecification with designers, software engineers,\n\nquality assurance staff, testers, and others to\n\nanswer questions. As soon as possible, a close\n\n\ncommunity of users, SEs, designers, developers,\n\nintegrators, and testers should be formed and\n\nmaintained.\n\nCapture lessons learned. Although MITRE inter­\n\nnal projects differ from MITRE direct customer\n\nprojects in many ways, there are commonali­\n\nties when it comes to requirements and lessons\n\nlearned. When a project ends, project members,\n\nincluding managers, are encouraged to document\n\ntheir experiences, good and bad, using the LAMP\n\n(Lessons About My Project) template. A facilitator\n\nconducts a “Lessons Learned Review” with project\n\nparticipants and uses the LAMP to identify best\n\npractices and processes that need improvement.\n\nThe LAMP site provides many practical case\n\nexamples of eliciting requirements.\n\nEven when the requirements are good, complete,\n\nand correct, as a project is launched, changes\n\nare inevitable. Experience has shown that add­\n\ning, modifying, and deleting requirements after\n\na project is underway greatly increases cost. A\n\nformal requirements management process will\n\nhelp control cost, avoid requirements creep, and\n\nensure end-to-end traceability. However, changes\n\ndo occur and flexibility is needed to manage the\n\nchanges while concentrating on delivery of capa­\n\nbilities to users as soon as feasible.\n\nRelated articles include “Analyzing and Defining\n\nRequirements” and “Stakeholder Assessment and\n\nManagement.”\n\n\n-----\n\n###### References and Resources\n\nAmbler, S. W., September 1, 2005, “Seeking Stakeholders,” Doctor Dobbs Digest.\n\n[Ambler, S. W., October 1, 2005, “Requirements Wisdom,” Doctor Dobbs Digest.](http://drdobbs.com/architecture-and-design/184415414)\n\n[Ambler, S. W., September 16, 2008, “Strategies for Addressing Non-Functional Requirements,”](http://drdobbs.com/architecture-and-design/210601918)\n_Doctor Dobbs Digest._\n\nAssociation for Computing Machinery, ACM On-line Course on Eliciting Requirements,\naccessed March 3, 2010.\n\nBahill, T., “Eliciting Requirements in Use Cases and What is Systems Engineering? A\nConsensus of Senior Systems Engineers,” accessed March 3, 2010.\n\nBruegge, B., A. H. Dutoit, and A. A. Dutoit, October 1999, “Object-Oriented Software\n_Engineering: Conquering Complex and Changing Systems,” Pearson Education._\n\nFlorence, A., April 2002, “Reducing Risks Through Proper Specification of Software\nRequirements,” CrossTalk: The Journal of Defense Software Engineering.\n\nFlorence, A., October 2003, “Requirements Validation and Verification,” QAI Journal.\n\nFlorence, A., and W. Bail, 2005, “Effective Requirements Practices,” The MITRE Corporation.\n\nGottesdiener, E., March 2008, “Good Practices for Developing User Requirements©,” CrossTalk:\n_The Journal of Defense Software Engineering._\n\nThe MITRE Corporation, “ISIS Systems Technology and Engineering Projects (iSTEP),”\naccessed March 3, 2010.\n\nThe MITRE Corporation, “Lessons About My Project (LAMP),” accessed March 2, 2010.\n\nNuseibeh, B., and S. Easterbrook, 2000, Requirements Engineering: A Roadmap, Department of\nComputing, Imperial College of Science, Technology & Medicine, London, UK.\n\nRobertson, J., and S. Robertson, February 2006, Volere Requirements Specification Template.\n\nWest Pole, Inc., 1996–2005, Use Case and Interviewing Techniques for Focused Requirements\nCapture.\n\n\n-----\n\nDefinition: The engineering\n\n_analysis that ties the needs_\n\n_of users and other stakehold­_\n\n_ers to the system to be built in_\n\n_a quantifiable and traceable_\n\n_manner._\n\nKeywords: analyze, develop,\n\n_development methods,_\n\n_measures of effectiveness,_\n\n_measures of performance,_\n\n_performance engineering,_\n\n_requirements_\n\n\nREQUIREMENTS ENGINEERING\n###### Analyzing and Defining Requirements\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be able\n\nto analyze systems requirements to determine if\n\nthey can be tested, verified, and/or validated, and\n\nare unique, complete, unambiguous, consistent,\n\nand obtainable, and to trace all requirements\n\nto original business and mission needs. They\n\nare expected to review requirements to deter­\n\nmine conformance with government policy for\n\ndeveloping the system and identify potential\n\nintegration and interoperability challenges.\n\n\n-----\n\n###### Background\n\nHow can we judge if a system meets the needs of a user community? One way is to look\nat the system requirements and compare those statements to how the system was built,\ninstalled, and operated. For large enterprise systems, traditionally there has been a sub­\nstantial lag between requirements definition and field operation of a system. This often\naffects both the effectiveness of a system and how the system is perceived (e.g., the system\nis stale and outdated: it does not meet my needs). In part, this lag is addressed by spiral\nor incremental approaches to enterprise capability development (see the article “Eliciting,\nCollecting, and Developing Requirements”). The implication for requirements is that they\nshould be defined in detail in time to support the increment or spiral in which they will\nbe developed. This mitigates the problem of locking down a requirement so early that it\ncontributes to the perception of staleness when it is finally delivered. Additionally, when\nallocating requirements to an increment or spiral, they should be stable enough over the\nincrement’s planned development and testing period that the capability can still be expected\nto meet the user needs when delivered. Beyond that, requirements analysis during concept\ndevelopment must be efficient, accurate, rational, and traceable.\n\n###### Characteristics of Good Requirements\n\nMITRE SEs encounter many types of projects and systems—from research and development,\nto technical consulting work, to acquisition. Whatever the context, a good requirements state­\nment typically has these characteristics [1]:\n\n###### �Traceable: A requirement must be traceable to some source such as a system-level\n\nrequirement, which in turn needs to be traced back to an operational need and be\nattributable to an authoritative source, whether a person or document. Each require­\nment should have a unique identifier allowing the software design, code, and test proce­\ndures to be precisely traced back to the requirement.\n###### �Unambiguous: Test the wording of the requirement from different stakeholders’ per­\n\nspectives to see if it can be interpreted in multiple ways.\n###### �Specific and singular: Needed system attributes (e.g., peak load) are described clearly\n\nas atomic, singular thoughts.\n###### �Measurable: System functions can be assessed quantitatively or qualitatively. �Performance specified: Statements of real-world performance factors are associated\n\nwith a requirement.\n###### �Testable: All requirements must be testable to demonstrate that the end product satis­\n\nfies the requirements. To be testable, requirements must be specific, unambiguous, and\nquantitative whenever possible. Vague, general statements are to be avoided.\n\n\n-----\n\n###### �Consistent: Requirements must be consistent with each other; no requirement should\n\nconflict with any other requirement. Check requirements by examining all require­\nments in relation to each other for consistency and compatibility.\n###### �Feasible: It must be feasible to develop software that will fulfill each software require­\n\nment. Requirements that have questionable feasibility should be analyzed during\nrequirements analysis to prove their feasibility. If they cannot be implemented, they\nshould be eliminated.\n###### �Uniquely identified: Each need is stated exactly once to avoid confusion or duplicative\n\nwork. Uniquely identifying each requirement is essential if requirements are to be trace­\nable and able to be tested. Uniqueness also helps in stating requirements in a clear and\nconsistent fashion.\n###### �Design-free: Requirements should be specified at the requirements level and not at\n\nthe design level. Describe the requirement functionally from a requirement point of\nview, not from a design point of view (i.e., describe the functions that the system must\nsatisfy). A requirement reflects “what” the system shall accomplish, while the design\nreflects “how” the requirement is implemented.\n###### �Uses “shall” and related words: In specifications, using “shall” indicates a binding\n\nprovision (i.e., one the specification users must implement). To state nonbinding provi­\nsions, use “should” or “may.” Use “will” to express a declaration of purpose (e.g., “The\ngovernment will furnish...”) or to express future tense.\nEach of these characteristics contributes to the integrity and quality of the require­\nments and the system or capability to be developed. Enforcing these characteristics during\nthe requirements activity helps keep the entire development effort organized and reproduc­\nible, and avoids issues later in the life cycle. The goal of a requirements process is to define a\nsystem or capability that ties the needs of the users and other stakeholders to the system to\nbe built so that it satisfies the needs within a specified schedule and cost, and possesses the\nrequired performance characteristics, including characteristics like information assurance,\nquality, reliability, internationally enabled, and sustainability. Observing the above require­\nments characteristics will help to maintain engineering rigor, content, and value of the engi­\nneering analysis.\n\n###### Measures Associated with Requirements Analysis [2]\n\nThe typical categories of measures associated with determining if a system complies with the\nrequirements include:\n\n###### �Measures of Effectiveness (MOEs): MOEs are measures of mission success stated\n\nunder specific environmental and operating conditions, from the users’ viewpoint. They\n\n\n-----\n\nrelate to the overall operational success criteria (e.g., mission performance, safety, avail­\nability, and security).\n###### �Measures of Performance (MOPs): MOPs characterize specific physical or functional\n\ncharacteristics of the system’s operation, measured under specified conditions. They\ndiffer from MOEs in that they are used to determine whether the system meets perfor­\nmance requirements necessary to satisfy the MOE.\n###### �Key Performance Parameters (KPPs): KPPs are a stakeholder-defined measure that\n\nindicates a minimal and critical system performance and level of acceptance.\n\n###### Best Practices and Lessons Learned\n\n\nBaseline and agree. Developing requirements\n\nis usually a collaborative activity, involving users,\n\ndevelopers, maintainers, integrators, etc., so avoid\n\nplacing the responsibility of requirements analysis\n\nsolely on one stakeholder. When all team mem­\n\nbers acknowledge a set of requirements is done,\n\nthis is called a baseline (realizing that this will\n\nevolve—see “Be flexible” below).\n\nRequirements analysis is an iterative process,\n\nso plan accordingly. At each step, the results\n\nmust be compared for traceability and consis­\n\ntency with users’ requirements, and then verified\n\nwith users, or go back into the process for further\n\nanalysis, before being used to drive architecture\n\nand design.\n\nPay special attention to interface requirements.\n\nRequirements must clearly capture all the interac­\n\ntions with external systems and the external envi­\n\nronment so that boundaries are clear. Remember\n\nthat external interfaces can be influenced by the\n\narchitecture of a system or subsystems. In some\n\ncases, hidden external interfaces will be estab­\n\nlished because internal subsystem-to-subsystem\n\ncommunications use an external medium (e.g.,\n\nradios connect subsystems via airwaves) or other\n\n\nassets that are not part of the system (e.g., satel­\n\nlite relay, external network). Examples of tools for\n\nrecognizing external interfaces include the DoD\n\nArchitecture Framework (DoDAF) operational and\n\nsystem views referred to as OV-1, OV-2, and SV-1\n\ndiagrams.\n\nBe flexible. To balance out rigidness of baselin­\n\ning requirements, a development team should\n\nconsider what constitutes a “change of require­\n\nments” as distinguished from a valid interpreta­\n\ntion of requirements. The key is to find a balance\n\nbetween adherence to a baseline and sufficient\n\nflexibility (e.g., to encourage innovation, support\n\nthe changing mission).\n\nUse templates and tools that suit your needs.\n\nTo get started quickly, make use of resources\n\nprovided by your sponsor organization or internal\n\nMITRE resources.\n\nUse storyboarding, use cases, campaign modeling\n\nand simulation tools, and other tools that capture\n\nusers, their activities, and the information flows.\n\nPrototyping and experiments are effective ways to\n\ncollect the information.\n\n\n-----\n\nMIL-STD-490A, “Specification Practices,” is a\n\nmilitary standard for defining specification of mili­\n\ntary systems. Although it is officially cancelled, it\n\nprovides descriptions of various types of specifi­\n\ncations and their contents.\n\nData Item Description DI-IPSC-81431A,\n\n“System/Subsystem Specification,” is a\n\ntemplate for a traditional military system or\n\nsubsystem specification. It describes the format\n\n###### References and Resources\n\n\nand contents of a traditional specification. It often\n\nrequires tailoring to a particular application. It is a\n\nuseful tool to stimulate thought on topics to be\n\nincluded in the requirements. There are special­\n\nized database tools that are designed to capture\n\nand manage requirements. A good source for\n\ninformation on available tools is the INCOSE web­\n\nsite [3] under Requirements Management. The list\n\nis free to the public.\n\n\n1. Florence, A., April 2002, “Reducing Risks Through Proper Specification of Software\n\nRequirements,” STSC CrossTalk, Vol. 15, No. 4.\n\n2. Roedler, G. J., and C. Jones, 2005, Technical Measurement: A Collaborative Project of PSM,\n\n_INCOSE, and Industry._\n\n3. INCOSE website, http://www.incose.org/.\n\n###### Additional References and Resources\n\nHo, C-W., et al., 2006, “On Agile Performance Requirements Specification and Testing,” IEEE\nAgile Conference.\n\nKossiakoff, A., and W. Sweet, December, 2002, Systems Engineering Principles and Practice,\nWiley-Interscience.\n\nLight, M., April 18, 2005, “Agile Requirements Definition and Management Will Benefit\nApplication Development,” Gartner RASCore Research Note G00126310, R1741.\n\nPerron, J., Fall 2007, “User-Centered Requirements Analysis: Who, What, When, Where, Why\n& How,” SEPO Collaborations, Vol. 5, Issue 2, The MITRE Corporation.\n\n\n-----\n\nDefinition: Prototyping and\n\n_experimentation are two closely_\n\n_related methods that can help_\n\n_systems engineers (SEs) drive_\n\n_requirements uncertainty out of_\n\n_the requirements process._\n\nKeywords: CONOPS, experi­\n\n_mentation, exploration, pro­_\n\n_totyping, requirements,_\n\n\nREQUIREMENTS ENGINEERING\n###### Special Considerations for Conditions of Uncertainty: Prototyping and Experimentation\n\n\n_uncertainty_ **MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to identify\n\nuncertainty in requirements and actively take\n\nsteps to manage and mitigate it, including con­\n\nsidering uncertainty in associated areas such as\n\noperational concepts and others (see the SEG’s\n\nConcept Development topic). MITRE SEs are\n\nexpected to understand the range and styles\n\nof prototyping and experimentation, and the\n\npotential impact of each when applied during\n\nrequirements engineering. SEs are expected to\n\nunderstand the value in having MITRE execute a\n\nprototyping activity as opposed to (or in conjunc­\n\ntion with) a contractor. They are also expected\n\nto be aware of experimental venues, events, and\n\nlaboratories that exist to support these activities.\n\n\n-----\n\n###### Background\n\nSuccessfully developing systems or capabilities to meet customers’ needs requires the ability\nto manage uncertainty when defining requirements. For example, how will analytical assess­\nments of performance or functional requirements match the reality of their implementation\nwhen the system or capability is fielded? What unintended technical, operational, or perfor­\nmance issues are likely to occur? Will technology essential to meet a requirement perform as\nexpected when using realistic user data, or when injected in operational environments and\ncontexts? Are the user concepts of operations really supportable given new technical capabili­\nties? Prototyping and experimentation are two methods that can help address these issues.\n\nPrototyping\n\n_Prototyping is a practice in which an early sample or model of a system, capability, or process_\nis built to answer specific questions about, give insight into, or reduce uncertainty or risk in\nmany diverse areas, including requirements. This includes exploring alternative concepts and\ntechnology maturity assessments as well as requirements discovery or refinement. It is a part\nof the SE’s toolkit of techniques for managing requirements uncertainty and complexity and\nmitigating their effects.\n\nThe phase of the systems engineering life cycle and the nature of the problem the pro­\ntotype is intended to address influence the use and type of prototyping. Prototyping may be\nidentified immediately after a decision to pursue a material solution to meet an operational\nneed. In this situation, prototypes are used to examine alternative concepts as part of the\nanalysis of alternatives to explore the requirements space to determine if other approaches can\nbetter meet the requirements. Prototyping to explore and evaluate the feasibility of high-level\nconceptual designs may be performed early in technology development as part of govern­\nment activities to assess and increase technology maturity, discover or refine requirements,\nor develop a preliminary design. A prototype may even be developed into a reference imple­\nmentation—a well-engineered example of how to implement a capability, often based upon a\nparticular standard or architecture—and provided to a commercial contractor for production\nas a way of clarifying requirements and an implementation approach.\n\nFor more information on prototyping, see the article “Competitive Prototyping.”\n\nExperimentation\n\n_Experimentation adds a component of scientific inquiry to the above, often supported by_\nrealistic mission/domain context. Performing experiments with a realistic context allows the\nevaluator to assess and evaluate hypotheses about concept of operations (CONOPS), feasi­\nbility of technology, integration with other systems, services, and data, and other concepts\nthat support requirements refinement. Experimentation environments, or laboratories, allow\n\n\n-----\n\nacquisition personnel, real-world users, operators, and technologists to collaboratively evaluate\nconcepts and prototypes using combinations of government, open source, and commercial-offthe-shelf products. In these environments, stakeholders can evolve concepts and approaches—\nin realistic mission contexts—and quickly find out what works and what doesn’t, ultimately\nreducing risks by applying what they’ve learned to the acquisition process.\n\nIt is useful to consider three broad stages of experimentation, which form a pipeline (see\nFigure 1):\n\n###### �Lightweight Exploration: Driven by operator needs, this stage is distinctive for its\n\nquick brainstorming and rapid assembly of capabilities with light investment require­\nments. It allows for a “first look” insight into new concepts and newly integrated\ncapabilities that can support requirements generation. MITRE’s ACME (Agile Capability\nMashup Environment) Lab is an example of a lightweight experimentation venue.\n###### �Low/Medium-Fidelity Experimentation: This stage involves significant engagement\n\nwith users, operators, and stakeholders, and is typified by human-in-the-loop simu­\nlations and possibly real-world capabilities and data with experimental design and\nattempts to control independent variables. MITRE’s Collaborative Experimentation\nEnvironment (CEE) and iLab-based “Warfighter Workshops” are two examples of low/\nmedium-fidelity experimentation venues. These venues allow concept exploration and\nalternative evaluations that can support requirements clarification.\n###### �High-Fidelity Experimentation: These experiments are planned with sponsors to\n\nrefine existing CONOPS that can support\n\nI ACME (Agile Capability requirements refinement. They often fea­\n\nLightweight Mashup Environment)\nExploration ture highly realistic models and simulations\n\nof entities, timing, sensors, and commu­\nnication networks along with some realworld applications. MITRE’s Naval C4ISR\n\nI CEE (Collaborative\n\nLow/Med. Experimentation Experimentation Lab (NCEL) is an example\nFidelity\n\nEnvironment) of a high-fidelity experimentation venue.\n\nExperimentation\n\n\nFigure 1. Experimentation Pipeline\n\n\nPrototype solutions from any of the pre­\nceding experimental stages can be used to\nsupport the requirements management pro­\ncess. Generally the products from the light­\nweight end of the venue spectrum support\nthe early stages of requirements manage­\nment (CONOPS, concept development, etc.),\nwhereas those at the high-fidelity end tend\nto support refinement of relatively mature\n\n\n-----\n\nrequirements. These solutions can also be transitioned, given appropriate circumstances, to\noperators in the field, to industry, or to other parties for evaluation or use.\n\n###### Best Practices and Lessons Learned\n\n\nBe opportunistic. The acquisition process is\n\nstructured, linear, and can, in some circum­\n\nstances, seem to stifle innovation. Embrace\n\nrequirements uncertainty as an opportunity to\n\ninject innovation and think freely.\n\nAct early in the acquisition life cycle. Prototyping\n\nearly in the acquisition life cycle serves as a\n\nmethod to reduce requirements risk and may\n\nbe of interest to program managers attempting\n\nto avoid late-acquisition-life-cycle change (e.g.,\n\nrequirements creep), especially if there is require­\n\nments uncertainty.\n\nSeek early/frequent collaboration among\n\nthe three critical stakeholders. Purely techni­\n\ncal prototyping risks operational irrelevance. It\n\nis vital to involve technologists, operators, and\n\nacquirers/integrators early and often in proto­\n\ntyping and experimentation dialogs. Operator\n\ninvolvement is particularly critical, especially in\n\nsolidifying requirements, yet it is often deferred or\n\nneglected. Three of the four recommendations\n\nfrom the 2007 Department of Defense Report to\n\nCongress on Technology Transition address this\n\ncollaboration [1]:\n\n_... early and frequent collaboration is required_\n\namong the developer, acquirer, and user. This early\n\nplanning can then serve to mitigate the chasm\n\nbetween Technology Readiness Level (TRL) 5 and\n\nTRL 7 by identifying technical issues, resource\n\nrequirements/sources, avoiding unintended\n\n\nconsequences, and ultimately gaining the most\n\nyield for the science and technology (S&T)\n\ninvestment.\n\n... if the program manager were to conduct early\n\n_and frequent communication with the developer_\n\nabout user requirements and companion acquisi­\n\ntion plans, much of the development risk could be\n\naddressed earlier in the process.\n\n... the pace at which new technologies are discov­\n\nered/innovated/developed/deployed in the private\n\nsector is staggering, and at odds with the linear,\n\ndeliberate nature of some government acquisi­\n\ntions ... . Finding ways to include these innovators\n\n_in our process could serve both the government_\n\nand America’s economic competitiveness in the\n\nworld market.\n\nUse realistic data. Prototypes using unrealistic\n\ndata often result in failure to address the require­\n\nments uncertainty or complexity the prototype\n\nwas designed to examine. MITRE SEs should take\n\nevery opportunity to capture real data from their\n\nwork and help build a repository of this data that\n\ncan be used across MITRE’s activities.\n\nUse loose couplers and open standards to\n\nisolate requirements changes. Providing loosely\n\ncoupled data integration points across com­\n\nponents in the prototype allows for changes\n\nin one area to explore some aspects of the\n\nrequirements space, while controlling others.\n\nUse open standards, including RESTful services,\n\n\n-----\n\nwhenever possible. (See articles “Design Patterns,”\n\n“Composable Capabilities on Demand (CCOD),”\n\nand “Open Source Software” in the SEG’s\n\nEnterprise Engineering section.)\n\nDevelop scalable prototypes. Consider the\n\nintended operational or systematic use of the\n\nprototype being developed. Does the technology\n\nscale? Does the operator workload scale? The\n\nperformance? Do the results collected from the\n\nprototype provide the necessary insight to ensure\n\nthat modifications to requirements are appropri­\n\nate for the actual full-scale system?\n\nPrefer rapid increments. Execute quick experi­\n\nmental iterations or spirals of prototype capability\n\ndevelopment with operator involvement to ensure\n\n###### References and Resources\n\n\nadequate feedback flow and requirements refine­\n\nment for future spirals.\n\nLook beyond MITRE for resources. MITRE\n\nresources on any given program are limited. If the\n\nMITRE project resources cannot support a proto­\n\ntyping or experimentation activity by MITRE staff,\n\nlook to other mechanisms, such as government\n\nteaming, the SBIR (Small Business Innovative\n\nResearch) process, or willing industry participants.\n\nLook beyond MITRE for venues. Consider holding\n\nan experiment on-site at a contractor facility, in an\n\noperational setting, at a sponsor training facility, or\n\nat other locations to reduce real or perceived bar­\n\nriers to participation and to promote awareness of\n\nparticular stakeholder contexts and points of view.\n\n\n1. Deputy Under Secretary of Defense, August 2007, DoD Technology Transition Report to\n\nCongress.\n\n\n-----\n\n##### System Architecture\n\nDefinition: An architecture is “the fundamental organization of a system, embod­\n\n_ied in its components, their relationships to each other and the environment, and_\n\n_the principles governing its design and evolution [1, 2].”_\n\nKeyword: architecture\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to understand the role\n\nthat an architecture plays in system development (e.g., conceptualiza­\n\ntion, development, and certification), the various purposes for architec­\n\nture, and the different types of architectures. They are also expected\n\nto understand various architecture frameworks, models and modeling,\n\nviews and viewpoints, as well as when and why each would apply. MITRE\n\nSEs are expected to understand different architectural approaches and\n\ntheir applications, including the use of architectural patterns.\n\n###### Context\n\nAt this point in the systems engineering life cycle, an operational need\n\nhas been expressed and turned into a concept and set of operational\n\nrequirements (see the SEG’s Concept Development topic). They are\n\nthen analyzed and transformed into a set of system requirements (see\n\nthe SEG’s Requirements Engineering topic). The next step is to develop\n\n\n-----\n\nan architecture (or update an existing architecture for fielded systems) as a basis or founda­\ntion to guide design and development.\n\nThe article “Architectural Frameworks, Models, and Views” discusses the ways in\nwhich an architecture can be described. Various frameworks are used in different domains.\nTwo well-known examples are the Zachman framework and the Department of Defense\nArchitecture Framework. Whatever their specific form, all frameworks focus on defining a\nset of models, views, and viewpoints to support a range of systems engineering and program\nmanagement activities and decisions across the system life cycle.\n\nThe article “Approaches to Architecture Development” provides an overview of ways\nto tailor and apply architecture approaches, process, and methodologies to support decision\nmaking.\n\nArchitectural patterns are a method of arranging blocks of functionality. They can be\nused at the subsystem (component), system, or enterprise level. The article “Architectural\nPatterns” describes patterns and discusses how they can simplify and expedite the develop­\nment process.\n\n###### Architecture Best Practices\n\n\nEnsure purpose before architecting.\n\nEnsure that stakeholders have an opportunity\n\nto vet architectural trade-offs as they occur.\n\nEvaluate the architecture throughout sys­\n\ntem development. Although an architecture is\n\nintended to be a persistent framework during\n\nthe life cycle (and life) of a system, unforeseen\n\n###### References and Resources\n\n\nchanges (e.g., new missions) can influence the\n\nbest of “first version” architectures.\n\nConstruct the architecture to help understand\n\ntechnology readiness and evolution, and avoid\n\ngetting locked in to proprietary or potentially\n\nobsolete technologies or captured by a specific\n\nvendor.\n\n\n1. ANSI/IEEE 1471-2000, “Recommended Practice for Architecture Description of Software\nIntensive Systems.”\n\n2. ISO/IEC 42010:2007, “Systems and Software Engineering—Recommended Practice for\n\nArchitectural Description of Software-Intensive Systems.”\n\n###### Additional References and Resources\n\n“Architecture,” MITRE Systems Engineering Competency Model, vol. 1.13, section 2.3,\naccessed February 23, 2010.\n\n\n-----\n\nBass, L., P. Clements, and R. Kazman, December 30, 1997, Software Architecture in Practice,\n1st Ed., Addison-Wesley Professional.\n\n“System Architecture,” MITRE Project Leadership Handbook.\n\n\n-----\n\nDefinition: An architecture\n\n_framework is an encapsulation_\n\n_of a minimum set of practices_\n\n_and requirements for artifacts_\n\n_that describe a system’s archi­_\n\n_tecture. Models are representa­_\n\n_tions of how objects in a system_\n\n_fit structurally in and behave as_\n\n_part of the system. Views are a_\n\n_partial expression of the system_\n\n_from a particular perspective. A_\n\n_viewpoint is a set of represen­_\n\n_tations (views and models) of_\n\n_an architecture that covers a_\n\n_stakeholder’s issues._\n\nKeywords: architecture, archi­\n\n_tecture description, architecture_\n\n_frameworks, models, viewpoint,_\n\n_views_\n\n\nSYSTEM ARCHITECTURE\n###### Architectural Frameworks, Models, and Views\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nassist in or lead efforts to define an archi­\n\ntecture, based on a set of requirements\n\ncaptured during the concept development\n\nand requirements engineering phases of the\n\nsystems engineering life cycle. The archi­\n\ntecture definition activity usually produces\n\noperational, system, and technical views. This\n\narchitecture becomes the foundation for\n\ndevelopers and integrators to create design\n\nand implementation architectures and views.\n\nTo effectively communicate and guide the\n\nensuing system development activities, MITRE\n\nSEs should have a sound understanding of\n\narchitecture frameworks and their use, and the\n\ncircumstances under which each available\n\n\n-----\n\n1 1,*\n\nSystem Non-model\narchitecture artifacts\n\n\nProvides 0,*\n\nIs communicated by requirements\n\nfor the Consist of\ndevelopment of\nIs\ngoverned\n\n1 Architecture 1 by 1,* Architecture 1 *n\n\nViews\n\ndescription framework(s)\n\n0,*\n\nConsist of\n\n1,*\n\nModels\n\nFigure 1. Architecture Framework, Models, and Views Relationship [1]\n\nframework might be used. They also must be able to convey the appropriate framework that\napplies to the various decisions and phases of the program.\n\n###### Getting Started\n\nBecause systems are inherently multidimensional and have numerous stakeholders with dif­\nferent concerns, their descriptions are as well. Architecture frameworks enable the creation of\nsystem views that are directly relevant to stakeholders’ concerns. Often, multiple models and\nnon-model artifacts are generated to capture and track the concerns of all stakeholders.\n\nBy interacting with intra- and extra-program stakeholders, including users, experiment­\ners, acquirers, developers, integrators, and testers, key architectural aspects that need to be\ncaptured and communicated in a program are determined. These architecture needs then\nshould be consolidated and rationalized as a basis for the SE’s recommendation to develop\nand use specific models and views that directly support the program’s key decisions and\nactivities. Concurrently an architecture content and development governance structure should\nbe developed to manage and satisfy the collective needs. Figure 2 highlights the architecture\nplanning and implementation activities.\n\nMITRE SEs should be actively involved in determining key architecture artifacts and\ncontent, and guiding the development of the architecture and its depictions at the appropriate\n\n|e 1|*n|\n|---|---|\n|)|1,*|\n\n\n-----\n\nFigure 2. Architecture Planning and Implementation Activities\n\nlevels of abstraction or detail. MITRE SEs should take a lead role in standardizing the archi­\ntecture modeling approach. They should provide a “reference implementation” of the needed\nmodels and views with the goals of: (1) setting the standards for construction and content of\nthe models, and (2) ensuring that the model and view elements clearly trace to the concepts\nand requirements from which they are derived.\n\n###### Determining the Right Framework\n\nThough many MITRE SEs have probably heard of the Department of Defense Architecture\nFramework (DoDAF), other frameworks should be considered. Figure 3 shows that an SE\nworking at an enterprise level should also be versed in the Federal Enterprise Architecture\nFramework (FEAF). To prevent duplicate efforts in describing a system using multiple\nframeworks, establish overlapping description requirements and ensure that they are under­\nstood among the SEs generating those artifacts. The article “Approaches to Architecture\nDevelopment” details the frameworks.\n\n\n-----\n\nObjectives\n\nInvestment\n\n\nProcess\n\nEnterprise\n\nOrganization\n\n\nFederal Enterprise\nArchitectural\nFramework (FEAF)\n\nDepartment of Defense\nArchitectural\nFramework (DODAF)\n\nThe Open Group\nArchitectural\nFramework (TOGAF)\n\nDepartment of Defense\nArchitectural\nFramework (DODAF)\n\nDepartment of Defense\nArchitectural\nFramework (DODAF)\n\n4+1 Architectural\nView Model, Krutchen\n\n\nSystems-of-systems\nconsisting of\nDoD & Federal Agencies\n\n\nSystem (SW intensive)\n\n\nFigure 3. Applying Frameworks\n\n###### Best Practices and Lessons Learned\n\n\nA program may elect not to use architectural\n\nmodels and views, or elect to create only those\n\nviews dictated by policy or regulation. The\n\nresources and time required to create archi­\n\ntecture views may be seen as not providing a\n\ncommensurate return on investment in systems\n\nengineering or program execution. Consider\n\nthese cultural impediments. Guide your actions\n\nwith the view that architecture is a tool that\n\nenables and is integral to systems engineering.\n\nConsider the following best practices and les­\n\nsons learned to make architectures work in your\n\nprogram.\n\nPurpose is paramount. Determine the purpose\n\nfor the architecting effort, views, and models\n\nneeded. Plan the architecting steps to generate\n\n\nthe views and models to meet the purpose only.\n\nUltimately models and views should help each\n\nstakeholder reason about the structure and\n\nbehavior of the system or part of the system they\n\nrepresent so they can conclude that their objec­\n\ntives will be met. Frameworks help by establish­\n\ning minimum guidelines for each stakeholder’s\n\ninterest. However, stakeholders can have other\n\nconcerns, so use the framework requirements as\n\ndiscussion to help uncover as many concerns as\n\npossible.\n\nA plan is a point of departure. There should\n\nbe clear milestone development dates, and the\n\nneeded resources should be established for the\n\ndevelopment of the architecture views and mod­\n\nels. Some views are precursors for others. Ensure\n\n\n-----\n\nthat it is understood which views are “feeds” for\n\nothers.\n\nKnow the relationships. Models and views that\n\nrelate to each other should be consistent, con­\n\ncordant, and developed with reuse in mind. It is\n\ngood practice to identify the data or information\n\nthat each view shares, and manage it centrally to\n\nhelp create the different views. For guidance on\n\npatterns and their use/reuse, see the article “SEG\n\nArchitectural Patterns.”\n\nBe the early bird. Inject the idea of architectures\n\nearly in the process. Continuously influence your\n\nproject to use models and views throughout\n\nexecution. The earlier the better.\n\nNo one trusts a skinny cook. By using models as\n\nan analysis tool yourself, particularly in day-to\nday and key discussions, you maintain focus on\n\nkey architectural issues and demonstrate how\n\narchitecture artifacts can be used to enable\n\ndecision making.\n\nWhich way is right and how do I get there from\n\nhere? Architectures can be used to help assess\n\ntoday’s alternatives and different evolutionary\n\npaths to the future. Views of architecture alterna­\n\ntives can be used to help judge the strengths and\n\nweaknesses of different approaches. Views of\n\n“as is” and “to be” architectures help stakehold­\n\ners understand potential migration paths and\n\ntransitions.\n\nTry before you buy. Architectures (or parts of\n\nthem) can sometimes be “tried out” during live\n\nexercises. This can either confirm an archi­\n\ntectural approach for application to real-world\n\nsituations or be the basis for refinement that\n\nbetter aligns the architecture with operational\n\n\nreality. Architectures also can be used as a basis\n\nfor identifying prototyping and experimentation\n\nactivities to reduce technical risk and engage­\n\nments with operational users to better illumi­\n\nnate their needs and operational concepts.\n\nTaming the complexity beast. If a program or\n\nan effort is particularly large, models and views\n\ncan provide a disciplined way of communicat­\n\ning how you expect the system to behave.\n\nSome behavioral models such as business\n\nprocess models, activity models, and sequence\n\ndiagrams are intuitive, easy to use, and easy to\n\nchange to capture consensus views of sys­\n\ntem behavior. For guidance on model char­\n\nacterization, see the article “Approaches to\n\nArchitecture Development.”\n\nKeep it simple. Avoid diagrams that are com­\n\nplicated and non-intuitive, such as node con­\n\nnectivity diagrams with many nodes and edges,\n\nespecially in the early phases of a program. This\n\ncan be a deterrent for the uninitiated. Start with\n\nthe operational concepts, so your architecture\n\nefforts flow from information that users and\n\nmany other stakeholders already understand.\n\nDetermining the right models and views. Once\n\nthe frameworks have been chosen, the models\n\nand views will need to be determined. It is not\n\nunusual to have to refer to several sets of guid­\n\nance, each calling for a different set of views and\n\nmodels to be generated.\n\nBut it looked so pretty in the window. Lay out\n\nthe requirements for your architectures—what\n\ndecisions it supports, what it will help stakehold­\n\ners reason about, and how it will do so. A simple\n\nspreadsheet can be used for this purpose. This\n\n\n-----\n\nshould happen early and often throughout the\n\nsystem’s life cycle to ensure that the architecture\n\nis used. .\n\nHow do I create the right views? Selecting the\n\nright modeling approach to develop accurate\n\nand consistent representations that can be used\n\nacross program boundaries is a critical systems\n\nengineering activity. Some of the questions to\n\nanswer are:\n\n###### � [Is a disciplined architecture approach ]\n\nembedded in the primary tool my team will\n\nbe using, as in the case of Activity-Based\n\nModeling (ABM) being embedded in sys­\n\ntem architecture, or do we have to enforce\n\nan approach ourselves?\n###### � [Are the rules/standards of the modeling ]\n\nlanguage enforced in the tool, as in the\n\ncase of BPMN 2.0 being embedded in\n\niGrafix?\n###### � [Do I plan to generate executable models? ]\n\nIf so, will my descriptions need to adhere\n\nto strict development guidelines to easily\n\nsupport the use of executable models to\n\nhelp reason about performance and timing\n\nissues of the system?\n\nBringing dolls to life. If your program is develop­\n\ning models for large systems supporting missions\n\nand businesses with time-sensitive needs, insight\n\ninto system behavior is crucial. Seriously consider\n\nusing executable models to gain it. Today, many\n\narchitecture tools support the development of\n\nexecutable models easily and at reasonable cost.\n\nMission-Level Modeling (MLM) and Model Driven\n\n\nor Architecture-Based/Centric Engineering [2]\n\nare two modeling approaches that incorporate\n\nexecutable modeling. They are worth investigating\n\nto support reasoning about technology impacts\n\nto mission performance and internal system\n\nbehavior, respectively.\n\nHow much architecture is enough? The most\n\ndifficult conundrum when deciding to launch\n\nan architecture effort is determining the level\n\nof detail needed and when to stop producing/\n\nupdating artifacts. Architecture models and views\n\nmust be easily changeable. There is an investment\n\nassociated with having a “living” architecture that\n\ncontains current information, and differing levels\n\nof abstraction and views to satisfy all stakehold­\n\ners. Actively discuss this sufficiency issue with\n\nstakeholders so that the architecture effort is\n\n“right-sized.” See the Architecture Specification\n\nfor CANES [3].\n\nPenny wise, pound foolish. Generating archi­\n\ntecture models and views can seem a lot easier\n\nto not do. Before jumping on the “architecture is\n\ncostly and has minimal utility” bandwagon, con­\n\nsider these questions:\n\n###### � [Will there be a need to tell others how the ]\n\nsystem works?\n###### � [Will there be a need to train new person­]\n\nnel on a regular basis (every one to three\n\nyears) in system operations?\n###### � [Will there be a need to tell a different ]\n\ncontractor how the system works so that\n\ncosts for maintaining and refreshing the\n\nsystem remain competitive?\n\n\n-----\n\n###### � [Will there be a need to assess the sys­]\n\ntem’s viability to contribute to future mis­\n\nsion needs?\n\n###### References and Resources\n\n\nIf you answer yes to one or more of these ques­\n\ntions, consider concise, accurate, concordant, and\n\nconsistent models of your system.\n\n\n1. IEEE Standard 1471, accessed February 26, 2010.\n\n2. Wheeler, T., and M. Brooks, 2006, Experiences in Applying Architecture-Centric Model\n\nBased Systems engineering to Large-Scale, Distributed, Real-Time Systems, The MITRE\nCorporation.\n\n3. Navy PMW 160 Tactical Networks, May 20, 2009, “Architecture Specification for\n\nConsolidated Afloat Network and Enterprise Services (CANES), Increment 1.”\n\n###### Additional References and Resources\n\nCIO Council, September 1999, “Federal Enterprise Architecture Framework.”\n\nDoDAF Architecture Framework, ver. 2.0, 2008.\n\nKrutchen, P., 1995, “Architectural Blueprints—The ‘4+1’ View Model of Software Architecture.”\n\nRing, S. J., et al., 2004, “An Activity-based Methodology for Development and Analysis of\nIntegrated DoD Architectures.”\n\nThe Open Group Architecture Framework (TOGAF), version 9.\n\n\n-----\n\nDefinition: Architecture devel­\n\n_opment can be thought of as_\n\n_both a process and a discipline_\n\n_that aids the development of_\n\n_mission-effective systems._\n\nKeywords: DoDAF 6-step\n\n_architecture development_\n\n_process, functional decomposi­_\n\n_tion, object-oriented analysis,_\n\n_structured analysis, TOGAF_\n\n_ADM_\n\n\nSYSTEM ARCHITECTURE\n###### Approaches to Architecture Development\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand how to tailor and apply approaches,\n\nprocesses, and methodologies to develop archi­\n\ntectures that support decision making. They\n\nshould understand the scope, methodology,\n\nstrengths, and weaknesses of various approaches\n\nso they can apply them, separately and in com­\n\nbination, to architecture development efforts.\n\n\n-----\n\n###### Introduction\n\nMultiple complementary approaches and methodologies are used to develop enterprise\nand system architectures. Some of the most popular approaches used in government\ndepartments and agencies are:\n\n###### �U.S. Department of Defense Architecture Framework (DoDAF) �The Open Group Architecture Framework (TOGAF) �Object-oriented with Unified Modeling Language �Spewak architecture process and Zachman Framework\nThe key steps of any architecture development approach are:\n\n###### �Define the architecture purpose, value, and decisions it will support. �Get information needed to define the architecture from stakeholders as early as possible. �Create, refine, and update the architecture in an iterative way throughout the acquisi­\n\ntion life cycle.\n###### �Validate that the architecture will meet expectations when implemented. �Define roles for team members to guide and coordinate their efforts. �Create estimates and schedules based on the architectural blueprint. �Use the architecture to gain insight into project performance. �Establish a lightweight, scalable, tailorable, repeatable process framework [1].\n\n Determining the Right Process/Method\n\nMany SEs believe there is an “either-or” decision to be made regarding different architectural\nframeworks (e.g., DoDAF or TOGAF), but this is not necessarily the case. Some architectural\nstandards address completely different elements of the architecting process; thus there may\nbe a natural synergy among the frameworks. For example, TOGAF has a primary focus on\narchitecture methodology—the “how to” aspect of architecting, without prescribing architec­\nture description constructs. DoDAF has a primary focus on architecture description via a set\nof viewpoints, without a detailed specification of methodology [2].\n\n###### DoDAF 6-Step Architecture Process\n\nThe primary focus of DoDAF is architecture description—the architecture depiction consist­\ning of several models (called products in DoDAF-2004). Initially the primary objective of\nDoDAF was to facilitate interoperability among DoD systems; however, that objective has\nbeen broadened to assist decision making by DoD managers at all levels on issues relating to\nDOTMLPF—Doctrine, Organization, Training, Materiel, Leadership and Education, Personnel,\nand Facilities—and DoD information technology systems.\n\nAlthough a 6-step architecture process (see Figure 1) is described, it is meant to remain\nsimple, tailorable, and able to be augmented by other architecture development processes.\n\n\n-----\n\n|I Target obje I Key tradeof I Decision oo I Probable an|ctives fs ints alysis methods|\n|---|---|\n\n\n\n- Architecture\npresentations and\nviews\n\n- Reusable\narchitecture data\n\n- Analysis reports\n\n\n\n- Geographical,\noperational, and\nfunctional bounds\n\n- Technological\nbounds\n\n- Time frame(s)\n\n - Architecture\nresource and\nschedule\nconstraints\n\n\nRequired Architectural\nCharacteristics:\n\n- Architectural data\nentities\n\n- Levels of detail\n\n- Units of measure\n\n- Associated\nmetadata\n\n\n\n- Automated\nrepositories\n\n- Activity models\n\n- Data models\n\n- Dynamic models\n\n- Organizational\nmodels\n\n- Metadata\nregistration\n\n\n\n- Shortfall analyses\n\n- Capacity analyses\n\n- Interoperability\nassessments\n\n- Business process\nanalysis\n\n- Test architecture\ncompleteness,\naccuracy, and\nsufficiency\n\n\nFigure 1. 6-Step Architecture Process\n\nThe method described within DoDAF is generic and can be used with other frameworks. The\nprocess supports both the structured analysis and object-oriented analysis and design model­\ning techniques and their specific notations [3].\n\nTOGAF Architecture Development Method (ADM)\n\nThe TOGAF Architecture Development Method (ADM) provides a tested and repeatable pro­\ncess for developing architectures. It is a generic method for architecture development that is\ndesigned to deal with most systems. However, it will often be necessary to modify or extend\nthe ADM to suit specific needs. One of the tasks before applying the ADM is to review its com­\nponents for applicability, and then tailor them as appropriate.\n\nThe phases within the ADM are as follows:\n\n\n-----\n\n###### �The Preliminary Phase: describes the preparation and initiation activities required to\n\nprepare to meet the operational directive for a new architecture, including the definition\nof an organization-specific architecture framework and the definition of principles.\n###### �Phase A–Architecture Vision: describes the initial phase of an architecture develop­\n\nment cycle. It includes information about defining the scope, identifying the stakehold­\ners, creating the architecture vision, and obtaining approvals.\n###### �Phase B–Business Architecture: describes the development of a business architecture\n\nto support an agreed architecture vision.\n###### �Phase C–Information Systems Architectures: describes the development of information\n\nsystems architectures for an architecture project, including the development of data and\napplication architectures.\n\n\nDetected or suspected malfunction, or item is\nscheduled for bench-check\nIn-service\nasset Remove\n\nReplaced asset\n\nand\nreplace\n\nSpare Reparable\nasset asset\n\nStatus records\n\nSchedule\ninto\nshop\n\nSupply\nparts\n\nReplacement\n\nAsset\n\nor original\n\n(before\n\n(repaired)\n\nrepair) Inspect\n\nor\nrepair\n\nAssets Asset Completed\nawaiting (after asset\nparts repair)\n\nMonitor\nand\nroute\n\nNode Title Number\n\nAGF Maintain Reparable Spares pg. 4-5\n\n\nSpare\n\n|scheduled for bench-check In-service asset Replaced asset Remove and|Col2|\n|---|---|\n|and replace pare Reparable sset asset Status records Schedule into shop Supply parts Replacement Asset or original (before (repaired) repair) Inspect or repair Assets Asset Completed awaiting (after asset parts repair) Monitor and route Spar||\n|pare sset||\n||Spar|\n\n\nFigure 2. Integrated Definition for Function Modeling (IDEF0)\n\n\n-----\n\n###### �Phase D–Technology Architecture: describes the development of the technology archi­\n\ntecture for an architecture project.\n###### �Phase E–Opportunities and Solutions: conducts initial implementation planning and\n\nidentifies delivery vehicles for the architecture defined in the previous phases.\n###### �Phase F–Migration Planning: addresses the formulation of a set of detailed sequences\n\nof transition architectures with a supporting implementation and migration plan.\n###### �Phase G–Implementation Governance: provides an architectural oversight of the\n\nimplementation.\n###### �Phase H–Architecture Change Management: establishes procedures for managing\n\nchange to the new architecture.\n###### �Requirements Management: examines the process of managing architecture require­\n\nments throughout the ADM [4].\nAs a generic method, the ADM may be used in conjunction with the set of deliverables\nof another framework where these have been deemed to be more appropriate (e.g., DoDAF\nmodels).\n\n###### Modeling Techniques\n\nOnce the decision about the architecture development methodology is resolved, selecting a\ntechnique to discover the architectural structure and processes is important. Currently two\n\n\nWaitstaff\n\nPatron\n\nCashier\n\n\napproaches are in use—object-oriented\nanalysis and design, and structured\n\nOrder analysis and design. Both have strengths\nfood\n\nPrepare and weaknesses that make them suitable\nfood for different classes of problems; however,\n\nChef\nServe the object-oriented methodology is better\nfood\n\nfor complex, interactive, and changing sys­\ntems with many interfaces, which are the\nkinds of systems most MITRE SEs face.\n\nEat\nfood Structured functional techniques tend\n\nto be rigid and verbose, and they do not\n\nSystem\n\nDrink boundary address commonality. Functional decom­\nwine position does not lend itself well to cases of\n\nhighly complex interactive problems and is\n\nPay for generally not used in modern development\nfood\n\nenvironments. Functional solutions are\noften difficult to integrate horizontally and\nare costly to sustain.\nFigure 3. Use Case Diagram\n\n\n-----\n\nThe object-oriented method takes a value-based approach to discovering system capabili­\nties. Use cases describe the behavior between the system and its environment (see Figure 3).\nFrom the use case, the services the system must provide are derived. Those services are then\nrealized by the internal structure of the system elements in iterative steps until system ele­\nments are simple enough to build. The resultant set of diagrams traces the composition of the\nsystem from its parts to the aggregated behavior captured within the set of use cases.\n\nObject-oriented approaches focus on interaction from the beginning, which has the\nbeneficial side-effect of defining the boundary between the system and its environment.\nThe use cases identify the ways in which the operator will use the system. Sequence dia­\ngrams (see Figure 4) illustrate the interactions the system must support. The “lifelines” of the\ndiagram gather the behavioral responsibilities of each “object” participating in the use case.\nThese responsibilities are the requirements to share data across the collection to produce the\nrequired result.\n\nThe advantages of the object-oriented method are that it embraces the concept of effectsdriven process development, and it promotes reuse, facilitating the federation of cross-func­\ntional domain architectures. The focus on system interfaces also supports the service-oriented\narchitecture implementation pattern [5].\n\n\nRemote User\n\n\nLogon Page User Profiles Database Entry Page\n\nSet User Name,\nPassword\n\nValidate User\n\nGet User Data\n\n[valid]\nDisplay Page\n\n[invalid]\n\nFigure 4. Sequence Diagram\n\n|Enter User Name Enter Password Set User Name, Password Validate User Get User Data [valid] Display Page [invalid] Login Invalid|Get User Data|[valid] Display Page|\n|---|---|---|\n\n\n-----\n\nFor an example of a functional specification using use cases and sequence diagrams, see\nthe Functional Specification for Consolidated Afloat Network and Enterprise Services [6].\n\n###### Best Practices and Lessons Learned\n\n\nPurpose before architecture. Purpose must drive\n\nthe architecting effort or the effort will be subject\n\nto the criticism of architecting for its own sake.\n\nArchitecting is integral to systems engineer­\n\ning. Significant analytical insight into the system is\n\ngained through the process of architecting.\n\nThink “both-and.” Various architecture meth­\n\nodologies and approaches exist. When properly\n\nunderstood, they can be complementary. Some\n\napproaches and frameworks address architec­\n\nture content, and others address the architecture\n\n###### References and Resources\n\n\nprocess. Understand the value of all to apply the\n\nbest course of actions for the purpose. Actively\n\nconsider mixing and matching them to achieve\n\nyour purpose.\n\nDifferent models for different situations.\n\nBasic modeling techniques include a structured\n\napproach and an object approach. Understand\n\ntheir application strengths and weaknesses.\n\nThe object approach provides many features to\n\nsupport complex system architectures and their\n\ninteractions.\n\n\n1. Lattanze, A., 2005, “The Architecture Centric Development Method,” Carnegie Mellon\n\nUniversity report CMU-ISRI-05-103.\n\n2. Blevins, T., F. Dandashi, and M. Tolbert, 2010, TOGAF ADM and DoDAF Models, The\n\nOpen Group White Paper.\n\n3. U.S. Department of Defense, 2009, DoD Architecture Framework (DoDAF) Ver. 2.0.\n\n4. The Open Group, 2009, The Open Group Architecture Framework (TOGAF).\n\n5. Folk, T., 2006, Architectural Structures and Specifications.\n\n6. Navy PMW 160 Tactical Networks, May 20, 2009, “Architecture Specification for\n\nConsolidated Afloat Network and Enterprise Services (CANES), Increment 1.”\n\n###### Additional References and Resources\n\nDraft Federal Information Processing Standards Publication 183, December 1993 (withdrawn\n[September 2008), Integration Definition for Function Modeling (IDEF0).](http://www.itl.nist.gov/fipspubs/idef02.doc)\n\n[Federal CIO Council, 2001, A Practical Guide to Federal Enterprise Architecture.](http://www.gao.gov/bestpractices/bpeaguide.pdf)\n\n\n-----\n\nDefinition: Architectural pat­\n\n_terns are a method of arrang­_\n\n_ing blocks of functionality_\n\n_to address a need. Patterns_\n\n_can be used at the software,_\n\n_system, or enterprise levels._\n\n_Good pattern expressions_\n\n_tell you how to use them, and_\n\n_when, why, and what trade-offs_\n\n_to make in doing so. Patterns_\n\n_can be characterized according_\n\n_to the type of solution they are_\n\n_addressing (e.g., structural or_\n\n_behavioral)._\n\nKeywords: architecture, archi­\n\n_tecture patterns, patterns_\n\n\nSYSTEM ARCHITECTURE\n###### Architectural Patterns\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are frequently the\n\nstewards of an enterprise, system, or software\n\narchitecture over its life cycle. The MITRE SE\n\nis expected to understand how architecture\n\npatterns can simplify and expedite the devel­\n\nopment of the system, and to mandate and\n\nencourage their use when appropriate.\n\n\n-----\n\nSystem Level Patterns\nusage (less mature)\nMITRE SEs should\nencounter them during\ndesign reviews\n\n\nSW Patterns\nusage (more\nmature)\nSW producers\nshould use as a\npart of best\npractice\n\n\nSW SW SW\n\n|e Architect rprise SW SW|SW Pattern usage (mor mature) SW produc should use part of best practice SW|\n|---|---|\n\n\nSW SW SW\n\nFigure 1. Architectural Pattern Usage\n\n\nSW\n\nSW\n\n\n###### Background\n\n“A key aspect to enterprise architecting is the reuse of knowledge. In most organizations\ntoday, the experience gained while doing a similar endeavor in the past is rarely utilized, or\ngrossly underutilized, while dealing with a need today. Through better utilization of experi­\nences and knowledge from the past, one can obtain major strategic advantages [1].” Pattern\nusage is an excellent way to reuse knowledge to address various problems. Figure 1 shows the\nlevels of pattern application and how mature the pattern practice currently is for each one.\n\nDefinition\n\nThe architecture of an object, system, or enterprise is recognizable from the organization of\nfeatures that contribute either structurally or behaviorally to the subject. A “pattern” has been\ndefined as “an idea that has been useful in one practical context and will probably be useful\n_in others [2, 3].”_\n\n\n-----\n\nComplexity Management\n\nA major problem facing MITRE’s sponsors today is constructing large, complex “systems of\nsystems.” These initiatives attempt to integrate dozens of legacy applications into a “system of\npre-existing systems” to solve new and unexpected problems. The use of patterns can make\nthese systems more efficient and effective. For instance, a system might have a tightly coupled\narchitecture to address low-latency performance needs. Likewise, loosely coupled architec­\ntures may provide more opportunities to flexibly combine existing functions. As an example,\none pattern used to enable loose coupling is the façade pattern in software architecture. This\nstructural pattern provides a simple interface easily understood by many customers, hiding\nthe complexity of function it provides, and is typically used when a service is to be provided\nto many objects in the environment.\n\nPattern Expression\n\nOne of the reasons why “experience gained while doing a similar endeavor in the past is\nrarely utilized” is because problems and their solutions are not expressed in a form suitable\nfor reuse. Patterns provide a form for expressing technical solutions in the context of business\nproblems and capturing them as reusable corporate knowledge assets.\n\nIn 1979, the (building) architect Christopher Alexander published The Timeless Way of\n_Building, which describes a way to organize common solutions to architectural problems using_\npatterns. In the early 1990s, software engineers began applying these ideas to systems architec­\ntures. Here is an example of a layered enterprise architecture expressed in Alexander’s format:\n\n###### �Name\n\n     - _Layering_\n###### �Context (situation giving rise to a problem)\n\n     - Systems need to evolve to accommodate changing user requirements and new\n\ntechnologies\n\n     - Managing change in complex systems\n###### �Problem (set of forces repeatedly arising in the context)\n\n     - Applications built as monolithic structures\n\n     - Changing one part propagates costly changes everywhere\n\n     - Migration timelines are long and expensive\n###### �Solution (configuration to balance the forces)\n\n     - Structure a system into layers\n\n     - Each layer is a “black box” with well-defined interfaces\n\n     - Implementation details of each layer are hidden behind the interface\n\n\n-----\n\nFigure 2 illustrates the Layering pattern.\nA pattern can be expressed using both\nhuman language such as prose, and more\nformal representations such as Unified\nModeling Language diagrams. Patterns may\nalso provide fragments of code to illustrate\na design solution; however, it is not the\nintent of a pattern to provide a fully coded\nimplementation.\n\n###### Applications of Patterns\n\n\nClient Client Tier\ndevices\n\n\nAppliances\n\n\nPresentation\n& content\n\n\nWeb server\n\n\nPresentation Tier\n\n\nBusiness\nprocessing\n\n\nApp server\n\n\nBusiness Tier\n\n\nData\nmanagement\n\n\nInformation Tier\n\n\nInfo server\n\n\nAs the value of patterns becomes recognized\n\nFigure 2. Layering Pattern\n\nin the federal government, agencies are\nbeginning to build pattern repositories in\nthe context of the Federal Enterprise Architecture Framework. For example, the Department\nof Veterans Affairs has established a Technical Reference Model that includes 18 patterns that\naddress such issues as router configurations and email address conventions.\n\nWhen problem spaces are pervasive in an enterprise, there is an opportunity to develop\nguidelines in the form of patterns to address and govern solutions to that problem. The\nTactical Edge Characterization Framework [4] contains patterns that address solutions to\nproblems that occur at the edge of an enterprise where the users do not have large-scale and\nrobust infrastructures.\n\nThe Navy has successfully applied patterns for their surface combat systems software\nproduct line. They use a layered presentation approach and a catalog of pattern elements.\n\nAnother set of problems occurs in the security domain of enterprises. The use of differ­\nent approaches and a lack of patterns in developing security solutions lead to interoperability\nproblems.\n\n###### Best Practices and Lessons Learned\n\n\nTo be effective, patterns need to be incorpo­\n\nrated into the corporate culture and adopted by\n\nmanagement, business, and technical organi­\n\nzations. The effective use of patterns involves\n\nactivities across technical, organizational, and\n\nprocess dimensions (see Figure 3).\n\n\nIn addition to internal corporate use, patterns\n\ncan leverage collective solutions among part­\n\nners across corporate, government, and national\n\nboundaries.\n\nSeek out pattern sources. For systems you are\n\nthe steward of, seek out sources of architectural\n\npatterns. Examples include Net-centric Enterprise\n\n\n-----\n\nFigure 3. Dimensions of Effective Pattern Use\n\n\nSolutions for Interoperability (NESI) [5], and the\n\nElectronic Systems Center Strategic Technical\n\nPlan [6]. These two are particularly applicable to\n\nproblems of enterprise-level net-centricity.\n\nBe a pattern steward. Recognize and capture\n\npatterns for reuse by others. Base patterns on\n\nproven experience in a common form or expres­\n\nsion that is amenable to capture as a corporate\n\nknowledge asset. This is one way to help our\n\ncustomers solve their hard problems.\n\nLead the way in pattern usage. Enable and\n\nstimulate the selection of technical solutions\n\n###### References and Resources\n\n\nbased on successful patterns by using them in\n\nkey documents such as Technical Requirements\n\nDocuments, Software Development Plans,\n\nSystems Engineering Management Plans, and\n\nother key architecture documents.\n\nPatterns, patterns, everywhere! Adopt patterns\n\nnot only in technology-based SE work but also\n\norganizationally and in the process arenas. Works\n\nsuch as the Mission Level Modeling done by Prem\n\nJain [7] contain workflow patterns that can be\n\nreused in architecture modeling efforts.\n\n\n1. Peloquin, J. J., 2001, An Essay on Knowledge as a Corporate Asset: Building the Case for\n\nHuman Capital.\n\n2. Fowler, M., 1997, Analysis Patterns—Reusable Object Models, Addison-Wesley, ISBN\n\n0-201-89542-0.\n\n\n-----\n\n3. The Open Group, The Open Group Architecture Framework (TOGAF), ver. 8.1.1.\n\n4. Dandashi, F., et al., November 2007, Tactical Edge Characterization Framework—Vol. 1:\n\nCommon Vocabulary for Tactical Environments, The MITRE Corporation.\n\n5. Department of the Navy, SPAWAR Systems Center Pacific, “NESI Public Site—Net-Centric\n\nEnterprise Solutions for Interoperability,” accessed February 25, 2010.\n\n6. “ESC Strategic Technical Plan,” MITREpedia.\n\n7. “Mission Level Modeling,” MITREpedia.\n\n###### Additional References and Resources\n\nAdams, J., S. Koushik, G. Vasudeva, and G. Galambos, Patterns for e-Business, IBM Press,\nISBN 1-931182-027.\n\nBuschmann, F., R. Meunier, H. Rohnert, P. Sommerlad, and M. Stal, Pattern-Oriented Software\n_Architecture, A System of Patterns, ISBN 0-471958-697._\n\nGamma, E., R. Helm, R. Johnson, and J. Vlissides, Design Patterns: Elements of Reusable\n_Object-Oriented Software, ISBN 0-201633-612._\n\nHalley, M. R., and C. Bashioum, Enterprise Transformation to a Service Oriented Architecture:\nSuccessful Patterns, Proceedings of the IEEE International Conference on Web Services\n_(ICWS’05)._\n\nHohpe, G., and B. Woolf, Enterprise Integration Patterns: Designing, Building and Deploying\n_Messaging Solutions, ISBN 0-321-20068-3._\n\nLapkin, A., October 22, 2004, A User’s Guide to Architectural Patterns, Gartner Research Note\nG00123049.\n\nLeganza, G., and J. Meyer, April 13, 2001, Using Patterns in Enterprise Architecture: Part 1—\n_Benefits and Drawbacks of the Patterns Methodology, Giga Information Group._\n\nMITRE, Information Sharing, Biopedia, accessed February 1, 2011 (includes instructions to\naccess the Sharing Among International Partners Initiative [SAIPI] wiki).\n\nNavy PEO Integrated Warfare Systems, July 31, 2009, Surface Navy Combat Systems\n_Architecture Description_ _Document._\n\nSchulman, J., October 19, 2004, Overcome Architecture Pattern Pitfalls and Problems, Gartner\nResearch Note G00123461.\n\nSchulman, J., October 20, 2004, Architecture Patterns Lead to Better Solutions, Gartner\nResearch Note G00123458.\n\nThe Open Group, The Open Group Architecture Framework (TOGAF), ver. 8.1.1, Part IV\n(Resource Base), Architecture Patterns.\n\n\n-----\n\n##### System Design and Development\n\nDefinition: System design is the process of defining the components, modules,\n\n_interfaces, and data for a system to satisfy specified requirements. System devel­_\n\n_opment is the process of creating or altering systems, along with the processes,_\n\n_practices, models, and methodologies used to develop them._\n\nKeywords: contractor, design, design review, development, evaluation, require­\n\n_ments, specifications, strawman, traceability, validation, verification_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to have a sound under­\n\nstanding of what a system requirement is intended to convey, what\n\nconstitutes a good system requirement, how to identify a poorly written\n\nrequirements statement, and what constitutes a good set of systems\n\nrequirements. MITRE SEs are expected to be able to transform busi­\n\nness/mission and operational needs into system requirements. Typically\n\nMITRE SEs lead the government acquisition program office effort to\n\ndevelop these requirements or are heavily involved in it. Collectively the\n\ndescriptions and constraints comprising the system-level technical\n\nrequirements are one of the most important products that MITRE can\n\ndevelop for the customer.\n\nMITRE SEs are expected to help lead the government effort to create\n\nrealistic top-level designs and associated risk mitigation activities so\n\n\n-----\n\nthat planning will be based on a realistic foundation. Cost, schedule, and performance projec­\ntions based on the top-level system design can be instrumental in mitigating and managing\nprogram risks. MITRE SEs are expected to be able to evaluate and influence the contractor’s\ndesign and development effort, including making independent performance assessments and\nleading design review teams. In some programs, contractors will have primary responsibility\nfor the top-level design with MITRE SEs providing guidance and verification of their efforts.\nIn other programs, the government will develop a top-level design as part of its early systems\nengineering activities. Often MITRE will have a lead role or substantial responsibility for\ndeveloping the government’s top-level system design.\n\nMITRE SEs are expected to understand the importance of system design in meeting\nthe government’s mission and goals. They are expected to be able to review and influence\nthe contractor’s preliminary design so that it meets the overall business or mission objec­\ntives of the sponsor, customer, and user. MITRE SEs are expected to be able to recom­\nmend changes to the contractor’s design activities, artifacts, and deliverables to address\nperformance shortfalls and advise the sponsor or customer if a performance shortfall\nwould result in a capability that supports mission requirements, whether or not the design\nmeets technical requirements. MITRE SEs are expected to be thought leaders in influ­\nencing decisions made in government design review teams and to appropriately involve\nspecialty engineering.\n\n###### Context\n\nCore activities in system design and development include developing system-level techni­\ncal requirements and top-level system designs and assessing the design’s ability to meet the\nsystem requirements.\n\nSystem-level technical requirements describe the users’ needs, and provide information\nfor the finished system to meet legal restrictions, adhere to regulations, and interoperate or\nintegrate effectively with other systems. The system-level technical requirements are used\nby the government to acquire a capability, system, or product to meet a user need. They are\nused as part of a procurement contract solicitation or prototyping/experimentation effort\nand by the product vendors as their design criteria. The decisions made when defining\nsystem-level technical requirements can affect the number of potential solutions, the techni­\ncal maturity of the potential solutions, system cost, system evolution, and development time\nand phasing.\n\nSystem-level technical requirements are a critical precursor to and foundation of system\ndesign and development. A top-level system design is generally under the stewardship of the\ngovernment team and represents the government team’s independent projection of the way a\nsystem could be implemented to meet requirements with acceptable risk. The primary reason\n\n\n-----\n\nfor developing a top-level system design is to provide a technical foundation for planning the\nprogram. It is the government’s de facto technical approach to meeting the customer’s needs.\nA top-level system design developed early in an acquisition program can be used to assess\nsystem feasibility and provide some assurance that the implemented design will satisfy system\nrequirements. Done early in a program, a government design effort can be a powerful basis\nfor developing fact-based government projections of cost, schedule, performance, and risk and\nprovide the foundation for subsequent contractor design efforts.\n\nRequirements traceability is a critical activity during the design, development, and\ndeployment of capability that starts with the translation of the users’ operational needs into\ntechnical requirements and extends throughout the entire system life cycle. It is a technique\nto develop a meaningful assessment of whether the solution delivered fulfills the opera­\ntional need. Traceability is also the foundation for the change process within a project or\nprogram. Without the ability to trace requirements from end to end, the impact of changes\ncannot be effectively evaluated. In addition, change should be evaluated in the context of\nthe end-to-end impact on other requirements and overall performance (e.g., see the SEG’s\nEnterprise Engineering section). This bi-directional flow of requirements must be managed\ncarefully throughout a project/program and be accompanied by a well-managed require­\nments baseline.\n\nThe articles in this topic highlight important elements of system design and develop­\nment. The article “Develop System-Level Technical Requirements” provides guidance on\nselecting the right level of detail in writing technical requirements, highlights common\nchallenges in achieving stakeholder agreement on requirements, suggests ways to handle\nthem, and provides a checklist to help ensure that all bases have been covered in develop­\ning the system-level requirements. The article “Develop Top-Level System Design” pro­\nvides guidance on early design efforts. The article is written from the perspective of an\nin-house government design activity, but many of the best practices and lessons learned\ncan be used to shape, guide, and monitor contractor design efforts. The article “Assess the\nDesign’s Ability to Meet the System Requirements” provides guidance in establishing and\naccomplishing traceability, the importance of two-way traceability (both up and down),\nthe need to have testing in mind when beginning traceability efforts, and the value of\nengaging with the operational user. It describes the technique of using a requirements\ntraceability matrix to manage the specific traceability and verification from user need to\nrequirements to design and development modules to test cases and measures/metrics for\nsuccess.\n\nFor related information, see the SEG’s Concept Development, Requirements Engineering,\nand System Architecture topics.\n\n\n-----\n\n###### References\n\nWikipedia contributors, “Systems Design,” Wikipedia, accessed December 4, 2009.\n\nWikipedia contributors, “Systems Development Life Cycle,” Wikipedia, accessed December 4,\n2009.\n\n\n-----\n\nDefinition: System-level techni­\n\n_cal requirements is a general_\n\n_term used to describe the set_\n\n_of statements that identifies a_\n\n_system’s functions, character­_\n\n_istics, or constraints._\n\nKeywords: acquisition develop­\n\n_ment program, requirement,_\n\n_specification_\n\n\nSYSTEM DESIGN AND DEVELOPMENT\n###### Develop System-Level Technical Requirements\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to have a sound\n\nunderstanding of what a system requirement is\n\nintended to convey, what constitutes a good system\n\nrequirement, how to identify a poorly written require­\n\nments statement, and what constitutes a good set\n\nof systems requirements. MITRE SEs are expected\n\nto be able to transform business/mission and opera­\n\ntional needs into system requirements, including [1]:\n\n###### �Exploring and recommending creative ways to elicit,\n\nanalyze, and document user requirements\n\n###### �Transforming and integrating business/mission\n\nand operational needs into system requirements,\n\nincluding unstated or implied needs\n\n###### �Promoting shared understanding and facilitat­\n\ning stakeholder agreement about systems\n\nrequirements\n\n\n-----\n\n###### �Integrating new requirements generated by prototypes into the system requirements �Analyzing the interrelationships, priorities, cost, implementation, and environmental\n\nimplications of system requirements\n###### �Defining system boundaries, including how the system interacts with both inputs from\n\nand outputs to users, equipment, or other systems.\n\n###### Background\n\nFrequently MITRE plays a central role in developing system-level technical requirements,\ninterpreting them, and assessing designs against them. Developing the right system requires\nthe right information to communicate what the system is intended to do and what conditions\nor constraints its design must accommodate.\n\nCollectively the descriptions and constraints that make up the system-level technical\nrequirements are one of the most important products that MITRE can develop for the cus­\ntomer. Often, the system-level technical requirements are used in a government activity with\nthe objective of acquiring a capability, system, or product to meet a user need. They are used\nas part of a procurement contract solicitation or prototyping/experimentation effort and by\nthe product vendors as their design criteria. System-level technical requirements describe the\nusers’ needs, and provide information for the finished system to meet legal restrictions, adhere\nto regulations, and interoperate or integrate effectively with other systems. The decisions that\nare made when defining system-level technical requirements can affect the number of poten­\ntial solutions, the technical maturity of the potential solutions, system cost, system evolution,\nand development time and phasing. Requirements definition (see the SEG’s Requirements\nEngineering topic) confers an obligation and an opportunity for MITRE to influence the\nsystem development in ways that improve the ability of systems to interoperate with other\nsystems. Poor requirements often result in solutions that are at high risk of failing to meet the\nuser need, cost, schedule, and ability to interoperate.\n\n###### Application\n\nIdeally system-level technical requirements are developed after user requirements are\ndefined. When not the case, a preliminary version of the system requirements may be drafted\nfor developing a prototype or experimentation system to confirm, clarify, or discover user\nrequirements. Prototypes or experimentation systems are also instrumental in validating\nthat key technologies needed to meet the requirements are sufficiently mature and can meet\nthe requirements that exist. In most government departments and agencies, system require­\nments are needed before development, manufacture, or construction. Once established, the\nsystem requirements exist and evolve for the life of the system. They may be and frequently\nare updated as the user defines new needs, or when the environment in which the system\n\n\n-----\n\noperates changes [2, 3]. In evolutionary or incremental acquisitions, the system requirements\ngenerally get defined in greater detail as the increment in which they are implemented draws\nnear.\n\nMany projects capture their system requirements using formal “shall” requirement state­\nments in a system specification document. Use of the term “shall” in acquisition programs\ndrives a mandatory implementation by the developer of a capability to meet the “shall.”\nSelective use of “shalls” can be beneficial in situations in which needs are likely to evolve over\ntime. Exhaustive and excessive use of “shall” statements can be constricting and costly. A\nterm such as “should” can be used to both show government preference and allow some free­\ndom in design and management/negotiation (e.g., time-driven, capability-driven, cost-driven)\nof system requirements intended to meet user needs over time.\n\nThe term “document” or “documented” has a long history in referring to requirements\nand is commonly used to this day. They should not be construed generally to mean that the\ninformation exists in paper form or is even organized as a paper substitute, such as a wordprocessed document [4]. Many projects today use electronic software tools to capture and\nmanage requirements and other information. MITRE SEs are expected to understand the\nvarious types of tools and media, and their advantages and disadvantages, when capturing\nsystem requirements in different situations. Some projects, especially those involving soft­\nware-intensive systems, use modeling tools or languages to describe major system elements,\nrequired behaviors, and interfaces. Examples include Unified Modeling Language (UML) and\nSystem Modeling Language (SysML).\n\n###### Development of System-Level Requirements\n\nWhen developing systems requirements, a good rule of thumb is to provide designers and test\nengineers with what they must know, but leave as much “white space” as possible for clever\ndesigners to explore design options (many times through prototypes of different forms or\nexperiments).\n\nAn obvious place to start developing system requirements is with the user require­\nments—high-level expressions of user needs that the system is expected to satisfy. Examples\nof Department of Defense (DoD) user requirement sources include the initial capabilities\ndocument (ICD), the capability development document (CDD), and the capability production\ndocument (CPD).\n\nThere are two cautions when working with user requirement documents. First, the needs\nare frequently expressed in user operational lingo that may not be meaningful to engineers.\nAn even more insidious problem is that while the language may be meaningful to both opera­\ntors and engineers, it may also convey different interpretations, resulting in a lack of clarity\nabout intent by the operators and the engineers. SEs need to explicitly account for and resolve\n\n\n-----\n\nthis language divide and may have to translate operational terminology into engineering\nrequirements. Second, user requirements often are not expressed in ways that unambiguously\ncover acceptance or test criteria. What appears to be a clear user requirement (e.g., “detect\nairborne objects out to a range of 100 miles”) often requires the SE to do substantial work to\nachieve a set of requirements that are testable and can be built with reasonable technological\nrisk.\n\nIn the requirement example above, there appears to be a straightforward performance\nrequirement of 100 miles, but to a radar engineer it begs the question of “detect what?”\nand “how well?” There are both physics-related and technological challenges with detect­\ning objects that are small and far away. Even for large objects, it is impossible to guarantee\ndetection 100 percent of the time. To write an effective system requirement for a designer to\nimplement a solution, an SE would have to derive additional requirements and/or verification\ncriteria to define how small an object must be to be detected, and what the likelihood is that a\ngiven object within range will be detected.\n\nSimilarly, in a Maglev train development, the user’s need to transit a great distance in\na short time will eventually establish requirements for train speed parameters. The need to\ntransport passengers will form the basis for safety requirements and maximum noise toler­\nances. In the vast majority of cases, the MITRE SE will benefit from working with the end\nusers to create a mutual understanding of their needs and the capabilities and technology\navailable and feasible to support them.\n\nConsider using the checklist in Table 1 when developing a set of system-level require­\nments [4, 5, 6]:\n\nTable 1. System-Level Requirements Checklist\n\n|Checklist Item|4|\n|---|---|\n|The system-level technical requirements are traceable to the user requirements.||\n|Each system requirement describes something relevant: a function the system must perform, performance a function must provide, a constraint on the design, or a refer­ ence such as to an interface definition.||\n|The level of detail that the requirements provide about system functionality is appropriate. The requirements are sufficient to describe what the overall system must do, what its performance must be, and what constraints an engineer should consider. There are few requirements that specifically affect the design of only one component of the system. The major requirements drivers (e.g., those stressing the design) and associated risks should be identified.||\n\n\n-----\n\n|The requirements include any legal or regulatory constraints within which the system must perform. Example: There may be restrictions on the use or quantity of certain hazardous materials in a system.|Col2|\n|---|---|\n|The requirements include enterprise architecture constraints within which the sys­ tem must integrate (or toward which the system is desired to migrate). Requirements include appropriate open systems and modularity standards. Examples: DoD Net-Ready requirements, modular open system architecture con­ cepts, Electronic Systems Center strategic technical plan goals.||\n|Environmental design requirements are specified. Example: A control unit may be in a controlled office environment and the other major components may be outdoors, thus two environments must be defined and associated with the functionality operating in each environment.||\n|All external interfaces for the system are included. Major internal interfaces may also be included if they are important to system modularity, or future growth in capability. These may include physical (mechanical fastening, electrical wiring, connectors), func­ tional (mechanical stress transfer points, cooling, power sources, antennas, wire mes­ sage formats, data exchanges), and software (software interface specifications, library calls, data formats, etc.). Remember that an internal interface between two subsystems that use a transport mechanism that is not part of the system is a hidden external interface. For exam­ ple, two subsystems that communicate internally with each other over a sensitive but unclassified network as the internal interface (the data exchanged between them) and an external interface (the wiring and internet protocols to enable the data exchanges with the network).||\n|Requirement statements use the word “shall” or “should.” The word “shall” has meaning in contractual language and is enforceable legally. Other words like “will,” “may,” “should,” and “must” may show intent but are not legally binding in contracts. In some situations, it may be desirable to use “should” to show the govern­ ment’s intent and preference while at the same time allowing flexibility and latitude. Example: “The system shall have a mean time between failures of greater than 500 hours.”||\n|Requirements statements are unambiguous. Terminology is clear without the use of informal jargon. Statements are short and concise.||\n\n\n-----\n\n|Performance requirements statements (including logistics/sustainment/support) are quantifiable, testable, and/or verifiable. Avoid the phrase “shall not.” It is very difficult to prove a negative. Avoid qualitative words like “maximize” or “minimize.” They force an engineer to judge when the design is good enough. The user may think that the engineer did not “mini­ mize enough” and get into a legal argument with the contractor. Note: Every user requirements document includes: “the system shall be easy to use” requirement. Talk to other MITRE staff for examples from other projects and seek out a human factors specialist for requirements wording that is suitable both for specifying these requirements and methodologies for verifying them. Avoid specific, one-point values when defining requirements. Use ranges (minimum of, more than, less than, maximum of, within, etc.) to accommodate appropriate interpre­ tation. Using a single point value may cause arguments if the system is tested at that exact value only, or if a test appears to be successful from an intent perspective, but does not meet the exact value stated in the system requirement. Example: The system shall process a minimum of 100 transactions/sec. Example: The system shall be operable up to and including 30,000 ft. Example: The system shall operate in temperatures between 5 and 35 degrees Celsius.|Col2|\n|---|---|\n|If objective performance values are included as goals, ensure they are clearly identified and distinguished from firm requirements. User requirement documents refer to threshold requirements (those that must be pro­ vided), and objective requirements (better performance has value to the user, but not above the objective requirement). Example: The system shall detect and display up to 100 targets within the surveil­ lance volume with a goal of detecting and displaying up to 125 targets.||\n|The operational and support environment is described and defined. Example: The system shall be maintainable by an Air Force level 5 technician. Example: The system shall be reparable while in flight.||\n|The requirements include appropriate use of Government and industry specifications, standards, and guides. Only include them if they are relevant and ensure that the correct version is listed in a list of reference documents.||\n|Verification approaches for all system performance and sustainability requirements are complete and appropriate. Every requirement must have a verification method identified. If a requirement cannot easily be verified by a direct inspection, measurement, or one- time demonstration of the requirement, the verification requirement should include an expanded test criteria description to ensure that there is no disagreement later in the program. This can include describing the number of trials, statistical criteria to be used, conditions of the test such as simulated inputs, etc.||\n\n\n-----\n\nSystem requirements should be tracked or traced to the user requirements. Tracing a\nrequirement means to cross-reference the source (in this case user) requirement on which a\nsystem-level requirement is based, and also to reverse reference which system requirement(s)\nimplement the source requirements. Tracing user to system-level requirements helps ensure\nthat all requirements have some user basis and that all user requirements are included in the\nsystem requirements for development. It is advisable to place the assumptions, constraints,\nand analyses associated with any derived requirements into a decision and/or requirements\ndatabase as well.\n\nIt is possible to manually manage user to system-level requirement cross-references. Many\nprojects use spreadsheets, databases, or word processors to manage requirement information.\nHowever, it is recommended that a project adopt a commercial requirements tool to aid in the\nprocess. Specialized database tools, such as DOORS (by IBM Rational), can be used to capture\ntext requirements statements, diagrams, verification requirements, and other electronic files.\n\nFor related information, see the article “Assess the Design’s Ability to Meet the System\nRequirements.”\n\n###### Best Practices and Lessons Learned\n\n\nThe devil’s in the (right level of) details. The\n\nprimary challenge associated with developing\n\nsystem-level requirements is to provide enough\n\ndetail so that there is sufficient information to\n\nimplement the right system, yet not too much\n\ndetail to restrict designers unnecessarily. One\n\ncan think of a car analogy. If you were specifying\n\nrequirements for a public transportation vehicle,\n\nyou might specify the number of passengers,\n\nspeed the vehicle must be capable of, and dis­\n\ntance that must be covered. If it were intended\n\nto be a concept like an automobile, one would\n\nadd requirements associated with single-user\n\noperation and with the regulations that affect\n\nautomobile design to allow it to operate on public\n\nroadways.\n\nWith too high of a level of detail, the designer\n\nhas insufficient information to provide a system\n\nthat will meet the user need, and the designer\n\n\nwill have to guess what was intended. In the\n\nautomobile example, failing to include a require­\n\nment for a minimum amount of cargo space\n\ncould result in a design with insufficient cargo\n\nspace, yet the system would be compliant with\n\nthe requirements. Do not assume that another\n\nperson’s view of a logical, detailed design choice\n\nwill match yours.\n\nToo low of a level of detail can artificially constrain\n\nthe design. Additional requirements may prevent\n\na designer from making choices that can pro­\n\nvide innovative solutions. This problem is often\n\nencountered because people have a natural\n\ntendency to want something they have seen and\n\nwould like to be included in the final product.\n\nAs a design concept is explored, people on the\n\ncustomer/sponsor side discover design details\n\nand then try to ensure that the particular feature\n\nof interest is included in the requirements. In the\n\n\n-----\n\ncar example, such a person might try to specify\n\na particular high-end sound system with custom\n\ninterfaces for their favorite player when all that is\n\nrequired is compatibility with commonly pur­\n\nchased media such as compact disks.\n\nSystem cost is likely to increase from add­\n\ning too many low-level requirements to the\n\nsystem specification. Every system requirement\n\nprovided to a contractor has a cost, even if they\n\nhad already planned to include it in their design.\n\nEvery requirement must be documented,\n\nallocated to design specifications, tracked, and\n\nformally tested. Contractors will bid that work.\n\nIn addition, the detail requirements become\n\na permanent part of the top-level system\n\nrequirements for all future upgrades until they\n\nare explicitly changed.\n\nClose communications with the users and use of\n\nprototypes, experiments, etc., are mechanisms\n\nto ensure a collective understanding of what is\n\nneeded and to manage level-of-details require­\n\nments issues.\n\nStakeholders’ agreement. Because system\nlevel technical requirements are central to the\n\ndefinition of what is to be procured and have\n\nsignificant effects on program acquisition risk,\n\ncost, and schedule, reaching agreement on\n\nsystem requirements is challenging but critical.\n\nMany stakeholder organizations and individuals\n\noften have differing opinions. Contractors may\n\nargue to mitigate their risk or slant a requirement\n\nin their favor. Users may try to guide design or get\n\nadditional capability. Co-workers and the spon­\n\nsor have opinions based on their experience and\n\nperceptions. Sometimes political implications\n\n\n(real or assumed) exist when a requirement chal­\n\nlenge is initiated by a sponsor senior leader or\n\nend user. If you are an engineer defining sys­\n\ntem technical requirements, you will eventually\n\nencounter arguments about:\n\n###### � [What a requirement statement or a spe­]\n\ncific word within a requirement statement\n\nmeans\n###### � [Whether a requirement should be included ]\n\nor excluded\n###### � [Whether a requirement is specifying too ]\n\nmuch or too little performance\n###### � [Whether a requirement directs design and ]\n\nshould be removed\n###### � [Whether a requirement should be added ]\n\nto guide design and ensure that a specific\n\nfeature is provided.\n\nTo resolve requirement arguments, it is often\n\nhelpful to understand the source of the objec­\n\ntion to your system requirement. The source of\n\nsomeone’s argument often comes from one of\n\nthe following situations:\n\n###### � [They have experience and are really trying ]\n\nto help.\n\nyy They might not always be right, but they\n\nbelieve they are helping build a better\nset of requirements.\n\nyy You may have to do more research or\n\nmake a judgment as whether their posi­\ntion has merit.\n###### � [They have a concern about how someone ]\n\nelse may interpret the requirement.\n\nyy Contractors: They are afraid of how\n\ntheir words could be interpreted by\nothers. Losing an argument about an\n\n\n-----\n\ninterpretation of a requirement late in\nthe program is highly undesirable to\nthe contractor because of the higher\ncost and schedule impact due to late\ndesign changes.\n\nyy Program Office person or user repre­\n\n_sentative: They are afraid the require­_\nment will not force the contractor to\nprovide a specific design feature that\nthey want included in the system.\n###### � [They want the program to adopt a com­]\n\npeting technical approach.\n\nyy Contractors: They want to slant the\n\nacquisition toward their specific solu­\ntion to get the contract award or allow\nthem to meet the requirement with a\nsolution they have already identified.\n\nyy Any Party: They may have a valid\n\ntechnical solution or they may want to\nhave the project adopt their require­\nment over yours to demonstrate their\ncontribution.\n###### � [They insist on a specific detail or feature ]\n\nthe system must have, and they want\n\n###### References and Resources\n\n\nspecific words to include as a require­\n\nment. Any party can fear that if you don’t\n\nspecify the requirement explicitly, they\n\nwill not get it.\n\nResolving any of these issues requires a mixture\n\nof negotiation ability, technical expertise, and\n\nan ability to understand the root cause of the\n\nother person’s concern. Choosing clear require­\n\nments language is a good starting point. Being\n\nconsistent with specification wording and using\n\nterminology similar to that employed in past\n\nprojects with the contractor can also help.\n\nUnderstanding and experiencing the operational\n\nenvironment will give the MITRE SE additional\n\nknowledge on which to judge the requirements.\n\nIn other cases, the SE will have to explore\n\nthe other person’s objections and determine\n\nwhether their position has merit, technical or\n\notherwise.\n\nFor related information, see the SEG top­\n\nics Requirements Engineering and System\n\nArchitecture.\n\n\n1. The MITRE Corporation, “Requirements Engineering,” MITRE Systems Engineering\n\nCompetency Model, section 2.2.\n\n2. Department of Defense, December 8, 2008, DoD Instruction 5000.02.\n\n3. Chairman of the Joint Chiefs of Staff, March 1, 2009, Joint Capabilities Integration and\n\nDevelopment System (CJCSI 3170.01G).\n\n4. Stevens, R., P. Book, K. Jackson, and S. Arnold, 1998, Systems Engineering: Coping\n\n_with Complexity, Prentice Hall._\n\n5. Blanchard, B., and W. Fabrycky, 1998, Systems Engineering and Analysis, Prentice\n\nHall.\n\n6. International Council on Systems Engineering (INCOSE), January 2010, INCOSE\n\nSystems Engineering Handbook, Ver. 3.2, INCOSE-TP-2003-002-03.2.\n\n\n-----\n\n###### Additional References and Resources\n\nNavy PEO Integrated Warfare Systems, October 27, 2008, Surface Navy Combat Systems\n_Development Strategy Acquisition Management Plan (AMP)._\n\n\n-----\n\nDefinition: In acquisition\n_oriented systems engineering, a_\n\n_top-level system design repre­_\n\n_sents the envisioned implemen­_\n\n_tation of a system in sufficient_\n\n_detail to support credible_\n\n_projections of cost, schedule,_\n\n_performance, evolution, and_\n\n_risk. It helps in assessing sys­_\n\n_tem feasibility at the program’s_\n\n_outset, performing analyses_\n\n_of alternatives, and finalizing_\n\n_requirements and budgets_\n\n_prior to contract solicitation. If_\n\n_carefully developed, the design_\n\n_becomes the program’s early_\n\n_technical baseline for acquisi­_\n\n_tion planning activities._\n\nKeywords: cost analysis,\n\n_cost analysis requirements_\n\n_document, early design, early_\n\n_system design, early systems_\n\n_engineering, requirements_\n\n_optimization, technical baseline,_\n\n_top-level system design_\n\n\nSYSTEM DESIGN AND DEVELOPMENT\n###### Develop Top-Level System Design\n\n**MITRE SE Roles and Expectations: During initial**\n\ncapability planning activities, the MITRE systems\n\nengineer (SE) is often involved in establish­\n\ning a sound program baseline, which includes\n\nan understanding of the system operational\n\nrequirements, the system design concept, the\n\narchitecture, the technical requirements, and\n\nthe associated program cost and schedule. In\n\nthese situations, a MITRE SE is expected to:\n\n###### �Understand the purpose and role of top-level\n\nsystem design in the acquisition process\n\n###### �Understand how and when a top-level system\n\ndesign should be undertaken\n\n###### �Understand the associated benefits and risks\n\n �Identify and engage subject matter experts\n\n(SMEs) with core technical skills appropriate for\n\ndeveloping the top-level system design\n\n\n-----\n\n###### �Apply the top-level system design baseline and lessons learned from the design activity\n\nin program acquisition planning activities.\n\n###### Background\n\nA top-level system design represents the government team’s independent projection of the\nway a system could be implemented to meet the prevailing requirements with acceptable\nrisk. Although a top-level system design could be mandated for eventual implementation by\na development contractor, it is generally under the stewardship of the government team. The\nprimary reason for developing a top-level system design is to provide a technical foundation\nfor planning the program. It is the government’s de facto technical approach to meeting the\ncustomer’s needs. Once defined, a top-level system design represents an approach that can be\nused to develop a program schedule and cost estimates consistent with the program’s techni­\ncal content, as well as risk assessments, acquisition strategies, a logistics approach, etc. If a\nprogram is required to develop a cost analysis requirements document (CARD), the top-level\ndesign can be the foundation of that work.\n\nBecause the government is not in the business of designing systems, the top-level system\ndesign is only representative of what can be built with conventional technology; therefore,\nthe design may sometimes indicate lower performance or greater cost than the design that\neventually will be developed by the contractor, which may include proprietary innovations\n(particularly in a competitive environment). MITRE SEs are expected to help lead the govern­\nment effort to create realistic top-level designs and associated risk mitigation activities so that\nplanning will be based on a realistic foundation. Cost, schedule, and performance projections\nbased on the top-level system design should include margins that will tend to help mitigate\nprogram risks.\n\nAs with many aspects of acquisition-oriented systems engineering, the role played by\ntop-level system design varies considerably across programs. In some programs, any design—\neven a “top-level” one—is viewed as treading excessively into implementation territory, and is\ntherefore considered the responsibility of the development contractor rather than the govern­\nment. Even programs that do not share this philosophical view often do not have enough\nsystems engineering resources for development of a top-level system design and the mandated\narchitectural frameworks. The effort needs to be a teaming approach of the government,\nMITRE SEs, support or development contractors, and industry. In some cases, academia will\nalso have a role in helping with ideas for technology and capabilities that could be included in\na top-level design.\n\nThe process of developing a top-level system design engages the MITRE SE in the program\ntechnical content and provides numerous benefits:\n\n\n-----\n\n###### �By analyzing the known/draft system operational requirements, the SE can discover\n\nthe design implications of these requirements, identify any potential requirements\nconflicts or technically infeasible requirements, review and comment on the opera­\ntional documents being developed, identify and possibly initiate requirements trades\nand/or risk reduction activities such as prototypes and experiments, and assess the\naffordability of the required system. This allows for smarter early interaction with the\nauthors of the operational requirements document(s), producing clearer expectations\nand better defined operational documents (see the SEG’s Requirements Engineering\ntopic).\n###### �By developing a top-level system design, the SE discovers the complexities, depen­\n\ndencies, and interactions within the system, gaining a better understanding of the\nprogram’s technical concerns, issues, evolution needs, and risks that will have to be\nmanaged during program acquisition. The natural interplay between a top-level sys­\ntem design and system architecture should be captured in each (see the SEG’s System\nArchitecture topic).\n###### �The development of the top-level system design requires the SE to explore industry’s\n\n(and, at times, academia’s) capabilities and the available technologies. This includes an\nassessment of interest by, and capability of, the potential contractors, and maturity and\navailability of required technologies. Early involvement with the potential contractors\nfor the program tells the SE what questions to ask in the Request for Information (RFI)\nsolicitation, enables intelligent discussions with industry during Industry Days, and\nresults in a Request for Proposal (RFP) package that will be clearer, resulting in better\nquality proposals. A good understanding of the maturity (or lack thereof) of relevant\ntechnologies helps the SE define appropriate program acquisition and risk mitigation\nstrategies. Communicating the users’ needs and requirements together with the toplevel design to industry partners is key in helping them understand the need and formu­\nlating the best solutions for the capabilities.\n###### �The development of the top-level system design requires the SE to investigate availabil­\n\nity, maturity, and applicability of products (systems, subsystems, components, algo­\nrithms, etc.) that could be used to provide some of the required system functionality\nand performance, thus getting an early assessment of the risk/benefit trade-offs associ­\nated with these products.\n###### �The top-level system design process reviews the technical content and lessons learned\n\nfrom any precursor programs. The deployed capabilities, technologies, products,\napproaches, and solutions are available to the SE to consider for the new program.\n\n\n-----\n\nThe development of a top-level system design essentially mimics the first part of a system\ndesign and development program, stepping through abbreviated phases of requirements\ndefinition, decomposition, analysis, assessment of alternative solutions, and development of a\ndesign.\n\nA top-level system design may be independent of technology, contractors, and exist­\ning systems. It needs to be detailed only enough to provide inputs into program cost and\nschedule estimation activities to allow for program planning activities (acquisition strat­\negy, risk reduction, etc.). However, with many interfacing capability needs, use of services\nfrom repositories, and constraints of the technical environment where the capability must\nreside, a design cannot be completely agnostic to the technology and existing systems, and\nmust consider the overall context and enterprise into which the capability will reside (see\nEnterprise Engineering section).\n\nFinally, top-level system designing is a recurring activity. Initially, it can start as a\nrefinement of the alternative selected from the analysis of alternatives; in time, it is updated\nand fleshed out as more information becomes available after the technology readiness\nassessment, RFI/Industry Days, user requirements working groups, completion of risk\nreduction activities, etc.\n\nICD/ORD\n\nMITRE-led SE Team Program Office External\n\nCDD\n\nTradeoffs\n\nLoop times as short\n\nCONOPS Spec as a few weeks\n\nTechnology Contractor\nassessment proposals\n\nCost Cost\n\nCARD\n\nmodel estimate\n\nBaseline Risk\ndesign assessment\n\nPerf. Performance Effectiveness\nmodel analysis analysis\n\nFigure 1. The Top-Level System Design Loop\n\n|Col1|Col2|\n|---|---|\n|||\n\n\nCDD\n\n\nCONOPS\n\n\nSpec\n\n\n-----\n\n###### The Top-Level System Design Process\n\nFigure 1 shows how the top-level system design serves as the core of the program office’s\nsystems engineering analysis loop.\n\nThe figure represents a snapshot taken just prior to issuance of the RFP for the system\ndevelopment phase, at which point the major systems engineering challenge facing the\nprogram was the finalization of system requirements and the program budget. However, the\nsame process was used earlier in the program to establish system feasibility, and identify and\nassess the maturity of critical technologies. Subsequently, after system acceptance, planning\nand budgeting for upgrades can be established.\n\n###### Best Practices and Lessons Learned\n\n\nRepresentative design. During the top-level sys­\n\ntem design activities, the SE has to be continu­\n\nally aware that the top-level system design may\n\nbe just one of many possible solutions. The SE\n\nshould keep an open mind to all available designs\n\nand ensure as “generic” a top-level system design\n\nas possible, one that will enable competition by\n\nall relevant industry members. Absent compel­\n\nling performance reasons, the top-level system\n\ndesign solution should avoid locking in on a\n\none-party approach (e.g., using an idea, solu­\n\ntion, or existing product available to only some\n\ncontractors).\n\nCompeting designs. Sometimes two mutually\n\nexclusive approaches are possible for the system\n\ndesign (e.g., software intensive vs. hardware\n\nsolutions, or reuse of extensive existing products\n\nvs. new development). In this case, basing the\n\nprogram plans on a single top-level system design\n\nmay have negative consequences because it\n\ncould result in development of system require­\n\nments that would preclude the bid of one (or\n\nmore) contractors, or result in unrealistic program\n\ncost/schedule estimates. In this instance, the SE\n\n\nshould help evaluate the alternatives, identify and\n\nconsider their pros and cons, and carefully decide\n\nwhich top-level system designs should be devel­\n\noped and carried forward into program planning\n\nto cover the range of implementation options.\n\nApplicable, feasible, affordable, phased. Another\n\nchallenge for the SE is to keep the top-level\n\nsystem design feasible and affordable and not\n\nfall into the trap of capturing the best features\n\nand all the additional capabilities of the various\n\npossible approaches, alternatives, technologies,\n\nand products. It is important to ensure the top\nlevel system design meets the stated operational\n\nneeds and is affordable and available in useful\n\ntime. The implementation of solutions to complex\n\nproblems is likely to be time-phased. Therefore,\n\nthe top-level design should plan for evolutions of\n\ncapabilities based on urgent user needs, technical\n\nfeasibility, and affordability over time. Additional\n\n“bells and whistles” may be exciting, but they\n\nshould be avoided because in the long run they\n\nrisk breaking the program’s bank.\n\nDesign depth. Developing a top-level system\n\ndesign is hard for SEs. By definition, the top-level\n\n\n-----\n\nsystem design is not intended to resolve all prob­\n\nlems, address all issues, or mitigate all technical\n\nrisk. Letting go of a partially completed design can\n\nbe hard. The trick is to understand which parts of\n\nthe top-level design need what degree of depth\n\nin order to address the critical uncertainties to\n\nbe explored at this stage of the life cycle. That\n\ndepends on an assessment of the key risks for the\n\nparticular program and the relevant technologies.\n\nAn example of a comprehensive top-level system\n\ndesign comes from a government acquisition pro­\n\ngram that developed its capability within budget\n\nand ahead of schedule, earning the government\ncontractor program team accolades for exem­\n\nplary acquisition excellence. MITRE serves as the\n\nlead systems engineering organization for the\n\ngovernment program office team, which includes\n\nmultiple systems engineering and technical assis­\n\ntance contractors, government laboratories, and\n\nother federally funded research and development\n\ncenters.\n\nOne of the cornerstones of the program’s sys­\n\ntems engineering approach is the conviction that\n\nthe best way for program offices (POs) to obtain\n\nreliable projections of cost, schedule, perfor­\n\nmance, and risk is through independent analysis. A\n\ncomprehensive top-level system design, together\n\nwith a government team with enough subject\n\nmatter expertise to develop and use it, is the cru­\n\ncial enabler for such analysis.\n\nDepth is key. To support meaningful assess­\n\nments of feasibility, cost, and performance, a\n\ntop-level system design must be detailed enough\n\nto identify and describe critical items. The level of\n\ndetail needed to get to “critical items” depends on\n\nenabling technology, requirements, and the like. In\n\n\nthis particular program, sufficient detail required\n\nidentification and description of critical items\n\nsuch as enabling chips and components (e.g., low\nnoise amplifier monolithic microwave integrated\n\ncircuits, high-power-density direct-current to\n\ndirect-current converters, heat exchangers), key\n\nalgorithms (e.g., false target mitigation), and all\n\nrequired computer software configuration items.\n\nThe required depth was about the same as that\n\nof the technical volume of a system development\n\nproposal; in the case of the referenced program,\n\nthe top-level system design document was more\n\nthan 300 pages, and extended to work breakdown\n\nstructure (WBS) Level 4—and in some cases,\n\nLevel 5.\n\nA typical objection against this degree of depth\n\nis that “nobody is ever going to build this gov­\n\nernment design anyway, so it’s better to base\n\nprogrammatic projections on conceptual designs\n\nprovided by the competing contractors.” One\n\nanswer to this objection is that costs and risks are\n\nlargely determined by decisions made early in a\n\nprogram’s life, well before any contractor-devel­\n\noped conceptual designs exist and often before\n\ncandidate contractors have been identified. A\n\nmore compelling answer is that, in a competitive\n\nenvironment, there are strong incentives for even\n\nthe best intentioned contractors to underesti­\n\nmate the challenges associated with meeting\n\nthe requirements. Conversely, in a sole-source\n\nenvironment, there may be strong incentives\n\nto overstate those challenges, particularly by a\n\ncontractor who is already producing a system that\n\ncould be viewed as a competitor to the envisioned\n\nsystem.\n\n\n-----\n\nFor example, in the referenced program the origi­\n\nnal operational requirements called for capabilities\n\nthat in many ways exceeded those of any extant\n\nairborne system. At that time, other airborne\n\nsensors were tightly integrated into the host air­\n\nframes, requiring extensive aircraft modifications.\n\nIn contrast, to minimize cost and maximize opera­\n\ntional availability, the operational user organization\n\nwanted the bulk of the system mission equipment\n\nto be encapsulated in a self-contained pod that\n\ncould be rapidly moved between small—and lightly\n\nmodified—host aircraft. At the time these require­\n\nments were floated, the industry consensus was\n\nthat they were infeasible. However, the compre­\n\nhensiveness and depth of the top-level system\n\ndesign provided the sponsor with the confidence\n\nto properly budget and proceed with the acquisi­\n\ntion, which eventually proved highly successful.\n\nDon’t skimp on the team. The team required to\n\ndevelop a sound top-level system design of the\n\nrequisite technical depth is substantial—about\n\n30 to 40 people in the case of the referenced\n\nprogram—with total effort of about 30 staff-years.\n\nOf these, about 70 percent were SMEs, including\n\nhardware and software engineers, cost analysts,\n\noperational analysts, and experts in modeling\n\nand simulation. Approximately 20 percent were\n\nprocess experts, responsible for planning, con­\n\nfiguration management, requirements traceability,\n\nand acquisition process compliance. Finally, 10\n\npercent were technical leaders, including a chief\n\nengineer and system architect. Critical to all of this\n\nis involving the end-user community in the design\n\nprocess all along the way to help with trade-off\n\nanalysis and negotiation of needs.\n\n\nFor this particular program, the same demo­\n\ngraphic makeup was also about right for virtually\n\nall of the other technically intensive government\n\nsystems engineering activities necessary to sup­\n\nport this acquisition, including proposal evalua­\n\ntion. For example, the top-level system design\n\nteam eventually formed the core of the cost and\n\ntechnical boards that advised during proposal\n\nevaluations; their experience in developing and\n\ncosting the top-level system design was invalu­\n\nable in ensuring thorough proposal evaluations.\n\nInvolve cost analysts in the design synthesis. A\n\nkey use for a top-level system design is to serve\n\nas the basis for cost projections; a properly docu­\n\nmented top-level system design meets all the\n\nrequirements for the DoD-mandated CARD.\n\nThe referenced program’s experience shows\n\nthere is substantial room for error in the subse­\n\nquent cost estimate, no matter how thoroughly a\n\ndesign is documented, unless the cost analysts\n\nare involved in the design synthesis. Cost analysts\n\ncan make substantive contributions to the design\n\neffort by helping to define an appropriate WBS,\n\nwhich effectively serves as the outline for the\n\ntop-level system design document, and identify­\n\ning the type of technical information that should\n\nbe documented for each WBS area to facilitate\n\ncosting.\n\nAn added benefit of involving the cost analysts\n\nup front is that cost models can be developed\n\nin parallel with the design synthesis, enabling the\n\ncost estimates to be provided sooner (about two\n\nmonths sooner, in the case of the referenced\n\nprogram) than those developed in a sequential\n\nprocess.\n\n\n-----\n\nAs previously mentioned, a crucial benefit to\n\nparticipating in the design process is that it gives\n\nthe cost analysts greater insight into the salient\n\ntechnical issues, enabling them to perform a thor­\n\nough evaluation of the cost proposals submitted\n\nby the contractors. Although difficult to quantify,\n\nthis benefit was evident in the program’s source\n\nselection.\n\nMITRE’s organic cost analysis capability and\n\nexperience in tightly integrating that capability\n\nwith extensive in-house technical subject matter\n\nexpertise was unique among the many organiza­\n\ntions supporting the acquisition program, and\n\nthe resulting synergy that was achieved provided\n\nmajor added value to the program.\n\nInvolve performance modelers in the design\n\nsynthesis. The top-level system design also\n\nserves as the basis for system performance pro­\n\njections. As with cost estimation, involving opera­\n\ntional analysts and performance modelers up\n\nfront allows the required models to be developed\n\nsooner, while improving the fidelity of the results.\n\nEarly development of the performance model\n\nprovided a key benefit for the referenced pro­\n\ngram: it enabled the government to include the\n\nMITRE-developed model—but without the design\n\nparameters specific to the government design—as\n\ngovernment-furnished information in the RFP\n\npackages provided to candidate offerors. Offerors\n\nwere required to provide design-specific input\n\nfiles for the model as part of their proposals. This\n\nenabled the government to use a common tool to\n\nrapidly evaluate the performance of the compet­\n\ning designs—the same tool that was used to help\n\nset the requirements in the first place.\n\n\nThis continued to pay dividends beyond the\n\nsource selection. The program’s development\n\ncontract called for the selected offeror to include\n\na performance model as part of the delivered\n\nsystem (e.g., to support mission planning). The\n\ncontractor was free to adapt an existing model\n\nfor this purpose or to develop one from scratch.\n\nBecause the MITRE/government model had been\n\ndeveloped and already had been validated within\n\nthe government team, the contractor elected to\n\nuse it as the basis for the delivered model, thereby\n\nreducing cost and risk.\n\nOrganizationally collocate the key members of\n\nthe team. Developing a sound top-level system\n\ndesign requires extensive face-to-face commu­\n\nnication, necessitating that most team members\n\nbe physically collocated. However, even with\n\nphysical collocation, risks increase when the\n\nteam is spread across multiple organizations and\n\nmultiple management chains of command. The\n\nexperience of the referenced program shows that\n\nsynthesizing and leveraging a top-level system\n\ndesign becomes problematic unless key hardware\n\nSMEs, key software SMEs, and key cost analysts\n\nare all resident within the lead systems engineer­\n\ning organization. Although detailed-level design\n\nand implementation activities may sometimes be\n\nwell accomplished by allocating tasks to hardware,\n\nsoftware, and cost support organizations that are\n\n“best” in each discipline, early design work should\n\nbe accomplished by a close-knit team represent­\n\ning all the required specialties, as well as overall\n\nsystems engineering leadership.\n\nEnsure the top-level system design is an\n\nindependent government product. Contractor\ndeveloped conceptual designs may already exist\n\n\n-----\n\nwhen the government team undertakes devel­\n\nopment or update of a top-level system design.\n\nBecause the top-level system design is used to\n\nhelp set requirements and budgets, inclusion\n\nof any of the contractors’ proprietary innova­\n\ntions in the government design, thereby reveal­\n\ning an ostensibly preferred design approach,\n\ncould destroy the integrity of a source selection.\n\nThe government and MITRE must be cautious\n\nstewards of the top-level design. If a decision is\n\nmade to share the design with industry, it must\n\nbe shared with all interested industry partners\n\n\nequally. Even if care is taken to avoid inclusion\n\nof proprietary features, however, a top-level\n\nsystem design often resembles at least one of\n\nthe contractor-developed conceptual designs.\n\nThis should present no problems, provided that\n\na paper trail can show the design was inde­\n\npendently derived. For example, the referenced\n\nprogram’s top-level system design provided an\n\nextensive design rationale that illustrated how the\n\ndesign flowed naturally from the requirements,\n\nbased on technologies and sensor design prac­\n\ntices documented in the open literature.\n\n\n-----\n\nDefinition: The ability of a sys­\n\n_tem design to meet operational,_\n\n_functional, and system require­_\n\n_ments is needed to meet a sys­_\n\n_tem’s ultimate goal of satisfying_\n\n_mission objective(s). One way_\n\n_to assess the design’s ability to_\n\n_meet the system requirements_\n\n_is through “requirements trace­_\n\n_ability.” This is the process of_\n\n_creating and understanding the_\n\n_bidirectional linkage between_\n\n_requirements (operational_\n\n_need), organizational goals, and_\n\n_solutions (performance)._\n\nKeywords: assessment, con­\n\n_cept of operations (CONOPS),_\n\n_functional requirements,_\n\n_mission and needs, operational_\n\n_requirements, performance_\n\n_verification, requirements,_\n\n_requirements traceability,_\n\n_requirements traceability_\n\n_matrix, system requirements,_\n\n_traceability, verification_\n\n\nSYSTEM DESIGN AND DEVELOPMENT\n###### Assess the Design’s Ability to Meet the System Requirements\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to under­\n\nstand the importance of system design in meeting\n\nthe government’s mission and goals. They are\n\nexpected to be able to review and influence the\n\ncontractor’s preliminary design so that it meets\n\nthe overall business or mission objectives of the\n\nsponsor, customer, and user. MITRE SEs are\n\nexpected to be able to recommend changes to\n\nthe contractor’s design activities, artifacts, and\n\ndeliverables to address performance shortfalls and\n\nadvise the sponsor or customer if a performance\n\nshortfall would result in a capability that supports\n\nmission requirements, whether or not the design\n\nmeets technical requirements. They are expected\n\nto be thought leaders in influencing decisions\n\nmade in government design review teams and\n\nto appropriately involve specialty engineering.\n\n\n-----\n\nIn requirements traceability and performance verification, MITRE SEs are expected to\nmaintain an objective view of requirements and the linkage between the system end-state\nperformance and the source requirements and to assist the government in fielding the best\ncombination of technical solution, value, and operational effectiveness for a given capability.\n\n###### Background\n\nTraceability and Verification Process\n\nA meaningful assessment of a design’s ability to meet system requirements centers on the\nword “traceability.” Traceability is needed to validate that the solution delivered fulfills the\noperational need. For example, if a ship is built to have a top speed of 32 knots, there must be\na trail of requirements tied to performance verification that justifies the need for the additional\nengineering, construction, and sustainment to provide a speed of 32 knots. The continuum of\nrequirements generation and traceability is one of the most important processes in the design,\ndevelopment, and deployment of capability.\n\nTraceability is also the foundation for the change process within a project or program.\nWithout the ability to trace requirements from end to end, the impact of changes cannot be\neffectively evaluated. Furthermore, change should be evaluated in the context of the end-toend impact on other requirements and overall performance (e.g., see the SEG’s Enterprise\nEngineering section). This bidirectional flow of requirements should be managed carefully\nthroughout a project/program and be accompanied by a well-managed requirements baseline.\n\nRequirements Flow\n\nThe planned functionality and capabilities offered by a system need to be tracked through\nvarious stages of requirements (operational, functional, and system) development and evolu­\ntion. Requirements should support higher level organizational initiatives and goals. It may\nnot be the project’s role to trace requirements to larger agency goals. However, it can be a best\npractice in ensuring value to the government. In a funding-constrained environment, require­\nments traceability to both solutions as well as organizational goals is essential in order to\nmake best use of development and sustainment resources.\n\nAs part of the requirements traceability process, a requirement should flow two ways: (1)\ntoward larger and more expansive organizational goals, and (2) toward the solution designed\nto enable the desired capability.\n\nRequirements Verification\n\nBecause the ability to test and verify is a key element of project/program success, require­\nments tied to operational needs should be generated from the outset and maintained\n\n\n-----\n\nthroughout the requirements life cycle with test and verification in mind. Advice on the ability\nto test requirements can be extremely effective in helping a project or program. Techniques\nsuch as prototyping and experimentation can help assess requirements early and provide a\nvaluable tool for subsequent verification and validation. For more information, see the article\n“Competitive Prototyping.”\n\nTest and verification plan development and execution should be tied directly back to the\noriginal requirements. This is how the effectiveness of the desired capability will be evaluated\nbefore a fielding decision. Continual interaction with the stakeholder community can help\nrealize success. All test and verification efforts should relate directly to enabling the fielding\nof the required capability. Developing test plans that do not facilitate verification of required\nperformance is an unnecessary drain on resources and should be avoided.\n\nBefore a system design phase is initiated, it should be ensured that the system require­\nments, captured in repositories or a system requirements document, can be mapped to func­\ntional requirements (e.g., in a functional requirements document [FRD]). The requirements in\nthe FRD should be traceable to operational requirements (e.g., in an operational requirements\ndocument or a capabilities development document). If all this traceability is ensured, there is\na better likelihood that the design of the system will meet the mission needs articulated in a\nconcept of operations and/or the mission needs statement of the program.\n\nDesign Assessment Considerations\n\nThe design of a system should clearly point to system capabilities that meet each system\nrequirement. This two-way traceability between design and system requirements will enable\nhigher probability of a successful test outcome of each system requirement, the system as a\nwhole, and delivery of a useful capability.\n\nAs the service-oriented architecture (SOA) approach matures, there is increased emphasis\non linking system requirements to specific services. Therefore, the artifacts or components of\nsystem design should be packaged in a manner that supports provisioning of services offered\nwithin SOA (assuming that deployment of SOA is a goal of an enterprise).\n\nFor example, assume that the system requirements can be grouped in three catego­\nries: Ingest, Analysis, and Reporting. To meet system requirements within these categories,\nthe design of the system needs to point to each component of the system in a manner that\naddresses how it would fulfill the system requirements for Ingest, Analysis, and Reporting,\nrespectively. The design information should include schematics or other appropriate artifacts\nthat show input, processing, and outputs of system components that collectively meet system\nrequirements. Absent a clear roadmap that shows how input, processing, and outputs of a sys­\ntem component meet a given system requirement, there is risk in whether that specific system\nrequirement will be met.\n\n\n-----\n\n###### The Requirements Traceability\n\nRequirements\n\n###### Matrix: Where the Rubber Meets the Road\n\nTypically the project team develops a\n\nDesign Test\n\nrequirements traceability matrix (RTM)\nthat shows linkage between functional\nrequirements, system requirements, and\nsystem capabilities of system design com­ Development\nponents. An RTM that can clearly point to\nsystem components that are designed to Figure 1. Traceability Matrix Relationships\nmeet system requirements is more likely\nto result in a well-designed system, all\nother considerations being equal. Additional linkages can be included in an RTM to show\nmechanisms to test functionality of system components for testing the design of the system\nto meet system requirements. An RTM as described above (i.e., one that ranges from state­\nment of a requirement to methodology to test the system component that satisfies the system\nrequirement) will go a long way in successfully assessing a system design to meet system\nrequirements.\n\nA traceability matrix is developed by linking requirements with the design components\nthat satisfy them. As a result, tests are associated with the requirements on which they are\nbased and the system is tested to meet the requirement. These relationships are shown in\nFigure 1.\n\nA sustained interaction is needed among members of a requirements team and those of\ndesign and development teams across all phases of system design and its ultimate develop­\nment and testing. This kind of dialog helps ensure that a system is being designed properly\nwith an objective to meet system requirements. In this way, an RTM provides a useful mecha­\nnism to facilitate the much-needed interaction among project team members.\n\nTable 1 is a sample RTM that spans “Requirement Reference” to “Design Reference.” The\nmatrix can be extended to include testing mechanisms for further assurance that the system\ndesign will meet system requirements. The RTM in Table 1 links a system requirement to a\ndesign component (e.g., a name of a module).\n\nThe assessment of a system design should consider how well the design team presents the\nlinkage of its design to system requirements (i.e., through documentation, presentations, and/\nor critical design reviews). A traceability matrix can be an important device in communicat­\ning a design’s ability to meet system requirements.\n\nAs part of the system design approach, the design team may develop mock-ups and/or\nprototypes for periodic presentation to the end users of the system and at design reviews. This\n\n\n-----\n\nTable 1. Sample RTM Linking System Requirement to Design Component\n\nProject Name: Author:\n\nDate of Review: Reviewed By:\n\n**Requirement** **Requirement** **Design** **System Feature**\n**Req. ID**\n\n**Reference** **Description** **Reference** **Module Name**\n\nAPP 1.1 APP SRS Ver 2.1 Better GUI APP Ver 1.2 Module A\n\nSend Alert\nAPP 1.2 APP SRS Ver 2.1 APP Ver 1.2 Module B\nmessages\n\nAPP 1.3 APP SRS Ver 2.1 Query handling APP Ver 1.2 Module C\n\nAPP 1.4 APP SRS Ver 2.1 Geospatial Analysis APP Ver 1.2 Module D\n\nTable 2. Sample RTM Linking a Test Case Designed to Test a System Requirement\n\n**Acceptance** **Requirement**\n**Unit Test Case #** **System Test Case #**\n**Test Case #** **Type**\n\nAPP_GUI.xls TC_APP_GUI.xls UAT_APP_GUI.xls New\n\nAPP_MSG.xls TC_APP_MSG.xls UAT_APP_MSG.xls Change Request\n\nAPP_QRY.xls TC_APP_QRY.xls UAT_APP_QRY.xls New\n\nAPP_GA.xls TC_APP_GA.xls UAT_APP_GA.xls Change Request\n\napproach provides an opportunity for system designers to confirm that the design will meet\nsystem requirements. Therefore, in assessing a system design, the design team’s approach\nneeds to be examined to see how the team is seeking confirmation of its design in meeting\nsystem requirements.\n\n###### Best Practices and Lessons Learned\n\n|Project Name:|Col2|\n|---|---|\n|Date of Review:||\n\n|Author:|Col2|\n|---|---|\n|Reviewed By:||\n\n|Req. ID|Requirement Reference|Requirement Description|Design Reference|System Feature Module Name|\n|---|---|---|---|---|\n|APP 1.1|APP SRS Ver 2.1|Better GUI|APP Ver 1.2|Module A|\n|APP 1.2|APP SRS Ver 2.1|Send Alert messages|APP Ver 1.2|Module B|\n|APP 1.3|APP SRS Ver 2.1|Query handling|APP Ver 1.2|Module C|\n|APP 1.4|APP SRS Ver 2.1|Geospatial Analysis|APP Ver 1.2|Module D|\n\n|Unit Test Case #|System Test Case #|Acceptance Test Case #|Requirement Type|\n|---|---|---|---|\n|APP_GUI.xls|TC_APP_GUI.xls|UAT_APP_GUI.xls|New|\n|APP_MSG.xls|TC_APP_MSG.xls|UAT_APP_MSG.xls|Change Request|\n|APP_QRY.xls|TC_APP_QRY.xls|UAT_APP_QRY.xls|New|\n|APP_GA.xls|TC_APP_GA.xls|UAT_APP_GA.xls|Change Request|\n\n\nTraceability and Verification\n\nDevelopment of project/program scope. The\n\noverall goals or desired impact for a project/\n\n\nprogram must be understood and delineated\n\nfrom the beginning of the effort. The solutions\n\nand technologies required can and should evolve\n\n\n-----\n\nduring the systems engineering process, but the\n\ndesired capability end state should be well under­\n\nstood at the beginning. “What problem are we\n\ntrying to solve?” must be answered first.\n\nQuality of written requirements. Poorly written\n\nrequirements make traceability difficult because\n\nthe real meaning is often lost. Ambiguous termi­\n\nnology (e.g., “may,” “will”) is one way requirements\n\ncan be difficult to scope, decompose, and test.\n\nAssist with written requirements by ensuring a\n\ncommon and clear understanding of terminology.\n\nExcessive reliance on derived requirements. As\n\nwork moves away from a focus on original require­\n\nments, there is a danger of getting off course for\n\nperformance. Over-reliance on derived require­\n\nments can lead to a loss of context and a dilution\n\nof the true nature of the need. This is an area\n\nwhere traceability and the bidirectional flow of\n\nrequirements are critical.\n\nUnique challenges of performance-based\n\nacquisition. Performance-based projects pres­\n\nent a unique set of issues. The nature of perfor­\n\nmance-based activity creates ambiguity in the\n\nrequirements by design. This can be extremely\n\ndifficult to overcome in arriving at a final, user\nsatisfactory solution. As a matter of practice,\n\nmuch greater scrutiny should be used in this\n\nenvironment than in traditional project/program\n\ndevelopment.\n\nRequirements baseline. A requirements base­\n\nline is essential to traceability. There must be\n\na trail from the original requirements set to\n\nthe final implemented and deployed capability.\n\nAll of the changes and adjustments that have\n\nbeen approved must be incorporated in order\n\n\nto provide a seamless understanding of the end\n\nstate of the effort. It should also include require­\n\nments that were not able to be met. In order to\n\nadequately judge performance, the requirements\n\nmust be properly adjusted and documented.\n\nProject/program risk impacts. The influence of\n\nrequirements on project/program risk must be\n\nevaluated carefully. If sufficient risk is generated\n\nby a requirement, then an effective mitigation\n\nstrategy must be developed and implemented.\n\nEliminating a requirement can be an outcome of\n\nthis analysis, but it must be weighed carefully. This\n\nis an area where an FFRDC trusted agent status\n\nis especially critical. Chasing an attractive yet\n\nunattainable requirement is a common element\n\nin project/program delays, cost overruns, and\n\nfailures. See the SEG’s Risk Management topic.\n\nWatch for requirements that are difficult to\n\ntest. If requirements are difficult or impossible to\n\ntest, the requirements can’t be traced to results\n\nif the results can’t be measured. System-of\nsystems engineering efforts can greatly exac­\n\nerbate this problem, creating an almost insur­\n\nmountable verification challenge. The language\n\nand context of requirements must be weighed\n\ncarefully and judged as to testability; this is\n\nespecially true in a system-of-systems context.\n\nSee the article “Test and Evaluation of Systems of\n\nSystems.”\n\nRequirements creep. Requirements creep—\n\nboth up and down the spectrum—is an enduring\n\nconundrum. As requirements flow through the\n\nsystems engineering process, they can be diluted\n\nto make achieving goals easier, or they can be\n\n“gold plated” (by any stakeholder) to provide more\n\nthan is scoped in the effort. Increasing capability\n\n\n-----\n\nbeyond the defined requirements set may seem\n\nlike a good thing; however, it can lead to difficulty\n\nin justifying program elements, performance, and\n\ncost. Adding out-of-scope capability can drasti­\n\ncally change the sustainment resources needed\n\nto maintain the system through its life cycle.\n\nRequirements creep is insidious and extremely\n\ndetrimental. On the other hand, the evolution of\n\nneeds and requirements must be accommodated\n\nso that flexibility in creating capabilities can match\n\nchanging operations and missions and provide\n\ntimely solutions.\n\nInteraction with end users. Interaction with end\n\nusers is critical to the requirements traceability\n\nand verification cycle. The ability to get feedback\n\nfrom people who will actively use the project or\n\nprogram deliverables can provide early insight into\n\npotential performance issues.\n\nBidirectional requirements traceability. There\n\nmust be a two-way trace of requirements from\n\nthe requirements themselves to both larger\n\norganizational goals and to applicable capability\n\nsolutions.\n\nVerification of test plans. Pay careful attention to\n\nthe development of the requirements verification\n\ntest plans. An overly ambitious test plan can por­\n\ntray a system that completely meets its require­\n\nments as lackluster and perhaps even unsafe. On\n\nthe other hand, a “quick and dirty” test plan can\n\nmiss potentially catastrophic flaws in a system or\n\ncapability that could later lead to personnel injury\n\nor mission failure.\n\nDesign Assessment\n\nImportance of documentation and team com­\n\nmitment. A thorough review of documentation\n\n\nand an evaluation of the design team’s com­\n\nmitment to engage with the stakeholders in the\n\ndesign process are key to conducting a meaning­\n\nful assessment of whether the system design\n\nmeets the system requirements.\n\n###### � [Review system development team’s strat­]\n\negy/approach to assess team’s commit­\n\nment in meeting system requirements.\n\nyy Interview design team lead and key\n\npersonnel.\n\nyy Review system documentation.\n###### � [Focus assessment review on: ]\n\nyy Existence of an RTM and its accuracy\n\nand currency (this does not have to be\nexhaustive, but a systematic audit of\nkey system functionality will suffice)\n\nyy Participation in critical design reviews\n\nyy Design team’s approach toward out­\n\nreach to system concept designers and\nuser community (stakeholders)\n\nyy Design team’s procedures to capture\n\nstakeholder comments\n\nyy Design team’s methodology to vet sys­\n\ntem requirements and process change\nrequest.\n\nImportance of documented and validated find­\n\nings. Document your assessment and validate\n\nyour findings.\n\n###### � [Re-validate the audit trail of how you ]\n\narrived at each finding and make any cor­\n\nrections (if needed).\n###### � [If possible, consult with design team rep­]\n\nresentative and share key findings.\n###### � [Document your re-validated findings and ]\n\nmake recommendations.\n\n\n-----\n\n###### References and Resources\n\nAgouridas, V., H. Winand, A. McKay, and A. de Pennington, 2008, “Early Alignment of Design\nRequirements with Stakeholder Needs,” Proceedings of the Institute of Mechanical Engineers,\nvol. 222, pp. 1529–1549.\n\nArkley, P., S. Riddle, and T. Brookes, 2006, “Tailoring Traceability Information to Business Needs,”\n_Proceedings of the 14th IEEE International Conference on Requirements Engineering, Sept. 11–15._\n\nChairman of the Joint Chiefs of Staff, March 1, 2009, Joint Capabilities Integration and\nDevelopment System (CJCSI 3170.01G).\n\nDefense Acquisition Guidebook.\n\nFederal Aviation Administration, May 14, 2007, Systems Engineering Manual.\n\nInternational Council on Systems Engineering (INCOSE), January 2010, INCOSE Systems\nEngineering Handbook Version 3.2, INCOSE-TP-2003-002-03.2.\n\nNASA Systems Engineering Handbook.\n\nRamesh, B., and M. Jarke, January 2001, “Towards Reference Models for Requirements\nTraceability,” IEEE Transactions on Software Engineering, vol. 27, issue 1.\n\nRamesh, B., C. Stubbs, T. Powers, and M. Edwards, April 1995, “Lessons Learned from\nImplementing Requirements Traceability,” CrossTalk.\n\nTran, E., Spring 1999, “Requirements and Specifications,” Carnegie Mellon University.\n\nCase Studies\n\nGovernment Accountability Office, March 2006, Business Systems Modernization, IRS\nNeeds to Complete Recent Efforts to Develop Policies and Procedures to Guide Requirements\nDevelopment and Management, GAO-06-310.\n\nMinardo, K., September 24, 2007, Tier 1 Case Study of TBMCS/TBONE (1999 - 2006), Social\nContexts of Enterprise Systems Engineering MITRE-Sponsored Research Project MITRE\nTechnical Report 070074.\n\nNordmann, J. C., January 2007, Overview of the Single Integrated Air Picture (SIAP) Enterprise\nand Lessons Learned to Date (2000 - 2007), An Enterprise Systems Engineering (ESE)\nPerspective.\n\nToolkits\n\n[FAA Acquisition System Toolset, http://fast.faa.gov, accessed March 12, 2010.](http://fast.faa.gov)\n\nNordmann, J. C., January 2007, The SEPO Requirements Process Toolkit, accessed March 12,\n2010.\n\n\n-----\n\n##### Systems Integration\n\nDefinition: Systems integration is the composition of a capability by assembling\n\n_elements in a way that allows them to work together to achieve an intended_\n\n_purpose._\n\nKeywords: acquisition, capability, people, process, program, reward, solution, SoS,\n\n_system of systems, systems integration_\n\n###### Context\n\nSystems integration creates a mission capability by composing sub­\n\ncomponents of the capability. It is the logical next step between design\n\nand development, and testing, verification, validation, and deployment.\n\nAlways important, integration is increasingly critical to success as the\n\nprograms MITRE supports migrate to service-oriented, composable\ncapability architectures.\n\nAfter components are developed, they must be integrated together\n\nwith or in the environment in which they are expected to operate. The\n\nassembled system is then tested to verify that it performs in accor­\n\ndance with its requirements.\n\nThere are different forms of integration. Vertical integration is when\n\nthe components of a system, developed by a single acquisition program,\n\nare integrated to produce the desired capability. Horizontal integration\n\ncreates new capabilities across individual systems developed by differ­\n\nent acquisition programs. Often, the individual systems were originally\n\n\n-----\n\ndeveloped for different customers and purposes. One of the first government resources avail­\nable on engineering these types of systems is the Systems Engineering Guide for Systems of\nSystems [1].\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to identify integration and interoperability chal­\nlenges and create integration strategies that meet the business/mission needs of end-users and\ntheir stakeholders. MITRE SEs develop and evaluate integration and interoperability options\nand observe and assess integration testing. The SE is expected to address integration and\ninteroperability issues associated with the system, including technical, programmatic, social,\nand business dimensions.\n\n###### Articles in This Topic\n\nKey aspects of systems integration include (1) identifying and assessing integration and\ninteroperability (I&I) challenges, (2) developing and evaluating I&I solutions, (3) assessing\nintegration testing approaches, and (4) interface management.\n\nThe article “Identify and Assess Integration and Interoperability (I&I) Challenges”\ndescribes the dimensions of integration and interoperability, the systems engineer’s role in\naddressing them, and best practices and lessons learned in recognizing and evaluating their\nchallenges.\n\nOnce the I&I challenges are understood, the SE is expected to develop and evaluate\nstrategies to address them, and recommend a way forward. The article “Develop and Evaluate\nIntegration and Interoperability (I&I) Solution Strategies” discusses the second half of the\nthread from I&I challenges to solutions. Read the articles together to understand the entire\nchallenge-to-solution thread.\n\nIntegration testing often poses significant challenges. The article “Assess Integration\nTesting Approaches” discusses the problems associated with testing approaches, suggests\npossible testing strategies and when integration testing, system-of-system testing, or other\napproaches are appropriate for a program.\n\nManaging interfaces also presents special challenges to systems integration. The article\n“Interface Management” discusses general principles and best practices of managing inter­\nfaces, with special attention to doing so in a complex, interconnected, service-based enter­\nprise, where the consequences of interface decisions can have significant ripple effects.\n\nFor complex integration environments, see the article “Systems Engineering Strategies for\nUncertainty and Complexity” in the SEG’s Enterprise Engineering section.\n\n\n-----\n\n###### References and Resources\n\n1. Director, Systems and Software Engineering, Deputy Under Secretary of Defense\n\n(Acquisition and Technology), August 2008, Systems Engineering Guide for System of\nSystems, Ver. 1.0, Office of the Under Secretary of Defense, Acquisition, Technology and\nLogistics (AT&L), Department of Defense (DoD), Washington, DC.\n\n###### Additional References and Resources\n\nCMMI Product Team, November 2007, “CMMI® for Acquisition,” Version 1.2.\n\n[Dahmann, J. S., J. Lane, and G. Rebovich, Jr., November 2008, “Systems Engineering for](http://www.crosstalkonline.org/storage/issue-archives/2008/200811/200811-Dahmann.pdf)\n[Capabilities,” CrossTalk—The Journal of Defense Software Engineering.](http://www.crosstalkonline.org/storage/issue-archives/2008/200811/200811-Dahmann.pdf)\n\nHybertson, D. W., 2009, Model-Oriented Systems Engineering Science, New York: Taylor &\nFrancis.\n\n_Improving processes for acquiring better products and services, Technical Report CMU/_\nSEI-2007-TR-017, ESC-TR-2007-017, Software Engineering Institute (SEI), Carnegie Mellon\nUniversity, Pittsburgh, PA.\n\nKelley, M., and R. Kepner, August 2008, Achieving Effective Results in Acquisition –\nGuidelines for Recommended Action, The MITRE Corporation.\n\nWhite, B. E., May 12–15, 2008, “Complex Adaptive Systems Engineering (CASE),” 8th\n_Understanding Complex Systems Symposium, University of Illinois at Champaign-_\nUrbana, IL.\n\n\n-----\n\nDefinitions: Integration is merg­\n\n_ing or combining two or more_\n\n_components or configuration_\n\n_items into a higher level system_\n\n_element, and ensuring that the_\n\n_logical and physical interfaces_\n\n_are satisfied and the integrated_\n\n_system satisfies its intended_\n\n_purpose [1]. Interoperability_\n\n_is the ability of two or more_\n\n_systems or components to_\n\n_exchange information and use_\n\n_the information that has been_\n\n_exchanged [2]._\n\nKeywords: integration,\n\n_interoperability_\n\n\nSYSTEMS INTEGRATION\n###### Identify and Assess Integration and Interoperability (I&I) Challenges\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be\n\nable to identify and assess integration and\n\ninteroperability (I&I) issues in the program\n\nthey support and in the enterprises of which\n\ntheir system is a part. They are expected to\n\ntake a broad view of I&I, including technical,\n\norganizational, operational, and environmental\n\nissues and interactions across systems.\n\n\n-----\n\n###### Integration and Interoperability\n\nIdentification and assessment of I&I challenges are often system-dependent. Experience\nshows that integration and interoperability are two sides of the same coin, and SEs need to be\nconcerned about both. Integration is typically addressed when a system is being developed—\nensuring that the interfaces are well understood and documented, and that the physical\nenvironment has been thoroughly addressed in the design and implementation. Interoperation\nis more about the role of the developed system—how the various components interact to\nmeet the operational business needs of the customer. A critical first step in identifying and\nassessing I&I challenges is to understand the systems engineer’s responsibilities in addressing\nintegration and the complexities of the associated problems.\n\n###### Systems Engineering Responsibilities\n\nSEs should adopt the point of view that they own the I&I issues associated with their system,\nwhether they are formally responsible for them or not. This includes all aspects of I&I: techni­\ncal, programmatic, social, and business. The degree to which any one of these aspects domi­\nnates depends on the system being developed, the cultures and agendas of the stakeholder\norganizations, and the environments into which the system is expected to be deployed.\n\nI&I can cover a broad range of issues, such as:\n\n###### �Electronic components being incorporated onto a motherboard �Computer subsystems and software forming a personal computer �Mechanical components being included in a vehicular drive train �Radio transceiver and antenna being installed in a vehicle or aircraft �Multiple computers and applications being built into a command center �Business interactions crossing many different commands or operating center\n\nboundaries.\nCommon to all of these issues is the need to manage requirements for performance, size,\nweight, power, and cooling. Environmental constraints must also be considered. When soft­\nware is a component, processing platforms, operating systems, and languages are all concerns\nfrom multiple perspectives, including intended use, future supportability, and modernization\nroadmaps.\n\nI&I can take on a nested nature. In a worst-case scenario, the SE may have to worry about\neverything from the development of integrated circuits to the collection of business processes\nspanning organizations, and everything in between. Fortunately, this extreme situation is\nrare, and SEs often rely on modern information technology (IT) to provide significant compo­\nnents for their systems. Due to the evolution of IT and the systems engineer’s relationship to it,\nalong with the government’s increasing focus on capability development, MITRE’s role in I&I\n\n\n-----\n\nis moving from integration of components and subsystems to integration of systems, systemsof-systems, and enterprises.\n\n###### Complexities of I&I\n\nSEs can identify and assess I&I challenges in several interacting areas: technical, program­\nmatic, social, and business.\n\nThe technical area focuses on the system itself, usually without regard for people. There\nare physical, logical, and environmental aspects to consider. The SE must ensure that physical\nsubsystems fit together and interact properly. Items such as cabling or mechanical fit and fin­\nish at the interfaces must be verified. Logically the SE needs to ensure signals are interpreted\ncorrectly, and that data exchanged at the interfaces conforms to a defined structure and\nintended semantics. Further, the SE must understand how the system under consideration fits\noperationally into the enterprise in which it will exist, and how the technical capabilities work\nin conjunction with other systems to meet mission needs. Finally, the SE may need to address\nhow the system supports installation and the possibility of dual side-by-side operations with\nan existing system to support transition.\n\nThe programmatic and social areas are dominated by the system stakeholders.\nStakeholders include everyone who needs to be involved in the development of the system:\nowner, administrator, operator, etc. Each will have a different perspective on risk associated\nwith the project, and often these risks are not technical. The SE needs to listen to and consider\nall stakeholder views, while understanding that the goal is not to accommodate all stake­\nholder requests. This can be a driver in complexity of system development. Although juggling\nexpectations, budgets, and schedules is a program manager’s responsibility, the SE will have a\nmajor stake in working out informed decisions.\n\nThe SE also must understand the business environment in which the program operates—\nfunding, relationships and dependencies with other programs and organizations, business\nstrategies, and motivations—so integration issues can be identified and understood in the\ncontext of this environment.\n\nFinally, enterprise constraints must be addressed. The enterprise is typically concerned\nwith broad-based interoperability, and it levies requirements on developers to help ease inte­\ngration as the business environment evolves. These restrictions are usually expressed as tech­\nnical standards to be employed in system development. There are also enterprise processes\nthat will affect the development and fielding of a system (e.g., the Department of Defense\nInformation Assurance Certification and Accreditation Process). It is incumbent on the SE\nto maintain awareness of enterprise standards and processes. For more information, see the\narticle “Standards Boards and Bodies” in the SEG’s Enterprise Engineering section.\n\n\n-----\n\nSEs are also encouraged to read “Develop and Evaluate Integration and Interoperability\n(I&I) Solution Strategies,” the companion article to this one in the SEG’s Systems Integration\ntopic.\n\n###### Best Practices and Lessons Learned\n\n\nInterfaces—live or die by them. Interfaces are\n\nwhere the SE can exert control, particularly in\n\nthe technical area. Internal and external inter­\n\nfaces must be established and their configuration\n\nmanaged. Identify in detail as many interfaces\n\nas possible. External interfaces represent the\n\nboundary and scope of the system for which the\n\nSE is responsible. Interfaces among the stake­\n\nholders are equally important. Interfaces to the\n\nbusiness process or operations also should not\n\nbe forgotten.\n\nCommunication—essential for success. In addi­\n\ntion to providing high-quality documentation and\n\ninterface specifications that communicate how\n\nthe system is intended to operate, the SE must\n\nalso monitor dialog interfaces among the vari­\n\nous stakeholders to ensure the program stays on\n\ntrack. Encouragement of open, factual communi­\n\ncations among the stakeholders can be the lubri­\n\ncation that makes it all happen. This nontechnical\n\nskill is often overlooked.\n\nSubsystems—use them wholesale if possible,\n\nbut verify their appropriateness. Use of com­\n\nmercial off-the-shelf assemblies for both hard­\n\nware and software systems is common. To ensure\n\nsuccess, all subsystems should be qualified for\n\nperformance and acceptance-tested commen­\n\nsurate with the risks posed by the component.\n\nProblems in the technical area—plan for the\n\nunexpected. Realize and accept that system\n\n\nintegration will not be flawless the first time. The\n\nmore “moving parts” the system has, the big­\n\nger the challenge. Provide time in the schedule\n\nand funds in the budget to accommodate the\n\noccasional failure. Increased testing may also help\n\nminimize errors. Close attention to the deploy­\n\nment environment is also warranted. For more\n\ninformation, see the article “Systems Engineering\n\nStrategies for Uncertainty and Complexity” in the\n\nSEG’s Enterprise Engineering section.\n\nLet risk drive your focus. Use risk identification\n\nand management strategies to help you decide\n\nthe specific areas and techniques you will use to\n\nfocus your work. For example, if the component or\n\nsubsystem integration activities appear to be well\nmanaged by the contractor compared to some\n\nsystem or enterprise integration issue, focus your\n\nattention where it is needed and balance your\n\ntasks based on the severity of the risks. For details\n\non risk identification and management strate­\n\ngies, see the Risk Management topic in the SEG’s\n\nAcquisition Systems Engineering section.\n\nChange—anticipate it. New things are not always\n\nwelcomed by the people who are expected to use\n\nthem. A rollout plan and training will be critical to\n\nthe acceptance of a system. The more stakehold­\n\ners involved, the greater the degree of difficulty\n\nwill be.\n\nUnintended system usage—count on it. To\n\naccommodate the reality that a system will not\n\n\n-----\n\nbe employed exactly as the original designers\n\nintended, build in as much flexibility as you can.\n\nThis is especially true for IT-based systems, where\n\nadoption of popular standards at the exter­\n\nnal interfaces can pay dividends as the system\n\nand other systems evolve in their operational\n\nenvironment.\n\nRecognize and address I&I gaps. This is critical,\n\nespecially when they appear to be outside your\n\narea of responsibility. It is the age-old problem of\n\ndoubles tennis—both players think it is the other’s\n\nresponsibility. System I&I is such a multifaceted\n\nand complex area that there is always a risk that\n\n###### Summary\n\n\nan issue or consideration has slipped through the\n\ncracks of the integration team. Each SE should\n\ntake ownership to ensure that I&I occurs thor­\n\noughly and correctly across the team’s span of\n\ninfluence, both vertically and horizontally.\n\nStandards—a helpful nuisance. Standards\nbased interfaces are easy to enforce in theory. In\n\nreality, they mature over time, compete across\n\nstandards organizations, and often do not exist for\n\nthe specialized interfaces you need. Nevertheless,\n\nstandards provide a meaningful starting place for\n\ninterface definition, when they are available.\n\n\nIntegration is a difficult topic due to its many facets. Technical integration has its challenges,\nbut the real difference between success and failure is in dealing with people. Time and energy\nspent on constructive stakeholder interactions is a good investment for SEs. Well-defined inter­\nfaces help the process.\n\n###### References and Resources\n\n1. Kossiakoff, A., and W. Sweet, 2003, Systems Engineering: Principles and Practice,\n\nHoboken, NJ: John Wiley & Sons.\n\n2. Institute of Electrical and Electronics Engineers (IEEE), 1990, IEEE Standard Computer\n\n_Dictionary:_ _A Compilation of IEEE Standard Computer Glossaries, New York, NY._\n\n\n-----\n\nDefinitions: Integration is merg­\n\n_ing or combining two or more_\n\n_components or configuration_\n\n_items into a higher level system_\n\n_element, and ensuring that the_\n\n_logical and physical interfaces_\n\n_are satisfied and the integrated_\n\n_system satisfies its intended_\n\n_purpose [1]. Interoperability_\n\n_is the ability of two or more_\n\n_systems or components to_\n\n_exchange information and use_\n\n_the information that has been_\n\n_exchanged [2]._\n\nKeywords: integration,\n\n_interfaces, interoperability,_\n\n_system(s)_\n\n\nSYSTEMS INTEGRATION\n###### Develop and Evaluate Integration and Interoperability (I&I) Solution Strategies\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be able\n\nto develop and evaluate integration and interop­\n\nerability (I&I) solution strategies for the program\n\nthey support and the enterprises of which their\n\nsystem is a part. They are expected to take a\n\nbroad view of I&I, including technical, organi­\n\nzational, operational, and environmental issues\n\nand interactions across systems. In general, the\n\nMITRE SE “owns” the overall I&I problem space.\n\n\n-----\n\n###### Background\n\nIntegration and interoperability co-exist with each other. To be successful at integrating two or\nmore elements requires the elements to be interoperable. The elements must co-exist at both\nthe physical and functional levels to be interoperable, and various communication or interface\nstandards must be adhered to. These can be as simple as a 120-volt AC outlet, or as complex\nas the Transmission Control Protocol and the Internet Protocol that control the exchange of\ninformation on a computer network.\n\nAs discussed in the companion article “Identify and Assess Integration and\nInteroperability (I&I) Challenges,” to be successful, I&I must consider technical, program­\nmatic, social, and business dimensions.\n\nI&I of information technology (IT)–intensive systems is increasingly important as the\nconcept “the network is the computer” becomes a reality. Almost all products in service\ntoday, from televisions to jet aircraft, are either wholly or partially controlled by computers,\nand therefore I&I strategies for IT-intensive systems require an understanding of two forms of\ninteroperability:\n\n###### �Syntactic Interoperability: If two or more systems are capable of communicating and\n\nexchanging data, they are exhibiting syntactic interoperability. Specified data formats,\ncommunication protocols, and the like are fundamental. In general, Extensible Markup\nLanguage or Structured Query Language standards provide syntactic interoperability.\nSyntactic interoperability is required for any attempts at further interoperability.\n###### �Semantic Interoperability: Beyond the ability of two or more computer systems to\n\nexchange information, semantic interoperability is the ability to automatically interpret\nthe information exchanged meaningfully and accurately to produce useful results as\ndefined by the end users of both systems. To achieve semantic interoperability, both\nsides must agree to a common information exchange reference model, whether it is one\nused only by themselves or, preferably, a standard model that has been agreed on by a\ncommunity, so additional interoperability with other systems in the community can be\nfacilitated. Regardless, the content of the information exchange requests is unambigu­\nously defined: what is sent is the same as what is understood.\nSo what does this mean for the SE in developing and evaluating I&I solution strategies? On\nthe most basic level, it requires an understanding of what is needed to effectively integrate the\nelements of interest while providing a solution that will be interoperable. The detailed answer\nto this question should be based on the level of interoperability desired, which needs to be\nrooted in an assessment of operational needs. A key question is whether syntactic interopera­\nbility, semantic interoperability, or both are required. This is not necessarily a simple question\nto answer, but once it has been, the integration strategy can be developed.\n\n\n-----\n\nI&I solution strategies must be scrutinized to ensure they satisfy program cost and\nschedule constraints. Other considerations, such as contract structure, implications to other\nsystems (both the systems that directly interface to the system[s] impacted by the strategies\nand those that are indirectly impacted), probability of success, and risks must be factored\ninto the evaluation. (Several articles cover subjects relevant to strategy formulation under the\nAcquisition Program Planning and Program Acquisition Strategy Formulation topics in the\nSEG’s Acquisition Systems Engineering section.)\n\nI&I solution strategies need to account for stakeholder agendas and objectives. This is\noften called the social or political dimension, and it includes actions to address potential\nissues or objections certain organizations or personalities may pose. One strategy is to social­\nize alternate solutions with supporting rationale for preferred options. In doing so, be sure\nespecially to encourage or tease out stakeholder inputs in areas important to them as a way to\nfacilitate their buy-in.\n\nBusiness considerations should focus on plans and strategies of those stakeholders with\nthe greatest equity in the system. Implications for future work programs, systems, and road­\nmaps should be part of the evaluation process. Naturally, the operational users should figure\nprominently in this dimension, but don’t forget the government’s prime commercial contrac­\ntors’ interests or equities in the system. Factoring them into your business considerations can\nhelp shape the I&I strategy so it gives the government better leverage with the contractor.\n\n###### Best Practices and Lessons Learned\n\n\nOperational needs are key. The single most\n\nimportant consideration in assessing integration\n\nand/or interoperability solutions is to first under­\n\nstand the operational requirement for exchanging\n\ndata. Ask yourself: “How much and what type of\n\ninformation is required to be exchanged for the\n\nsystem to perform its mission?”\n\nImportance of early and continuous operator\n\ninvolvement. Get users involved early and include\n\nthem in the operational implications of the I&I\n\nsolution strategies being considered. This is a\n\ncontinuous process, not a one-time activity.\n\nWorking groups that work. Establish an interop­\n\nerability working group, including all stakeholders,\n\n\nearly in the development cycle and meet regularly.\n\nThere is no better way to ensure nothing “falls\n\nthrough the cracks” than to regularly meet with all\n\nstakeholders and discuss the specifics of integra­\n\ntion and interoperability. Something as simple as a\n\ndifference in nomenclature can result in opposite\n\npolarities on either end of a signal, with the result\n\nthat interoperability doesn’t happen.\n\nThink broadly. Be sure you step back and con­\n\nsider the broad implications of candidate I&I solu­\n\ntions. Most systems no longer stand alone but\n\nare part of one or more systems-of-systems or\n\nenterprises. This makes developing a successful\n\nI&I solution strategy more difficult. The problems\n\nof integrating boards and boxes into a system\n\n\n-----\n\nare being joined and sometimes supplanted by\n\nintegrating systems into enterprises with only the\n\nloosest of defined interfaces. This is of particu­\n\nlar importance when an enterprise exists today\n\nand a much more sophisticated (i.e., complex)\n\nenterprise will replace it, gradually, over time. One\n\nexample is the FAA/DoD Next Generation Air\n\nTransportation System (NextGen) [3] being devel­\n\noped to address the extreme growth in air traffic\n\nexpected over the next two decades. Much of the\n\nresponsibility, currently allocated to ground-based\n\nsystems and air traffic controllers, will move to an\n\nairborne network and place more responsibility on\n\ncockpit personnel. I&I issues and their potential\n\nsolutions need to be carefully addressed. These\n\nissues involve human cognition and decision\nmaking dimensions, not just questions of enabling\n\ntechnologies.\n\n###### References and Resources\n\n\nThink incremental. Consider iterative or incre­\n\nmental steps to achieve the desired level of I&I.\n\nPrototypes and experiments. Consider the use\n\nof prototyping and experimentation to understand\n\nthe I&I environment, especially if the objectives\n\nare new or the solution is innovative in nature.\n\nBottom-up. Work integration issues from the\n\nlowest level upward. Do not assume that the “box\n\nwill work” just because it did in another applica­\n\ntion, unless all integration aspects have been\n\nverified first. Make sure the integration strategy\n\nand subsequent verification are documented\n\nand agreed on by all involved. Don’t arrive at the\n\nfinal test and hear “Well, that’s not really what we\n\nmeant.”\n\n\n1. Kossiakoff, A., and W. Sweet, 2003, Systems Engineering: Principles and Practice,\n\nHoboken, NJ: John Wiley & Sons.\n\n2. Institute of Electrical and Electronics Engineers (IEEE), 1990, IEEE Standard Computer\n\n_Dictionary:_ _A Compilation of IEEE Standard Computer Glossaries, New York, NY._\n\n[3. Next Generation Air Transportation System (NextGen) website, http://www.faa.gov/next­](http://www.faa.gov/nextgen/)\n\n[gen/, accessed May 24, 2010.](http://www.faa.gov/nextgen/)\n\n\n-----\n\nDefinition: When components\n\n_of a system are developed in_\n\n_isolation, all the pieces must be_\n\n_brought together to ensure that_\n\n_the integrated system functions_\n\n_as intended in its operational_\n\n_configuration. Integration_\n\n_testing should exercise key_\n\n_interfaces between system_\n\n_components to ensure that_\n\n_they have been designed and_\n\n_implemented correctly. In_\n\n_addition, the total operational_\n\n_architecture, including all the_\n\n_segments of the system that_\n\n_are already fielded, should be_\n\n_included in an end-to-end test_\n\n_to verify system integration_\n\n_success._\n\nKeywords: end-to-end testing,\n\n_integration testing, operational_\n\n_architecture, system-of-sys­_\n\n_tems testing_\n\n\nSYSTEMS INTEGRATION\n###### Assess Integration Testing Approaches\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to understand\n\nthe purpose and role of integration (or system\nof-systems) testing in the acquisition process,\n\nwhere it occurs in systems development, and the\n\nbenefits and risks of employing it. MITRE SEs are\n\nalso expected to understand and recommend\n\nwhen integration testing, or system-of-systems\n\ntesting, is appropriate within a program develop­\n\nment. They should be able to take a broader look\n\nat the system acquisition within the context of its\n\nintended operational environment, beyond simply\n\nthe core piece of equipment or software that is\n\nbeing developed, to the overarching operational\n\narchitecture. MITRE SEs should develop and\n\nrecommend integration testing strategies and\n\nprocesses that encourage and facilitate active\n\n\n-----\n\nparticipation of end users and other stakeholders in the end-to-end testing process. They are\nexpected to monitor and evaluate contractor integration testing and the acquisition program’s\noverall testing processes, and recommend changes when warranted.\n\n###### Background\n\nFrom a software development perspective, system integration testing (SIT) is defined as the\nactivities involved with verifying the proper execution of software components and proper\ninterfacing between components within the solution. The objective of SIT is to validate that all\nsoftware module dependencies are functionally correct and that data integrity is maintained\nbetween separate modules for the entire solution. While functional testing is focused on test­\ning all business rules and transformations and ensuring that each “black box” functions as it\nshould, SIT is principally focused on testing all automated aspects of the solution and integra­\ntion touch points [1].\n\nModern systems provide great value through multifunctionality. However, for the sys­\ntems engineer, the multifunctionality brings the challenge of increased complexity. Humans\ndeal with complexity by partitioning the challenge into smaller pieces—sometimes called\ncomponents or modules, although at times these are full systems in and of themselves. The\ndownside of partitioning the problem into manageable pieces is that the pieces have to be put\ntogether (integration) and shown to work together. This integration is best achieved through a\ndisciplined systems engineering approach containing good architecture, interface definitions,\nand configuration management.\n\nIn most cases, systems being acquired through the government’s acquisition process are\nnot complete, stand-alone entities. The newly acquired system will almost always need to fit\ninto a larger operational architecture of existing systems and/or operate with systems that\nare being separately acquired. To be completely effective and suitable for operational use, the\nnewly acquired system must interface correctly with the other systems that are a part of the\nfinal operational architecture. Integration testing, or system-of-systems testing, verifies that\nthe building blocks of a system will effectively interact, and the system as a whole will effec­\ntively and suitably accomplish its mission. This article expands the strict software-focused\ndefinition of system integration testing to a broader look at complete systems and the integra­\ntion, or system-of-systems, testing that should be conducted to verify the system has been\n“assembled” correctly.\n\nThe conundrum that a MITRE systems engineer, or any independent party charged with\nassessing a system’s integration test strategy, will encounter in attempting to recommend or\ndevelop integration test strategies is the lack of requirements written at a system-of-systems\nor operational architecture level. By way of example, although the Department of Defense\n(DoD) Joint Capabilities Integration and Development System (JCIDS) was developed to\n\n\n-----\n\naddress shortfalls in the DoD requirements generation system, including “not considering new\nprograms in the context of other programs” [2], operational requirements documents con­\ntinue to be developed without a system-of-systems focus. A typical Capabilities Development\nDocument will provide requirements for a system, including key performance parameters,\nbut will not provide requirements at the overarching architecture level. As a result, to develop\na recommendation for integration testing, some creativity and a great deal of pulling infor­\nmation from diverse sources are required. Once the test is developed, the task of advocating\nand justifying the test’s need within the system development process will be the challenge at\nhand.\n\nThe following discussion provides examples of systems-of-systems, the recommended\nintegration testing that should be conducted, and both good and bad integration testing\nexamples. Note that best practices and lessons learned are generally interspersed throughout\nthe article. A few cautionary remarks are also listed at the end.\n\n###### Systems-of-Systems: Definition and Examples\n\nWhile the individual systems constituting a system-of-systems can be very different and\noperate independently, their interactions typically deliver important operational properties.\nIn addition, the dependencies among the various systems are typically critically important to\neffective mission accomplishment. The interactions and dependencies must be recognized,\nanalyzed, and understood [3]. Then the system-of-systems test strategy can be developed to\nensure that the integration of the individual systems has been accomplished successfully to\ndeliver a fully effective and suitable operational capability.\n\nThe examples used in this article are drawn from a particular domain. But most MITRE\nSEs should see a great deal of similarity in the essentials of the following examples, regardless\nof the sponsor or customer they support.\n\nThe Global Positioning System (GPS) is an example of a system-of-systems. Typical\nGPS users—ranging from a hiker or driver using a GPS receiver to navigate through the\nwoods or the local streets, to a USAF pilot using GPS to guide a munition to its target—don’t\nusually consider all the components within the system-of-systems required to guide them\nwith GPS navigation. The constellation of GPS satellites is only a small piece, albeit an\nimportant one, within the system-of-systems required to deliver position, navigation, and\ntiming information to the GPS user. Other essential pieces include the ground command\nand control network needed to maintain the satellite’s proper orbit; the mission process­\ning function needed to process the raw collected data into usable information for the end\nuser; the external communication networks needed to disseminate the information to the\nend user; and the user equipment needed for the end user to interface with the system and\nuse its information. The dependencies and interfaces among all these elements are just as\n\n\n-----\n\ncritical to accomplishing the user’s goal as is the proper functioning of the constellation of\nGPS satellites.\n\nA second system-of-systems example is an interoperable and information assurance (IA)\nprotected cross-boundary information sharing environment where federal government users\nfrom different departments and agencies, commercial contractors, allies, and coalition mem­\nbers can share information on a global network. Multiple separate but interrelated products\ncomprise the first increment suite of information technology services, including Enterprise\nCollaboration, Content Discovery and Delivery, User Access (Portal), and a Service-Oriented\nArchitecture Foundation to include Enterprise Service Management.\n\nFinally, an example of a more loosely coupled system-of-systems (SoS)—i.e., a surveil­\nlance system-of-systems for which a single government program office is not responsible for\nacquiring and sustaining the entire SoS. The surveillance network comprises a number of\nsensors that contribute information in the form of observations to a central processing center\n(CPC) that uses the sensor-provided observations to maintain a database containing the loca­\ntion of all objects being monitored. The CPC is updated and maintained by one organization\nwhile each type of surveillance network contributing sensor has its own heritage and acqui­\nsition/sustainment tail. A new sensor type for the surveillance network is currently being\nacquired. While it will be critically important for this new sensor type to integrate seamlessly\ninto and provide data integrity within the overall surveillance network, the road to SoS inte­\ngration testing is fraught with difficulty primarily because there are no overarching require­\nments at the surveillance network level to insure adequate integration of the new sensor.\n\n###### System-of-Systems Testing\n\nAlthough they are challenging to plan and execute, system-of-systems tests for programs\nwhere a single government program office is responsible for the entire SoS are generally\naccomplished better as part of the system acquisition process. If nothing else, a final system\nintegration test is typically planned and executed by the development contractor prior to\nturning the system over for operational testing. Then the operational test community plans,\nexecutes, and reports on an operationally realistic end-to-end system test as a part of the sys­\ntem’s Congressionally mandated Title 10 Operational Test and Evaluation.\n\nA good example of an integration/SoS test is that being done to inform some GPS\nupgrades. As the new capability is fielded within the GPS constellation, the development com­\nmunity will combine their Integrated System Test (IST) with the operational test community’s\nOperational Test and Evaluation into an integrated test that will demonstrate the end-to-end\ncapability of the system. This SoS/end-to-end test will include the full operational process,\nfrom user request for information, through command generation and upload to the constella­\ntion, to user receipt of the information through the user’s GPS receiver. During the final phase\n\n\n-----\n\nof the IST, a number of operational vignettes will be conducted to collect data on the end-toend system performance across a gamut of operational scenarios.\n\n###### Best Practices and Lessons Learned\n\n\n###### � [Integration testing should include sce­]\n\nnarios that demonstrate the capability to\n\nperform mission-essential tasks across\n\nthe SoS segments.\n###### � [Don’t assume integration testing will ]\n\nnecessarily happen or be adequate just\n\nbecause the full SoS is under the con­\n\ntrol of a single program office. There are\n\nexamples of such single program office\n\nSoS acquisitions comprising a number of\n\n###### Summary and Conclusions\n\n\ndifferent products and segments that only\n\ntested each product separately.\n###### � [Failure to conduct adequate SoS integra­]\n\ntion testing can lead to potentially cata­\n\nstrophic failures. If the new sensor type in\n\nthe surveillance network example provides\n\nthe quality and quantity of data antici­\n\npated, there is the real possibility that it will\n\noverwhelm the CPC’s processing capabil­\n\nity, thus degrading the accuracy and time­\n\nliness of the surveillance database.\n\n\nA strict software-development view of integration testing defines it as a logical extension of\nunit testing [4]. In integration testing’s simplest form, two units that have already been tested\nare combined into a component and the interface between them is tested. A component, in\nthis sense, refers to an integrated aggregate of more than one unit. In a realistic scenario,\nmany units are combined into components, which are in turn aggregated into even larger\nparts of the program. The idea is to test combinations of pieces and eventually expand the\nprocess to test your modules with those of other groups. Eventually all the modules making\nup a process are tested together. Integration testing identifies problems that occur when units\nare combined. By using a test plan that requires you to test each unit and ensure the viability\nof each before combining units, you know that any errors discovered when combining units\nare likely related to the interface between them. This method reduces the number of possibili­\nties to a far simpler level of analysis.\n\nThis article has focused on making the logical extension of this definition to a full-up\nsystem, and expanding the integration testing definition to one of system-of-systems testing.\nMITRE SEs charged with assessing integration testing approaches should ensure a system-ofsystems view of the program, and develop and advocate for full end-to-end testing of capabili­\nties within the complete operational architecture.\n\n\n-----\n\n###### References and Resources\n\n1. MIKE 2.0, System Integration Testing, accessed March 5, 2010.\n\n2. Joint Capabilities Integration Development System, Wikipedia, accessed March 5, 2010.\n\n3. System of Systems, Wikipedia, accessed March 5, 2010.\n\n4. DISA/JITC, October 2008, Net-Centric Enterprise Services Initial Operational Test and\n\nEvaluation Plan, p. i.\n\n###### Additional References and Resources\n\nMSDN, Integration Testing, accessed March 5, 2010.\n\nSMC/GPSW, September 9, 2009, Integrated System Test (IST) 2-4 Test Plan draft, pp. 2-21–2-22.\n\nWikipedia contributors, “United States Space Surveillance Network,” Wikipedia, accessed\nMay 25, 2010.\n\n\n-----\n\nDefinition: Interface manage­\n\n_ment includes the activities_\n\n_of defining, controlling, and_\n\n_communicating the information_\n\n_needed to enable unrelated_\n\n_objects (including systems,_\n\n_services, equipment, software,_\n\n_and data) to co-function._\n\n_Most new systems or services_\n\n_require external interfaces with_\n\n_other systems or services. All_\n\n_of these interfaces must be_\n\n_defined and controlled in a way_\n\n_that enables efficient use and_\n\n_change management of these_\n\n_systems or services. Therefore,_\n\n_the practice of interface man­_\n\n_agement begins at design and_\n\n_continues through operations_\n\n_and maintenance._\n\nKeywords: change manage­\n\n_ment, coupling, interface_\n\n\nSYSTEMS INTEGRATION\n###### Interface Management\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to under­\n\nstand the general principles and best practices of\n\nmanaging interfaces for Information and Com­\n\nmunications Technology (ICT) systems. They\n\nare expected to identify the most efficient and\n\neffective processes and methods for implement­\n\ning them, and to understand the complexities\n\nthat result in an increasingly interoperable world.\n\n\n-----\n\n###### Background\n\nInterfaces are the functional and physical connections at the boundaries of ICT systems that\nare designed to interoperate with other systems. There are many types of interfaces, including\ncommunications interfaces, signaling interfaces, service interfaces, data interfaces, hardware\ninterfaces, software interfaces, application program interfaces, etc. These interfaces are criti­\ncal elements supporting the complex nature of today’s systems, which are becoming more\ngeographically distributed and interconnected with other independently developed systems.\nThe strong dependencies on external interfaces require that special attention be given to how\nthey are designed, managed, and publicized beyond what programs typically control within a\ntraditional configuration management process.\n\nThe practice of interface management (IxM) is related to requirements and configuration\nmanagement, but is applied more specifically to the management of interfaces as a subcompo­\nnent of ICT systems. IxM is a technical systems engineering activity, focused on the archi­\ntecture, design, and implementation of the interface. As such, the lead or chief SE typically\nhas primary responsibility for this life-cycle management process [1]. The major inputs and\noutputs might include:\n\n###### �Inputs: interface management plan, interface requirements, and interface requirements\n\nchanges\n###### �Outputs: interface specifications, interface control documents/drawings, and interface\n\naction control sheets.\nFor this article, the purpose of interface management activity is to:\n\n###### �Provide all necessary design information needed to enable co-functionality of items,\n\nsuch that separately designed and produced items will be able to work together.\n###### �Use the interface performance, functional, and physical attributes as constraints to\n\ndesign changes, ensuring sustainable use of that interface by an increasing and chang­\ning body of users.\n\n###### Interface Management in an Enterprise Engineering Service-Oriented Environment\n\nInteroperability requires a minimum of two items, each with its own interfaces configured in\na way that enables effective joining. A plug and an outlet are examples of that interface. In\nICT, interfaces need to be characterized along multiple dimensions, including function, perfor­\nmance, behavioral process, and data. Multiple public standards exist to help with this charac­\nterization (such as WSDL, SOAP, UDDI, XML, and KML), but these alone are insufficient. It is\nnecessary to understand how the interface will behave and function, how it can transmit or\nconsume data, and what sequences are required for the exchange of data. Interface manage­\nment addresses this complexity through use of an engineering management process that is\n\n\n-----\n\nwell defined in various engineering bodies of knowledge, such as Software Engineering Body\nof Knowledge, Software Engineering Institute, International Council on Systems Engineering,\nand the Defense Acquisition Guidebook.\n\nIn a service-oriented environment, the key to successful system or service usage lies in\nthe ability to effectively publicize the requirements to users in a way they understand and\nneed. In addition, users will want some level of assurance or understanding of how that\nservice provider plans to mature and evolve the interface so that users have some level of con­\nfiguration control over their side of the interface. From a provider’s perspective, it may become\ndifficult to understand who is using the service, and the provider may not recognize the scale\nof the larger set of users that will be affected by any interface changes.\n\n###### Best Practices and Lessons Learned\n\n\nThe following rules of practice can be used for\n\nboth internal and external interfaces, as part of\n\nthe interface management process.\n\nDon’t underestimate the need for governance.\n\nInterfaces play a crucial role in all systems, which,\n\nby definition, consist of multiple components that\n\nmust interact to deliver a collective capability.\n\nComplex systems consist of numerous inter­\n\nfaces of various types; loosely coupled architec­\n\ntures entail higher degrees of abstraction than\n\ntightly coupled architectures. In the absence of\n\nproper governance, interface sprawl and varia­\n\ntion can quickly devolve into degraded system\n\nperformance, maintainability, and sustainability.\n\nTherefore, IxM, which is always important, is con­\n\nsiderably more challenging in systems character­\n\nized by loosely coupled architectures. The need\n\nfor comprehensive governance throughout the\n\ninterface life cycle is essential, and early deliberate\n\nplanning is necessary.\n\nThe major IxM activities needing governance\n\ninclude build-time and run-time contexts.\n\nRequirements management, architecture and\n\n\ndesign standards, technical reviews, and test­\n\ning are some of the major build-time governance\n\nareas, and tend to come from a system develop­\n\nment life-cycle perspective. Release manage­\n\nment, configuration management, and change\n\nmanagement are focus areas for run-time\n\ngovernance, and tend to come from an informa­\n\ntion technology service management perspec­\n\ntive. While it is customary to distinguish between\n\nbuild-time and run-time governance activities, it is\n\nclear that life-cycle management spans the two,\n\nas change management tends to fold back onto\n\nrequirements management, and testing unfolds\n\ninto release management. How an enterprise\n\nelects to organize and categorize the governance\n\nprocesses is not as important as the need to\n\nensure that such oversight is executed. Also, rec­\n\nognize that good governance structures mature\n\nand change based on the needs of the activities\n\nbeing governed. Deliberately plan a mechanism for\n\nperiodically reviewing the governance construct\n\nfor opportunities to improve.\n\nEstablish an interface characterization frame­\n\nwork and IxM processes early. Often, creation\n\n\n-----\n\nof the interface characterization framework does\n\nnot occur until after interfaces are developed. As\n\na result, end users are not engaged, the frame­\n\nwork is lagging based on information that needs\n\nto now be captured as opposed to created during\n\nthe design phase, and there are other competing\n\npriorities for the systems engineer’s time (such as\n\ndelivering the system or service). This results in\n\ninterfaces that are not well documented, con­\n\ntrolled, or publicized. The best time to start is\n\nduring design, when everyone is thinking about\n\nwhat the system or service should or will be.\n\nCreate the framework and begin capturing the\n\nplanned interface information when it is being\n\ndecided. Establish the teams that are responsible\n\nfor executing IxM processes to give them time to\n\ndetermine the most efficient way to engage in the\n\nsystems engineering life cycle. Allocating suffi­\n\ncient time to planning improves the opportunity to\n\ndeliver good results.\n\nImplement the simplest IxM approach pos­\n\nsible. The simplest and most straightforward\n\napproach that will satisfy the objective(s) should\n\nalways be chosen. Complex interface manage­\n\nment processes or tools should only be employed\n\nwhen other methods do not meet the needs of\n\nthose trying to use or manage the interfaces.\n\nWhy create unnecessary overhead? Engineering\n\nresources are scarce and should be leveraged in\n\nareas where they are most needed.\n\nAlways adhere to standards. Interoperability\n\ndepends on the use of standards to successfully\n\npromulgate and grow a service-oriented envi­\n\nronment. The use of standards-based inter­\n\nfaces helps minimize the amount of specialty\n\n\ndevelopment and therefore improves the likeli­\n\nhood of service reuse.\n\nEstablish service-level agreements for inter­\n\nfaces. Trust is earned. Users need to understand\n\nwhat performance to expect from the interface\n\nand its underlying system. Establish service-level\n\nagreements (SLAs) to document the performance\n\nparameters for the interface and underlying\n\nsystems, then report performance against those\n\nmeasures. Remember that SLAs are as much\n\nabout the provider of the interface as they are\n\nabout the user. If the service level will vary based\n\non the volume of use, then clear articulation of\n\nthat expected performance variation will help\n\nusers understand up front, before performance\n\nbegins to degrade. It will also trigger various man­\n\nagement activities to manage that degradation in\n\na thoughtful and planned way.\n\nUse a well-defined framework to describe the\n\ninterfaces. Having different sets of information\n\nattributes for different interfaces complicates\n\nand raises the cost of achieving interoperability. It\n\nis critical that each system or service categorize\n\nand describe the interfaces under their control\n\nin a consistent way. In addition, this framework\n\nmust focus on communicating at different layers\n\nof abstraction about how other systems/services\n\nneed to use the interface. There is resistance\n\nto providing additional information beyond that\n\nrequired by some common repository, such as a\n\nUniversal Description, Discovery, and Integration\n\nregistry. It is critical that the framework address\n\nattributes of using the interfaces (e.g., business\n\nprocess or data flow, SLA, functionality, data map)\n\nand not just technical aspects of the interface\n\nitself. ITIL suggests using the service catalog as a\n\n\n-----\n\ntype of framework to describe services and their\n\ninterfaces. An interface specification is another\n\nalternative. Regardless of which mechanism\n\nor framework is used, it is important to include\n\ninformation on current and future versions, their\n\ndeprecation dates, the technical specification,\n\nstandards, and other use-related information or\n\ntools in one logical place for user access.\n\nSimplify the end-user development challenge\n\nwith tools where possible. Develop, deploy, and\n\nmanage development and implementation tools\n\nwhere possible. The best way to support end\n\nusers is to help them understand when they have\n\ncorrectly configured their end of the interface. For\n\nexample, conformance test kits can significantly\n\nhelp during the development phase to ensure\n\nthat inadvertent configuration problems do not\n\narise. This will improve end user satisfaction and\n\nincrease interoperability and usage. Sample code\n\nand even configuration schemas for commercial\noff-the-shelf–based interfaces are other tools to\n\nconsider providing.\n\nEnsure persistent, active engagement of all\n\nstakeholders in the IxM process. Users or\n\ncustomers really like to be heard, particularly\n\nwhen they are being asked to make their busi­\n\nness dependent on a system or service that is\n\nnot within their control. So, provide them with\n\nthat user forum and engage them in IxM activities.\n\nInterface Control Working Groups (ICWGs) can\n\nbe effective mechanisms to publicize and man­\n\nage interfaces with customer/user participation.\n\nWhere multiple end users depend on your exter­\n\nnal interfaces, an ICWG may encourage active and\n\nconstant participation, which will in turn promote\n\nconsistency, prevent development of unnecessary\n\n\ninterfaces, and reduce risk associated with inter­\n\nface changes. An ICWG is a specialized integrated\n\nproduct team comprising cognizant technical\n\nrepresentatives from the interfacing activities.\n\nIts sole purpose is to solve interface issues that\n\nsurface and cannot be resolved through simple\n\nengineer-to-engineer interaction.\n\nPlan to deprecate prior versions. It is important\n\nnot to strand users of prior versions of the inter­\n\nface; however, there is a limit to which backward\n\ncompatibility should extend. Backward compat­\n\nibility requirements can constrain future flexibility\n\nand innovation and create maintenance and\n\nmanagement headaches. However, it is critical\n\nfor end users to have some level of stability in the\n\ninterface so they can manage their own develop­\n\nment life cycles and mission objectives. Create\n\nan opportunity to discuss an interface depreca­\n\ntion strategy and establish a core set of busi­\n\nness rules that drive that plan. Ensure that cost/\n\nprice considerations are understood from both\n\nperspectives, provider and end user. Then, publish\n\nthe business rules in the service catalog or other\n\nmechanism in a widely visible and accessible\n\nlocation, alongside other user-related information.\n\nThis will enable users to manage their side of the\n\ninterface in a way that does not strand them and\n\nalso supports growth and innovation.\n\nPublish interface information in an easily acces­\n\nsible and visible location. Often, users do not\n\nhave ready and easy access to the information\n\nthey need to use a system or service. Accessibility\n\nis required not only for the government, but also\n\ntheir supporting contractors who may or may not\n\nhave access to particular networks or knowledge\n\ncenters. Interface information must be readily\n\n\n-----\n\navailable from a known location accessible to all\n\nresources responsible for making decisions about\n\nand developing to the interface. Too often, system\n\nand service developers forget that users have\n\ndifferent competency levels, and they misun­\n\nderstand the depth of information users need to\n\n###### References and Resources\n\n\nbe able to effectively configure their systems to\n\ninterface properly. Good interface management\n\nincludes understanding the various types of use\n\nfor the interface information, and presenting the\n\ndata in a way that supports getting that informa­\n\ntion easily to the user.\n\n\n1. Per DoDI 5000.02, Enclosure 12, Section 4.1.6, each Program Executive Officer, or equiva­\n\nlent, shall have a lead or chief systems engineer in charge of reviewing assigned pro­\ngrams’ Systems Engineering Plans and overseeing their implementation.\n\n###### Additional References and Resources\n\nDefense Acquisition University, Defense Acquisition Guidebook, accessed December 10, 2009.\n\nDoD Directive (DODD) 8320.02, Data Sharing in a Net-Centric Department of Defense, April\n23, 2007.\n\nDoD Instruction (DoDI) 5000.2, Operation of the Defense Acquisition System, December 8,\n2008.\n\n[International Council on Systems Engineering (INCOSE), http://www.incose.org/, accessed](http://www.incose.org/)\nDecember 15, 2009.\n\nMilitary Handbook Configuration Management Guidance MIL-HDBK-61A(SE), February 7,\n2001.\n\nNational Information Exchange Model, accessed December 10, 2009.\n\nSEPO Configuration Management Toolkit, The MITRE Corporation.\n\nSEPO Enterprise Integration Toolkit, The MITRE Corporation.\n\nSoftware Engineering Body of Knowledge (SWEBOK), accessed December 15, 2009.\n\nSoftware Engineering Institute (SEI), Service-Oriented Architectures as an Interoperability\nMechanism, July 31, 2009.\n\nUniversal Core Overview, https://metadata.ces.mil/ucore/index.html, November 2008.\n\n\n-----\n\n##### Test and Evaluation\n\nDefinition: Test and Evaluation (T&E) is the process by which a system or com­\n\n_ponents are compared against requirements and specifications through testing._\n\n_The results are evaluated to assess progress of design, performance, support­_\n\n_ability, etc. Developmental test and evaluation (DT&E) is an engineering tool used_\n\n_to reduce risk throughout the acquisition cycle. Operational test and evaluation_\n\n_(OT&E) is the actual or simulated employment, by typical users, of a system under_\n\n_realistic operational conditions [1]._\n\nKeywords: analysis, DT&E, evaluation, OT&E, performance, testing, verification\n\n###### Context\n\nTesting is a mechanism to emsure quality of a product, system, or capa­\n\nbility (e.g., right product, built right). To be effective, testing cannot occur\n\nonly at the end of a development. It must be addressed continuously\n\nthroughout the entire life cycle.\n\nTest and Evaluation involves evaluating a product from the compo­\n\nnent level, to stand-alone system, integrated system, and, if appropriate,\n\nsystem-of-systems, and enterprise. Figure 1 highlights these levels of\n\nevaluation and how they align with government DT, OT, and accreditation\n\nand certification testing.\n\nThe articles in this topic are arranged according to a notional\n\nV-model life cycle [2] chronology of a program acquisition: (1) creating\n\nand assessing T&E strategies, (2) assessing T&E plans and procedures,\n\n\n-----\n\nComponent Stand-Alone Integrated System System-of-System\nTesting System Testing Testing Testing\n\n- Unit testing - CI-to-CI testing - I&I testing - Field testing\n\n- CI testing - Exercise\n\n\nI&I OT\nFQT\nCert Cert\n\n\nContractor DT\n\n\nGovernment DT\n\n\nGovernment OT\n\n\nCompliance Testing\n\n\nAccreditation and Certification\nTesting\n\n\nFigure 1. Product Life-cycle Test Phases\n\n(3) verification and validation, and (4) creating and assessing certification and accreditation\nstrategies throughout the process. As noted elsewhere in the SE Guide, the system life cycle is\nrarely, if ever, as linear as the simplified V-model might imply.\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to be able to create test and evaluation strategies\nto field effective, interoperable systems that include making recommendations on certification\nand accreditation processes. They assist in developing and defining test and evaluation plans\nand procedures. MITRE SEs participate in developmental and operational testing, observe and\ncommunicate test results, influence re-test decisions, recommend mitigation strategies, and\nassist the customer/sponsor in making system acceptance decisions.\n\n###### Best Practices and Lessons Learned\n\n\nEmploy prototypes and M&S to advantage.\n\nPrototypes and/or modeling and simulation (M&S)\n\nused early in a program can help predict system\n\nperformance and identify expected results, both\n\ngood and bad. Both techniques can be used in\n\ndesigning, evaluating, or debugging portions of a\n\nsystem before incurring the expense of “bending\n\nmetal.”\n\n\nCommon sense—sometimes a rarity. Use com­\n\nmon sense in testing. For example, although it is\n\nnecessary to ensure that environment testing cov­\n\ners the environment that your system is expected\n\nto operate in, specifying and testing a system to\n\noperate to -70°C when it will be used in an office\nlike environment is a sure way to either fail the test\n\nor drive the cost of the system beyond reason. This\n\n\n-----\n\nis an especially common pitfall when designing sys­\n\ntems for mobile or airborne environments. Extreme\n\ntemperature, vibration, radiated emissions, etc., are\n\nnot always encountered in these environments.\n\nEnsure that the tests are realistic.\n\nMatch testing method with purpose. There are\n\nmany forms of testing. Some involve instrumented\n\nmeasurements of system performance during “live”\n\noperations. Others, in decreasing order of com­\n\nplexity, are analysis, demonstration, or inspection.\n\nSelect the testing method that suits the purpose.\n\nThe performance of a critical operational capabil­\n\nity (e.g., identification of airborne objects as friend,\n\nhostile, or neutral) will likely require all or most of\n\nthe methods and culminate in a “live” test. Analysis\n\nis suited to testing requirements like long-term\n\nreliability of electronic components, and when\n\nassessing inspection is appropriate (e.g., number of\n\noperator consoles in a command center). Selecting\n\nthe right verification methods produces the right\n\nresults and saves time and cost.\n\n###### References and Resources\n\n\nTest strategy—start early and refine continu­\n\nally. Plan the test strategy from the onset of the\n\nprogram and refine it throughout the program’s\n\nlife cycle. Involve the right stakeholders in the\n\ndevelopment and review of the test strategy and\n\nplans.\n\nDon’t overlook the basics. Ensure that tests have\n\nbeen developed to be objective and capable of\n\nassessing compliance with a requirement. Make\n\nsure that if one test is intended to validate many\n\nlower level requirements, you are sufficiently\n\nversed with the details of the system design and\n\nhave the results of the component level tests\n\navailable. This is particularly important in preparing\n\nfor operation testing.\n\nReady or not? Determining suitability to enter a\n\ntest is  a key decision that can substantially affect\n\nthe overall success of the program. Know when\n\nyou are ready, and know when you are not. When\n\nunsure, postponing or deferring testing may be\n\nthe most prudent long-term course of action.\n\n\n[1. Defense Acquisition University website, https://acc.dau.mil/t&e.](https://acc.dau.mil/t&e)\n\n[2. Wikipedia contributors, “V-Model,” Wikipedia, accessed January 13, 2010.](http://en.wikipedia.org/w/index.php?title=V-Model&oldid=328588516))\n\n###### Additional References and Resources\n\n[Defense Acquisition University, “Test and Evaluation,” Defense Acquisition Guidebook,](https://acc.dau.mil/CommunityBrowser.aspx?id=315920)\nChapter 9.\n\nHaimes, Y., 1998, Risk Modeling, Assessment, and Management, Wiley & Sons.\n\n[International Test and Evaluation Association website, http://itea.org/.](http://itea.org/)\n\nStevens, R., R. Brook, K. Jackson, and S. Arnold, 1998, Systems Engineering: Coping with\n_Complexity, Prentice Hall._\n\n[The MITRE Institute, September 1, 2007, “MITRE Systems Engineering (SE) Competency](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n[Model, Ver. 1,” Section 2.6.](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n\n\n-----\n\nDefinition: A Test and\n\n_Evaluation strategy “...provide[s]_\n\n_information about risk and_\n\n_risk mitigation, ...[and] empiri­_\n\n_cal data to validate models_\n\n_and simulations, evaluate_\n\n_technical performance and_\n\n_system maturity, and determine_\n\n_whether systems are opera­_\n\n_tionally effective, suitable, and_\n\n_survivable... [1].”_\n\nKeywords: evaluation, gover­\n\n_nance, strategy, test_\n\n\nTEST AND EVALUATION\n###### Create and Assess Test and Evaluation Strategies\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) work with sponsors to\n\ncreate or evaluate test and evaluation strategies\n\nin support of acquisition programs. They are\n\noften asked to recommend test and evaluation\n\napproaches, which provide insights that can\n\nbe used to manage acquisition risks. They also\n\nmonitor government and contractor test and\n\nevaluation processes, and recommend changes\n\nwhen they are warranted. Subsequent to contract\n\naward, MITRE SEs evaluate test and evaluation\n\nmaster plans produced by both contractors and\n\ngovernment test organizations. They also evalu­\n\nate test plans and procedures that are applied\n\nduring development testing, operational testing,\n\nand for some customers, live fire testing; occa­\n\nsionally, they help to formulate the plans and\n\n\n-----\n\nprocedures as a member or advisor to the government test team. As a consequence, MITRE\nSEs are expected to understand the rationale behind the requirement for acquisition programs\nto create and execute a test and evaluation strategy. They are expected to understand where\ntest and evaluation activities such as interoperability testing, information assurance testing,\nand modeling and simulation fit in the acquisition life cycle, and where they can be used most\neffectively to identify and mitigate risk. Finally, it is expected that MITRE SEs, in the course of\ntheir other activities such as requirements and design analysis, will include test and evalua­\ntion concerns in their analysis.\n\n###### Background\n\nThe fundamental purpose of test and evaluation (T&E) is to:\n\nprovide knowledge to assist in managing the risks involved in developing, producing,\noperating, and sustaining systems and capabilities. T&E measures progress in both sys­\ntem and capability development. T&E provides knowledge of system capabilities and\nlimitations to the acquisition community for use in improving the system performance,\nand the user community for optimizing system use in operations. T&E expertise must\nbe brought to bear at the beginning of the system life cycle to provide earlier learn­\ning about the strengths and weaknesses of the system under development. The goal is\nearly identification of technical, operational, and system deficiencies, so that appropri­\nate and timely corrective actions can be developed prior to fielding the system.[1]\n\nThe program manager is responsible for creating and submitting a test and evaluation strategy\nafter the decision is made to pursue a materiel solution. The creation of the test and evaluation\nstrategy involves planning for technology development, including risk; evaluating the system\ndesign against mission requirements; and identifying where competitive prototyping and\nother evaluation techniques fit in the process.\n\nThe content of a test and evaluation strategy is a function of where it is applied in the\nacquisition process, the requirements for the capability to be provided, and the technolo­\ngies that drive the required capability. A test and evaluation strategy should lead to the\nknowledge required to manage risks; the empirical data required to validate models and\nsimulations; the evaluation of technical performance and system maturity; and a determi­\nnation of operational effectiveness, suitability, and survivability. In the end, the goal of the\nstrategy is to identify, manage, and mitigate risk, which requires identifying the strengths\nand weaknesses of the system or service being provided to meet the end goal of the acqui­\nsition program. Ideally, the strategy should drive a process that confirms compliance with\nthe Initial Capabilities Document (ICD), instead of discovering later that functional, perfor­\nmance, or non-functional goals are not being met. The discovery of problems late in the test\n\n\n-----\n\nand evaluation phase can have significant cost impacts as well as substantial operational\nrepercussions.\n\nHistorically, test and evaluation consisted of testing a single system, element, or compo­\nnent, and was carried out in a serial manner. One test would be performed, data would be\nobtained, and then the system would move to the next test event, often at a new location with\na different test environment. Similarly, the evaluations themselves were typically performed\nin a serial manner, with determinations of how well the system met its required capabilities\nestablished through the combination of test results obtained from multiple sites with differ­\ning environments. The process was time-consuming and inefficient, and with the advent of\nnetwork-centric data-sharing strategies, it became insufficient. In large part this was due to an\napproach to acquisition that did not easily accommodate the incremental addition of capabili­\nties. Creating and maintaining an effective test and evaluation strategy under those condi­\ntions would have been difficult at best. A test and evaluation strategy is a necessity today\nbecause of the addition of capabilities via incremental upgrades, which is now the norm, and\nthe shift to a network-centric construct where data is separated from the applications; data\nis posted and made available before it is processed; collaboration is employed to make data\nunderstandable; and a rich set of network nodes and paths provide the required supporting\ninfrastructure.\n\nWhen there is a need to deliver a set of capabilities as quickly as possible, further com­\nplexity in creating a test and evaluation strategy can be introduced, especially in cases where\nICDs are largely nonexistent, ambiguous, inconsistent, or incomplete. In this situation, the\ndevelopment of a test and evaluation strategy represents a significant challenge, and in some\ncases it may be largely ignored to get a capability in the field as quickly as possible. However,\nthis approach is not without attendant risk assessments and mitigation strategies—they are\njust accomplished at a high level very early in the process. Quick reaction capabilities (QRCs)\nof this sort are often followed by a more formal acquisition effort, a program of record.\nNonetheless, test and evaluation of QRCs cannot be completely ignored. At the outset, the\ncritical capabilities must be identified, and their risks must be identified, managed, and miti­\ngated through some level of test and evaluation.\n\n###### Government Interest and Use\n\nGovernment acquisition communities are recognizing the need for a test and evaluation\nstrategy that is in concert with evolving department and agency network-centric data-sharing\nstrategies. Although a test and evaluation strategy is created early in the acquisition process\n(Figure 1), it has to be refined as the acquisition process evolves and system details become\nmore specific. A test and evaluation strategy needs to be developed early in the acquisition\nprocess to ensure that it is consistent with the acquisition strategy, identifies the required\n\n\n-----\n\nMilestone review Decision point if PDR is not\nconducted before milestone B\n\n\nFollow-on DT and OT\n\nVerification of\ncorrections for\ndeficiencies\n\nDevelop T&E\nprograms to support\nupgrades, modifications, increments\n\n\nTest and evaluation\nmaster plan (TEMP)\n\nExecute T&E\nprogram\n\nProvide T&E results\nfor OIPT/DAB\n\nSupport SE tech\nreview (PDR)\n\nCPD requirements\nfor testability and\nevaluation\n\nDefine system\ncapabilities and\nlimitations\n\nDiscovery and\ndeficiencies\n\nAnnual report\n\n\nTest and evaluation\nmaster plan\n\nVerification of\ncorrections for\ndeficiencies\n\nT&E results for\nOIPT/DAB\n\nOTRR\n\nIOT&E\n\nAnnual report\n\n\nTechnology\ndevelopment\nstrategy (TDS)\n\nTest evaluation\nstrategy (TES)\n\nID emerging T&E\ncapability\nrequirements\n\nID T&E requirements\nin RFP\n\nAnnual report\n\n\nTest and evaluation\nmaster plan (TEMP)\n\nExecute T&E\nprogram\n\nProvide T&E results\nfor OIPT/DAB\n\nCDD requirements\nfor testability and\nevaluation\n\nTRL evaluation\n\nT&E requirements in\nRFP\n\nAnnual report\n\n\nFigure 1. T&E in the Defense Acquisition Management System [4]\n\nresources (facilities, ranges, personnel, and equipment, including government-furnished\nequipment), encourages shared data access, engages the appropriate government test agencies,\nidentifies where and when modeling and simulation will be employed, and establishes both\nthe contractor’s and government’s test and evaluation efforts.\n\nMITRE can and should influence how a test and evaluation strategy evolves and is\napplied, and, in particular, should ensure that it is consistent with the acquisition strategy\nand the systems engineering plan, if there is one. It is rare for MITRE, or any other single\norganization, to be asked to independently create a test and evaluation strategy. It is far more\ncommon for MITRE to collaborate with the government stakeholders to create a test and\n\n\n-----\n\nevaluation strategy, or to be employed to evaluate and recommend changes to a strategy that\nis the product of a test and evaluation working group or other test and evaluation stakeholder\norganization. In these instances, it is important that MITRE become a collaborator and con­\nsensus builder.\n\nIn most instances, the government establishes a working group to execute the test and\nevaluation strategy. This group is often referred to as a test and evaluation working integrated\nproduct team, and it consists of test and evaluation subject matter experts from the program\noffice, customer headquarters, customer user representatives, test and evaluation organiza­\ntions, higher oversight organizations (e.g., Office of the Secretary of Defense for DoD systems),\nsupporting FFRDCs, and other stakeholders. The test and evaluation strategy is a living docu­\nment, and this group is responsible for any updates that are required over time. The program\nmanager looks to this group to ensure that test and evaluation processes are consistent with\nthe acquisition strategy and that the user’s capability-based operational requirements are met\nat each milestone in the program. Finally, as a program progresses from pre-systems acquisi­\ntion to systems acquisition, the test and evaluation strategy begins to be replaced by a test and\nevaluation master plan, which becomes the guiding test and evaluation document (Figure 1).\nThe DoD’s interest in and application of a test and evaluation strategy is documented in\nIncorporating Test and Evaluation into Department of Defense Acquisition Contracts [2] and\nChapter 9 of the Defense Acquisition Guidebook [3].\n\n###### Best Practices and Lessons Learned\n\n\nNew thinking required for T&E in net-centric\n\nand SOA environments. The transition to\n\nnetwork-centric capabilities has introduced\n\nnew test and evaluation challenges. Network\n\ncapabilities can reside in both nodes and links,\n\nand the basic system capabilities can reside in\n\nservice-oriented architecture (SOA) infrastruc­\n\ntures, with the remaining capabilities provided\n\nby services that are hosted on the SOA infra­\n\nstructure. The test and evaluation of capabilities\n\nin this type of framework requires new thinking\n\nand a new strategy. For example, evaluating the\n\nperformance of the network itself is probably\n\nnot going to be accomplished without extensive\n\nuse of modeling and simulation because the\n\n\nexpense of adding live nodes in a lab increases\n\ndramatically with the number of nodes added\n\nto the test apparatus. This places a greater\n\nburden on the veracity of the modeling and\n\nsimulation because one of the keys to obtaining\n\nthe metrics that will support risk mitigation is\n\ngaining an understanding of the effect of a new\n\nhost platform on the network infrastructure, as\n\nwell as the effect of the network infrastructure\n\non the new host platform. A test and evaluation\n\nstrategy that mitigates risk in the development\n\nof a network infrastructure that will support\n\nnetwork-centric warfare requires a balance\n\nof theoretical analysis and laboratory testing.\n\nMITRE can help develop a strategy that employs\n\n\n-----\n\na mix of modeling and simulation that has been\n\nverified, validated, and accredited; labora­\n\ntory testing; and distributed testing that takes\n\nadvantage of other network-enabled test com­\n\nponents and networks. The capabilities required\n\nto execute a network-centric test and evalu­\n\nation strategy have evolved over the past few\n\nyears, and today we have a rich set of networks\n\n(such as the DREN the and SDREN) that host\n\nnodes that constitute government laboratories,\n\nuniversity facilities, test centers, operational\n\nexercise sites, contractor facilities, and coalition\n\npartner facilities.\n\nThere are emerging technology aspects of the\n\nnetwork-centric transformation where test\n\norganizations have limited experience, and these\n\naspects are where MITRE can help create and\n\nassess test and evaluation strategies. These\n\nnew technology areas constitute the heart of\n\nthe SOA that will make up the enterprise, as well\n\nas the services themselves that make up new\n\ncapabilities.\n\nAccounting for governance in T&E. The transi­\n\ntion to a service-based enterprise introduces\n\nsome new complexities that must be accounted\n\nfor in the test and evaluation strategy. Service\nbased enterprises rely on a more formalized\n\nbusiness model for the identification of required\n\ncapabilities. While this is not a new concept, the\n\nformalization of business processes into the\n\nengineering process, and the addition of the con­\n\ncomitant governance, add new complexities to\n\nboth the systems engineering and test and evalu­\n\nation processes. A test and evaluation strategy\n\nmust account for governance of capabilities (e.g.,\n\nservices) as well as the capabilities themselves.\n\n\nService repositories become critical parts of the\n\ntest and evaluation strategy and must encompass\n\nhow services are distributed, populated, managed,\n\nand accessed, since a critical aspect of service\nbased capabilities is reuse of existing services to\n\ncompose new capabilities.\n\nAccounting for business process re-engi­\n\nneering and scalability of service-based\n\ninfrastructure in T&E. The shift to network\ncentric service-based enterprise capabilities is\n\nrarely accomplished in a single stroke; instead\n\nit is accomplished incrementally, beginning\n\nwith business process re-engineering and\n\nthe identification of scalable service-based\n\ninfrastructure. Both of these activities need to\n\nbe incorporated into the test and evaluation\n\nstrategy, and their evaluation should begin as\n\nearly as possible. Prototyping or competitive\n\nprototyping are common techniques used to\n\nevaluate service-based infrastructures, espe­\n\ncially the ability of the infrastructure to scale to\n\nmeet future needs and extend to accommodate\n\nfuture capabilities.\n\nThe importance of factoring in refactoring.\n\nBusiness process re-engineering leads to segre­\n\ngating capabilities into those that will be provided\n\nby newly developed services, and those that will\n\nbe provided by refactored legacy components.\n\nIt also enables a block and spiral upgrade strat­\n\negy for introducing new capabilities. An evalu­\n\nation of how it is decided which capabilities will\n\nbe newly developed and which will be refactored\n\nlegacy components is critical to the health of the\n\nprogram and should constitute another early and\n\ncritical aspect of the test and evaluation strategy.\n\nEach legacy component selected for refactoring\n\n\n-----\n\nmust be analyzed to determine how tightly cou­\n\npled it is to both the data and other processes.\n\nFailure to do so can lead to the sort of “sticker\n\nshock” some current programs have experienced\n\nwhen attempting to add capabilities through spiral\n\nupgrades.\n\nDistributed test environments. A key distinction\n\nof, and enabling concept in, the network-centric\n\nservice-based construct is the ability to reuse\n\ncapabilities through a process referred to as\n\nfinding and binding. Achieving the true acquisition\n\nbenefits of service-based programs requires that\n\ncapabilities that can be reused be discoverable\n\nand accessible. To do this, service registries must\n\nbe established and a distributed test environ­\n\nment be employed, which in turn places new\n\nrequirements on the test and evaluation strategy\n\nfor these types of programs. Distributed test\n\nand evaluation capabilities must be planned for,\n\nresourced, and staffed, and shared data reposito­\n\nries must be established that will support distrib­\n\nuted test and evaluation. Network infrastructures\n\nexist that host a wide variety of nodes that can\n\nsupport distributed test and evaluation (e.g., DREN\n\nand SDREN). However, early planning is required to\n\nensure they will be funded and available to meet\n\nprogram test and evaluation needs.\n\nImportance of metrics for loose coupling in\n\nT&E strategy. Another area where a test and\n\nevaluation strategy can be effective early in a\n\nservice-based acquisition program is in the\n\ncontinuous evaluation and measurement of the\n\nloose coupling that maintains separation of data\n\nand applications, and enables changes in ser­\n\nvices with minimal impact to other services. The\n\naverage contractor business model leans toward\n\n\ntight coupling simply because it ensures that the\n\ncontractor is continuously engaged throughout\n\nthe program’s life cycle. Failure to establish and\n\napply metrics for loose coupling as part of the\n\ntest and evaluation strategy will lead to a lack of\n\ninsight into system performance; the impact of\n\ntight coupling with respect to interfaces will be\n\nunknown until the interfaces are actually in play,\n\nwhich is often too late to mitigate the risk involved.\n\nConsequently the test and evaluation strategy\n\nmust include an identification and metrics-based\n\nanalysis of interfaces to mitigate the risk that data\n\nand applications are tightly coupled; the earlier\n\nthis is accomplished, the easier it is to mitigate the\n\nproblem.\n\nData sharing implications for T&E strategy.\n\nOften overlooked in development and test and\n\nevaluation of service-based enterprises are\n\nthe core capabilities for data sharing. While\n\ntime is devoted to the test and evaluation of\n\nservices that enable data sharing, the underly­\n\ning technologies that support it are often not\n\nbrought into the test and evaluation process\n\nuntil late. The technologies critical to data dis­\n\ncovery and sharing are embedded in metadata\n\ncatalog frameworks and ontology products,\n\nboth of which require a skill set that is more\n\nesoteric than most. The consequence of this\n\nis that aspects of discovery and federation\n\nthrough the use of harmonized metadata are\n\noverlooked, and instead individual contractor\n\nmetadata is employed for discovery. This leads\n\nto a downstream need for resource adapt­\n\ners that bridge metadata used in one part of\n\nthe enterprise or for one type of data to other\n\nparts of the enterprise. In several instances, the\n\n\n-----\n\ndownstream requirement for resource adapters\n\nhas ballooned to account for nearly every data\n\nstore in the enterprise. A test and evaluation\n\nstrategy that incorporated the harmonization of\n\n###### Summary\n\n\nmetadata, the development of a single ontology,\n\nand the early test and evaluation of these items\n\nwould have saved time and money, and deliv­\n\nered a capability to the warfighter earlier.\n\n\nThe shift to a network-centric data-sharing strategy has introduced a new set of challenges in\nthe acquisition process. Incremental development of capabilities has become the norm, and\ndistributed enterprise capabilities are the desired end-state. Test and evaluation must evolve\nto keep pace with the shift in development processes. In this article we have captured a few of\nthe best practices and lessons learned, but the list could go on at length to include those prac­\ntices that still provide significant risk identification, management, and mitigation. In addition,\nas information technology in particular evolves, the risk areas will shift and coalesce, driving\nthe need for new and updated test and evaluation strategies.\n\n###### References and Resources\n\n1. Department of Defense Instruction Number 5000.02, December 8, 2008, Operation of the\n\nDefense Acquisition System, USD(AT&L), Enclosure 6 Integrated T&E, p. 50.\n\n2. _Incorporating Test and Evaluation into Department of Defense Acquisition Contracts,_\n\n2009, Office of the Deputy Under Secretary of Defense for Acquisition and Technology,\nWashington, DC.\n\n3. _Defense Acquisition Guidebook._\n\n4. DoD Presentation: Test and Evaluation Working Integrated Product Team, August 17,\n\n2009.\n\n###### Additional References and Resources\n\nDepartment of Defense Directive Number 5000.01, May 12, 2003, The Defense Acquisition\nSystem, USD(AT&L).\n\n\n-----\n\nDefinition: Test and evalua­\n\n_tion is the set of practices and_\n\n_processes used to determine if_\n\n_the product under examination_\n\n_meets the design, if the design_\n\n_correctly reflects the functional_\n\n_requirements, and if the product_\n\n_performance satisfies the_\n\n_usability needs of personnel in_\n\n_the field._\n\nKeywords: acceptance test,\n\n_integration test, operational_\n\n_test, peer reviews, system test_\n\n\nTEST AND EVALUATION\n###### Assess Test and Evaluation Plans and Procedures\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to be familiar\n\nwith different kinds of tests, which group con­\n\nducts the tests, how to evaluate test documents,\n\nand the developer’s procedures for test control.\n\nMITRE SEs are also expected to analyze test data.\n\n\n-----\n\n###### Background\n\nTesting is the way a product, system, or capability under development is evaluated for cor­\nrectness and robustness, and is proved to meet the stated requirements. Testing is done at\neach stage of development, and has characteristics unique to the level of test being performed.\nAt a macro level, testing can be divided into developer testing conducted before the system\nundergoes configuration management, and testing conducted after the system undergoes\nconfiguration management. Testing done before configuration management includes peer\nreviews (sometimes called human testing) and unit tests. Testing done after configuration\nmanagement includes integration test, system test, acceptance test, and operational test. An\noperational test is normally conducted by government testing agencies. The other tests are\nconducted by the developer; in some cases, such as acceptance test, government observers are\npresent.\n\n###### Assessing Test and Evaluation Plans and Procedures\n\nAssessment normally begins with the Test and Evaluation Master Plan (TEMP), which is the\ndriver for much of what follows. The TEMP is developed by the government; detailed test\nplans and procedures are created by the developer. The scope, direction, and content of the\nTEMP are driven by the nature of the program, the life cycle, the user needs, and the user\nmission. For example, testing software developed for the program is quite different from test­\ning systems that are largely based on, and require considerable integration of, commercialoff-the-shelf (COTS) products. The TEMP will influence the testing documents produced by\nthe developer, but the developer’s documents are largely driven by what it produces and is\nworking to deliver.\n\nThe government program management office (PMO) is tasked with assessing the devel­\noper’s test and evaluation plans and procedures. Often MITRE plays a central role in helping\nthe PMO perform this assessment. The requirements on which the developer’s test plans and\nprocedures are based must be well crafted. A valid requirement is one that is measurable and\ntestable. If it is not measurable and testable, it is a poor requirement. Developer test plans and\nprocedures should be based on the functional requirements, not the software design. Both the\ntest community within the developer organization and the development community should\nbase their products on the functional requirements.\n\nWhen assessing the developer’s test plans and procedures, the focus should be the\npurpose of the test—that is, to assess the correctness and robustness of the product, system,\nor service. The tests should prove the product can do what it is intended to and, second, can\nwithstand anomalous conditions that may arise. This second point requires particular care\nbecause there are huge differences in how robustness is validated in a COTS-based system\nversus software developed for a real-time embedded system. The environment in many\n\n\n-----\n\nCOTS-based business systems can be tightly bound. A name or address field can be limited\nin terms of acceptable characters and field length. In a real-time embedded system, you know\nwhat the software expects to receive if all is going as it should, but you do not always know\nwhat possible input data might actually arrive, which can vary in terms of data type, data\nrate, and so on. Denial-of-service attacks often try to overwhelm a system with data, and the\ndeveloper’s skill in building robustness into the system that allows it to handle data it is not\nintended to process has a great deal to do with the eventual reliability and availability of the\ndelivered product. It is not unusual for the error protection logic in complex government sys­\ntems to be as large as, or larger than, the operational software.\n\nAssessment of the test plans and procedures must take all of these issues into account.\nThe assessor must understand the nature and purpose of the system and the kind of software\ninvolved, and must have the experience to examine the test plans and procedures to assure\nthey do an appropriate job of verifying that the software functions as intended. The assessor\nmust also verify that, when faced with anomalous data conditions, the software will respond\nand deal with the situation without crashing. The test conditions in the test plans and proce­\ndures should present a wide variety of data conditions and record the responses.\n\nFor software systems, especially real-time systems, it is impossible to test all possible\npaths through the software, but it should be possible to test all independent paths to ensure\nall segments of the software are exercised by the tests. There are software tools to facilitate\nthis, such as the McCabe suite that will identify paths as well as the test conditions needed to\nput into a test case. However it is accomplished, this level of rigor is necessary to assure the\nrequisite reliability has been built into the software.\n\nUnlike the unit test, the integration test plans and procedures focus on the interfaces\nbetween program elements. These tests must verify that the data being passed between\nprogram elements will allow the elements to function as intended, while also assuring that\nanomalous data conditions are dealt with at their entry point and not passed to other pro­\ngrams within the system. The assessor must pay particular attention to this when assess­\ning the integration test plans and procedures. These tests must be driven by the functional\nrequirements, because those drive what the software must do for the system to be accepted by\nthe sponsor.\n\n###### Test and Evaluation Phases\n\nPre-Configuration Management Testing\n\nThe two primary test practices conducted prior to configuration management are:\n\n###### �Peer Reviews: Peer reviews are performed to find as many errors as possible in\n\nthe software before the product enters the integration test. They are one of the key\n\n\n-----\n\nperformance activities at Level 3 of the Software Engineering Institute’s (SEI) Capability\nMaturity Model. The SEI accepts two kinds of peer reviews: code walkthroughs, and\nsoftware inspections (the SEI preferred process, sometimes called Fagan Inspections in\nreference to Mike Fagan, who developed the process). Software inspections have a welldefined process understood throughout the industry. Done properly, software inspec­\ntions can remove as much as 87 percent of the life-cycle errors in the software. There is\nno standard process for walkthroughs, which can have widely differing levels of rigor\nand effectiveness, and at best will remove about 60 percent of the errors in software.\n###### �Unit Test: The developer conducts the unit test, typically on the individual modules\n\nunder development. Unit test often requires the use of drivers and stubs because other\nmodules, which are the source of input data or receive the output of the module being\ntested, are not ready for test.\n\nPost-Configuration Management Testing\n\nTesting conducted after the product is placed under developer configuration control includes\nall testing beyond unit test. Once the system is under configuration management, a problem\ndiscovered during testing is recorded as a trouble report. This testing phase becomes progres­\nsively more expensive because it involves integrating more and more modules and functional\nunits as they become available; the system therefore becomes increasingly more complex.\nEach test requires a documented test plan and procedure, and each problem encountered is\nrecorded on a trouble report. Each proposed fix must be validated against the test procedure\nduring which it was discovered, and must also verify that the code inserted to correct the\nproblem does not cause another problem elsewhere. With each change made to respond to\na problem, the associated documentation must be upgraded, the fix must be documented as\npart of the configuration management process, and the fix must be included in the next sys­\ntem build so that testing is not conducted with patches. The longer it takes to find a problem,\nthe more rework is likely, and the more impact the fix may have on other system modules;\ntherefore, the expense can continue to increase. Thus performing good peer reviews and unit\ntests is very important.\n\n###### �Integration Test: This is a developer test that is successively more complex. It begins by\n\nintegrating the component parts, which are either the modules that have completed the\nunit test or COTS products, to form functional elements. The integration test progresses\nfrom integration of modules to form entire functional elements, to integration between\nfunctional elements, to software-hardware integration testing. Modeling and simulation\nare often used to provide an operational-like testing environment. An integration test is\ndriven by an integration test plan and a set of integration test procedures. Typically an\nintegration test will have embedded within it a subset of tests identified as regression\n\n\n-----\n\ntests, which are conducted following a system build. Their objective is to verify that the\nbuild process did not create a serious problem that would prevent the system from being\nproperly tested. Often regression tests can be automated.\n###### �Test Data Analysis: When conducting peer reviews, unit tests, integration testing, and\n\nsystem tests, a significant amount of data is collected and metric analysis is conducted\nto show the condition state of the system. Significant metric data is produced related to\nsuch things as defect density, pass-fail data on test procedures, and error trend analy­\nsis. MITRE SEs should be familiar with test metrics, and they should evaluate the test\nresults and determine the likelihood of the system being able to meet the requirements\nof performance delivered on time and within budget.\n###### �System Test: This is an operational-like test of the entire system being developed.\n\nFollowing a successful system test, a determination is made whether the system is ready\nfor acceptance test. After the completed system test, and before the acceptance test, a\ntest readiness review (TRR) may be conducted to assess the readiness of the system to\nenter the acceptance test.\n###### �Acceptance Test: Witnessed by the government, this is the last test before the govern­\n\nment formally accepts the system. Similar to the system test, the acceptance test is often\na subset of the procedures run during system test.\n###### �Operational Test: Performed by an operational unit of the government, this is the final\n\ntest before the system is declared ready for general distribution to the field.\n\n###### Best Practices and Lessons Learned\n\n\n###### � [Examine the reports on the pre-configu­]\n\nration management tests to evaluate the\n\nerror density information and determine\n\nthe expected failure rates that should\n\nbe encountered during subsequent test\n\nperiods.\n###### � [Review the peer review and unit test results ]\n\nprior to the start of integration testing. Due\n\nto the expense and time needed to correct\n\nproblems discovered in the post-config­\n\nuration management tests, the SE should\n\nunderstand how thorough the prior tests\n\nwere, and whether there is a hint of any\n\n\nissues that need to be addressed before\n\nthe integration test starts.\n###### � [If peer reviews and unit tests are done ]\n\nproperly, the error density trend data dur­\n\ning the integration test should show an\n\nerror density of 0.2 to 1.2 defects per 1,000\n\nsource lines of code.\n###### � [Consider modeling and simulation options ]\n\nto support or substitute for some aspects\n\nof integration that are either of lower risk\n\nor extremely expensive or complex to\n\nperform with the actual system.\n###### � [Complete a thorough independent review ]\n\nof the test results to date prior to sup­\n\n\n-----\n\nporting the TRR. This is especially true for\n\nperformance or design areas deemed to\n\nbe of the greatest risk during the design\n\nphase. Once the TRR is passed and the\n\nprogram enters acceptance testing, cor­\n\nrecting problems is extremely expensive\n\nand time-consuming.\n###### � [Involve the government Responsible Test ]\n\nOrganization (RTO) early (during the con­\n\n###### References and Resources\n\n\ncept development phase is not too early)\n\nso they understand the programmatic and\n\ntechnical issues on the program. Includ­\n\ning the RTO as part of the team with the\n\nacquisition and engineering organizations\n\nwill lessen conflicts between the acquisi­\n\ntion organization and RTO due to lack of\n\ncommunication and misunderstanding of\n\nobjectives.\n\n\n“Best Practices,” SEPO Library via Onomi, accessed March 15, 2010.\n\n“Capability Maturity Model Integration (CMMI),” SEPO Library via Onomi, accessed March\n15, 2010.\n\n“Commercial Off the Shelf Software (COTS),” SEPO Library via Onomi, accessed March 15,\n2010.\n\nDepartment of Justice, Test and Evaluation Master Plan, accessed May 28, 2010.\n\nFederal Aviation Administration, System Process Flowcharts/Test and Evaluation Process and\nGuidance, accessed May 28, 2010.\n\n“Interoperability,” SEPO Library via Onomi, accessed March 15, 2010.\n\n“System Test Plan,” SEPO Library via Onomi, accessed March 15, 2010.\n\nTufts University, Department of Computer Science, The Test Plan, accessed May 28, 2010.\n\n\n-----\n\nDefinition: Verification is\n\n_“the process for determining_\n\n_whether or not the products_\n\n_of a given phase of develop­_\n\n_ment fulfill the requirements_\n\n_established during the previ­_\n\n_ous phase [1].” Validation is the_\n\n_“evaluation of the capability of_\n\n_the delivered system to meet_\n\n_the customer’s operational_\n\n_need in the most realistic envi­_\n\n_ronment achievable [1].”_\n\nKeywords: systems engineering\n\n_life cycle, validation, verification_\n\n\nTEST AND EVALUATION\n###### Verification and Validation\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to understand\n\nwhere verification and validation fit into the sys­\n\ntems engineering life cycle, and how to accom­\n\nplish them to develop effective and suitable\n\nsystems. They are expected to assist in develop­\n\ning and defining requirements, specifications for\n\ntest and evaluation plans, and verification and\n\nvalidation procedures. MITRE SEs are expected\n\nto participate in developmental and operational\n\ntesting, observe and communicate test results,\n\ninfluence retest decisions, recommend mitiga­\n\ntion strategies, and assist the customer/spon­\n\nsor in making system acceptance decisions.\n\nThey are expected to evaluate test data and\n\nverify that specified requirements are met and\n\nvalidated to confirm operational capabilities.\n\n\n-----\n\n###### Background\n\nMITRE’s government customers may describe the systems engineering life-cycle model differ­\nently. The Department of Defense (DoD) customer uses the DoD 5000.02 process to describe\na “five-stage” systems engineering life cycle [2]. This DoD 5000.02 life cycle model maps to\nother equivalent models described (e.g., International Organization for Standardization [ISO]\n15288 Systems and Software Engineering Life Cycle Processes, and Institute of Electrical\n& Electronics Engineers [IEEE] 1220-2005 Standard for Application and Management of\nthe Systems Engineering Process). Figure 1, as depicted in the “Engineering for Systems\nAssurance” Manual Ver. 1.0 published by the National Defense Industrial Association (NDIA),\nshows the interrelationships between the different life-cycle processes [3].\n\nRegardless of the life-cycle model our customer uses, they all track to three basic systems\nengineering stages: concept development, engineering development, and post-development.\nEach of these engineering stages may be separated into supporting phases. The concept\ndevelopment phase is critical because it describes the ultimate operational requirements that\nwill be used to “validate” the ultimate material solution. The supporting system, subsystem,\nand component-level requirements leading to preliminary design and critical design will\nbe iteratively verified through various types of testing and analysis during materialization,\nintegration, and testing. Verification is the critical feedback element that confirms the require­\nments specified in the previous phase were satisfied. Validation is final confirmation that the\nuser’s needs were satisfied in the final material solution. It cannot be overemphasized that\nVerification and Validation (V&V) and Test and Evaluation (T&E) are not separate stages or\nphases, but integrated activities within the SE process. Figure 2, from the Washington State\nDepartment of Transportation (DOT), illustrates how V&V provide feedback to the systems\nengineering process [4].\n\n###### Government Interest and Use\n\nUsing the ISO 15288 Systems and Software Engineering Life Cycle Processes as a model,\nV&V are critical activities that are executed continuously throughout the process. During the\ninitial concept development stage, verification activities confirm that the operational and\nperformance requirements and functional specifications are viable. These requirements and\nspecifications may be developed by the government, by MITRE, and/or by other Systems\nEngineering and Technical Assistance (SETA) contractors and must be verified. The opera­\ntional requirements will be used by the government for ultimate validation of the material\nsolution. The performance and functional specification must also be validated because they\nwill be used by the developing contractor to drive preliminary and critical design and to\ndevelop the material solution. During the engineering development stage, the subcomponents\nand components that comprise the material solution must be verified, integrated, and tested.\n\n\n-----\n\n|Utilization Support Utilization Supp S C Production & deployment EP Final produc A baseline LRIP|Col2|\n|---|---|\n\n\n-----\n\nTime\n\nFigure 2. Systems Engineering “V,” Washington State DOT\n\nOperational testing is the venue that gathers data to validate that the ultimate material solu­\ntion satisfies required operational capabilities. V&V are critical activities that confirm that the\n“contracted for” material solution provides the required operational capability.\n\n###### Best Practices and Lessons Learned\n\n\nContinuously coordinate process execution.\n\nIn many cases, the capabilities development,\n\nsystems acquisition, and systems engineering\n\nprocesses, although interdependent, are exe­\n\ncuted independently by different organizations (or\n\ndifferent parts of the same organization, such as\n\nthe prime contractor). Disconnects between the\n\n\nactivities executed by the different stakeholders\n\ncan create serious problems. This can lead to cost\n\nand schedule overruns, and reduced opera­\n\ntional capability. Active verification of the output\n\nfrom each step of the process and an active risk\n\nmanagement program can go far to identify and\n\naddress disconnects as they occur. The earlier\n\n\n-----\n\na problem is identified and addressed, the less\n\nexpensive the resolution will be in terms of cost,\n\nschedule, and performance. Early and continuous\n\ninvolvement of subject matter experts is required.\n\nOperational requirements verification—a\n\nteam sport. Verified operational requirements\n\nare critical to validation of the system. In many\n\ncases, the operational requirements are poorly\n\ndocumented or change during the course of an\n\nacquisition. Verification of operational require­\n\nments must involve all potential stakeholders,\n\nincluding the acquisition program manager,\n\nsystems engineering team, and validation agent\n\n(operational tester).\n\nSmart contracting. The government develops\n\noperational capability needs, functional require­\n\nments, and systems specifications that are placed\n\non contract to develop preliminary and critical\n\ndesigns, and to materialize the system. The con­\n\ntract should include Contract Data Requirements\n\nListings (CDRLs) that require the contractor to\n\ndevelop, and the government to approve, test\n\nplans; monitor test execution; and deliver reports\n\nthat support subcomponent, component, and\n\nsystem verification. This may involve additional\n\nupfront cost for the program. However, failure to\n\ndo so is likely to result in additional cost, longer\n\nschedule, and performance shortfalls to the\n\nrequired operational capability. The acquisition\n\nprogram manager, SE, and government contract­\n\ning officer must work carefully to shape requests\n\nfor proposal, evaluate responses, and form the\n\ncontract to include these CDRLs.\n\nHarmonize use of modeling and simulation\n\n(M&S) in verification. M&S can be used as part\n\nof the T&E process to verify subcomponents,\n\n\ncomponents, and systems. The program manager\n\nshould involve the contractor, and development\n\nand operational test agencies to identify where\n\nM&S can be used to generate data for use in\n\nverification. Establish the intended use of M&S by\n\neach of the testing stakeholders at the beginning\n\nof the systems engineering process. The M&S\n\napproach can then be harmonized across several\n\nintended users and phases of V&V.\n\nIntegrated testing supports continuous veri­\n\nfication and operational validation. The goal\n\nof Operational Test and Evaluation (OT&E) is to\n\nconfirm that the “concept” developed on the\n\nleft side of the systems engineering “V” can be\n\nvalidated in the “material solution” on the right\n\nside. The Operational Testing Agent (OTA) often\n\nseeks contractor and developmental test data to\n\nvalidate capabilities. Often requirements cannot\n\nbe validated because CDRLs were not specified\n\nin the contract and/or developmental test data\n\nwere not clearly specified by the OTA or delivered\n\nby the program manager/developer. In some\n\ncases, the verification activities were haphazard\n\nor not properly executed. In cases where there\n\nhas been an undisciplined approach to verifica­\n\ntion, test and evaluation (missing entry or exit\n\ncriteria for each step), significant gaps and holes\n\nmay exist in the material solution that are not\n\nevident until OT&E is executed. CDRLs, inte­\n\ngration, security, interoperability, development\n\ntest events, and supporting data requirements\n\nshould be clearly specified in T&E Master Plans.\n\nTime to fix and retest also would be included in\n\nthe process. The ultimate goal is to execute an\n\nintegrated testing approach where a compo­\n\nnent/system/ system-of-systems testing and\n\n\n-----\n\nverification can be executed by one stakeholder,\n\nand the data accepted by all stakeholders. Any\n\nsuch testing/verification approach must be\n\n###### References and Resources\n\n\ndocumented in test and evaluation plans and\n\nresources to execute documented.\n\n\n1. Kossiakoff, A., and W. Sweet, 2003, Systems Engineering: Principles and Practice, John\n\nWiley and Sons, ISBN 0-471-23443-5\n\n2. U.S. Department of Defense, December 8, 2008, “Operation of the Defense Acquisition\n\nSystem,” DoD Instruction 5000.02.\n\n3. National Defense Industrial Association System Assurance Committee, “Engineering for\n\nSystem Assurance,” Ver. 1.0.\n\n4. Washington State DOT, July 2010, WSDOT Design Manual, Chapter 1050.03, Systems\n\nEngineering: Systems Engineering “V.”\n\n\n-----\n\nDefinition: “Certification is the\n\n_comprehensive evaluation and_\n\n_validation of a[n] ... information_\n\n_system (IS) to establish the_\n\n_degree to which it complies_\n\n_with assigned information_\n\n_assurance (IA) controls based_\n\n_on standardized procedures._\n\n_An accreditation decision is a_\n\n_formal statement by a des­_\n\n_ignated accrediting authority_\n\n_(DAA) regarding acceptance of_\n\n_the risk associated with operat­_\n\n_ing a[n] ... IS and [is] expressed_\n\n_as an authorization to operate_\n\n_(ATO), interim ATO (IATO), interim_\n\n_authorization to test (IATT), or_\n\n_denial of ATO (DATO) [1].“_\n\nKeywords: accreditation, certifi­\n\n_cation, DIACAP_\n\n\nTEST AND EVALUATION\n###### Create and Assess Certification and Accreditation Strategies\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the principles of certification and\n\naccreditation (C&A), how a government develop­\n\nment organization initiates the C&A process,\n\nand how the government sponsor maintains\n\naccreditation status following product delivery.\n\nThey are also expected to understand informa­\n\ntion assurance (IA) and C&A requirements and\n\nprocesses so they can advise when the govern­\n\nment or the contractor is not complying with\n\nthe letter or intent of department or agency\n\npolicies and processes. MITRE SEs are expected\n\nto understand how systems engineering deci­\n\nsions may impact the IA posture of a system.\n\n\n-----\n\n###### Introduction\n\nThis article is intended to provide general guidance on C&A of all government systems. It fol­\nlows the Department of Defense (DoD) C&A process and is directly applicable to DoD systems.\nC&A processes for other U.S. government systems are similar in their essentials but otherwise\nmay vary. In the latter case, the guidance presented here should serve as a general reference\nfor the conduct of C&A activities. Non-DoD department or agency guidance should always\ntake precedence for C&A of their systems.\n\n###### Certification and Accreditation Process Overview\n\nC&A processes applied to federal and DoD systems are similar. These similarities include use\nof a common set of functional roles as shown in Table 1.\n\nTable 1. Functional Roles\n\n**Role** **Function/Responsibility**\n\nAn official with statutory or operational authority for specified infor­\n\nInformation Owner mation and responsibility for establishing the controls for its gen­\n\neration, collection, processing, dissemination, and disposal.\n\nIndividual, group, or organization responsible for ensuring the sys­\n\nInformation System Owner tem is deployed and operated according to the agreed-on security\n\nrequirements.\n\nIndividual, group, or organization responsible for conducting a secu­\n\nCertifying Authority/Agent rity certification, or comprehensive assessment of the manage­\n(CA) ment, operational, and technical security controls in an information\n\nsystem.\n\nDesignated Accrediting\n\nAn official with the authority to formally assume responsibility for\n\nAuthority (DAA) or Autho­\n\noperating a system at an acceptable level of risk.\n\nrizing Official\n\nThe following generic C&A process overview is based on the functional roles described\nabove.\n\n1. The information owner establishes data sensitivity and security protection\n\nrequirements.\n2. The information system owner implements technical, administrative, and operational\n\nsecurity controls in accordance with security protection requirements provided by the\ninformation owner.\n3. The CA evaluates the security controls incorporated by the system and makes a recom­\n\nmendation to the DAA on whether the system satisfies its security requirements.\n\n|Role|Function/Responsibility|\n|---|---|\n|Information Owner|An official with statutory or operational authority for specified infor­ mation and responsibility for establishing the controls for its gen­ eration, collection, processing, dissemination, and disposal.|\n|Information System Owner|Individual, group, or organization responsible for ensuring the sys­ tem is deployed and operated according to the agreed-on security requirements.|\n|Certifying Authority/Agent (CA)|Individual, group, or organization responsible for conducting a secu­ rity certification, or comprehensive assessment of the manage­ ment, operational, and technical security controls in an information system.|\n|Designated Accrediting Authority (DAA) or Autho­ rizing Official|An official with the authority to formally assume responsibility for operating a system at an acceptable level of risk.|\n\n\n-----\n\n4. The DAA assesses the residual security risk, based on the CA’s recommendation, and\n\nmakes an accreditation decision.\n5. The information system owner operates the accredited system, which must undergo\n\nperiodic review and/or re-accreditation.\n\n###### DoD Information Assurance Certification and Accreditation Process (DIACAP) [2, 3]\n\nDIACAP is the C&A process applied to systems that store or process DoD information. It is\ndefined in DoD Instruction 8510.01 as the “process to manage the implementation of IA capa­\nbilities and services and provide visibility of accreditation decisions regarding the operation\nof DoD information systems (IS), including core enterprise services and Web services–based\nsoftware systems and applications.” [1]\n\nIn supporting C&A of a system, MITRE should help the program manager (PM) assemble\nthe DIACAP team, identify requirements, design solutions, implement the system, and inte­\ngrate testing. The entire DIACAP team should be assembled at program inception to determine\nthe IA Strategy, to agree on the mission assurance category (MAC) and confidentiality level,\nnegotiate a baseline set of IA controls, and assign responsibilities. If there is no team review\nof system design for compliance with IA requirements, then testing of IA and functional\nrequirements, which sometimes can conflict, will likely not be integrated. It is important that\nthe DIACAP team be assembled to resolve discrepancies throughout the acquisition life cycle;\nwithout that cooperation, it is more likely the PM or engineers will make unilateral deci­\nsions the DAA may not be able to accept. To help ensure a successful positive C&A outcome,\nMITRE, often acting as “lead integrator” for the activity, should at the outset reach back to\nstaff members who support the CA and DAA to ensure coordination and agreement regarding\nthe scope of the C&A process.\n\n###### Process Artifacts\n\nExecution of the DIACAP produces a number of engineering artifacts that are summarized in\nTable 2.\n\nThese artifact documents, together with all other documents resulting from the DIACAP\nprocess, are typically produced by the program office and/or the acquisition team. When a\ncontractor produces a DIACAP document, it is reviewed and approved by the program office,\noften with a MITRE SE involved.\n\n###### Data Sensitivity and Mission Assurance Category\n\nEach DoD system can be characterized by two pieces of information: the confidentiality\nlevel of the data that it processes and its MAC. These characteristics drive the selection of IA\n\n\n-----\n\ncontrols that the system must implement and the level of robustness (i.e., strength of mecha­\nnism) required.\n\nThe confidentiality level of a system is based on the highest classification or sensitivity of\ninformation stored and processed by the system. The confidentiality level is expressed in three\ncategories: public, sensitive, and classified. More stringent authentication, access control,\nand auditing requirements apply to systems that process classified data than to systems that\nprocess sensitive data, while systems that process public data enforce minimal authentication,\naccess control, or auditing requirements.\n\nTable 2. Engineering Artifacts\n\n**Artifact** **Description**\n\nInformation to register about the system being\nSystem Information Profile (SIP)\ndeveloped.\n\nEnumerates, assigns, and tracks the status of IA con­\nDIACAP Implementation Plan (DIP)\ntrols being implemented.\n\nRecords the results of test procedures/protocols used\nDIACAP Scorecard\nto validate implemented IA controls.\n\nIdentifies tasks or workarounds to remediate identified\nPlan of Action & Milestones (POA&M)\nvulnerabilities.\n\nA compilation of IA controls validation artifacts provided\nSupporting Certification Documents\nto the CA.\n\nAn accreditation decision is a special case for authoriz­\n\nInterim Approval to Test (IATT) ing testing in an operational information environment or\n\nwith live data for a specified time period.\n\nAn accreditation decision intended to manage IA secu­\nrity weaknesses while allowing system operation for up\n\nInterim Approval to Operate (IATO)\n\nto 180 days, with consecutive IATOs totaling no more\nthan 360 days.\n\nAn accreditation decision that the system should not\noperate because the IA design, IA controls implemen­\n\nDenial of Approval to Operate (DATO)\n\ntation or other security is inadequate and there are no\ncompelling reasons to allow system operation.\n\nAn accreditation decision for a system to process,\nstore, or transmit information for up to three years;\n\nApproval to Operate (ATO)\n\nindicates a system has adequately implemented all\nassigned IA controls and residual risk is acceptable.\n\nAs described in DoD Instruction 8500.2, p. 22, the MAC assesses the value of the system\n“relative to the achievement of DoD goals and objectives, particularly the warfighters’ combat\nmission [3].” For systems designated as MAC I systems, loss of integrity or availability would\n\n|Artifact|Description|\n|---|---|\n|System Information Profile (SIP)|Information to register about the system being developed.|\n|DIACAP Implementation Plan (DIP)|Enumerates, assigns, and tracks the status of IA con­ trols being implemented.|\n|DIACAP Scorecard|Records the results of test procedures/protocols used to validate implemented IA controls.|\n|Plan of Action & Milestones (POA&M)|Identifies tasks or workarounds to remediate identified vulnerabilities.|\n|Supporting Certification Documents|A compilation of IA controls validation artifacts provided to the CA.|\n|Interim Approval to Test (IATT)|An accreditation decision is a special case for authoriz­ ing testing in an operational information environment or with live data for a specified time period.|\n|Interim Approval to Operate (IATO)|An accreditation decision intended to manage IA secu­ rity weaknesses while allowing system operation for up to 180 days, with consecutive IATOs totaling no more than 360 days.|\n|Denial of Approval to Operate (DATO)|An accreditation decision that the system should not operate because the IA design, IA controls implemen­ tation or other security is inadequate and there are no compelling reasons to allow system operation.|\n|Approval to Operate (ATO)|An accreditation decision for a system to process, store, or transmit information for up to three years; indicates a system has adequately implemented all assigned IA controls and residual risk is acceptable.|\n\n\n-----\n\nresult in immediate and sustained loss of mission effectiveness and cannot be tolerated. MAC\nII systems also have stringent data integrity requirements but may be unavailable for short\nperiods of time without impacting mission effectiveness. With MAC III systems, the loss of\ndata integrity and/or availability has no significant impact on mission effectiveness or opera­\ntional readiness.\n\n###### Information Assurance Controls\n\nInformation assurance controls used in DIACAP are detailed in DoD Instruction 8500.2 [3].\nThese safeguards are grouped into IA baselines, where the selection of an IA baseline is\ngoverned by the confidentiality level and MAC of the system. Table 3 specifies the number of\nIA controls that a system must satisfy as a function of the sensitivity and MAC level of that\nsystem.\n\nTable 3. IA Controls Requirements\n\n**Sensitivity/MAC** **MAC III** **MAC II** **MAC I**\n\nPublic 75 81 81\n\nSensitive 100 106 106\n\nClassified 105 110 110\n\nThese numbers reflect the upper bound on the number of IA controls; in reality, many\nof the IA controls in a given IA baseline may not apply or may be inherited from an external,\ninterconnected system. It should also be noted that while the number of IA controls required\nfor MAC I and MAC II are the same for a given sensitivity level, the level of effort to satisfy\nthose controls is often significantly higher for MAC I systems, where system availability\nrequirements are considerably more stringent.\n\nIf IA requirements are not identified early in the acquisition/development process at the\ntime functional requirements are identified, IA requirements cannot be built into the system\nand tested along with functional requirements. Although inappropriate, C&A is often per­\nformed after the system has been built; therefore, when IA controls validation is performed\nand the C&A documentation is presented to the CA and/or DAA, missing IA requirements\nmay be identified far too late in the acquisition life cycle. It is much more costly to modify a\nsystem to comply with IA requirements after it has been built than it is to build in IA up front.\n\n###### Robustness\n\nA number of IA controls specify system robustness, which DoD Instruction 8500.2 defines\nas “the strength of mechanism and assurance properties of an IA solution.” IA robustness\n\n|Sensitivity/MAC|MAC III|MAC II|MAC I|\n|---|---|---|---|\n|Public|75|81|81|\n|Sensitive|100|106|106|\n|Classified|105|110|110|\n\n\n-----\n\nrequirements are expressed by IA controls as basic, medium, or high, and depend on the MAC\nand sensitivity level of the system and the threat environment in which the system will be\ndeployed.\n\nCommercial off-the-shelf (COTS) IA products or IA-enabled products selected for use in a\nsystem must satisfy IA robustness requirements established for that system. The robustness\nof COTS IA products is evaluated through the National Information Assurance Partnership\n(NIAP) [4]. An NIAP evaluation assigns an evaluated assurance level (EAL) rating to each\nproduct as a means for selecting IA products for use in system acquisition or development\nprograms. Table 4 summarizes the IA characteristics of each robustness level and associated\nproduct EAL ranges.\n\nTable 4. IA Characteristics\n\n**Robustness Level**\n\n**Characteristics**\n\n**Basic** **Medium** **High**\n\nGeneral Commercial-grade best High-end High assurance\nDescription practice commercial-grade design\n\nStrong (e.g., PKI-based)\n\nAuthenticated access NSA-endorsed\nAccess Control authenticated access\ncontrol access control and\n\ncontrol\n\nkey management\nNIST-approved key NSA-approved key\nKey Management capabilities\nmanagement management\n\nNIST FIPS-validated NIST FIPS-validated NSA-certified\nCryptography\ncryptography cryptography cryptography\n\nAssurance proper­\n\nAssurance proper­\n\nAssurance properties ties consistent with\n\nties consistent with\n\nconsistent with NSA- NSA-endorsed high\n\nProtection Profiles NSA-endorsed medium\n\nendorsed basic robust­ robustness protec­\n\nrobustness protection\n\nness protection profiles tion profiles, where\n\nprofiles\n\navailable\n\nEvaluated Assur­\nEAL1 – EAL3 EAL4 – EAL5 EAL6 – EAL7\nance Level (EAL)\n\n###### Best Practices and Lessons Learned\n\n|Characteristics|Robustness Level|Col3|Col4|\n|---|---|---|---|\n||Basic|Medium|High|\n|General Description|Commercial-grade best practice|High-end commercial-grade|High assurance design|\n|Access Control|Authenticated access control|Strong (e.g., PKI-based) authenticated access control|NSA-endorsed access control and key management capabilities|\n|Key Management|NIST-approved key management|NSA-approved key management||\n|Cryptography|NIST FIPS-validated cryptography|NIST FIPS-validated cryptography|NSA-certified cryptography|\n|Protection Profiles|Assurance properties consistent with NSA- endorsed basic robust­ ness protection profiles|Assurance proper­ ties consistent with NSA-endorsed medium robustness protection profiles|Assurance proper­ ties consistent with NSA-endorsed high robustness protec­ tion profiles, where available|\n|Evaluated Assur­ ance Level (EAL)|EAL1 – EAL3|EAL4 – EAL5|EAL6 – EAL7|\n\n\nIA, C&A, and security in general are not viewed as\n\nfundamental requirements and are often traded\n\noff when program funding is limited or cut (IA is\n\n\nnot funded as a separate line item in the budget).\n\nPMs and engineers often don’t realize that IA\n\nrequirements are critical to the functionality of\n\n\n-----\n\na system; IA ensures the appropriate amount of\n\nconfidentiality, integrity, and availability are built\n\nin—something the warfighter demands.\n\nDevelopmental testing (DT) is often performed\n\nin a vacuum. IA controls are not identified early\n\non; therefore, the IA testing cannot be integrated\n\ninto the DT. DT is planned and performed without\n\nconsideration of the Operational Test & Evaluation\n\n(OT&E) certification requirement. Deficiencies are\n\nidentified in OT&E that should have been caught\n\nin DT and fixed.\n\nOnce an ATO is issued, the tendency is to place\n\nthe C&A package on the shelf for three years\n\nuntil the next accreditation is needed. If the\n\nsystem is not monitored constantly, with new\n\nvulnerabilities mitigated as they are discov­\n\nered and as threats become increasingly more\n\nadvanced, the IA posture of the system quickly\n\ndegrades.\n\nEmploy information systems security engi­\n\nneering (ISSE) and reference the IA Technical\n\nFramework. The intent is for ISSEs to work with\n\nSEs throughout the acquisition life cycle to build\n\nIA into the system. This cooperative effort will\n\nyield functional capability that is also secure. The\n\nSE and ISSE effort will also yield documentation\n\nthat can be used as evidence of compliance with\n\nassigned IA controls—no need to generate special\n\ndocuments just to satisfy the CA and DAA.\n\nDon’t wait until the system is completely built\n\nto begin testing the IA controls. As capabil­\n\nity is developed, test it—integrate the DT with IA\n\ncontrols testing. Also, integrate the DT with OT&E.\n\nOT&E teams can reuse the results of DT, but\n\n\nperform their own analysis. The test and evalu­\n\nation master plan should identify, integrate, and\n\ntrack all types of testing.\n\nIf the CA can’t participate in the DIACAP team\n\nmeetings, employ the agent of the certifying\n\nauthority (ACA) (or Service equivalent). The\n\nACAs were established to stand in for the CA and\n\nhandle the day-to-day certification activities.\n\nThe ACAs also perform hands-on validation of IA\n\ncontrols, not a desktop review as may be done at\n\na headquarters level. The ACAs are trusted by the\n\nCA, and the CA can take a DIACAP scorecard at\n\nface value—the CA need not dig into the details of\n\na C&A package, so staffing goes faster.\n\nPMs (with the help of the SE, ISSE, and ACA)\n\nmust build a realistic POA&M. In conjunction\n\nwith the DIACAP scorecard, the POA&M should\n\naccurately convey the residual risk to the DAA.\n\nThe PM must aggressively resolve weaknesses\n\nand constantly update the POA&M, submitting it\n\nquarterly to the DAA for review/acceptance.\n\nKeep the system current, relevant, and secure\n\nwith a robust incident response and vulner­\n\nability management program. Threats evolve\n\nand exploit new vulnerabilities, thereby increasing\n\nrisk. Constantly monitor the system to identify\n\nchanges (threats, vulnerabilities, operations,\n\nenvironment, etc.) that could impact the IA pos­\n\nture. Ensure the IA manager is a member of the\n\nconfiguration control board to review all changes\n\nimpacting IA.\n\n\n-----\n\n###### References and Resources\n\n1. DoDI 8510.01, November 28, 2007, “DoD Information Assurance Certification and\n\nAccreditation Process (DIACAP).”\n\n2. Information Assurance Certification and Accreditation Process (DIACAP).\n\n3. DoD Instruction 8500.2, February 2003, “Information Assurance (IA) Implementation.”\n\n4. The NIAP Evaluated Products List.\n\n\n-----\n\n##### Implementation, O&M,\n and Transition\n\nDefinitions: Implementation is the realization of a system (application, plan execu­\n\n_tion, idea, model, design, specification, standard, algorithm, or policy) into an_\n\n_operational environment._\n\nO&M (Operations & Maintenance): When a system is fielded, it enters an operations\n\n_phase. Preventive maintenance is a schedule of actions aimed at preventing break­_\n\n_downs and failures before they occur and at preserving and enhancing equipment_\n\n_reliability by replacing worn components before they fail. When a system fails, correc­_\n\n_tive maintenance is performed._\n\n_Transition is when the system moves from development to the manufacturing/­_\n\n_fielding/sustainment phase._\n\nKeywords: corrective, implementation, maintenance, operations, preventive,\n\n_transition_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to take into account\n\nsuccessful sustainment of their system during pre-acquisition and\n\nacquisition phases. They are expected to be able to develop transition\n\nstrategies for delivering and deploying systems, including simultaneous\n\nsystem operation, cutover, and retirement/disposal of systems to be\n\ndecommissioned. SEs develop technical requirements and strategies to\n\nenable and facilitate system operation, maintenance, and operator\n\n\n-----\n\ntraining, and evaluate those developed by others. MITRE SEs develop approaches to enable\nsystem modifications and technology insertion.\n\n###### Context\n\nAlthough most of sustainment is typically not within MITRE’s primary systems engineering\npurview, activities during the concept development, design, and verification phases, where\nMITRE does have significant influence, can promote or inhibit successful operation and sus­\ntainment of fielded systems.\n\n###### Discussion\n\nAlthough no underlying articles exist yet for this topic, see the topic-level best practices and\nlessons learned below. When available, the articles in this topic will describe best practices\nand lessons learned for transitioning a system from its acquisition phase to the remainder of\nits life cycle.\n\n###### Best Practices and Lessons Learned\n\n\nView transition as a process. Though it is com­\n\nmon to think of the transition from development\n\nto fielding as a point in time, as described in this\n\narticle’s definition, transition is actually a process\n\nthat takes place over time. The transition process\n\nis a set of activities that encompasses (1) planning\n\nfor transition, (2) implementation, and (3) O&M in\n\na sequential, phased order. The transition process\n\nshould not be deferred until after the product is\n\ndeveloped and ready to be produced and fielded,\n\nor the likelihood of failure is greater. Begin planning\n\nthe transition process early in the development\n\nphase to account for any uniqueness in manufac­\n\nturing, fielding, or maintenance activities. Deciding\n\nhow to insert technology improvements in the\n\nO&M phase, often necessary multiple times,\n\nneeds to be addressed in the initial design and\n\ndevelopment of the system.\n\nStart planning for the transition early in the prod­\n\nuct development phase, even before the initial\n\n\ndesign review. Figure 1 represents a Department\n\nof Defense perspective of a product’s life cycle,\n\nand is fairly representative of any product devel­\n\nopment/manufacture/sustainment processes.\n\nAs indicated in the figure, the transition strategy\n\nneeds to begin as early as possible, even during\n\nthe product planning/strategy sessions. Include\n\nmanufacturing engineers and field-service engi­\n\nneers in the product planning phase to ensure\n\nthat the development engineers understand how\n\nthe product will be produced and maintained.\n\nMinor changes in the design, if done early on, can\n\nprovide a significant benefit to the person who\n\nmust build or maintain the product.\n\nWalk the manufacturing floor. Have the SEs\n\ntour the manufacturing or integration facility and\n\nunderstand how a product “flows” through the\n\nplant. Many times, simple modifications to the\n\nprocess can greatly improve the quality of the\n\nproduct being built. Talk to the personnel who\n\n\n-----\n\nTransition Planning Transition\nPhase Phase\n\nFull\n\nInitial Operational Operational\n\nA B (Program initiation) C Capability Capability\n\nMateriel Engineering and Production and Operations\nSolution Manufacturing Deployment and\nAnalysis Development Support\n\nMateriel FRP\nDevelopment Post CDR LRIP/ Decision\nDecision Assessment IO T&E Review\n\nPre-Systems Acquisition Systems Acquisition Sustainment\n\n= Decision Point = Milestone Review\n\nFigure 1. Transition Activities in Acquisition Life Cycle\n\n\nwork on the floor, to understand what works well\n\nand what does not. The more you understand\n\nabout the actual manufacturing and integration\n\nprocess, the smoother the transition will be.\n\nUnderstand the customer’s maintenance\n\nconcept. In the commercial world, a field engi­\n\nneering force typically services all aspects of the\n\nsystem. In the government sector, “two-level”\n\nmaintenance “boxes” are typically swapped in the\n\nfield (many times by the user), and boxes needing\n\nrepair are sent back to a repair depot or contrac­\n\ntor facility. Regardless of the method used, know­\n\ning the responsibilities of maintenance personnel\n\n###### References and Resources\n\n\nand the tools at their disposal helps develop a\n\nproduct that is easier to maintain.\n\nHave a flexible technology roadmap. Understand\n\nand have a plan for when, how, and why technol­\n\nogy improvements should be inserted into the\n\nsystem after deployment. Understand technology\n\nreadiness assessments and develop a roadmap\n\nthat balances technology maturity, the ability to\n\ninsert the technology into the system (e.g., from a\n\nmanufacturing or retrofit perspective), and when it\n\nis operationally appropriate to make the technol­\n\nogy upgrade.\n\n\nDefense Acquisition University, Systems Engineering, Defense Acquisition Guidebook,\nChapter 4.\n\nDefense Acquisition University, Life Cycle Logistics, Defense Acquisition Guidebook, Chapter 5.\n\n\n-----\n\n##### Other SE Life-Cycle Building\n Blocks Articles\n\nThis topic is a staging area for articles on subjects of relevance to SE\n\nLife-Cycle Building Blocks that don’t neatly fit under one of its other\n\ntopics. In most cases, this is because the subject matter is at the edge\n\nof our understanding of systems engineering, represents some of the\n\nmost difficult problems MITRE systems engineers work on, and has not\n\nyet formed a sufficient critical mass to constitute a separate topic. This\n\nis definitely not the “et cetera” pile of articles.\n\nAs more articles are added and an organizing theme among several\n\nof them becomes evident, a new topic will be created. In other cases, if a\n\nconnection between an article and an existing topic becomes apparent,\n\nthe article will be moved and the topic description revised to reflect the\n\nchange. Last, a subject may remain a critical, special area of interest and\n\na related article will continue to be carried under this topic.\n\nThe article “Spanning the Operational Space—How to Select Use\n\nCases and Mission Threads” covers use cases and mission threads as\n\nuseful mechanisms to document an existing system’s functionality or\n\nto establish a user’s needs for a new system. They reflect functional\n\nrequirements in an easy-to-read format. Use cases and mission threads\n\nare used during many stages of system development (e.g., to capture\n\nrequirements, to serve as the basis for the design process itself, to\n\nvalidate the design, for testing, etc.).\n\n\n-----\n\nThe article “Acquiring and Incorporating Post-Fielding Operational Feedback into Future\nDevelopments: The Post-Implementation Review” follows the fielding of a system and informs\nfuture efforts. As such, it follows the last life-cycle phases that we have defined here.\n\nThe article “Test and Evaluation of Systems of Systems” addresses test and evaluation\nplans in the SoS environment to help MITRE SEs apply systems engineering processes and\nmake plans for systems that are constituents of systems of systems (SoS).\n\nThe article “Verification and Validation of Simulation Models” describes best practices\nand lessons learned for developing and executing a plan to verify and validate a simulation\nmodel of a system or system component, and in collecting and analyzing different types of\nverification and validation data.\n\nThe article “Affordability, Efficiency, and Effectiveness (AEE)” describes the various prac­\ntices and analyses that MITRE SEs can apply to achieve three key success measures—afford­\nability, efficiency, and effectiveness (AEE)—in developing and shaping engineering solutions,\nmaking program recommendations, and evaluating engineering efforts on behalf of their\nsponsor’s mission.\n\n\n-----\n\nDefinition: Use cases describe\n\n_a system’s behavior (“who” can_\n\n_do “what”) as it responds to_\n\n_outside requests. The use case_\n\n_technique captures a sys­_\n\n_tem’s behavioral requirements_\n\n_by detailing scenario-driven_\n\n_threads through functional_\n\n_requirements._\n\n_A mission scenario/thread (an_\n\n_instance type of a use case)_\n\n_represents one path through_\n\n_the use case. In a sequence dia­_\n\n_gram, we can show one thread_\n\n_for the main flow through the_\n\n_use case and others for possible_\n\n_flow variations through it (e.g.,_\n\n_from options, errors, breaches)._\n\nKeywords: behavioral require­\n\n_ments, functional requirements,_\n\n_mission description, mission_\n\n_execution, operational require­_\n\n_ments, process description,_\n\n_sequence diagram, software_\n\n_engineering, systems engineer­_\n\n_ing, UML_\n\n\nOTHER SE LIFE-CYCLE BUILDING BLOCKS\nARTICLES\n###### Spanning the Operational Space—How to Select Use Cases and Mission Threads\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to understand\n\nthe purpose and roles of use cases and mission\n\nthreads, where to employ them in an acquisition\n\nprogram life cycle, and the benefits and risks of\n\nusing them. MITRE SEs are expected to under­\n\nstand when use cases and mission threads are\n\nappropriate to a situation and to develop detailed\n\nrecommendations and strategies for using them.\n\nThey are expected to monitor and evaluate con­\n\ntractor activities in using mission threads and use\n\ncases, as well as in the government’s overall strat­\n\negy, and recommend changes when warranted.\n\n\n-----\n\n###### Introduction\n\nThe use case technique captures a system’s behavioral requirements by detailing scenariodriven threads through the functional requirements.\n\nA use case defines a goal-oriented set of interactions between external actors and the sys­\ntem under consideration. Actors are parties outside the system that interact with the system.\nAn actor may be a class of users, roles users can play, or other systems. A primary actor is one\nhaving a goal requiring the assistance of the system. A secondary actor is one from which the\nsystem needs assistance.\n\nMission threads are associated with one or more operational vignettes. A vignette\ndescribes the overall environment: the geography, organizational structure and mission, strat­\negies, tactics, and information on any protagonists, including their composition, strategies,\ntactics, and timing. Mission threads can describe tactical operations, logistical operations,\nsupport operations, and maintenance, training, test, and development operations. Mission\nthreads serve as drivers for developing the architecture and as the basis for test cases during a\nverification cycle.\n\n###### What They Are and When to Use Them\n\nUse cases and mission threads are a useful mechanism to document an existing system’s\nfunctionality or to establish user needs for a new system. Use cases describe the system from\nthe user’s point of view and the interaction between one or more actors. (The interaction\nbetween the actor[s] and the system is represented as a sequence of simple steps.) Actors may\nbe end users, other systems, or hardware devices. Each use case is a complete series of events\ndescribed from the point of view of the actor. One or more scenarios or threads may be gener­\nated from a use case. The aggregate of all threads documents the detail of each possible way\nof achieving the system’s goal. Use cases typically avoid technical jargon, preferring instead\nthe language of the end user or domain expert.\n\nUse cases don’t make much sense when dealing with simple tasks or systems. There is no\npoint in modeling a task flow of how a visitor can go to the “About us” page on a website. This\nis better done with site map diagrams. Use cases are primarily useful when dealing with more\ncomplex flows.\n\nUse cases:\n\n###### �Reflect functional requirements in an easy-to-read, easy-to-track text format. �Represent the goal of an interaction between an actor and the system. (The goal repre­\n\nsents a meaningful and measurable objective for the actor.)\n###### �Record a set of paths (scenarios) that traverse an actor from a trigger event (start of the\n\nuse case) to the goal (success scenarios).\n\n\n-----\n\n###### �Record a set of scenarios that traverse an actor from a trigger event toward a goal but\n\nfall short of the goal (failure scenarios).\n###### �Are multi-level—one use case can use/extend the functionality of another.\nUse cases do not:\n\n###### �Specify user interface design—they specify the intent, not the action detail. �Specify implementation detail (unless it is of particular importance to the actor for\n\nassurance that the goal is properly met).\nUse cases are used during many stages of software/system development to:\n\n###### �Capture systems requirements. �Act as a springboard for the software/system design. �Validate the design. �Validate proper and complete implementation of the use cases for test and quality\n\nassurance.\n###### �Serve as an initial framework for the online help and user manual.\nThe use case model can be a powerful tool for controlling scope throughout a project’s\nlife cycle. Because a simplified use case model can be understood by all project participants, it\ncan also serve as a framework for ongoing collaboration as well as a visual map of all agreedupon functionality. It can, therefore, be a valuable reference during later negotiations that\nmight affect the project’s scope.\n\nUse case models can be used to:\n\n###### �Document the business process. �Illuminate possible collaborative business areas. �Separate business processes into functional system areas. �Serve as requirements documentation for system development (because they are\n\ndefined in a non-implementation/easy-to-read manner).\n###### �Identify possibilities of system or component reuse. �Categorize requirements (e.g., state of implementation, functional system). �Rank requirements (e.g., level of importance, risk, level of interoperability). �Publish requirements at various levels (e.g., detailed design requirements, hanger analy­\n\nsis requirements, document management, document creation requirements).\n###### �Identify the effects of functional changes on implementation systems or of implementa­\n\ntion changes on functional capabilities (because they are part of the object-oriented\nanalysis and design process).\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nValidate use cases. Use cases are great for\n\nmanaging boundaries, exploring scope options,\n\nexpanding scope, and controlling complexity, but\n\nthey must be validated thoroughly. In reviewing\n\nuse cases, ask these questions:\n\n###### � [Is the use case complete? Do any details ]\n\nneed to be added?\n###### � [Do I feel confident that the actor’s goal is ]\n\ngoing to be properly met?\n###### � [Can I suggest any procedural or require­]\n\nment changes that would simplify the\n\nprocess depicted in the use case?\n###### � [Are there any additional goals of the actors ]\n\nthat are not addressed?\n###### � [Are there any additional actors that are not ]\n\nrepresented (directly or indirectly)?\n\nHave enough use cases. When do you know you\n\nhave all the use cases or enough of them for your\n\npurposes? The simple answer is you have them all\n\nwhen the users, sponsor, and other stakeholders\n\ncannot think of any more.\n\nDevelop uses cases in a cooperative setting.\n\nUse cases can serve as a bridge between the\n\nneeds of requirement analysts, designers, and\n\nsystem developers. But this will only happen if the\n\ndifferent views of the participants are reflected\n\nin the use cases. Requirement analysts focus on\n\nclient needs, interaction designers look at the\n\noperational user needs, and developers focus on\n\nthe technological capabilities. If developed in a\n\ncooperative setting—and in context of budget and\n\nschedule—use cases can provide enduring utility.\n\n\nConsider these common use case mistakes and\n\npitfalls from Ellen Gottesdiener, a requirements\n\nexpert and principal at EBG Consulting:\n\n###### � [Other important requirements representa­]\n\ntions are unused or underused.\n###### � [Use case goals are vague or lack clarity. ] � [Use case scope is ambiguous. ] � [Use case text includes nonfunctional ]\n\nrequirements and user interface details.\n###### � [The initial use case diagrams excessively ]\n\nuse “extends” and “includes.”\n###### � [Use case shows inattention to business ]\n\nrule definition.\n###### � [Subject matter experts are not sufficiently ]\n\ninvolved in creating, reviewing, or verifying\n\nuse cases.\n###### � [Preparation for user involvement in use ]\n\ncase definition is insufficient.\n###### � [Too much detail appears too early in use ]\n\ncase definition amid expectation that it is\n\na one-pass activity.\n###### � [You failed to validate or verify your use ]\n\ncases.\n###### � [You have too few use cases. You’ve ]\n\ndescribed only a subsystem or a few\n\nduties of a few subsystems. You’ve missed\n\nthe greater, essential application.\n###### � [You have gone too far. Your use cases ]\n\nrepresent wishful thinking well outside of\n\nthe needs of the operational user. This is a\n\nform of “creeping featurism” in which there\n\nis a danger of wasting time on features\n\nyou’re not keeping and then spending\n\n\n-----\n\nmore time to cull the list when you realize\n\nwhat has happened.\n###### � [You captured the wrong use cases][.][ You’ve ]\n\ngot plenty and could do without some, but\n\nyou can’t do without some you’ve missed.\n\nMaintain a list of ways the system may be\n\nparameterized (vary the perceived importance\n\nof the actors, the types of input, future modes\n\nof operation, situations when good actors do\n\nbad things, exceptional circumstances). Iterate\n\nthrough your use cases to see if they cover all the\n\nsituations. This might help refine your use cases or\n\ndiscover new ones.\n\nExamine your nonfunctional requirements (e.g.,\n\nconstraints) to see if your use cases can address\n\nthem. You might be able to refine, add, or drop use\n\ncases based on this.\n\nMake a semantic network diagram (a quick\n\nbrainstorm of all the concepts and interactions\n\nand relationships from the problem domain).\n\nDecide which concepts fall within the system\n\n(will be part of the object model), which are on\n\nthe boundaries (probably will become actors or\n\nusages), and which are beyond the scope of the\n\nsystem being modeled (do not affect the software\n\nsystem being built and don’t show up in any of the\n\nmodeling efforts).\n\nDon’t make premature assumptions about the\n\ninterface. Use cases should be thought of as\n\nabstract prototypes. The interface is yet to be\n\ndesigned, and premature assumptions can con­\n\nstrain the design unnecessarily. This is particu­\n\nlarly important if you are not going to design the\n\ninterface yourself.\n\n\nDon’t describe internal workings of the applica­\n\ntion/system. Use cases are tools for modeling\n\nuser tasks; there are lots of other tools more suit­\n\nable for system modeling.\n\nLessons from the Field\n\nStay focused on the business processes and\n\nnot the information technology. Be clear that\n\nyou and your customer agree on the issue or\n\nquestion being addressed. Focus on the users’\n\nchallenges/issues. A multitude of use cases and\n\nmission threads can be pursued, but to do all of\n\nthem would be unrealistic. Be sure to focus on\n\nand have objectives up front of the particular\n\nproblem(s) that is being evaluated and solution(s)\n\nfor that problem.\n\nBe sure you have a firm grasp of the mission\n\ncontext. Thoroughly understand the end user’s\n\nneeds, concept of operations, and environment\n\nalong with emerging threats, courses of actions,\n\netc. (For example, don’t do force-on-force now\n\nwhen perhaps anticipated future operations are\n\ninsurgency oriented. Don’t do quick shock and\n\nawe in isolation when potential is for protracted\n\noperations.) Focus on the end users’ needs and\n\nnot on what an operational leader wants—keep\n\nthe use cases as close to reality as possible.\n\nExercise due diligence by looking for other\n\ndefinitions on the Web and elsewhere that are\n\nsimilar or related. Given the paucity of existing,\n\ndocumented use cases or mission threads, the\n\nfirst lesson learned is how to find them. The sec­\n\nond is that no one considers anyone else’s work\n\nauthoritative.\n\n\n-----\n\nConsider the varying environments as a key\n\naspect of the use case/mission thread. This\n\nincludes geographical/climate environments,\n\ncultural/social environments, and even military\n\nservice or agency organization environments.\n\nRemember that although we strive for cross\nagency, joint, or multinational considerations, the\n\nreality is that individual departments and agen­\n\ncies often drive many aspects of a use case;\n\nthe mission thread and these drivers need to be\n\nconsidered.\n\nUnderstand the level of operation that is being\n\nexamined. Put the particular use case in context\n\nacross the full spectrum of operations.\n\nAim to have some form of simulated results\n\nthat can feed into the particular use cases\n\nyou’re examining. Simulate the input and output\n\nfrom the particular use case to and from the\n\nintegrated set of activities involved in the par­\n\nticular operation or aspect of the execution. Use\n\nstimulation triggers to interplay with the particular\n\nuse case(s) or mission threads that are under\n\ncloser examination.\n\nExtend the use case/thread perspective across\n\nthe full suite of systems engineering activities:\n\narchitectures, development, testing, training/\n\nexercises, and experiments. These will help\n\nmove you beyond simply testing the buttons of a\n\ncapability to testing/exercising a capability in the\n\ncontext of the end users’ requirements.\n\nSolicit tools to help with use case/mission\n\nthread creation, management, and use. There\n\nare a variety of commercial and government tools\n\nto help capture the use case actors, scripts, and\n\nactions. There are also tools to capture the results\n\n\nof these activities from modeling and simula­\n\ntions or operator-driven exercises/experiments.\n\nRemember, it is not about the tool but about the\n\nprocess.\n\nShare use cases and mission threads with\n\nothers. Given the relatively wide perspective and\n\nencompassing value of use cases and mission\n\nthreads across the systems engineering life cycle,\n\nthere is value in employing use cases/mission\n\nthreads in multiple systems engineering activities.\n\nDon’t believe that you have the greatest use\n\ncases/mission threads in the world. You prob­\n\nably don’t, but you probably do have important\n\npieces of the overall puzzle and value; share use\n\ncases and mission threads to increase the overall\n\nvalue of capabilities to the end users.\n\nAn important part of using use cases and mis­\n\nsion threads is the results you get from archi­\n\ntectures, testing, experiments, and exercises.\n\nBe sure to make your results very visible. A lot of\n\ntime is used in capturing and assessing the use\n\ncase data and parameters. Make your analysis vis­\n\nible to others to use in exercises (perhaps physical\n\nexercises), models and simulations, etc. Seek out\n\nand contribute to use case/mission thread data\n\nrepositories so your work can be used and reused\n\nacross the systems engineering life cycle.\n\nRemember to document. It is especially impor­\n\ntant to know who vetted your descriptions, what\n\nthe scope of the analysis was, and who the user\n\nwas. It is also important to provide references to\n\nany MITRE document that illuminates your work.\n\nDocumentation is especially important when\n\ndiscussions occur later about scope.\n\n\n-----\n\n###### References and Resources\n\nArmour, F., and G. Miller, 2000, Advanced Use Case Modeling: Software Systems, AddisonWesley Professional.\n\nBittner, K., and I. Spence, 2002, Use Case Modeling, Addison-Wesley Professional, pp. 2–3.\n\nCarr, N., and T. Meehan, March 2, 2005, “Use Cases Part II: Taming Scope,” A List Apart.\n\n[Cockburn, A., May 10, 2006, Use Case Fundamentals, http://alistair.cockburn.us/](http://alistair.cockburn.us/Use+case+fundamentals)\nUse+case+fundamentals.\n\n[Cockburn, A., 2002, “Use cases, ten years later,” http://alistair.cockburn.us/](http://alistair.cockburn.us/Use+cases%2c+ten+years+later)\n[Use+cases%2c+ten+years+later, Originally published in STQE magazine, Mar/Apr 2002.](http://alistair.cockburn.us/Use+cases%2c+ten+years+later)\n\nCockburn, A., 2001, Writing Effective Use Cases, Addison-Wesley Longman Publishing Co.,\nInc., Boston, MA.\n\nDenney, R., 2005, Succeeding with Use Cases: Working Smart to Deliver Quality, AddisonWesley Professional.\n\nDepartment of Defense, July 16, 2008, DoD Instruction 5200.39, Critical Program Information\n(CPI) Protection Within the Department of Defense.\n\nGottesdiener, E., June 2002, “Top Ten Ways Project Teams Misuse Use Cases—and How to\nCorrect Them, Part I: Content and Style Issues,” The Rational Edge.\n\nJacobson, I., 1992, Object-Oriented Software Engineering, Addison-Wesley Professional.\n\nLiddle, S., 13 October 2009, Comment on “Stake holders for use case specifications,” BYU\nIsland, ISYS Core.\n\nMITRE Lean Six Sigma Community of Practice, accessed December 2009.\n\nSoftware Engineering Institute, “Evaluating System of Systems Architectures: Mission Thread\nWorkshop.”\n\n\n-----\n\nDefinition: The Post\n_Implementation Review (PIR) is_\n\n_an evaluation tool that com­_\n\n_pares the conditions prior to the_\n\n_implementation of a project (as_\n\n_identified in the business case)_\n\n_with the actual results achieved_\n\n_by the project. With reference_\n\n_to information technology solu­_\n\n_tion projects, the Information_\n\n_Systems Audit and Control_\n\n_Association (ISACA) defines the_\n\n_PIR as the first or subsequent_\n\n_review of an information tech­_\n\n_nology (IT) solution and/or the_\n\n_process of its implementation,_\n\n_performed after its implemen­_\n\n_tation [1]._\n\nKeywords: benefit analysis,\n\n_evaluation, implementation_\n\n_review, investment manage­_\n\n_ment, performance manage­_\n\n_ment, post-technology use_\n\n\nOTHER SE LIFE-CYCLE BUILDING BLOCKS\nARTICLES\n###### Acquiring and Incorporating Post-Fielding Operational Feedback into Future Developments: The Post- Implementation Review\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to understand\n\nthe purpose and role of a Post-Implementation\n\nReview (PIR) and the benefits and costs of\n\nemploying them. They are expected to be able\n\nto recommend techniques for PIRs, assist the\n\ngovernment in tailoring PIR procedures, lead PIRs,\n\nor perform individual PIR tasks, as appropriate\n\n(e.g., post-implementation technical performance\n\nanalyses). However, since PIRs should be con­\n\nducted by individuals not directly involved in the\n\nprevious steps of the acquisition process, MITRE\n\nSEs are frequently precluded from participating\n\nin the reviews themselves, other than as subject\n\nmatter experts, because of their role in recom­\n\nmending appropriate technology or providing\n\nguidance during earlier phases of the life cycle.\n\n\n-----\n\n###### Background\n\nThe PIR is used to evaluate the effectiveness of system development after the system has been\nin production for a period of time. The objectives of the PIR are to determine if the system\ndoes what it is designed to do: Does it support the user as required in an effective and efficient\nmanner? The review is intended to assess how successful the system is in terms of func­\ntionality, performance, and cost versus benefits, as well as to assess the effectiveness of the\nlife-cycle development activities that produced the system. The review results can be used to\nstrengthen the system as well as system development procedures. However, while the sys­\ntems engineering community is in general agreement that the PIR is a laudable thing to do, in\npractice the review generally remains an ignored stepchild at best, as departments and agen­\ncies content themselves with simply “running the numbers” (i.e., doing statistical analyses of\nperformance deltas).\n\n###### Government Interest and Use\n\nThe President’s Office of Management and Budget (OMB) outlines the purpose of the PIR in\nStep IV.3. Post-Implementation Review (PIR), of the Capital Programming Guide, first pub­\nlished in 1997, as a supplement to Circular A-11, Part 3: Planning, Budgeting, and Acquisition\nof Capital Assets. It is described as a diagnostic tool to evaluate the overall effectiveness of the\nagency’s capital planning and acquisition process as a complement to operational analysis.\nThis control mechanism is used during the operational life cycle of an asset to enable resource\nmanagers to optimize the performance of capital assets over the course of their life cycle and\neventual disposition [2, 3]. OMB stipulates that PIRs should be conducted by individuals not\ndirectly involved in the acquisition of the asset, but that they may include owners and users of\nthe asset or other personnel and consultants.\n\nIn response to the OMB mandate, agency system development life cycle (SDLC)\ndocumentation now specifies the preparation of a PIR report as part of the final phase\nof the SDLC, Operations and Maintenance. However, in its IT Investment Management\nFramework [4, 5], the General Accounting Office (GAO) observes that agencies will\ncontinue to have difficulty performing an effective PIR unless they have more compre­\nhensively established policies and procedures to assess the benefits and performance\nof their investments. This lack of documented processes is typically one of GAO’s pri­\nmary criticisms of agency performance in this area (e.g., GAO’s 2007 report on DoD\nBusiness System Modernization [GAO-07-538] which criticized the limited nature of the\nDefense Acquisition System [DAS] PIR procedures [6]). In its recently updated Acquisition\nDirective 102-01, Department of Homeland Security (DHS) requires a PIR of every pro­\ngram implemented in the agency. The most recent DoD Instruction 5000.02 extends the\nPIR requirement to acquisition category (ACAT) II and below [7].\n\n\n-----\n\nAccording to OMB, PIRs should be conducted three to twelve months after an asset\nbecomes operational. However, agencies vary in the length of that period, with a range of\nanywhere from three to eighteen months after implementation. GAO points out that the\ntiming of a PIR can be problematic. A PIR conducted too soon after an investment has been\nimplemented may fail to capture the full benefits of the new system, while a PIR conducted\ntoo late may not be able to draw adequately on the institutional memory of the development/\ninvestment process. GAO indicates that PIRs should also be conducted for initiatives that were\naborted before completion, to assist in the identification of potential management and process\nimprovements.\n\nTo ensure consistency of evaluations, OMB recommends the use of a documented meth­\nodology for the conduct of PIRs, requiring that the chosen methodology be in alignment with\nthe organization’s planning process. The required level of specificity for PIR reports varies\nfrom agency to agency. According to OMB, PIRs should address [2]:\n\n**Customer/User Satisfaction**\n\n###### �Partnership/involvement �Business process support �Investment performance �Usage\n**Internal Business**\n\n###### �Project performance �Infrastructure availability �Standards and compliance �Maintenance �Security issues and internal controls �Evaluations (accuracy, timeliness, adequacy of information)\n**Strategic Impact and Effectiveness**\n\n###### �System impact and effectiveness �Alignment with mission goals �Portfolio analysis and management �Cost savings\n**Innovation**\n\n###### �Workforce competency �Advanced technology use �Methodology expertise �Employee satisfaction/retention �Program quality\n\n\n-----\n\nIn assisting government sponsors to tailor the PIR to a particular project, MITRE SEs\nshould cast the net as broadly as possible, ensuring that the PIR also examines potential\nweaknesses and risks and the long-term maintainability of the solution.\n\n###### Best Practices and Lessons Learned\n\n\nIn 2004, the Information Systems Audit and\n\nControl Association (ISACA) published a set of\n\nbest practices specific to the PIR, the IS Auditing\n\nGuideline: Post-Implementation Review [1].\n\nProperly leveraged, the PIR provides an important\n\ncontrol mechanism and tool for the continual\n\nimprovement of the acquisition process. However,\n\nPIR templates are all too frequently limited to\n\nchecklists comparing baseline expectations to\n\nresults on a numerical value scale or summary\n\nbullets rather than providing serious analyses of\n\nroot causes and contributory factors. By contrast,\n\nthe Systems Engineering: Post-Implementation\n\nReview (PIR) Document Template, published by\n\nthe U.S. Army Information Technology Agency\n\n(ITA) as draft in October 2008, devotes the major­\n\nity of its review to the system itself rather than\n\nto the acquisition process, and emphasizes that\n\nthe review should result in a free-form report,\n\nthereby highlighting the analytical potential of the\n\nreview [8].\n\nThat agencies should underestimate the value\n\nof PIRs is not surprising, given that managers\n\nfrequently feel they are all too aware of the limita­\n\ntions of system releases as a result of compres­\n\nsion of schedules and inevitable scope creep prior\n\nto deployment. The typical strategy is simply to\n\n“move on and fix the bugs” in the next release.\n\n“Why do we need a PIR if we already know what\n\nis wrong?,” a rationale that is based on a failure to\n\n\nrecognize the potential benefit of the review when\n\nit is conducted as a true audit of technical and\n\nprocess performance. Significantly, the PIR is not\n\ntied to any life-cycle milestone, effectively robbing\n\nit of any determinative value.\n\nAn additional reason for the failure to fully lever­\n\nage the PIR as a process improvement tool for\n\nIT acquisitions may well be that because of the\n\ncapital planning and investment control (CPIC)\n\ncontext in which OMB and GAO place the review,\n\nit is seen more as a capital management/project\n\nmanagement tool than as a tool to improve sys­\n\ntem development from a technical perspective.\n\nIf, from a CPIC perspective, the PIR can be seen as\n\nthe initial iteration of the operational assessments\n\nconducted over the life cycle of an operational\n\nsystem, from the SDLC perspective it is only\n\nloosely tied in to the systems life cycle, being\n\nrelegated to the O&M phase rather than seen as\n\na closeout report for the systems implementation\n\nphase of the life cycle. From the SE perspective,\n\none could argue that the PIR is condemned to a\n\ntechnology limbo—too late for developers to care,\n\ntoo early for maintenance people to be interested.\n\nIf the PIR presents an excellent opportunity for\n\nMITRE SEs to assist their sponsors across all pro­\n\ngrams to improve the way they conduct business,\n\nthis opportunity is even more acute for those\n\nMITRE programs whose major focus is providing\n\nsystems engineering support to agencies involved\n\n\n-----\n\nin business systems modernization (BSM) efforts.\n\nThese efforts generally encompass the entire\n\nenterprise and frequently, as in the case of DHS,\n\ninvolve the insertion of technologies that revo­\n\nlutionize the way the agencies do business and\n\nprovide challenges far exceeding those faced in\n\nthe implementation of systems in the past. The\n\nissues resulting from the challenges posed by\n\nBSM and major technology insertions are com­\n\npounded by the fact that civilian agencies have\n\nincreasingly relied on contractor support not just\n\nto deliver systems, but to provide end-to-end\n\nservices. This is an environment where the PIR\n\ncan be particularly useful not only from a project\n\nand portfolio management perspective but also\n\nas an analysis of the operational impact of new\n\ntechnology (e.g., the use of biometrically enabled\n\npassports to verify traveler identity or the use of\n\ndigitized documentation in a traditionally paper\nbased environment).\n\nThe effectiveness of PIRs as a process improve­\n\nment tool in general, particularly in the case of\n\nBSM deployments that transform the operating\n\nenvironment, is dependent on the willingness\n\n###### References and Resources\n\n\nto address issues of workforce competence,\n\nadvanced technology use, and methodology\n\nexpertise (e.g., how database-driven and service\noriented architectures are received in organiza­\n\ntions previously responsible for the maintenance\n\nof long-outdated hardware and software).\n\nAlthough these areas of investigation are identi­\n\nfied in OMB’s PIR guidance, they are generally\n\nabsent from agency PIR procedures. However, it\n\nis frequently in these areas that implementations\n\nfail to achieve the expected results. This is either\n\nbecause the workforce is not ready to operate or\n\nmaintain the new systems, those responsible for\n\nmaintaining the new systems lack the expertise\n\nrequired to support next-generation technology\n\ndelivered through the BSM program, or a mis­\n\nmatch exists between the advanced technology\n\ndeployed and the requirements of the operational\n\nenvironment. If the PIR is not designed to explore\n\nthe root cause of potential issues, the result­\n\ning report will remain a paper exercise. However,\n\nfailure to confront difficult issues leaves agencies\n\nwith limited tools in their dialog with oversight\n\nbodies when it comes to requesting additional\n\nresources or postponement of mandates.\n\n\n1. ISACA, October 15, 2004, IS Auditing Guidelines Post-Implementation Review.\n\n2. OMB, June 2006, “Post Implementation Review and Post-Occupancy Evaluation,” Section\n\nIII.3.3, Capital Programming Guide, Version 2.0, Supplement to Office of Management and\nBudget Circular A-11, Part 7: Planning, Budgeting, and Acquisition of Capital Assets.\n\n3. OMB Circular No. A-130, February 8, 1996, Management of Federal Information\n\nResources, revised.\n\n4. GAO, 2004, Information Technology Investment Management. A Framework for Assessing\n\nand Improving Process Maturity (GAO-04-394G), pp. 83–89.\n\n\n-----\n\n5. Internal Revenue Service, June 30, 2000, IRS Enterprise Life Cycle, Investment Decision\n\nManagement, Post Implementation Review (PIR) Procedure.\n\n6. GAO, May 2007, Business Systems Modernization: DoD Needs to Fully Define Policies and\n\nProcedures for Institutionally Managing Investments (GAO-07-538), p. 31.\n\n7. Department of Defense, December 8, 2008, Department of Defense Instruction Number\n5000.02.\n\n8. U.S. Army Information Technology Agency, October 2008, ITA Systems Engineering. Post\nImplementation Review (PIR) Document Template, Ver. 1.0.\n\n\n-----\n\nDefinition: A system of systems\n\n_(SoS) is “a collection of systems,_\n\n_each capable of independent_\n\n_operation, that interoperate_\n\n_together to achieve additional_\n\n_desired capabilities [1].” Test &_\n\n_Evaluation (T&E) is the process_\n\n_by which an SoS and/or its_\n\n_constituents are compared_\n\n_against capability requirements_\n\n_and specifications._\n\nKeywords: system of systems\n\n_(SoS), SoS test, SoS test and_\n\n_evaluation, SoS testing, SoS_\n\n_validation, SoS verification_\n\n\nOTHER SE LIFE-CYCLE BUILDING BLOCKS\nARTICLES\n###### Test and Evaluation of Systems of Systems\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to understand\n\nthe characteristics of systems of systems (SoS)\n\nand the implications for systems engineering in an\n\nSoS environment, including SoS test and evalua­\n\ntion. SEs are expected to develop systems engi­\n\nneering and T&E plans for systems that are con­\n\nstituents of SoS as well as for SoS themselves.\n\n\n-----\n\n###### Background\n\nThis article is a special subject that addresses unique aspects of T&E of SoS and outlines strat­\negies and techniques for handling them.\n\nSystems of systems (SoS) differ from traditional systems in a number of ways. As a\nresult, the application of systems engineering to SoS requires that it be tailored to address the\nparticular characteristics of SoS. Likewise, the distinctive characteristics of SoS have implica­\ntions for the application of test and evaluation (T&E). This discussion specifically addresses\n“acknowledged SoS,” a type of SoS that is growing in significance in the Department of\nDefense (DoD). Acknowledged SoS have recognized objectives, a designated manager, and\nresources. However, the constituent systems (those that interoperate with each other to\nachieve the SoS capabilities) retain their independent ownership, objectives, funding, develop­\nment, and sustainment approaches. Changes in the constituent systems are based on collabo­\nration between the SoS and the systems levels.\n\nSoS raise unique development challenges consequent to the far-reaching SoS capability\nobjectives: the lack of control by the SoS over the constituent systems and the dependence of\nSoS capability on leveraging already fielded systems that address user and SoS needs. Further,\nSoS are often not formal programs of record but rather depend on changes made through\nacquisition programs or operations and maintenance of fielded systems. As a result, the ques­\ntion addressed here is not simply, how do we implement T&E for SoS but rather, what does it\nmean to test and evaluate SoS?\n\n###### SoS Characteristics Impacting Test and Evaluation\n\nTable 1 summarizes key differentiating characteristics between systems and acknowledged\nSoS. Most of the differences are a result of the independence of the SoS’s constituent systems.\nThe constituent systems may evolve in response to user needs, technical direction, funding,\nand management control independent of the SoS. SoS evolution, then, is achieved through\ncooperation among the constituent systems, instead of direction from a central authority, by\nleveraging the constituent systems’ efforts to improve their own individual capabilities.\n\nAn SoS will face T&E challenges that stem from the independence of its constituent\nsystems:\n\n###### �Independent development cycles mean that the delivery of systems’ upgrades to meet\n\nSoS needs is done asynchronously and is bundled with other changes to the system in\nresponse to other needs (beyond those of the SoS).\n###### �The number and variability of the systems that influence SoS results means that large\n\nSoS, in particular, are complex and that interactions among the constituents may lead to\nunintended effects or emergent behavior.\n\n\n-----\n\nTable 1. Comparing Systems and Acknowledged Systems of Systems\n\n|Aspect of Environment|System|Acknowledged System of Systems|\n|---|---|---|\n|Management and Oversight|||\n|Stakeholder Involvement|Clearer set of stakeholders|Stakeholders at both system level and SoS lev­ els (including system owners), with competing interests and priorities; in some cases, the system stakeholder has no vested interest in the SoS; all stakeholders may not be recognized.|\n|Governance|Aligned program manager and funding|Added levels of complexity due to management and funding for both the SoS and individual sys­ tems; SoS does not have authority over all the systems.|\n|Operational Environment|||\n|Operational Focus|Designed and developed to meet operational objectives|Called on to meet a set of operational objectives using systems whose objectives may or may not align with the SoS objectives.|\n|Implementation|||\n|Acquisition|Aligned to acqui­ sition category milestones, docu­ mented require­ ments, SE|Added complexity due to multiple system life cycles across acquisition programs involving legacy systems, systems under development, new devel­ opments, and technology insertion; Typically have stated capability objectives upfront which may need to be translated into formal requirements.|\n|Test and Evaluation|Test and evaluation of the system is generally possible|Testing is more challenging due to the difficulty of synchronizing across multiple systems’ life cycles, given the complexity of all the moving parts and potential for unintended consequences.|\n|Engineering & Design Considerations|||\n|Boundaries and Interfaces|Focuses on bound­ aries and inter­ faces for the single system|Focus on identifying the systems that contrib­ ute to the SoS objectives and enabling the flow of data, control, and functionality across the SoS while balancing needs of the systems.|\n|Performance and Behavior|Performance of the system to meet specified objectives|Performance across the SoS that satisfies SoS user capability needs while balancing needs of the systems|\n\n\n-----\n\n3. Systems implement changes as part of their own development\nprocesses\n4. Systems level T&E validates implementation of system requirements\n5. These system development processes are typically asynchronous\n\n\n1. Capability\nobjectives are\noften stated at\na higher level\n\n2. Requirements\nare specified at\nthe level of the\nsystem for each\nupgrade cycle\n\n\nTranslating Assessing\ncapability performance\nobjectives Orchestrating to capability\nupgrades objectives\nto SoS\n\nUnderstanding Developing &\nsystems & Addressing evolving SoS\nrelationships requirements architecture\n\n& solution\noptions\n\nMonitoring\n& assessing\nchanges\nExternal\nenvironment\n\n\n6. SoS performance\nis assessed in\nvarious settings\n\n|Translating capability objectives|Col2|Col3|Col4|\n|---|---|---|---|\n||Translating capability objectives|||\n|||Translating capability objectives||\n|||||\n\n|Assessing performance to capability objectives|Col2|Col3|Col4|\n|---|---|---|---|\n||Assessing performance to capability objectives|||\n|||Assessing performance to capability objectives||\n|||||\n\n|Orchestrating upgrades to SoS|Col2|Col3|Col4|\n|---|---|---|---|\n||Orchestrating upgrades to SoS|||\n|||Orchestrating upgrades to SoS||\n|||||\n\n|Understanding systems & relationships|Col2|Col3|Col4|\n|---|---|---|---|\n||Understanding systems & relationships|||\n|||Understanding systems & relationships||\n|||||\n\n|Developing & evolving SoS architecture|Col2|Col3|Col4|\n|---|---|---|---|\n||Developing & evolving SoS architecture|||\n|||Developing & evolving SoS architecture||\n|||||\n\n|Addressing requirements & solution options|Col2|Col3|Col4|\n|---|---|---|---|\n||Addressing requirements & solution options|||\n|||Addressing requirements & solution options||\n|||||\n\n|Monitoring & assessing changes|Col2|Col3|Col4|Col5|\n|---|---|---|---|---|\n||Monitoring & assessing changes||||\n|||Monitoring & assessing changes|||\n||||||\n\n\nFigure 1. SoS SE Core Elements and Their Relationships to T&E\n\n###### Test and Evaluation in the SoS SE Process\n\nThe SoS Guide [1] presents seven core SoS SE elements. Four are critical to T&E of the SoS.\nMore detail on these elements can be found in the above reference; this article summarizes\ntheir key aspects as shown in Figure 1. The discussion that follows shows how T&E activities\nfit into the SoS SE core elements and the challenges SoS pose for T&E.\n\n1. **Capability objectives of an SoS are often stated at a high level, particularly when**\n\n**the need for an SoS is first established.**\nTranslating capability objectives into high-level SoS requirements is a core element\nin the SoS SE process. In most cases, SoS capability objectives are framed in high-level\nlanguage that needs to be interpreted into high-level requirements to serve as the founda­\ntion of the engineering process.\n\n“SoS objectives are typically couched in terms of needed capabilities, and the SE is\nresponsible for working with the SoS manager and users to translate these into high-level\nrequirements that provide the foundation for the technical planning to evolve the capabil­\nity over time [1, p. 18].”\n\nThese objectives establish the capability context for the SoS, which grounds the assess­\nment of current SoS performance. In most cases, SoS do not have requirements per se; they\n\n\n-----\n\nhave capability objectives or goals that provide the starting point for specific requirements\nthat drive changes in the constituent systems to create increments of SoS evolution.\n2. **Requirements are specified at the level of the system for each SoS upgrade cycle.**\n\nIn the SoS SE core element, “assessing requirements and solution options,” incre­\nments of SoS improvements are planned collaboratively by managers and SEs at the SoS\nand system levels. Typically, there are specific expectations for each increment about\nsystem changes that will produce an anticipated overall effect on the SoS performance.\nWhile it may be possible to confidently define specifications for the system changes, it\nis more difficult to do this for the SoS, which is, in effect, the cumulative result of the\nchanges in the systems.\n\n“It is key for the systems engineer to understand the individual systems and their\ntechnical and organizational context and constraints when identifying viable options to\naddress SoS needs and to consider the impact of these options at the systems level. It is\nthe SoS systems engineer’s role to work with requirements managers for the individual\nsystems to identify the specific requirements to be addressed by appropriate systems (that\nis to collaboratively derive, decompose, and allocate requirements to systems) [1, p. 20].”\n\nAs a result, most SoS requirements are specified at the system level for each upgrade\ncycle, which provides the basis for assessing system-level performance. As discussed\nbelow, T&E of system changes is typically done by the systems as part of their processes.\n3. **Systems implement changes as part of their own development processes.**\n\nThe main source of T&E challenges arises from SoS upgrades that are the product of\nchanges in independent operating systems and in the SoS itself. The SoS SE team needs\nto work with the SE systems teams to plan and track these systems changes that will con­\ntribute to meeting the SoS capability objectives:\n\n“Once an option for addressing a need has been selected, it is the SoS systems\nengineer’s role to work with the SoS sponsor, the SoS manager, the constituent systems’\nsponsors, managers, SEs, and contractors to fund, plan, contractually enable, facilitate,\nintegrate, and test upgrades to the SoS. The actual changes are made by the consistent\nsystems’ owners, but the SoS systems engineer orchestrates the process, taking a lead role\nin the synchronization, integration, and test across the SoS and providing oversight to\nensure that the changes agreed to by the systems are implemented in a way that supports\nthe SoS [1, p. 20].”\n4. **Systems-level T&E validates implementation of system requirements.**\n\nConsequently T&E is implemented as part of this element at both the system and\nSoS levels. It seems fairly straightforward to assess whether the systems have made the\nchanges as specified in the plan; however, it is less clear how the results of these changes\nat the SoS level are to be tested and evaluated.\n\n\n-----\n\n“Throughout orchestration, the systems are implementing changes according to the\nnegotiated plans, and they are following their own SE and T&E processes. The SoS sys­\ntems engineer works with the SE teams of the constituent systems to enable SoS insight\ninto progress of the system developments as laid out in the SoS plan. The SoS SE team\nmembers are responsible for integration and for verification and validation of the changes\nacross the suite of system updates under an SoS increment, including T&E tailored to the\nspecific needs of the increments. Their efforts may result in both performance assess­\nments and a statement of capabilities and limitations of the increment of SoS capability\nfrom the perspectives of SoS users and users of the individual systems. These assessments\nmay be done in a variety of venues, including distributed simulation environments, sys­\ntem integration laboratories, and field environments. The assessments can take a vari­\nety of forms, including analysis, demonstration, and inspection. Often SoS SEs leverage\nsystem-level activities that are underway in order to address SoS issues [1, p. 68].”\n\nThere are significant challenges in creating an end-to-end test environment sufficient\nto addressing the met needs of the SoS capability. This can be mitigated by conduct­\ning T&E on a subset of systems prior to fielding the entire SoS increment, though at the\nexpense of some T&E validity. Contingency plans should be prepared for when the SoS\nT&E results don’t reflect expected improvements in case the systems are ready to be\nfielded based on system-level test results and owners’ needs.\n5. **Constituent system development processes are typically asynchronous.**\n\nThe asynchronous nature of the constituent systems development schedules presents\na challenge to straightforward T&E at the SoS level. While it is obviously desirable to coor­\ndinate the development plans of the systems and synchronize the delivery of upgrades,\nas a practical matter, this is often difficult or impossible. Even when it is possible to plan\nsynchronous developments, the result may still be asynchronous deliveries due to the\ninevitable issues that lead to development schedule delays, particularly with a large num­\nber of systems or when the developments are complex.\n\n“SoS SE approaches based on multiple, small increments offer a more effective way to\nstructure SoS evolution. Big-bang implementations typically will not work in this environ­\nment; it is not feasible with asynchronous independent programs. Specifically, a number\nof SoS initiatives have adopted what could be termed a ‘bus stop,’ spin, or block-withwave type of development approach. In this type of approach, there are regular timebased SoS ‘delivery’ points, and systems target their changes for these points. Integration,\ntest, and evaluation are done for each drop. If systems miss a delivery point because\nof technical or programmatic issues, they know that they have another opportunity at\nthe next point (there will be another bus coming to pick up passengers in 3 months, for\ninstance). The impact of missing the scheduled bus can be evaluated and addressed. By\n\n\n-----\n\nproviding this type of SoS battle rhythm, discipline can be inserted into the inherently\nasynchronous SoS environment. In a complex SoS environment, multiple iterations of\nincremental development may be underway concurrently.\n\n“Approaches such as this may have a negative impact on certification testing, espe­\ncially if the item is software related to interoperability and/or safety issues (such as Air\nWorthiness Release). When synchronization is critical, considerations such as this may\nrequire large sections of the SoS, or the entire SoS, to be tested together before any of the\npieces are fielded [1, pp. 68–69]. “\n\nAs these passages indicate, the asynchronous nature of system developments fre­\nquently results in other SoS constituents being unprepared to test with earlier delivering\nsystems, complicating end-to-end testing. However, as autonomous entities, constituent\nsystems expect to field based on results of their own independent testing apart from the\nlarger impact on SoS capability. Holding some systems back until all are ready to test suc­\ncessfully is impractical and undesirable in most cases. These dependencies form a core\nimpediment to mapping traditional T&E to SoS.\n6. **SoS performance is assessed in various settings.**\n\nSoS typically have broad capability objectives rather than specific performance\nrequirements as is usually the case with independent systems. These capability objectives\nprovide the basis for identifying systems as candidate constituents of an SoS, developing\nan SoS architecture, and recommending changes or additions to constituent systems.\n\n“In an SoS environment there may be a variety of approaches to addressing objec­\ntives. This means that the SoS systems engineer needs to establish metrics and methods\nfor assessing performance of the SoS capabilities which are independent of alternative\nimplementation approaches. A part of effective mission capability assessment is to iden­\ntify the most important mission threads and focus the assessment effort on end-to-end\nperformance. Because SoS often comprise fielded suites of systems, feedback on SoS\nperformance may be based on operational experience and issues arising from operational\nsettings, including live exercises as well as actual operations. By monitoring performance\nin the field or in exercise settings, SEs can proactively identify and assess areas needing\nattention, emergent behavior in the SoS, and impacts on the SoS of changes in constituent\nsystems [1, pp. 18–19].”\n\nThis suggests the necessity of generating metrics that define end-to-end SoS capabili­\nties for ongoing benchmarking of SoS development. Developing these metrics and col­\nlecting data to assess the state of the SoS is part of the SoS SE core element “assessing the\nextent to which SoS performance meets capability objectives over time.” This element pro­\nvides the capability metrics for the SoS, which may be collected from a variety of settings\nas input on performance, including new operational conditions [1, p. 43]. Hence, assessing\n\n\n-----\n\nSoS performance is an ongoing activity that goes beyond assessment of specific changes\nin elements of the SoS.\n\nT&E objectives, particularly key performance parameters, are used as the basis for\nmaking a fielding decision. In addition, SoS metrics, as discussed above, provide an ongo­\ning benchmark for SoS development which, when assessed over time, should show an\nimprovement in meeting user capability objectives. Because SoS typically comprise a mix\nof fielded systems and new developments, there may not be a discrete SoS fielding deci­\nsion; instead, the various systems are deployed as they are ready, at some point reaching\nthe threshold that enables the new SoS capability.\n\nIn some circumstances, the SoS capability objectives can be effectively modeled in\nsimulation environments that can be used to identify changes at the system levels. If the\nfidelity of the simulation is sufficient, it may provide validation of the system changes\nneeded to enable SoS-level capability. In those cases, the fidelity of the simulation may\nalso be able to provide for the SoS evaluation.\n\nIn cases where simulation is not practical, other analytical approaches may be used\nfor T&E. Test conditions that validate the analysis must be carefully chosen to balance\ntest preparation and logistics constraints against the need to demonstrate the objective\ncapability under realistic operational conditions.\n\n###### Best Practices and Lessons Learned\n\n\nApproach SoS T&E as an evidence-based\n\napproach to addressing risk. Full conven­\n\ntional T&E before fielding may be impractical for\n\nincremental changes to SoS because systems\n\nmay have asynchronous development paths. In\n\naddition, explicit test conditions at the SoS level\n\nmay not be feasible due to the difficulty in bring­\n\ning all constituent systems together to set up\n\nmeaningful test conditions. Thus an incremental\n\nrisk-based approach to identifying key T&E issues\n\nis recommended.\n\nFor each increment, a risk-based approach\n\nidentifies areas critical to success and areas that\n\ncould have adverse impacts on user missions.\n\nThis is followed by a pre-deployment T&E. Risk is\n\nassessed using evidence from a range of sources,\n\n\nincluding live test. In some circumstances, the evi­\n\ndence can be based on activity at the SoS level,\n\nin others it may be based on roll-ups of activity\n\nat the constituent systems level. The activity can\n\nrange from explicit verification testing, results of\n\nmodels and simulations, use of linked integration\n\nfacilities, and results of system level operational\n\ntest and evaluation.\n\nFinally, these risks must be factored into SoS\n\nand system development plans, in case the T&E\n\nresults indicate that the changes will have a nega­\n\ntive impact, which can then be discarded without\n\njeopardizing system update deliveries to users. The\n\nresults could be used to provide end-user feed­\n\nback in the form of “capabilities and limitations.”\n\n\n-----\n\nEncourage development of analytic methods\n\nto support planning and assessment. Analytical\n\nmodels of the SoS can serve as effective tools\n\nto assess system-level performance against SoS\n\noperational scenarios. They may also be used to\n\nvalidate the requirements allocations to systems,\n\nand provide an analytical framework for SoS-level\n\ncapability verification. Such models may used to\n\ndevelop reasonable expectations for SoS perfor­\n\nmance. Relevant operational conditions should\n\nbe developed with end user input and guided\n\nby “design of experiments” principles, so as to\n\nexplore a broad a range of conditions.\n\nAddress independent evaluation of networks\n\nthat support multiple SoS. Based on the\n\ngovernment vision of enabling distributed net\ncentric operations, the “network” has assumed\n\na central role as a unique constituent of every\n\nSoS. Realistic assessment of SoS performance\n\ndemands evaluation of both network perfor­\n\nmance and potential for degradation under\n\nchanging operational conditions. Because\n\ngovernment departments and agencies seek to\n\ndevelop a set of network capabilities for a wide\n\nrange of applications, consideration should be\n\ngiven to developing an approach to network\n\nassessment independent of particular SoS appli­\n\ncations as part of SoS planning and T&E.\n\nEmploy a range of venues to assess SoS perfor­\n\nmance over time. For SoS, evaluation criteria may\n\nbe end user metrics that assess the results of\n\nloosely defined capabilities. While these may not\n\nbe expressly timed to the development and field­\n\ning of system changes to address SoS capability\n\n\nobjectives, this data can support periodic assess­\n\nments of evolving capability and provide valuable\n\ninsight to developers and users.\n\nAssessment opportunities should be both\n\nplanned and spontaneous. For spontaneous\n\nopportunities, T&E needs to be organized in a way\n\nthat facilitates responding flexibly as they arise.\n\nEstablish a robust process for feedback once\n\nfielded. Once deployed, continuing evaluation of\n\nthe fielded SoS can be used to identify opera­\n\ntional problems and make improvements. This\n\ncontinuous evaluation can be facilitated through\n\nsystem instrumentation and data collection\n\nto provide feedback on constraints, incipi­\n\nent failures warnings, and unique operational\n\nconditions.\n\nBy establishing and exercising robust feedback\n\nmechanisms among field organizations and their\n\noperations and the SoS SE and management\n\nteams, SoS T&E can provide a critical link to the\n\nongoing operational needs of the SoS. Feedback\n\nmechanisms include technical and organizational\n\ndimensions. An example of the former is instru­\n\nmenting systems for feedback post-fielding. An\n\nexample of the latter is posting a member of the\n\nSoS SE and management team to the SoS opera­\n\ntional organization.\n\n\n-----\n\n###### References and Resources\n\n1. Office of the Under Secretary of Defense for Acquisition, Technology and Logistics (OUSD\n\nAT&L), August 2008, Systems Engineering Guide for Systems of Systems, Washington, DC.\n\n###### Additional References and Resources\n\nDahmann, J., G. Rebovich, J. Lane, R. Lowry, and J. Palmer, 2010, “Systems of Systems Test and\nEvaluation Challenges,” 5th IEEE International Conference on System of Systems Engineering.\n\n[The MITRE Institute, September 1, 2007, “MITRE Systems Engineering (SE) Competency Model,](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\nVer. 1,” Section 2.6.\n\n\n-----\n\nDefinitions: Verification is the\n\n_process of determining that a_\n\n_model implementation and its_\n\n_associated data accurately_\n\n_represent the developer’s_\n\n_conceptual description and_\n\n_specifications [1]._\n\n_Validation is the process of_\n\n_determining the degree to_\n\n_which a simulation model and_\n\n_its associated data are an_\n\n_accurate representation of the_\n\n_real world from the perspec­_\n\n_tive of the intended uses of the_\n\n_model [1]._\n\nKeywords: accreditation,\n\n_analysis, data, historical, SME,_\n\n_statistical, subject matter_\n\n_expert, validation, verification_\n\n\nOTHER SE LIFE-CYCLE BUILDING BLOCKS\nARTICLES\n###### Verification and Validation of Simulation Models\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to have\n\na sound knowledge of the system being mod­\n\neled and the software process for developing\n\nthe model in order to provide effective technical\n\nguidance in the design and execution of plans\n\nto verify and/or validate a model, or to provide\n\nspecialized technical expertise in the collection\n\nand analysis of varying types of data required\n\nto do so. They are expected to be able to work\n\ndirectly with the developer of the system and\n\nthe simulation model to provide technical\n\ninsight into model verification and validation. In\n\nmost cases, MITRE SEs will be responsible for\n\nassisting the government sponsoring organiza­\n\ntion in the formal accreditation of the model.\n\n\n-----\n\n###### Background\n\nModeling and simulation (M&S) can be an important element in the acquisition of systems\nwithin government organizations. M&S is used during development to explore the design\ntrade space and inform design decisions, and in conjunction with testing and analysis to gain\nconfidence that the design implementation is performing as expected, or to assist trouble­\nshooting if it is not. M&S allows decision makers and stakeholders to quantify certain aspects\nof performance during the system development phase, and to provide supplementary data\nduring the testing phase of system acquisition. More important, M&S may play a key role in\nthe qualification (“sell-off”) of a system as a means to reduce the cost of a verification test pro­\ngram. Here, the development of a simulation model that has undergone a formal verification,\nvalidation, and accreditation (VV&A) process is not only desirable, but essential.\n\nTable 1. Common Simulation Model Validation Methods\n\n|Model Validation Method|Description|\n|---|---|\n|Comparison to Other Models|Various results (e.g., outputs) of the simulation model being validated are compared to results of other (valid) models. For example, (1) simple cases of a simulation model are compared to known results of analytic models and (2) the simulation model is compared to other simulation models that have been validated.|\n|Face Validity|Asking individuals knowledgeable about the system whether the model and/or its behavior are reasonable. For example, is the logic in the con­ ceptual model correct and are the model’s input-output relationships reasonable?|\n|Historical Data Validation|If historical data exist (e.g., data collected on a system specifically for building and testing a model), part of the data are used to build the model and the remaining data are used to determine (test) whether the model behaves as the system does.|\n|Parameter Vari­ ability – Sensitiv­ ity Analysis|This technique consists of changing the values of the input and internal parameters of a model to determine the effect on the model’s behav­ ior of output. The same relations should occur in the model as in the real system. This technique can be used qualitatively—directions only of outputs—and quantitatively—both directions and (precise) magnitudes of outputs. Those parameters that are sensitive (i.e., cause significant changes in the model’s behavior or output) should be made sufficiently accurate prior to using the model.|\n|Predictive Validation|The model is used to predict (forecast) the system’s behavior, and then the system’s behavior and the model’s forecast are compared to deter­ mine if they are the same. The system’s data may come from an opera­ tional system or be obtained by conducting experiments on the system, e.g., field tests.|\n\n\n-----\n\nIV&V Phases\n\nRequirements Design Code\n\nValidation\n\nverification verification verification\n\nInterface\n\nIntegration\n\nConcept Requirements Design Code Operation\n\nand testing\n\nDevelopment Cycle\n\nFigure 1. The Relationship Between IV&V and Systems Engineering Processes\n\nConsider the following definitions for the phases of the simulation model VV&A\nprocess [1]:\n\n###### �Verification: “The process of determining that a model implementation and its\n\nassociated data accurately represent the developer’s conceptual description and\nspecifications.”\n###### �Validation: “The process of determining the degree to which a [simulation] model and\n\nits associated data are an accurate representation of the real world from the perspective\nof the intended uses of the model.”\n###### �Accreditation: “The official certification that a model, simulation, or federation of mod­\n\nels and simulations and its associated data are acceptable for use for a specific purpose.”\n###### �Simulation Conceptual Model: “The developer’s description of what the model or simu­\n\nlation will represent, the assumptions limiting those representations, and other capabili­\nties needed to satisfy the user’s requirements.”\nVerification answers the question “Have we built the model right?” whereas validation\nanswers the question “Have we built the right model?” [2]. In other words, the verification\nphase of VV&A focuses on comparing the elements of a simulation model of the system with\nthe description of what the requirements and capabilities of the model were to be. Verification\nis an iterative process aimed at determining whether the product of each step in the develop­\nment of the simulation model fulfills all the requirements levied on it by the previous step and\nis internally complete, consistent, and correct enough to support the next phase [3]. The vali­\ndation phase of VV&A focuses on comparing the observed behavior of elements of a system\nwith the corresponding elements of a simulation model of the system, and on determining\nwhether the differences are acceptable given the intended use of the model. If agreement is\nnot obtained, the model is adjusted in order to bring it in closer agreement with the observed\n\n|Col1|Requirements verification|Design verification|Code verification|Validation|Col6|\n|---|---|---|---|---|---|\n|Interface||||||\n|Concept|Requirements|Design|Code|Integration and testing|Operation|\n\n\n-----\n\nbehavior of the actual system (or errors in observation/experimentation or reference models/\nanalyses are identified and rectified).\n\nTypically, government sponsoring organizations that mandate the use of a formal VV&A\nprocess do not specify how each phase should be carried out. Rather, they provide broad\nguidance that often includes artifacts required from the process. Independent verification and\nvalidation (IV&V) activities occur throughout most of the systems engineering development\nlife-cycle phases and are actively connected to them, as depicted in Figure 1, rather than being\nlimited to integration and testing phases.\n\nA variety of methods are used to validate simulation models, ranging from comparison to\nother models to the use of data generated by the actual system (i.e., predictive validation). The\nmost commonly used methods are described in Table 1 [4].\n\nWith the exception of face validity, all the methods detailed in Table 1 are data-driven\napproaches to model validation, with predictive validation among the most commonly used\nmethods. The use of predictive validation generally requires a significant amount of effort to\nacquire and analyze data to support model validation.\n\n###### Best Practices and Lessons Learned\n\n\nDevelop and maintain a model VV&A plan.\n\nDevelop a detailed model VV&A plan before the\n\nstart of the acquisition program VV&A process.\n\nDistinct from the specification for the model\n\nitself, this plan provides guidance for each phase\n\nof VV&A and clarifies the difference between\n\nthe verification and validation phases. The plan\n\nshould also map model specification require­\n\nments to model elements, identify those model\n\nelements that require validation, and develop\n\nmodel validation requirements. The plan should\n\ndescribe the method(s) used for validation,\n\nincluding any supporting analysis techniques. If a\n\ndata-driven approach is used for model validation,\n\ndetail should be provided on either the pedigree\n\nof existing data or the plan to collect new data to\n\nsupport validation. Prototype simulations and ad\n\nhoc models are often developed where a VV&A\n\nplan is not considered beforehand. Then, when\n\n\nthe prototype becomes a production system, an\n\nattempt is made to perform V&V. It is extremely\n\ndifficult to perform verification after the fact\n\nwhen normal system development artifacts have\n\nnot been created and there is no audit trail from\n\nconcept to product.\n\nEstablish quantitative model performance\n\nrequirements. Often performance requirements\n\nare neglected while developing the domain model.\n\nComplex systems can have interactions that pro­\n\nduce unexpected results in seemingly benign situ­\n\nations. Prototypes developed with small problem\n\nsets may not scale to large problems that produc­\n\ntion systems will deal with. More model detail\n\ndoes not necessarily generate a better answer\n\nand may make a simulation intractable. In discrete\n\nevent simulation, the appropriate event queue\n\nimplementation, random number generators, and\n\n\n-----\n\nsorting/searching algorithms can make a huge\n\ndifference in performance.\n\nEstablish quantitative model validation require­\n\nments. Specify the degree to which each ele­\n\nment (component, parameter) of the model is\n\nto be validated. For model elements based on a\n\nstochastic process, validation is often based on\n\nstatistical tests of agreement between the behav­\n\nior of the model and that of the actual system. If a\n\nhypothesis test is used to assess agreement, both\n\nallowable Type I and Type II risks (errors) need to\n\nbe specified as part of an unambiguous and com­\n\nplete validation requirement. If, instead, a confi­\n\ndence interval is used to assess agreement, the\n\nmaximum allowable interval width (i.e., precision)\n\nand a confidence level need to be specified.\n\nIn certain instances, the decision to use either\n\na hypothesis test or a confidence interval may\n\nbe a matter of preference; however, there will be\n\ninstances when using a hypothesis test is neces­\n\nsary, due to the nature of the model element\n\nbeing validated. Regardless, the development of\n\na validation requirement is often an involved task\n\nrequiring the use of analytic or computational\n\nmethods to determine allowable levels of valida­\n\ntion risk or precision. Sufficient time, resources,\n\nand expertise should be allocated to this task.\n\nDevelop model validation “trade-off” curves. If\n\npredictive validation is used, the amount of data\n\nrequired to achieve a quantitative model validation\n\nrequirement (see above) will need to be deter­\n\nmined. Determining the amount of data required\n\n(N) to achieve, say, a maximum allowable confi­\n\ndence interval width at a specified confidence\n\nlevel for any model element may require extensive\n\nanalytical or computational methods. Related to\n\n\nthis is solving the “inverse” problem: determining\n\nthe maximum (or expected) confidence interval\n\nwidth given a particular value of N. From this, a\n\nset of curves may be constructed to allow for the\n\n“trade-off” between the amount of validation data\n\nrequired (which, generally, drives the cost of the\n\nvalidation effort) in order to achieve a validation\n\nrequirement. This problem should be addressed\n\nas early as possible during the validation phase of\n\nthe VV&A process.\n\nNot every model element requires validation.\n\nModel elements associated with model functional\n\nrequirements usually do not require validation.\n\nThese elements are only dealt with in the verifica­\n\ntion phase of the VV&A process. Trivial examples\n\nof this are model elements that allow the user to\n\nselect various model options (i.e., “switches” or\n\n“knobs”). Nontrivial examples are elements that\n\nhave been identified as not being relevant or criti­\n\ncal to the intended use of the model. However, a\n\nmodel element that has not been deemed critical\n\nmay, in fact, be fully functional when the simula­\n\ntion model is deployed. In this case, the model\n\nelement may still be exercised, but the model\n\naccreditation documentation should note that\n\nthis particular element has been “verified, but not\n\nvalidated.”\n\nAllow for simplifications in the model. Almost\n\nalways, some observed behaviors of the actual\n\nsystem will be difficult to model or validate given\n\nthe scope and resources of the model develop­\n\nment and validation efforts. In such cases, using\n\nsimplifications (or approximations) in the model\n\nmay provide an acceptable way forward. For\n\nexample, if a model element requires a stochastic\n\ndata-generating mechanism, a probability density\n\n\n-----\n\nRefine the\nmodel\n\n\nFigure 2. Simulation Model Development and the VV&A Process\n\n\nfunction with a limited number of parameters (e.g.,\n\na Gaussian distribution) may be used in place of\n\nwhat appears to be, based on analysis of data\n\nfrom the actual system, a more complex data\ngenerating mechanism. In doing this, a conser­\n\nvative approach should be used. That is, in this\n\nexample, employing a simplified data-generating\n\nmechanism in the model should not result in\n\noverly optimistic behavior with respect to the\n\nactual system.\n\nPlan for parallel (iterative) model development\n\nand VV&A. The model development and its VV&A\n\n\nprocess should be carried out in parallel, with suf­\n\nficient resources and schedule to allow for several\n\niterations. Figure 1 depicts a notional relationship\n\nbetween model development and the VV&A pro­\n\ncess. The focus here is the gathering and analysis\n\nof validation data for a particular model element\n\nand the resulting decision to: (1) adjust the value\n\nof one or more parameters associated with the\n\nmodel element to obtain closer agreement with\n\nobserved system behavior, (2) redesign the model\n\nelement by factoring in insights obtained from\n\n\n-----\n\nthe analysis of validation data, or (3) accredit the\n\nmodel with respect to this element.\n\nAlthough Figure 1 depicts the relationship\n\nbetween model development and VV&A in its\n\nentirety, it may be applied to individual, indepen­\n\ndent model elements. A model VV&A plan should\n\nidentify those model elements that may be vali­\n\ndated independently of others.\n\nConsider the partial model validation of\n\nselected model elements. When the validation\n\nof a particular model element becomes problem­\n\natic, it may be acceptable to validate the model\n\nelement over a subset of the element’s defined\n\nrange. This partial validation of a model element\n\nmay be a viable option when either insufficient\n\ndata is available to enable validation or the actual\n\nsystem behavior is difficult to model even if rea­\n\nsonable simplifications are made. However, the\n\nresulting model will be valid with respect to this\n\nelement only within this limited range. This fact\n\nshould be noted in accreditation documentation.\n\nUse multiple approaches to model validation.\n\nWhen data-driven model validation is not possible\n\nor practical, face validity may be used. However,\n\neven when data-driven validation can be carried\n\nout, face validity (i.e., expert review) may be used\n\nas a first step in the validation phase. If a similar\n\nbut already validated model is available, perform­\n\ning a comparison of the model being developed\n\nto this existing model may provide an initial (or\n\npreliminary) model validation. Following this initial\n\nvalidation, a more comprehensive approach such\n\nas predictive validation may be employed.\n\nSubsystem verification and validation do not\n\nguarantee system credibility. Each submodel\n\n\nmay produce valid results and the integration\n\nof models can be verified to be correct, but the\n\nsimulation can still produce invalid results. Most\n\noften this occurs when the subsystem conceptual\n\ndesign includes factors that are not considered in\n\nthe system conceptual design, or vice versa. For\n\nexample, in a recently reviewed simulation system\n\nwith multiple subcomponents, one of the sub­\n\ncomponents simulated maintenance on vehicles,\n\nincluding 2½ ton trucks. There was no need for\n\nthe submodel to distinguish between truck types;\n\nhowever, vehicle type distinction was important\n\nto another module. In the system, tanker trucks\n\nused for hauling water or petroleum were being\n\nreturned from the maintenance submodel and\n\nbecame ambulances—alphabetically the first type\n\nof 2½ ton trucks.\n\nKnow what your simulation tool is doing. If a\n\nsimulation language or tool is used to build the\n\nsimulation, “hidden” assumptions are built into the\n\ntool. Here are four common situations that are\n\nhandled differently by different simulation tools:\n\n###### � [Resource recapture:][ Suppose a part ]\n\nreleases a machine (resource) and\n\nthen immediately tries to recapture the\n\nmachine for further processing. Should\n\na more highly qualified part capture the\n\nmachine instead? Some tools assign the\n\nmachine to the next user without consid­\n\nering the original part a contender. Some\n\ntools defer choosing the next user until\n\nthe releasing entity becomes a contender.\n\nStill others reassign the releasing part\n\nwithout considering other contenders.\n###### � [Condition delayed entities:][ Consider ]\n\ntwo entities waiting because no units of\n\n\n-----\n\na resource are available. The first entity\n\nrequires two units of the resource, and\n\nthe second entity requires one unit of the\n\nresource. When one unit of the resource\n\nbecomes available, how is it assigned?\n\nSome tools assign the resource to the\n\nsecond entity. Other tools assign it to the\n\nfirst entity, which continues to wait for\n\nanother unit of the resource.\n###### � [Yielding control temporarily:][ Suppose ]\n\nthat an active entity wants to yield control\n\nto another entity that can perform some\n\nprocessing, but then wants to become\n\nactive and continue processing before the\n\nsimulation clock advances. Some tools\n\ndo not allow an entity to resume process­\n\ning at all. Other tools allow an entity to\n\ncompete with other entities that want\n\nto process. Still others give priority to\n\nthe relinquishing process and allow it to\n\nresume.\n###### � [Conditions involving the clock:][ Sup­]\n\npose an entity needs to wait for a com­\n\npound condition involving the clock (e.g.,\n\n“wait until the input buffer is empty or it is\n\nexactly 5:00 p.m.”). Generally the program­\n\nmer will have to “trick” the system to com­\n\nbine timed events with other conditions.\n\nAn event at 5:00 p.m. could check to see\n\nif the buffer was empty, and if so, assume\n\nthat the buffer-empty event occurred\n\nearlier.\n\nTest for discrete math issues. Computer\n\nsimulation models use discrete representations\n\nfor numbers. This can cause strange behaviors.\n\nWhen testing models, always include tests at the\n\n\nextremes. Examples of errors found during testing\n\ninclude overflow for integers and subtraction of\n\nfloating point numbers with insufficient mantissa\n\nrepresentation. Conversion from decimal to binary\n\nrepresentation could cause rounding errors that\n\nare significant in the simulation.\n\nEstablish a model validation working group. A\n\nregular and structured working group involving the\n\nsponsoring government organization, the system\n\ndeveloper, and the model developer will result in\n\na superior simulation model. The function of this\n\ngroup should be the generation and review of\n\nartifacts from the model validation phase. When\n\ndata-driven validation is used, the majority of the\n\neffort should focus on the information products\n\nproduced by the statistical analysis of validation\n\ndata. This group may also provide recommenda­\n\ntions regarding the need to collect additional\n\nvalidation data should increased quality in the\n\nvalidation results be necessary.\n\nInvest in analysis capabilities and resources.\n\nPlans for the availability of subject matter experts,\n\nsufficient computing and data storage capability,\n\nand analysis software should be made early in the\n\nVV&A process. For many VV&A efforts, at least\n\ntwo subject matter experts will be required: one\n\nwho has knowledge of the system being modeled\n\nand another who has broad knowledge in areas\n\nof data reduction, statistical analysis, and, pos­\n\nsibly, a variety of operations research techniques.\n\nIf analysis software is needed, consider using\n\nopen-source packages—many provide all the data\n\nreduction, statistical analysis, and graphical capa­\n\nbility needed to support validation efforts.\n\n\n-----\n\n###### Summary\n\nThe successful validation of a simulation model requires the government sponsoring organiza­\ntion to address VV&A early in the life of an acquisition program. Key activities include:\n\n###### �The development of a model VV&A plan with quantitative model validation require­\n\nments (where appropriate).\n###### �Detailed planning for the collection of validation data (if data-driven validation is\n\nneeded).\n###### �The assembly of a working group that includes both domain experts and analysts.\nModel development and model validation should not be carried out sequentially, but in a\nparallel and iterative manner.\n\n###### References and Resources\n\n1. DoD Modeling and Simulation (M&S) Verification, Validation, and Accreditation (VV&A),\n\nDecember 9, 2009, DoD Instruction 5000.61.\n\n2. Cook, D. A., and Skinner, J. M., May 2005, How to Perform Credible Verification,\n\nValidation, and Accreditation for Modeling and Simulation, CrossTalk: The Journal of\n_Defense Software Engineering._\n\n3. Lewis, R. O., 1992, Independent Verification and Validation, Wiley & Sons.\n\n4. Law, A. M., 2008, How to Build Valid and Credible Simulation Models, paper presented at\n\nthe Winter Simulation Conference.\n\n###### Additional References and Resources\n\nLaw, A. M., 2007, Simulation Modeling and Analysis, McGraw Hill.\n\nWinter Simulation Conference, http://wintersim.org/2014/\n\n\n-----\n\nDefinitions: Affordability,\n\n_Efficiency, and Effectiveness_\n\n_(AEE) are three success_\n\n_measures that guide systems_\n\n_engineers in developing and_\n\n_shaping engineering solutions,_\n\n_making program recommenda­_\n\n_tions, and evaluating engineer­_\n\n_ing efforts._\n\nKeywords: analysis of alterna­\n\n_tives, budgets, cost benefit_\n\n_analysis, life-cycle cost, port­_\n\n_folio analysis, program cost,_\n\n_return on investment_\n\n\nOTHER SE LIFE-CYCLE BUILDING BLOCKS\nARTICLES\n###### Affordability, Efficiency, and Effectiveness (AEE)\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to incor­\n\nporate and assess measures of affordability,\n\nefficiency, and effectiveness in engineering\n\nsolutions and supporting acquisition activi­\n\nties of sponsors. They are expected to:\n\n###### �Work with users to understand their mis­\n\nsion needs, capability gaps, and performance\n\nrequirements.\n\n###### �Develop alternative solutions or courses of action,\n\nand evaluate them for mission effectiveness as\n\nwell as life-cycle affordability and efficiency.\n\n###### �Understand operational and technical domains;\n\nrecommend and conduct engineering-based\n\ntrade-offs of requirements, design, performance,\n\ncost, and schedule to address affordability\n\nconstraints.\n\n\n-----\n\n###### �Understand life-cycle cost, schedule, risk, and affordability implications of alternatives\n\nunder consideration, and incorporate these dimensions in engineering products and\nrecommendations.\n###### �Encourage and facilitate active participation of the broad stakeholder community and\n\nacquisition decision makers in exploring alternatives; help them understand and use\nthe trade space to achieve program affordability and evaluate merits of alternatives from\noperational as well as business perspectives.\n###### �Monitor and evaluate contractor system development efforts; identify affordability risks\n\nand recommend changes when warranted as an acquisition program progresses.\n###### �Assist government sponsors in developing, adjusting, and implementing strategies,\n\nat program and enterprise levels, to ensure affordability and improve efficiency/\neffectiveness.\n###### �Communicate AEE best practices and lessons learned.\n\n Background\n\nThe measures of affordability, efficiency, and effectiveness can be characterized as follows:\n\n###### �Affordability: Ability to fund desired investment. Solutions are affordable if they can\n\nbe deployed in sufficient quantity to meet mission needs within the (likely) available\nbudget.\n###### �Efficiency: A measure of the “bang for the buck” or “unit benefit per dollar.” Solutions\n\nare efficient if they measurably increase the “bang” or “unit benefit” for the amount of\nresources required to deliver the capability.\n###### �Effectiveness: “The bang”; the ability to achieve an organization’s mission. Solutions\n\nare effective if they deliver capability of high value to accomplishing the user’s\nmissions [1].\nMITRE’s mission is to work in partnership with its government sponsors in applying sci­\nence and advanced technology to engineer systems of critical national importance. MITRE’s\nsystems engineering is conducted in the context of developing solutions to meet the needs and\nchallenges of our sponsors in conducting their missions, and in aiding sponsors in planning\nand managing programs to acquire such solutions. Sponsor success is achieved if the systems\nthey deploy are effective and available when needed to achieve their mission. Systems to be\nprocured, deployed, and sustained must be affordable, that is, within the means of the spon­\nsor’s available resources, and should be efficient, providing high value for the resources to be\nexpended.\n\nSponsor acquisition environments present many challenges. A number of system devel­\nopment programs have failed to deliver needed capabilities, or delivered reduced capability\nwith expenditure of time and funds well beyond what was planned. Current mounting federal\n\n\n-----\n\nbudget deficits place considerable economic stress on government agencies. Budget reductions\nmandate difficult decisions about where to invest limited resources, how to make current pro­\ngrams more affordable, and whether to terminate poorly performing programs. Investments\nfor new capabilities, replacements, or enhancements to existing systems as well as simple con­\ntinuation of existing programs require careful analysis and evaluation of their affordability,\nefficiency, and effectiveness. Systems engineering seeks to apply current and emerging tech­\nnologies flexibly to address sponsors’ dynamic threats and mission needs. At the same time,\n_affordability engineering helps sponsors respond to fiscal realities and be effective stewards of_\ntaxpayer dollars.\n\nAs depicted in Figure 1, affordability challenges exist at different sponsor levels and are\naddressed by varying engineering, analysis, or management approaches.\n\nEnterprises invest in and sustain systems, infrastructures, and organizations to accom­\nplish multiple missions. Investment decisions made by agency heads, Senior Acquisition\nExecutives (SAEs), Chief Information Officers (CIOs), and Program Executive Officers (PEOs)\nrequire a holistic view of the enterprise’s missions and objectives. Investment decisions are\nrequired for new or enhanced systems that provide additional capabilities as well as for sus­\ntainment of existing systems, infrastructures, manpower, and operations. A portfolio man­\nagement approach evaluates the benefits of or return on investment choices relative to filling\n\nSponsor Level Approaches/Examples\n\nI BPR; portfolio mgt.;\n\nAgency heads, SAEs, CIOs, efficiency initiatives;\nPEOs, system commands Enterprise manpower reductions;\n\ncurtail mission\n\nI Multi-statics\n\nVaries: Enterprise, Technical & II IT consolidation3-D printing\nwarfighters/operators, operational I Portable power\nPMOs, etc. innovation I Adaptive systems\n\nI Continuous competition\n\nI Acquisition reform\n\nAll the PMOs MITRE Acquisition I Affordability engineering\nsupports Programs I Knee-in-the-curve\n\nanalysis\n\nFigure 1. AEE Construct\n\n\n-----\n\nidentified capability gaps or improving mission functions. Efficiency initiatives and business\nprocess reengineering (BPR) look to reduce redundancy, overlap, or inefficient processes.\nEnterprise-level analysis and decision frameworks can be applied to help an agency achieve\ngreatest mission effectiveness, ensuring highest priority/highest value needs are met with its\nallocated budget [1].\n\nEach acquisition program is an element of the integrated capability delivered by the enter­\nprise. As such, it is vital that each be executed to deliver its target capability within available\ntechnology, funding, and time (i.e., be affordable). Each program and capability must contrib­\nute high value to users in terms of mission effectiveness achieved by the most efficient means.\nNot doing so can, like a chain reaction, have serious impacts across the enterprise. At this\nlevel, engineering solutions for affordability and adopting best acquisition systems engineer­\ning and management practices within the Program Management Office (PMO) are key to\nachieving success.\n\nTechnical and operational innovation contributes to AEE at both the acquisition program\nand enterprise levels. Adaptive systems and composable capabilities provide an enterprise\nwith the flexibility to respond to rapidly changing mission needs with existing resources and\nminimal new investment. Application of advances in IT and network design offer potential for\ngreat efficiency in the delivery of information and services to end users at the enterprise and\nprogram level. Advances within technology domains afford an opportunity to reduce life-cycle\ncosts in the acquisition of new or enhanced capabilities and to transform operations and sus­\ntainment approaches for greater efficiency.\n\n###### Government Interest and Use\n\nThe U.S. economy is experiencing an era of very slow growth, high unemployment, historic\nhigh debt, and mounting budget deficits at federal and lower levels. Considerable economic\nstress is being felt by all government agencies as they strive to accomplish their missions and\ndeliver services with constant or shrinking budgets. Agencies across the federal government\nare implementing new strategies to promote AEE in their acquisition decisions and manage­\nment practices.\n\nIn June 2010, the Office of the Secretary of Defense/Acquisition, Technology and Logistics\n(OSD/ATL) memo Better Buying Power (BBP): Mandate for Restoring Affordability and\nProductivity in Defense Spending set an “important priority” for DoD: “delivering better value\nto the taxpayer and improving the way the Department does business” [2]. OSD/ATL memo\nBetter Buying Power: Guidance for Obtaining Greater Efficiency and Productivity in Defense\nSpending, September 14, 2010, followed, setting the significance and breadth of this man­\ndate [3]. This 17-page memo outlined five key areas, with more than 20 specific initiatives.\nThe five areas are:\n\n\n-----\n\nInfra- Other\nThreat structure systems SE Team Program Office External\n\nTrade\nCONOPS Spec\n\n\nTradeoffs\n\n\nSpec\n\n\nCost and schedule should be\npart of trade space and\ntreated like size, weight,\npower, etc. - Cost analysis requirements description\n\nFigure 2. Affordability in Systems Engineering [6]\n\n\nCONOPS\n\n\nRisk\nassessment\nEffectiveness\nanalysis\n\n|Col1|Col2|Col3|Col4|Col5|Col6|\n|---|---|---|---|---|---|\n|Trad ONOPS Spec off Technology Contractor assessment proposals Affordability trades Cost Cost CARD* model estimate Baseline Ris design assess Perf. Performance Effectiveness model analysis analysis and schedule should be of trade space and||||||\n||||Cost Cost CARD* model estimate|||\n||||||Ris assess|\n||||Perf. Performance Effectiveness model analysis analysis|||\n|||||||\n\n\nEffectiveness\nanalysis\n\n\n1. Target Affordability and Control Cost Growth (mandating affordability as a require­\n\nment and implementing “should-cost” based management)\n2. Incentivize Productivity and Innovation in Industry\n3. Promote Real Competition\n4. Improve Tradecraft in Services Acquisition\n5. Reduce Non-Productive Processes and Bureaucracy\nAT&L has issued further guidance with specific requirements and management prac­\ntices to address affordability in program planning and execution and milestone decisions.\nIn response, acquisition leadership in the various services has also issued implementation\ndirectives. The Secretary of Defense has set targets for cost, budget, and personnel reductions\nin many areas of DoD’s operations as well. Additional references on the topic of affordability\nare available in Section 3 of “Affordability Engineering Capstone (Phase I) Volume 1 - Basic\nResearch” [4].\n\nThe Office of Management and Budget and the General Accounting Office (GAO) have\nalso been targeting AEE in government spending, addressing acquisition and contract­\ning practices, duplicative capabilities and services, and inefficient business operations [4].\nAlthough the practice of AEE in acquisition is not new (acquisition guidance and regulation\npre-BBP have long been concerned with delivering capabilities within cost and schedule tar­\ngets), what is new is the sense of urgency given the current economic crisis.\n\n\n-----\n\n###### Achieving AEE\n\nAt program levels, achieving AEE requires constant effort across the acquisition life cycle to\nexamine cost and schedule implications of choices. Decisions about the system design, what\nrequirements to meet, and how to structure and manage the acquisition impact affordability\nand introduce potential affordability risk. As illustrated in Figure 2, cost and schedule analy­\nsis is integral to the systems engineering process. It will reveal affordability risks and define\nthe trade space of mission needs, cost, schedule, and performance in which to explore and\ncritically evaluate alternatives. Engineering and cost analysis must be closely coupled in the\nprocess. Systems engineering and technical skills must be paired with cost estimating skills to\nexamine affordability trades and recommend analysis-based courses of action.\n\nAs the program moves through its acquisition stages, divergence of the technical baseline\nand cost estimate must be carefully monitored. Early discovery of divergence permits early\nintervention and correction, reducing affordability risk.\n\nAn Affordability Engineering Risk Evaluation (AERiE) tool [5] is being developed to\nfacilitate identification of affordability risk at multiple points across the acquisition life\ncycle. AERiE is also envisioned as part of an Affordability Engineering Framework (AEF)\nthat will facilitate the validation of the technical baseline and program cost estimate, sug­\ngest and analyze trade-offs to address affordability disconnects, and recommend alternative\ncourses of action.\n\nFrom several studies, the GAO identified proven acquisition practices to minimize the risk\nof cost growth on DoD programs. Such practices help “establish programs in which there is a\nmatch between requirements and resources—including funding—from the start and execute\nthose programs using knowledge-based acquisition practices [7].” Although referring to DoD,\nthese practices are generally applicable to acquisition programs of any government agency.\nThey require a strong systems engineering foundation be established early in a program and\ngreater reliance on a government systems engineering team to set and manage objectives.\nPractices include:\n\n###### �Early and continued systems engineering analysis: Ideally beginning before a\n\nprogram is initiated, early systems engineering is critical to designing a system that\nmeets requirements (or negotiates requirements) within available resources, such as\ntechnologies, time, money, and people. A robust analysis of alternatives and a prelimi­\nnary design review (PDR)—which analyze the achievability of required capabilities\nbefore committing to a program—can help ensure that new programs have a sound,\nexecutable business case that represents a cost-effective solution to meeting the criti­\ncal user needs. Such engineering knowledge can identify key trade-offs in require­\nments and technology that are essential to managing cost. Systems engineering\n\n\n-----\n\ncontinues to be an important tool through a program’s critical design review (CDR)\nand system demonstration.\n###### �Leveraging mature technologies and processes: Programs often have insufficient\n\nknowledge about the maturity of technology. Prototyping early in programs can pro­\nvide confidence that a system’s proposed design can meet performance requirements.\nFurther, having predictable manufacturing processes before decisions are made to move\ninto production can reduce unknowns. Naturally this assumes that the manufacturing\nprocess is used to develop the prototype.\n###### �Establishing realistic cost and schedule estimates matched to available resources:\n\nCost and schedule estimates are often based on overly optimistic assumptions. Without\nthe ability to generate reliable cost estimates, programs are at risk of experiencing cost\noverruns, missed deadlines, and performance shortfalls. Inaccurate estimates do not\nprovide the necessary foundation for sufficient funding commitments. Engineering\nknowledge and more rigorous technical baselines are required to achieve more accu­\nrate, reliable cost estimates at the outset of a program. Established cost estimating,\nschedule estimating, work-breakdown structures, risk management techniques, engi­\nneering analyses, and past performance help achieve realism in AEE assessments.\n###### �Clear, well-defined requirements: Government department and agency cultures\n\nand environments sometimes allow programs to start with too many unknowns, for\nexample, entering the acquisition process without a full understanding of requirements\n(technical, training, integration, fielding environment, etc.). Minimizing requirements\nchanges could decrease the amount of cost growth experienced by acquisition pro­\ngrams, but this has to be carefully managed and balanced in an evolving environment\nto ensure continued effectiveness against, for example, new or improved adversary\nthreats.\n###### �Incremental approach to acquiring capabilities: Programs can put themselves in a bet­\n\nter position to succeed by implementing incremental/evolutionary acquisition strategies\nthat limit the time in each incremental development.\nAt enterprise or portfolio levels, a number of analyses and approaches are applied to\nassess affordability and promote efficiency and effectiveness in investment decisions. Each\nis appropriate to a decision or management context. MITRE SEs are expected to understand\nkey aspects of the analyses that will need to be performed. They are expected to know the\nobjectives of the analysis, the decisions to be supported, and the general approaches that can\nbe applied. They are expected to enlist the support of and engage with analysts in conducting\nanalyses supporting AEE objectives.\n\nSEs are frequently called on to perform or support a number of different investment anal­\nysis types (analysis of alternatives, business case analysis, and cost benefit analysis—to name\n\n\n-----\n\na few). These are focused on informing sponsor funding and expenditure decisions and they\nprovide critical analysis for assessing affordability, efficiency, and effectiveness of alternatives\nin deciding to select a solution or course of action.\n\n###### �Analysis of Alternatives (AoA): An AoA is a technical assessment using distinct metrics\n\nand different criteria to objectively evaluate different potential courses of action (or\nalternatives). Typically the emphasis is focused on an analysis of alternative technical\napproaches, measuring their effectiveness in meeting a given set of functional require­\nments or mission need. The AoA also includes a life-cycle cost estimate for each alter­\nnative, a risk assessment for each alternative, and a recommendation(s) regarding a\npreferred alternative, pending the results of a more rigorous business case analysis.\n###### �Business Case Analysis (BCA): A BCA is used to determine if a new approach and\n\noverall acquisition should be undertaken. A BCA results in a justification, one way or\nthe other, based on the comparison of life-cycle costs and benefits and the results of\nfinancial analysis techniques such as return on investment (ROI), net present value\n(NPV), and payback for each alternative. A BCA may evaluate a single or multiple alter­\nnatives against the status quo. Based on the results of the financial analysis, a BCA will\nhelp to determine if a potential new acquisition is warranted and if the effort should go\nforward.\n###### �Cost Benefit Analysis (CBA): A cost benefit analysis is a structured assessment of\n\nalternative courses of action for achieving some objective. A CBA looks forward and\nevaluates specific courses of action to determine which would yield the maximum ROI.\nThe assessment informs a decision maker about financial, non-financial, and other nonquantifiable impacts—costs and benefits—of each course of action.\nThese analyses are described in more detail in the articles “Performing Analyses of\nAlternatives” and “Comparison of Investment Analyses.”\n\n###### Best Practices and Lessons Learned\n\n\nAEE is not achieved through the application of any\n\nsingle analytic approach or engineering or man­\n\nagement practice, or even a small set of the same.\n\nAEE practices need to be integrated throughout\n\nenterprise and program engineering and acquisi­\n\ntion management activities. Achieving AEE of\n\nacquisition programs or in enterprise operations\n\nrequires a continual conscious effort on the part\n\nof all stakeholders. The following practices are\n\n\nfundamental to engineering for AEE and achiev­\n\ning successful acquisitions. They reflect some\n\nexamples of best practices and are derived from l\n\nessons learned:\n\nUnderstand the operational mission, its con­\n\ntext, and the current systems or solutions\n\nemployed. Understand what is changing, and\n\nwhat is influencing these changes. What do these\n\nchanges imply in terms of new operational needs?\n\n\n-----\n\nAs an engineer, understand the current program\n\narchitecture and system operations to be able to\n\nevaluate impacts of these changes. Also under­\n\nstand the principles of the enterprise architecture,\n\nthe data and system interdependencies, and\n\nrequired interoperability. Affordability consider­\n\nations extend beyond the system boundaries. This\n\nunderstanding can be gained through discussions\n\nwith end users and participation in operational\n\nexercises and experiments.\n\nUnderstand the operational gaps, mission defi­\n\nciencies, or enhanced/new capabilities being\n\nsought by users. What are the users’ impera­\n\ntives (threat, time, consequences) to meet these\n\nneeds? Determine required vs. desired capabilities\n\nand performance levels. At what performance\n\nlevel would an improved capability provide no\n\nsubstantive value beyond current capabilities?\n\nAt what performance level would an improved\n\ncapability exceed that required to accomplish the\n\nmission? Resources spent delivering performance\n\nin excess of that needed might be more effec­\n\ntively applied to other needs. The understanding\n\nof operational gaps can be gained through exam­\n\nining the after-action assessments of operations,\n\nvarious operational lessons learned, etc.\n\nDerive solutions from consideration of\n\nDOTMLPF (Doctrine, Organization, Training,\n\nMateriel, Leadership, Personnel, Facilities)\n\nalternatives, not just material solutions. Where\n\ncan non-material solutions affect the desired\n\nenhanced capabilities and operational benefit? If\n\na material solution is deemed necessary, deter­\n\nmine the non-material changes also needed to\n\nachieve the desired capability. Are these changes\n\naccounted for in program plans and life-cycle\n\n\ncost estimates? Understanding up front the full\n\nDOTMLPF impact of a solution is key to avoiding\n\naffordability surprises later in the program.\n\nConduct market research to determine where\n\nexploiting or adapting commercial products or\n\nservices in devising solutions may be possible.\n\nUnderstand the product marketplace, product\n\nmaturity, and the business as well as the tech­\n\nnical/operational and logistics risks of reliance\n\non commercial or government products. Many\n\ntechnology and capability assessments as well as\n\nproduct reviews exist and can help. Reach out to\n\nothers via social media.\n\nAssess the value proposition. From a portfolio\n\npoint of view, evaluate the cost-effectiveness of\n\nsolutions compared to alternative expenditures\n\nof available resources on other needs or capa­\n\nbilities. Is the expenditure of resources “worth\n\nit?” Does the enhanced or new capability provide\n\nvalue to users higher than addressing other\n\nimportant needs? Engineering assessments\n\nhighlighted below (e.g., analysis of alternatives)\n\nprovide techniques for evaluating the value\n\nproposition.\n\nUse early systems engineering to define the\n\ntrade space in which alternatives can be devel­\n\noped and evaluated. Define multiple concepts\n\nand characterize them technically with sufficient\n\ninformation to support rough order of magnitude\n\ncost estimation. Use concept modeling, modeling\n\nand simulation, prototyping, or experimentation to\n\nexamine concept feasibility. Identify the cost and\n\nschedule drivers of the concepts as they relate\n\nto specific requirements. Involve system users\n\nto identify technical or performance require­\n\nments that can be traded off to achieve cost and\n\n\n-----\n\nschedule objectives, or to define what capabilities\n\ncan be affordably delivered. Identify the require­\n\nments that drive cost and/or schedule and that\n\nimpose greater risk to timely delivery of needed\n\ncapabilities. Work with the users and other\n\nstakeholders as needed to define evolutionary\n\napproaches to meeting these requirements.\n\nAssess and compare the life-cycle cost, effec­\n\ntiveness, and risks of alternatives in select­\n\ning a solution. Ensure decision processes drive\n\nefficient and effective solution choices. Measure\n\nthe affordability of each solution against a current\n\nbudget profile and assess the affordability risk\n\nif the budget is changed. Understand and use\n\nestablished cost estimating tools to help deter­\n\nmine cost drivers and major risks associated with\n\nthe AEE of a capability. (See the article “Life-Cycle\n\nCost Estimation.”)\n\nAssess user stakeholder expectations against\n\nrealism of budgets, time, and technology\n\nmaturity. Understand the basis of budgets and\n\nfunding profiles. Ensure they are consistent\n\nwith the chosen solution/technical approach,\n\nbased on a cost estimate of a suitable technical\n\nbaseline, and include assessment of cost and\n\nschedule risk. Be wary of downward-directed\n\nschedules. Develop engineering-based time­\n\nlines showing the critical paths and dependen­\n\ncies; ensure that risks and uncertainty have\n\nbeen incorporated. For developmental items,\n\nensure that a technology readiness assessment\n\n(see the article “Assessing Technical Maturity”)\n\naccurately characterizes the technology matu­\n\nrity, and that the effort and time to advance\n\nmaturity to achieve desired performance or\n\nother requirements are adequately assessed.\n\n\nPresent the realism in cost as well as opera­\n\ntional terms of what mission aspects will be\n\nand might not be totally satisfied by the recom­\n\nmended approach along with the feasibility/\n\nprojection of capability satisfaction over time/\n\nfuture evolutions to help stakeholders assess\n\ntrade-offs. Create a time-phased roadmap\n\nhighlighting the recommended AEE strategy for\n\nimplementation of capabilities.\n\nEstablish, document, and maintain a compre­\n\nhensive, stable technical baseline to sup­\n\nport timely cost analysis and design trades.\n\nThe technical baseline of a chosen solution\n\nbecomes the foundation for the program cost\n\nestimate and program planning and execution.\n\nThrough program implementation, it serves as\n\nthe basis for performance of design and strat­\n\negy trade-offs, risk management, and mitiga­\n\ntion analyses. For these purposes, the technical\n\nbaseline must provide a holistic description\n\nof the system that includes its technical and\n\nfunctional composition, its relationships and\n\ninterdependencies with other elements of the\n\nenterprise, and its acquisition strategy and pro­\n\ngram implementation.\n\nCommunicate the technical baseline to ensure\n\ncost analysts understand it. Work with the cost\n\nanalysts in developing a comprehensive work\n\nbreakdown structure that captures all aspects of\n\nthe technical baseline. Provide a credible engi­\n\nneering basis and make clear any assumptions\n\nregarding input to the technical baseline. Ensure\n\nthat stakeholders—user community, acquisition\n\ncommunity, oversight organizations, etc.—are\n\naware of, familiar with, and understand the trade\noffs of the technical baseline and its role in AEE.\n\n\n-----\n\nAssess the completeness and realism of\n\nthe program’s cost and schedule estimate.\n\nConsider the program’s alignment and com­\n\npleteness with respect to the technical baseline\n\nand any changes to it as well as the adequacy\n\nwith which uncertainty and risk have been inte­\n\ngrated. As system requirements and program\n\nstrategies change, the technical baseline as\n\nwell as the program cost estimate should be\n\nupdated.\n\nIntegrate management of cost and technical\n\nbaselines throughout the program. Ensure that\n\ncost, engineering, and management teams work\n\ntogether (ideally collocated) to keep the technical\n\nbaseline and Program Cost Estimate current, and\n\nmaintain a list of risks, cost drivers, and alternative\n\nCOAs/mitigations to address moderate/high-risk\n\nareas.\n\nTreat cost and schedule as part of the design\ncapabilities trade space, just like size, weight,\n\npower, security, throughput, and other engi­\n\nneering parameters. Understand user expecta­\n\ntions/targets for total system cost, particularly\n\nunit procurement and sustainment costs for\n\nsystems with large quantities to be installed or\n\nfielded. Assess the ability of the chosen design to\n\nmeet these targets.\n\nUnderstand and document all system inter­\n\nfaces, interoperability requirements, depen­\n\ndencies on other systems, programs, and\n\nresources, and assess their associated risk\n\nas it would impact the program. The inter­\n\nfaces and dependencies of capabilities from\n\nindependent, yet associated, efforts can be a\n\nbig contributor to cost due to schedule mis­\n\nmatches, reworking of misunderstood interface\n\n\nexchanges, increased complexity in testing,\n\netc. Include consideration of these tasks and\n\ndependencies in the technical and cost base­\n\nlines along with the operational utility/value of\n\nthe interfaces, dependencies, and interoper­\n\nability. Various crown jewel and map-to-mission\n\ntechniques can be used to help accomplish this.\n\nThese techniques are frequently used for cyber\n\nmission assurance assessments and are equally\n\nvaluable to these AEE analyses.\n\nManage affordability as a key risk parameter in\n\nthe contractor’s system development effort.\n\nUse periodic design reviews to ensure that each\n\ncomponent of the system is on track from a\n\nrisk perspective (technical, cost, and schedule)\n\nto meet functional, performance, and interface\n\nrequirements. Monitor design change for impacts\n\nto production and sustainment costs.\n\nInform key design and programmatic deci­\n\nsions with assessment and understanding of\n\naffordability implications and associated risks.\n\nMaintain and measure progress against AEE\n\nobjectives (metrics) in design, engineering, and\n\nmanagement reviews and decision processes.\n\nEnsure “affordability” is communicated to decision\n\nmakers. Conduct independent assessments when\n\nconfronted with significant change in affordability\n\nrisk.\n\nKeep users well informed and involved in major\n\nengineering decisions affecting requirements\n\nsatisfaction, trade-offs, and affordability.\n\nPresent the AEE risks (as highlighted earlier in\n\n“Achieving AEE”) to the user community for their\n\ndecisions in accepting the risks (e.g., increased\n\ncosts balanced against increased effectiveness)\n\nto achieve an overall best value solution.\n\n\n-----\n\n###### References and Resources\n\n1. Affordability, Efficiency, and Effectiveness, Corporate Brief, December 2011, The MITRE\n\nCorporation.\n\n[2. OSD/AT&L Memorandum for Defense Acquisition and Logistics Professionals, Better](https://acc.dau.mil/CommunityBrowser.aspx?id=380803)\n\n[Buying Power: Mandate for Restoring Affordability and Productivity in Defense Spending,](https://acc.dau.mil/CommunityBrowser.aspx?id=380803)\nJune 28, 2010.\n\n[3. OSD/AT&L Memorandum for Defense Acquisition and Logistics Professionals, Better](https://acc.dau.mil/CommunityBrowser.aspx?id=395003)\n\n[Buying Power: Guidance for Obtaining Greater Efficiency and Productivity in Defense](https://acc.dau.mil/CommunityBrowser.aspx?id=395003)\n[Spending, September 14, 2010.](https://acc.dau.mil/CommunityBrowser.aspx?id=395003)\n\n4. MITRE Working note WN110058V1 “Affordability Engineering Capstone (Phase I) Volume\n\n1 - Basic Research,” J. Duquette et al., September 2011.\n\n5. AERiE prototype tool Community Share site.\n\n[6. GAO-11-318SP Opportunities to Reduce Potential Duplication in Government Programs,](http://www.gao.gov/new.items/d11318sp.pdf)\n\n[Save Tax Dollars, and Enhance Revenue, March 2011.](http://www.gao.gov/new.items/d11318sp.pdf)\n\n7. GAO Testimony before the Committee on Homeland Security and Governmental Affairs,\nSubcommittee on Federal Financial Management, Government Information, Federal\n[Services and International Security, United States Senate. DOD COST OVERRUNS Trends](http://www.gao.gov/htext/d11499t.html)\n[in Nunn-McCurdy Breaches and Tools to Manage Weapon Systems Acquisition Costs,](http://www.gao.gov/htext/d11499t.html)\nMarch 29, 2011, pp. 6–8.\n\n\n-----\n\n-----\n\n### Acquisition Systems Engineering\n\n##### MITRE\n\n\n### Acquisition Systems Engineering\n\n##### MITRE\n\n\n-----\n\n###### Introduction\n\nMITRE systems engineers (SEs) perform systems engineering activities in various contexts.\nThis includes support to field users, operational headquarters, acquisition agencies and\nprogram offices, policy and oversight organizations, as well as independent efforts of all\nforms (e.g., red teams, blue teams) across a range of collaborating stakeholders, such as other\nFederally Funded Research and Development Centers (FFRDCs), industry, and academia. SEs\nare expected to adapt systems engineering principles to these different environments.\n\nMITRE SEs are expected to work with the government customer to plan and manage the\noverall FFRDC systems engineering activities to support government acquisition efforts. They\nplan and technically manage MITRE systems engineering efforts, and sometimes those of\nothers, for projects and programs throughout the system life cycle. For systems engineering\nactivities that cut across multiple phases of a government program or activity, MITRE SEs are\nexpected to make connections among them and plan and manage their execution.\n\nThe focus of this section is on applying MITRE systems engineering support to govern­\nment acquisition programs. Most of the topics and articles are directed exclusively to the\nsubject of acquisition. The others apply to systems engineering in support of government\nactivities that include but go beyond acquisition.\n\n###### Background\n\nMany processes and models have emerged to guide the execution of systems engineering\nactivities in government acquisition processes. Over time, important best practice themes have\nemerged. They can be found throughout this section and include:\n\n###### �Planning and management are iterative. Management is sometimes thought of as\n\nexecution, although a more common view is that management is both planning and\nexecution. Although planning is always done early in an activity, many of the best\npractices and lessons learned of this section have a common element of iteration. For\nexample, during execution of a planned acquisition, conditions often change, calling for\na change in the plan as well. One of the topics that addresses this is Continuous Process\nImprovement.\n###### �Risk management is the first job that needs to be done. Some program managers char­\n\nacterize program and project management as risk management because once a program\nor project is started, the work of the SEs and managers often focuses on identifying\npotential problems and their solutions. The Risk Management topic in this section pro­\nvides guidance on this aspect of systems engineering. The risk identification and miti­\ngation theme also appears in other SEG articles, for example, “Competitive Prototyping”\nand “Systems Engineering Strategies for Uncertainty and Complexity.”\n\n\n-----\n\n###### �Think “enterprise.” Most programs being developed or undergoing significant modifi­\n\ncations are already interfacing to a number of other systems, networks, databases, and\ndata sources over the Web, or are part of a family or system of systems. Explore the\nEnterprise Engineering section to gain more knowledge on how to approach systems\nengineering for these cases, and fold that thinking into the acquisition systems engi­\nneering efforts you are undertaking. For example, the Transformation Planning and\nOrganizational Change topic explores the criticality of stakeholders as champions of\nprogram success.\nThe topics addressed in this section are summarized below.\n\n###### Acquisition Program Planning\n\nAcquisition is the conceptualization, initiation, design, development, test, contracting, produc­\ntion, deployment, logistic support, modification, and disposal of systems, supplies, products,\nor services (including construction) to satisfy agency/department needs, intended for use in or\nin support of that organization’s mission.\n\nAcquisition program planning is concerned with the acquisition-related coordination\nand integration efforts undertaken to meet agency or department needs. The scope and\ntype of the systems engineering support MITRE provides is determined by where along the\nspectrum—from purchasing commodity to major development—the supported acquisition\neffort falls. MITRE SEs are expected to understand the central role that systems engineering\nplays in effectively managing acquisition programs (or projects). Because systems engineer­\ning and program management are so inextricably linked, MITRE SEs need to be cognizant\nof program management challenges and issues. MITRE SEs may be required to assist in\nplanning the technical work; create, staff, and direct a team or organization to do the work;\nmonitor progress against the plan; and take corrective action to control and redirect the\nwork when needed. Articles in this topic area include “Performing Analyses of Alternatives,”\n“Acquisition Management Metrics,” “Assessing Technical Maturity,” “Technology Planning,”\n“Life-Cycle Cost Estimation,” “Integrated Master Schedule (IMS)/Integrated Master Plan (IMP)\nApplication,” and “Comparison of Investment Analyses.”\n\n###### Source Selection Preparation and Evaluation\n\nThe purpose of source selection is to prepare for a government solicitation, evaluate responses\nto the solicitation, and select one or more contractors for delivery of a product or service.\nMITRE SEs are expected to create technical and engineering portions of request for proposal\n(RFP) documentation (requirements documents, statement of work, evaluation criteria) and to\nassist in the technical evaluation of bidders. The technical evaluation is an assessment of the\ndegree to which proposed solutions or courses of action will provide the capabilities required\n\n\n-----\n\nto meet the government’s needs. This role includes conducting assessments of risk inherent in\nproposed solutions, including strategies for acquiring (or implementing) them, and identifying\nactionable options for mitigating those risks. Articles in this topic area include “Picking the\nRight Contractor” and “RFP Preparation and Source Selection.”\n\n###### Program Acquisition Strategy Formulation\n\nAn acquisition strategy is a comprehensive, integrated plan developed as part of acquisition\nplanning activities. It describes the business, technical, and support strategies to meet pro­\ngram objectives and manage program risks. The strategy guides acquisition program execu­\ntion across the entire program (or system) life cycle. It defines the relationship among the\nacquisition phases and work efforts as well as key program events such as decision points,\nreviews, contract awards, test activities, production lot/delivery quantities, and operational\ndeployment objectives. The strategy evolves over time and should continuously reflect the\ncurrent status and desired end point of the program. MITRE SEs assist in articulating govern­\nment needs, translating those needs into mission/outcome-oriented procurement/solicitation\nrequirements, and identifying the issues, risks, and opportunities that shape and influence\nthe soundness of the acquisition strategy. MITRE SEs help agencies achieve what the Federal\nAcquisition Regulation (FAR) characterizes as “mission-oriented solicitations” (FAR 34.005-2).\nArticles in this topic area include “Agile Acquisition Strategy,” “Evolutionary Acquisition,” and\n“‘Big-Bang’ Acquisition.”\n\n###### Contractor Evaluation\n\nContractor evaluation assesses the contractor’s technical and programmatic progress,\napproaches, and deliverables. The purpose of contractor evaluation is to provide insight\ninto risks and the likelihood of meeting program and contractual requirements. MITRE SEs\nperform contractor evaluations and milestone reviews, influence sponsor/customer deci­\nsions during those reviews, monitor the contractor’s continued performance, and recommend\nchanges based on their performance. This topic is related to the MITRE FFRDC Independent\nAssessments topic in the Enterprise Engineering section and contributes significantly\nto the process of identifying and managing risks, as discussed in the Risk Management\ntopic. Articles in this topic area include “Data-Driven Contractor Evaluations and Milestone\nReviews,” “Earned Value Management,” and “Competitive Prototyping.”\n\n###### Risk Management\n\nDefining and executing an iterative risk management process is a significant component of\neffective acquisitions and programs. MITRE SEs propose the risk management approach that\nenables risk-informed trade-offs and decisions to be made throughout a system’s evolution,\n\n\n-----\n\nand they are actively involved in all steps of the process. Articles in this topic area include\n“Risk Management Approach and Plan,” “Risk Identification,” “Risk Impact Assessment and\nPrioritization,” “Risk Mitigation Planning, Implementation, and “Progress Monitoring,” and\n“Risk Management Tools.”\n\n###### Configuration Management\n\nConfiguration management (CM) is the application of sound program practices to estab­\nlish and maintain consistency of a product or system’s attributes with its requirements and\nevolving technical baseline over its lifetime. Configuration management is required by the\nDepartment of Defense (DoD), Federal Aviation Administration (FAA), Internal Revenue\nService (IRS), and other programs that MITRE supports. MITRE SEs assist in ensuring that\ngood CM processes are in place and followed by all contractors and program office personnel.\nArticles in this topic area include “How to Control a Moving Baseline” and “Configuration\nManagement Tools.”\n\n###### Integrated Logistics Support\n\nIntegrated logistics support is the management and technical process through which sup­\nportability and logistic support considerations are integrated into the design and taken into\naccount throughout the life cycle of systems and equipment. MITRE SEs need to understand\nthe impact of technical decisions on the usability and life-cycle support of systems and assist\nin ensuring that life-cycle logistics considerations are part of the SE process. Articles in this\ntopic area include “Reliability, Availability, and Maintainability” and “Managing Energy\nEfficiency.”\n\n###### Quality Assurance and Measurement\n\nQuality assurance (QA) and measurement are systematic means for ensuring that defined\nstandards and methods are applied. The rigorous application of quality assurance and mea­\nsurement is one mechanism to mitigate program risk. MITRE SEs recommend and assist in\nexecuting QA and measurement programs. Articles in this topic area include “Establishing\na Quality Assurance Program in the Systems Acquisition or Government Operational\nOrganization” and “How to Conduct Process and Product Reviews Across Boundaries.”\n\n###### Continuous Process Improvement\n\nContinuous process improvement is an aspect of quality assurance. It is the set of ongoing\nsystems engineering and management activities used to select, tailor, implement, and assess\nthe processes used to achieve an organization’s business goals. MITRE SEs influence the gov­\nernment’s approach to implementing and improving systems engineering processes. Articles\n\n\n-----\n\nin this topic area include “Implementing and Improving Systems Engineering Processes for\nthe Acquisition Organization” and “Matching Systems Engineering Process Improvement\nFrameworks/Solutions with Customer Needs.”\n\n###### Other Acquisition Systems Engineering Articles\n\nIn the future, any articles on subjects of relevance to enterprise engineering but that don’t\nneatly fit under one of the section’s existing topics will be added in a separate topic, Other\nAcquisition Systems Engineering Articles. Such articles are likely to arise because the subject\nmatter is at the edge of our understanding of systems engineering, represents some of the\nmost difficult problems MITRE SEs work on, and has not yet formed a sufficient critical mass\nto constitute a separate topic.\n\n###### References and Resources\n\nGeneral Services Administration, Department of Defense, National Aeronautics and Space\nAdministration, March 2005, Federal Acquisition Regulation, Vol. 1, 34.005-2.\n\n[The MITRE Corporation, September 1, 2007, MITRE Systems Engineering (SE) Competency](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n[Model, ver. 1.1E, section 3.0, pp. 34–48.](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n\n\n-----\n\n-----\n\n###### Acquisition Systems Engineering Contents\n\nAcquisition Program Planning 491\nPerforming Analyses of Alternatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .496\nAcquisition Management Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 502\nAssessing Technical Maturity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\nTechnology Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514\nLife-Cycle Cost Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518\nIntegrated Master Schedule (IMS)/Integrated Master Plan (IMP) Application . . . . . . . . . . . . . . . 524\nPerformance Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528\nComparison of Investment Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536\n\nSource Selection Preparation and Evaluation 543\nPicking the Right Contractor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548\nRFP Preparation and Source Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551\n\nProgram Acquisition Strategy Formulation 559\nAgile Acquisition Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563\nEvolutionary Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568\n“Big-Bang” Acquisition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572\n\nContractor Evaluation 576\nData-Driven Contractor Evaluations and Milestone Reviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\nEarned Value Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 585\nCompetitive Prototyping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593\n\nRisk Management 599\nRisk Management Approach and Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604\nRisk Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\nRisk Impact Assessment and Prioritization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 620\nRisk Mitigation Planning, Implementation, and Progress Monitoring . . . . . . . . . . . . . . . . . . . . . . . . . 627\nRisk Management Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634\n\nConfiguration Management 643\nHow to Control a Moving Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649\nConfiguration Management Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653\n\nIntegrated Logistics Support 658\nReliability, Availability, and Maintainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 664\nManaging Energy Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 670\n\nQuality Assurance and Measurement 675\nEstablishing a Quality Assurance Program in the Systems\nAcquisition or Government Operational Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 681\nHow to Conduct Process and Product Reviews Across Boundaries . . . . . . . . . . . . . . . . . . . . . . . . 686\n\nContinuous Process Improvement 690\nImplementing and Improving Systems Engineering Processes for the\nAcquisition Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\nMatching Systems Engineering Process Improvement Frameworks/\nSolutions with Customer Needs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 698\n\n\n-----\n\n##### Acquisition Program Planning\n\nDefinition: Acquisition planning is the process for coordinating and integrating\n\n_acquisition efforts by using a plan to fulfill agency needs in a timely manner and_\n\n_at a reasonable cost. It includes developing the overall strategy for managing the_\n\n_acquisition. Planning enables the coordinated execution of the various efforts that_\n\n_constitute acquisition management [1, part 2]._\n\nKeywords: acquiring capabilities, acquisition, acquisition management, analysis of\n\n_alternatives, contracting, information technology, program management, scope_\n\n_of acquisition, software engineering, systems engineering_\n\n###### Context\n\nMITRE systems engineers (SEs) support a wide spectrum of\n\nfederal acquisition management efforts through its federally funded\n\nresearch and development centers (FFRDCs). Government acquisi­\n\ntion efforts range from purchasing commodities (which MITRE is\n\nnot involved in), such as commercially available goods and services\n\nrequiring little or no modification or system integration, to developing\n\nmajor, unprecedented systems that provide new strategic capabili­\n\nties to resource the various dimensions of U.S. national security.\n\nThese federal agencies/departments each have governance frame­\n\nworks for managing acquisitions through contracts. Though there is a\n\n“family resemblance” across the frameworks, there are differences\n\n\n-----\n\nas well. Some (such as DoDI 5000.02 [2]) have evolved over many years and are fairly mature,\nwhereas other frameworks are in their infancy. Federal Acquisitions Regulations (FAR) pro­\nvides guidance to agencies on policies and procedures for implementing FAR requirements.\nThis includes conducting acquisition planning and execution to ensure that the government\nacquires systems and capabilities in an effective, economical, and timely manner [1, sub­\npart 1.3, part 7]. However, the FAR does not specify a particular acquisition management or\ngovernance framework. Instead, it provides the latitude to put in place an acquisition plan­\nning system appropriate for that agency/department [1, subpart 7.102]. The acquisition systems\nadopted by these departments and agencies are very diverse and require MITRE SEs to adapt\nsystems engineering concepts, principles, and processes to align with the supported agency’s\nacquisition processes and governance/management framework.\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to understand the central role systems engineer­\ning plays in effectively planning and managing government acquisition programs to acquire\nproducts, systems, capabilities, and services to satisfy mission and business needs or modern­\nize enterprises. MITRE SEs are expected to tailor and adapt systems engineering principles,\nprocesses, and concepts to match the scope and complexity of the acquisition effort as well\nas the agency or department acquisition regulations, policies, and governance approaches.\nThey need to be cognizant of program management challenges and issues so they can assume\nappropriate accountability for the success of the programs they support. SEs may be required\nto plan the technical work; create, staff, and guide a team or organization to do the work;\nmonitor progress against the plan; and advise the government regarding corrective action to\ncontrol and redirect the work when needed [3].\n\n###### Best Practices and Lessons Learned\n\n\nHelp your customer take a holistic perspective.\n\nMITRE experience addressing the challenges of\n\ndeveloping, operating, and sustaining complex\n\nsystems highlights the need to take a broad view\n\nof acquisition, with systems engineering as an\n\nintegral part of it to be applied across the acquisi­\n\ntion spectrum or life cycle.\n\nThis holistic perspective of acquisition has taken\n\nhold in information technology (IT)-intensive\n\nacquisitions (e.g., capital investments) that are\n\n\npart of enterprise modernization initiatives man­\n\naged under the Capital Planning and Investment\n\nControl framework, a component of the Clinger\nCohen Act [4] that contains several provisions\n\nfor improving the way agencies acquire IT. OMB\n\nCircular A-11 [5] provides additional guidance and\n\ninstructions for the planning, budgeting, acquisi­\n\ntion, and management of IT assets.\n\nMITRE SEs should articulate the role systems\n\nengineering can take in improving the outcomes\n\n\n-----\n\nof IT acquisition programs. Several key areas\n\nwhere systems engineering plays a major role\n\nin achieving desired outcomes are discussed in\n\nthe articles under this topic, along with sugges­\n\ntions on how to address the challenges typically\n\nencountered.\n\nKnow the customer’s acquisition governance\n\nframework. Because the maturity of agencies’\n\nacquisition governance frameworks differs, MITRE\n\nSEs should be aware of their agency’s level of\n\nmaturity and tailor systems engineering practices\n\nto the acquisition environment of the supported\n\nagency.\n\nMake use of MITRE’s collected experience.\n\nProvide value by drawing on lessons learned from\n\nsimilar MITRE efforts. Look for common prob­\n\nlems and potentially common solutions that can\n\nbe applied across multiple programs and agen­\n\ncies. This involves conversing in various termi­\n\nnologies to express similar concepts or ideas. In\n\nother cases, the same terminology may take on\n\na different meaning. MITRE SEs should articulate\n\nrecommendations, such as courses of actions to\n\naddress key technical risks, in the language of the\n\nsupported agency.\n\nAcquisition management metrics: Outcome.\n\nMITRE SEs should ensure that acquisition plan­\n\nning addresses needs using outcome-focused\n\nattributes and associated metrics. Questions\n\nsuch as these should be asked and answered:\n\n###### � [What are the success criteria for the ]\n\nacquisition effort? How will we know when\n\nwe are done?\n###### � [What mission or business shortfall (or gap) ]\n\nis the acquisition effort trying to address?\n\n\nCan the need (shortfall/gap) be charac­\n\nterized as attributes that broadly define\n\nthe solution space (i.e., range of potential\n\nalternatives for satisfying the need)?\n###### � [Are the attributes/metrics used to articu­]\n\nlate the need traceable to criteria estab­\n\nlished during analysis of alternative solu­\n\ntion concepts (e.g., types of investments)\n\nfor addressing the need?\n\nOutcome-focused metrics provide the infor­\n\nmation needed for program management and\n\nsystems engineering activities such as risk and\n\ntrade space management, test and verifica­\n\ntion/evaluation, modeling and simulation, design\n\nreviews, fielding/implementation decisions, and\n\nother acquisition-related milestone decision or\n\nknowledge points. These metrics also aid in devel­\n\noping criteria for contract incentive structures\n\nto motivate achievement of mission or business\n\noutcomes. For further details, see the article\n\n“Acquisition Management Metrics.”\n\nAnalyses of alternatives (AoAs) help justify\n\nthe need for starting, stopping, or continuing\n\nan acquisition program [6]. As such, they may\n\noccur at any point in the system (or acquisi­\n\ntion management) life cycle. Typically they occur\n\nbefore initiating an acquisition program, or in the\n\ncase of IT programs subject to the requirements\n\nof the Clinger-Cohen Act, they may be used to\n\nmake capital planning and investment decisions\n\nbased on a business case analysis. The decision\n\ncriteria and associated metrics used to select\n\nthe materiel (or capital investment) alternative\n\nshould serve as the basis for the metrics used to\n\nplan and manage the acquisition effort. For more\n\n\n-----\n\non AoAs, see the article “Performing Analyses of\n\nAlternatives.”\n\nPerformance-based acquisition. Identifying\n\noutcome-focused metrics to manage the\n\nacquisition effort is consistent with the concept\n\nof performance-based acquisition (PBA). PBA\n\n(formerly performance-based contracting) is a\n\ntechnique for structuring an acquisition based on\n\nthe purpose and outcome desired, instead of the\n\nprocess by which the work is to be performed [7].\n\nPBA provides insight into key technical enablers\n\nfor achieving the required level of performance for\n\na product or the level of service when acquiring\n\nservices. These factors and associated metrics\n\n(preferably quantitative) should serve as a subset\n\nof the key performance parameters that define\n\nsuccess of the acquisition effort. They should be\n\nfolded into the measurable criteria for an inte­\n\ngrated master plan and integrated master sched­\n\nule. Cost and schedule must also be considered\n\nsince achievable technical performance or quality\n\nof service is often related. For more informa­\n\ntion, see the “Integrated Master Schedule (IMS)/\n\nIntegrated Master Plan (IMP) Application” article\n\nin this topic. For more information on costing, see\n\nthe article “Life-Cycle Cost Estimation.”\n\nUsing technology as an advantage. Technology\n\ncan be both an enabler and an inhibitor. Acquiring\n\nnew technology for a system that largely depends\n\non IT or on emerging technology means planning,\n\nmanaging, and executing a program plan centered\n\non reducing the risks associated with IT or the\n\nnew technology. Use of research and develop­\n\nment activities, prototyping, continuous technical\n\nassessments, and planned incremental deliveries\n\nare ways to mitigate the risks. The program plan\n\n\nshould be designed to adapt to either technology\n\nevolution or user needs during the program life\n\ncycle. Transitioning new technology into the users’\n\nhands is also a risk to be considered and man­\n\naged. For more on technology and transition, see\n\nthe articles “Assessing Technical Maturity” and\n\n“Technology Planning.”\n\nLeveraging increased specialization. As the\n\ndiscipline of systems engineering evolves, it\n\nincreases in span or scope into areas like system\nof-systems engineering and enterprise systems\n\nengineering (see the SEG’s introductory article\n\n“The Evolution of Systems Engineering”). It also\n\nleads to finer division and differentiation of sys­\n\ntems engineering skills. As a result, more than ever,\n\nsystems engineering is a team sport in which SEs\n\nmust collaborate with and orchestrate the efforts\n\nof specialty SEs to achieve program success.\n\nAlthough performance engineering is recognized\n\nas fundamental in manufacturing and produc­\n\ntion, its importance earlier in the life cycle is often\n\noverlooked. Performance engineering activities\n\nare most often associated with hardware and\n\nsoftware elements of a system. But, its principles\n\nand techniques can be applied to other aspects\n\nof systems that can be measured in some\n\nmeaningful way, including, for example, business\n\nprocesses. The article “Performance Engineering”\n\ndiscusses these two issues and describes where\n\nand how to employ the discipline across the sys­\n\ntems engineering life cycle.\n\nSEs will frequently team with economic/cost\n\nanalysts at various stages of the acquisition\n\nlife cycle to perform investment analyses, and\n\ntheir familiarity with key aspects of prevalent\n\nanalytic approaches can improve the overall\n\n\n-----\n\nquality of completed analyses and the efficiency\n\nof supporting activities performed. The article\n\n“Comparison of Investment Analyses” describes\n\n###### References and Resources\n\n1. Federal Acquisition Regulation (FAR).\n\n\nthe key aspects of investment analyses, includ­\n\ning decisions supported; primary objectives; and\n\ngeneral approaches that can be applied.\n\n\n2. Department of Defense, December 8, 2008, Department of Defense Instruction No. 5000.2.\n\n3. Space and Missile Systems Center U.S. Air Force, April 29, 2005, SMC Systems Engineering\n\n_Primer and Handbook._\n\n4. 104th U.S. Congress, “Capital Planning and Investment Control,” Information Technology\n\nManagement Reform Act of 1996 (now the Clinger/Cohen Act), Section 5112.\n\n5. Office of Management and Budget (OMB), August 2009, OMB Circular A-11.\n\n6. Office of Aerospace Studies, July 2008, Analysis of Alternatives Handbook.\n\n7. U.S. General Services Administration (GSA), Performance-Based Acquisition.\n\n###### Additional References and Resources\n\n“Program Formulation and Project Planning,” MITRE Project Leadership Handbook.\n\n[“3.2 Government Acquisition Support,” MITRE Systems Engineering Competency Model.](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n\n\n-----\n\nDefinition: An analysis of alter­\n\n_natives (AoA) is an analytical_\n\n_comparison of the operational_\n\n_effectiveness, cost, and risks_\n\n_of proposed materiel solu­_\n\n_tions to gaps and shortfalls in_\n\n_operational capability. AoAs_\n\n_document the rationale for_\n\n_identifying and recommending_\n\n_a preferred solution or solutions_\n\n_to the identified shortfall(s) [1]._\n\nKeywords: analysis of alterna­\n\n_tives, AoA, baseline alternative,_\n\n_cost analysis, criteria, evalu­_\n\n_ation, materiel solutions, risk_\n\n_analysis_\n\n\nACQUISITION PROGRAM PLANNING\n###### Performing Analyses of Alternatives\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the purpose and role of AoA and\n\nwhere it occurs in the acquisition process.\n\nMITRE SEs are also expected to understand\n\nand recommend when an AoA is appropriate\n\nto a situation. They are expected to develop,\n\nrecommend, lead, and conduct technical por­\n\ntions of an AoA, including strategies and best\n\npractices for execution. SEs are expected to\n\nmonitor and evaluate AoA technical progress\n\nand recommend changes when warranted.\n\n\n-----\n\n###### Background\n\nAn AoA is one of the key documents produced in preparation for major milestones or pro­\ngrams reviews. It is especially key at the start of a new program. Very often, MITRE’s systems\nengineering support to programs that are in pre-Milestone A or B phases involves support to\nAoA efforts. Recommendations from the AoA determine the procurement approach for either\na new program or the continuance of an existing program. MITRE SEs involved in program\nplanning are frequently called on to participate in these analyses and lead the technical\nefforts associated with assessing other existing programs for applicability to the mission,\ncost-effectiveness, and risk. MITRE SEs provide necessary systems engineering skills (require­\nments analysis, technical evaluation, architecture, etc.), freedom from bias, and access to\nsubject matter expertise required for AoAs.\n\n###### Why Do We Do AoAs?\n\nAoAs are performed to allow decision makers to understand choices and options for starting a\nnew program or continuing an existing program. The bottom line is cost effectiveness through\nnon-duplication of effort and lowest risk to successful program delivery. An example is a\nMITRE customer who determined there were gaps in their current suite of systems to meet\nanticipated needs. One of the systems was limited in capability and nearing its end of life with\nno follow-on program of record to continue or improve it. The customer needed an analysis\nof existing systems and technologies to make informed decisions on the best path forward to\nprovide an integrated solution of systems to meet their emerging needs. The Departments of\nDefense (DoD), Homeland Security, and Treasury require AoAs for new procurements. The\nUSAF (Air Force Materiel Command) has one of the more robust documented processes for\nperforming an AoA [1]. Per DoDI 5000.02, the purpose of the AoA is to assess the potential\nmateriel solutions to satisfy the capability need documented in an initial capabilities docu­\nment [2]. Approval to enter into the development phase of a program is contingent on comple­\ntion of an AoA, identification of materiel solution options by the lead DoD component that\nsatisfy the capability need, and satisfaction of phase-specific entrance criteria for the develop­\nment milestone.\n\nCommercial industry also uses “alternative analyses,” but they are usually more focused\non life-cycle cost. An equally interesting and valuable feature of an AoA is an assessment of\nrisk—including operational, technical, and programmatic risks. This kind of assessment is\nnot widely seen in AoA processes, and is not always employed [3]. As has been seen repeat­\nedly with other systems engineering disciplines, a good process is a good start, but it does not\nguarantee success unless it is actually followed.\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nThe plan is important. A major step leading to\n\na successful AoA is the creation of a well-con­\n\nsidered study plan. The study plan establishes a\n\nroadmap for how the analysis should proceed,\n\nwho is responsible for doing what, and why it\n\nis being done [1]. It should include the following\n\ninformation:\n\n###### � [Understand the technology gaps and ]\n\ncapability gaps—what needs is the\n\nintended system supposed to meet?\n###### � [Develop viable alternatives: ]\n\nyy Define the critical questions.\n\nyy List assumptions and constraints.\n\nyy Define criteria for viable/nonviable.\n\nyy Identify representative solutions\n\n(systems/programs).\n\nyy Develop operational scenarios to use\n\nfor comparisons/evaluation.\n###### � [Identify, request, and evaluate data from ]\n\nthe representative systems/programs\n\n(determined to be viable).\n###### � [Develop models—work through scenarios. ]\n\nThe AoA should “assess the critical technology\n\nelements associated with each proposed materiel\n\nsolution, including technology maturity, integration\n\nrisk, manufacturing feasibility, and, where neces­\n\nsary, technology maturation and demonstration\n\nneeds [3].”\n\nSufficient resourcing is key. Allocate sufficient\n\nresources and time for completing each of the\n\nactions and assessments and for preparing the\n\nfinal analysis product. The biggest risk to success\n\nis the lack of time to adequately perform the AoA.\n\nCompressed schedules associated with preparing\n\n\nfor a new procurement, and the associated exe­\n\ncution of funding, can present major problems for\n\nthe AoA team. If faced with this issue, resources\n\nmay need to be increased and allocated full time\n\nto the effort.\n\nKnow the baseline before starting the AoA.\n\nFor many AoAs, a capability exists but it is either\n\nnearing its end of life or no longer satisfies current\n\nneeds. In these cases, it is critical to first under­\n\nstand the existing capability baseline. The set of\n\nalternatives considered in the AoA must include\n\nan upgrade path from the status quo. Unless\n\ninformation about the existing capability (referred\n\nto as the “baseline alternative”) is already in hand,\n\nit is necessary to ensure that sufficient effort is\n\nplanned during the AoA to capture that baseline\n\nfully enough for the comparison analysis to be\n\nperformed. The point of comparison is likely to\n\nbe in the future (at which time it is projected that\n\nthe new capability would field), so there may be a\n\nneed to project an upgrade path for the exist­\n\ning capability for fair comparison. SEs, design\n\nengineers, and software engineers should get\n\ninvolved early to review the baseline, understand\n\nit, and project an upgrade path for the “baseline\n\nalternative.”\n\nKnow your stakeholders. Understand the\n\nstakeholders and decision makers involved with\n\nthe AoA and how they will use the results. Assess\n\nthe political, operational, economic, and techni­\n\ncal motivations of the stakeholders to inform the\n\nscope of the analysis and its focus. Use commu­\n\nnity and user stakeholders to assist in determin­\n\ning the objectives, and then the measures and\n\n\n-----\n\nmetrics that will be used to identify the “best”\n\nsolution. Not only does this leverage their knowl­\n\nedge, it provides a means to obtaining their buy-in\n\non the result.\n\nBeware premature convergence. A recent GAO\n\n(Government Accountability Office) report on\n\ndefense acquisitions attributes premature focus\n\non a particular solution or range of solutions as\n\na failing of AoAs [3]. If stakeholders are already\n\nenamored of a particular solution, completing a\n\nfull AoA may be difficult. The intention is to survey\n\na broad range of alternatives to ensure the best\n\nvalue and technical match to the need. A narrow\n\nscope or attention paid to a particular solution\n\nrenders the AoA ineffective for decision mak­\n\ning and leads to increased risk in the resulting\n\nprogram.\n\nKnow your AoA team. Establish standards for the\n\nminimum level of expertise and experience that\n\nAoA study participants in each integrated project\n\nteam/working group must meet. Subject matter\n\nexperts (SMEs) should truly be recognized experts\n\nin their area, rather than just adding a particular\n\norganizational affiliation.\n\nUnderstand the mission. It takes focusing on\n\nboth the mission and the technical capabilities\n\nto be able to perform an adequate assessment.\n\nAoAs should make use of simulation and modeling\n\nto help determine the best solution; but a simula­\n\ntion is only as good as its fidelity and the input\n\nparameters (i.e., subject to “garbage in, garbage\n\nout”). Make use of community SMEs and users\n\nto ensure a good understanding of the objective\n\noperations needed to meet the gaps. MITRE SEs\n\nneed to possess both the operational knowl­\n\nedge and technical skills to adequately analyze\n\n\ntechnical solutions. The objective is to be credible,\n\nthorough, and comprehensive. A good AoA needs\n\nto address the full spectrum of mission, operat­\n\ning environment, technologies, and any other\n\nunique aspects of the program. It also needs to be\n\narticulated well enough to enable decision makers\n\nto make an informed decision.\n\nObtain technical descriptions of the materiel\n\nsolutions. Frequently the selection of alternatives\n\nfor the study is made by team members who are\n\nfocused on the operational mission and capabil­\n\nity gaps. Yet technical knowledge must also be\n\napplied by the AoA team or time and resources\n\nmay be wasted investigating alternatives that\n\nare not technically feasible. Early application of\n\ntechnical criteria will avoid this. Ask for technical\n\ndescriptions (e.g., technical description docu­\n\nments) as well as operational descriptions of the\n\nalternatives before starting the AoA. Doing an\n\nearly DOTMLPF (Doctrine, Organization, Training,\n\nMateriel, Leadership and Education, Personnel,\n\nand Facilities) analysis makes it possible for the\n\nAoA to focus its efforts primarily on dealing with\n\nfeasible material solutions.\n\nAnticipate problems. Analyzing a broad range\n\nof solutions involves collecting a considerable\n\namount of information on the representative\n\nsystems/programs as well as on other details like\n\noperating environment or communications infra­\n\nstructure. Industry requests for information can\n\nbe useful, but do not always produce the detail\n\nneeded. Look for other data sources through\n\ngovernment, contractor, or MITRE contacts. This\n\nissue can also be exacerbated by a compressed\n\nschedule, leading to inaccurate or incomplete\n\nanalyses due to lacking detailed information. In any\n\n\n-----\n\ncase, a good assumption going into an AoA is that\n\nall necessary information will not be forthcoming\n\nand will necessitate creating some workarounds\n\n(e.g., facsimile, assumptions). Be persistent!\n\nLeverage MITRE. A wide range of techni­\n\ncal expertise and system/program expertise is\n\nrequired for an AoA. Determine what skills are\n\nrequired for the AoA plan and leverage what you\n\ncan from the broader MITRE team. Not only\n\nshould MITRE expertise be used for the technical\n\nexpertise in the areas of technology applicable to\n\nthe solution set, but also for analysis techniques,\n\nand modeling and simulation to aid in the evalu­\n\nation process. MITRE has in-house expertise in\n\nengineering cost modeling and life-cycle cost\n\ntrade-off analysis that can be leveraged as well.\n\nAs an example, MITRE’s participation in a par­\n\nticular AoA involved creating and using models\n\nto assist in the evaluation as well as providing\n\nsubject matter expertise in the technical areas\n\nof radio frequency, electro-optical, infrared,\n\nhigh-power microwave, and electronic warfare\n\ntechnologies. This led to a very successful AoA.\n\nLast, nothing can replace a good face-to-face\n\ndiscussion among novice and experienced engi­\n\nneers on an AoA.\n\nBeware the compressed schedule. As men­\n\ntioned above, an inadequate timeframe to con­\n\nduct an AoA can render its conclusions ineffec­\n\ntive. The GAO found that many AoAs have been\n\nconducted under compressed timeframes—six\n\nmonths or less—or concurrently with other key\n\nactivities that are required for program initiation\n\nin order to meet a planned milestone decision\n\nor system fielding date. Consequently AoAs may\n\nnot have enough time to assess a broad range of\n\n\nalternatives and their risks, or may be completed\n\ntoo late in the process to inform effective trade\n\ndiscussions prior to beginning development [1].\n\nIncorporate the risk analysis. Risks are impor­\n\ntant to assess because technical, programmatic,\n\nor operational uncertainties may be associated\n\nwith different alternatives that should be consid­\n\nered in determining the best approach [1].\n\nThe GAO reported that some of the AoAs they\n\nreviewed did not examine risks at all; they\n\nfocused only on the operational effective­\n\nness and costs of alternatives. Other AoAs had\n\nrelatively limited risk assessments. For example,\n\nseveral AoAs did not include integration risks\n\neven though the potential solution set involved\n\nmodified commercial systems that would require\n\nintegration of subsystems or equipment. Based\n\non a recent Defense Science Board report on\n\nbuying commercially based defense systems,\n\nprograms that do not assess the systems\n\nengineering and programmatic risks of alterna­\n\ntives do not understand the true costs associ­\n\nated with militarizing commercial platforms or\n\nintegrating various commercial components [3].\n\nOther AoAs did not examine the schedule risks\n\nof the various alternatives, despite accelerated\n\nschedules and fielding dates for the programs.\n\nThey also found that programs with AoAs that\n\nconducted a more comprehensive assessment\n\nof risks tended to have better cost and schedule\n\noutcomes than those that did not [1]. For more\n\ninformation on risk identification and manage­\n\nment, see the Risk Management topic and articles\n\nwithin this section of the SEG.\n\n\n-----\n\n###### References and Resources\n\n1. Air Force Materiel Command Office of Aerospace Studies, July 2008, Analysis of\n\n_Alternatives Handbook: A Practical Guide to Analyses of Alternatives._\n\n2. Department of Defense Instruction 5000.02, December 8, 2008 (revised), Operation of the\n\nDefense Acquisition System.\n\n3. _Many Analyses of Alternatives Have Not Provided a Robust Assessment of Weapon System_\n\n_Options, September 2009, GAO Report._\n\n\n-----\n\nDefinition: According to\n\n_BusinessDictionary.com, met­_\n\n_rics are defined as standards_\n\n_of measurement by which effi­_\n\n_ciency, performance, progress,_\n\n_or quality of a plan, process,_\n\n_or product can be assessed._\n\n_Acquisition management met­_\n\n_rics are specifically tailored to_\n\n_monitor the success of govern­_\n\n_ment acquisition programs._\n\nKeywords: acquisition metrics,\n\n_leading indicators, program_\n\n_success_\n\n\nACQUISITION PROGRAM PLANNING\n###### Acquisition Management Metrics\n\n**MITRE SE Roles and Expectations: Within**\n\nthe role of providing acquisition support,\n\nMITRE systems engineers (SEs) are tasked\n\nwith understanding technical risk and assess­\n\ning success. Management metrics are used\n\nas a mechanism to report progress and\n\nrisks to management. MITRE staff should\n\nunderstand how these metrics influence and\n\nrelate to acquisition systems engineering.\n\nThe use of metrics to summarize a pro­\n\ngram’s current health, identify potential areas of\n\nconcern, and ability to be successful are com­\n\nmon practice among government departments,\n\nagencies, and industry. Metrics range from\n\ndetailed software metrics to more overarching\n\nprogram-level metrics. Some of the follow­\n\ning examples are derived primarily from the\n\n\n-----\n\nDepartment of Defense (DoD) program practice, but the principles are applicable to any\nprogram.\n\n###### Probability of Program Success Metrics\n\nAs an aid in determining the ability of a program to succeed in delivering systems or capabili­\nties, the military services developed the Probability of Program Success (PoPS) approach. PoPS\nstandardizes the reporting of certain program factors and areas of risk. Each service measures\na slightly different set of factors, but all the tools use a similar hierarchy of five factors at the\ntop level. These factors are Requirements, Resources, Execution, Fit in Vision, and Advocacy.\nAssociated with each factor are metrics, as indicated in Figure 1. These tools are scoring\nmethodologies where metric criteria are assessed by the program office, a metric score/point\nis determined, and metric scores are weighted and then summed for an overall program score.\nThe summary score is associated with a color (green, yellow, or red), which is the primary\nway of communicating the result. It is at the metric level and the criteria used to assess the\nmetric where the biggest differences between the tools exist. Each tool weighs metrics differ­\nently, with Air Force and Navy varying these weights by acquisition phase. Furthermore, each\nservice uses different criteria to assess the same or similar metric. See the references [1, 2, 3,\n4] for each service tool to better understand how metrics are scored.\n\n**Best Practices and Lessons Learned: Determining the value of each metric is the**\n**responsibility of the acquisition program team. Systems engineering inputs are relevant to**\nmost of the reporting items; some are more obvious than others. With respect to staffing/\nresources, it is important to understand the right levels of engineering staffing for the pro­\ngram management office and the prospective development contractor to ensure success. At\nthe outset of an acquisition, a risk management process should be in place (see the SEG’s Risk\nManagement section); the ability of this process to adequately identify and track risks is a\nmajor component of the PoPS tool. All technical risks should be incorporated in this assess­\nment, including those that may be included in the technical maturity assessment. Immature\ntechnology can be a considerable risk to program success if not managed appropriately; it also\ncan be scheduled for insertion into the program delivery schedule on maturation. For more\ndetail on technology maturity, see the article Assessing Technical Maturity.\n\n_Note: This structure is generic and meant to closely represent what the services capture in_\n_their respective PoPS tools and where._\n\nAlthough a metric name may be different or absent when comparing one tool to another,\nsame or similar qualities may be captured in a different metric. Conversely, metrics may have\nthe same or similar name but capture different qualities of the program. See the individual\nservice’s PoPS operations guide for details [1, 2, 3, 4].\n\n\n-----\n\nFigure 1. Generic Representation of Metrics Considered in PoPS Tools\n\n###### Earned Value Management (EVM) Metrics\n\nA subset of program management metrics is specific to contractor earned value. Typical\nEVM metrics are the Cost Performance Index and the Schedule Performance Index; both are\n\n\n-----\n\nincluded in each of the service’s PoPS tool [5, 6]. Although EVM is mostly considered a moni­\ntoring tool for measuring project performance and progress, it is also a planning tool. Using\nEVM effectively requires the ability to define, schedule, and budget the entire body of work\nfrom the ground up. This is something to be considered in the planning phase of an acquisi­\ntion program (see the article “Integrated Master Schedule (IMS)/Integrated Master Plan (IMP)\nApplication”) because it is closely linked to the Work Breakdown Structure (WBS).\n\n**Best Practices and Lessons Learned: Fundamental to earned value is linking cost and**\n**schedule to work performed. However, work performed is often specified at too high a level**\nto identify problems early. This is linked back to the generation of the WBS during the initial\nprogram planning and whether it was created at a detailed enough level (i.e., measureable\n60-day efforts) to clearly define work performed or product developed. In cases where the\ndetail is insufficient, EVM is unlikely to report real problems for several months. It is usually\nprogram engineers and acquisition analysts who are able to identify and report technical and\nschedule problems before the EVM can report them. Another method is the use of Technical\nPerformance Measures (TPMs). TPMs are metrics that track key attributes of the design to\nmonitor progress toward meeting requirements [7, 8]. More detailed tracking of technical\nperformance by contractors is becoming popular as a way to measure progress and surface\nproblems early using Technical Performance Indices at the lowest product configuration item\n(i.e., Configuration Item, Computer Software Configuration Item) [9].\n\nAppropriate insight into evaluating the work performed for EVM can be challenging. It\noften requires close engineering team participation to judge whether the EVM is accurately\nreporting progress. A case where this is particularly challenging is in large programs requir­\ning cross-functional teams and subcontracts or associate contracts. Keeping the EVM report­\ning accurate and timely is the issue. To do this, the contractor’s team must have close coordi­\nnation and communication. Check that forums and methods are in place to accurately report\nEVM data for the entire program.\n\n###### Systems Engineering Specific Metrics—Leading Indicators\n\nSeveral years ago, MITRE engineers assisted in the development of a suite of “leading indi­\ncators” to track more detailed systems engineering activities for a DoD customer. Table 1\nsummarizes some of these metrics (expanded to generically apply here), which were to be\nassessed as green, yellow, or red, according to specified definitions. An analysis of the overlap\nof the leading indicators with PoPS metrics was conducted. Although several metrics have\nsimilar names, the essence of what is captured is different, and there is very little overlap.\n\n\n-----\n\nTable 1. Leading Indicators\n\n|Leading Indica­ tor Area|Detailed Metrics|Measurement Comments|\n|---|---|---|\n|Program Resources|Required Staffing Applied Appropriate Skills Applied Churn Rate Training Available|Staffed to at least 80% according to plan Ideally churn is less than 5% per quarter|\n|Requirements|Volatility/Impact|Low volatility (< 5%) can still be a problem if the changing requirements have a large impact to the cost and schedule (like 10–15% cost growth)|\n|Risk Handling|Trend Appropriate Priority Applied|Are the risks coming to closure without signif­i cant impact? Alternatively, are there more risks being created over time (+ or – slope)? Are resources being applied to the risk? Is there appropriate engineering and PM oversight?|\n|Interoperability|Community of Interest Established and Active Data Sharing Addressed Network Accessibility|Ideally, a data sharing plan exists for members of the COI and addresses the data formats, visibility / discovery (to include metadata), and plan for exposure.|\n|Software Development|Sizing Quality Productivity Defects|(These are fairly standard metrics—for more information on software metrics, see NIST Special Publication 500-234.)|\n|Verification and Validation|Complete Requirements Documentation Requirements Test Verifica­ tion Matrix (RTVM) Verification and Validation (V&V) Plan|The degree of completeness and volatility of these items is the measurement.|\n|Technology Readiness|Technology Readiness|TRL of at least 6|\n|Risk Exposure|Cost Exposure|15% deviation indicates high risk.|\n||Schedule Exposure||\n|Watchlist Items (Risks)|Severity Closure Rate|As opposed to the risk handling metric above, this one looks at the severity of the risks— which ones are likely to occur with a med-high impact to the program.|\n\n\n-----\n\nAlthough this suite of “leading indicators” was not officially implemented by the cus­\ntomer, they make a good set of items to consider for metrics; they also capture areas not\ncovered in other models.\n\n###### Best Practices and Lessons Learned\n\n\nBe careful comparing metrics across different\n\ntools. Although the metric names in the three\n\ncited tools (and others) may be similar, they may\n\nbe assessed differently. When comparing metrics\n\nacross different tools, you need to understand\n\nthe details of metric definitions and assessment\n\nscales. Make sure this is conveyed when reporting\n\nso that the intended audience gets the right mes­\n\nsage; the assessment needs to stand on its own\n\nand not be misinterpreted.\n\nUnderstand what a metric is supposed to be\n\nmeasuring. For example, trends, current status,\n\nand the ability to be successful when resolution\n\nplans are in place. This will ensure that results are\n\ninterpreted and used properly.\n\n###### References and Resources\n\n\nUse the metrics that are most appropriate\n\nfor the phase of the program you are in. If the\n\nprogram has overlapping phases, use multiple\n\nmetrics. When a program in overlapping phases\n\nis assessed as if it were in a single program phase\n\n(as in PoPS), the resulting report is usually not an\n\naccurate representation of the program status.\n\nBe cautious of methodologies where subjec­\n\ntive assessments are converted to scores.\n\nDeveloping scoring methodologies can appear to\n\nbe simple, yet mathematical fundamentals must\n\nstill be followed for results to be meaningful [10].\n\nThis is particularly a concern when the resulting\n\nsingle score is taken out of context of the analysis\n\nand used as a metric in decision making.\n\n\n1. U.S. Army, May 2004, Probability of Program Success Operations Guide.\n\n2. U.S. Air Force, July 2008, Probability of Program Success (PoPS) Model, SMART\n\n_Integration, Operations Guide, Ver. 1.0._\n\n3. Department of the Navy, September 2008, Naval PoPS Guidebook, Guidance for the\n\n_Implementation of Naval PoPS, A Program Health Assessment Methodology for Navy and_\n_Marine Corps Acquisition Programs, Ver. 1.0._\n\n4. Department of the Navy, September 2008, Naval PoPS Criteria Handbook, Supplement for\n\n_the Implementation of Naval PoPS, Ver. 1.0._\n\n5. Meyers, B., Introduction to Earned Value Management, EVM 101, ESC/AE (Acquisition\n\nCenter of Excellence).\n\n6. Meyers, B., Analysis of Earned Value Data, EVM 401, ESC/AE (Acquisition Center of\n\nExcellence).\n\n\n-----\n\n7. Ferraro, M., “Technical Performance Measurement,” Defense Contract Management\nAgency, Integrated Program Management Conference, November 14–17, 2001.\n\n8. Pisano, Commander N. D., Technical Performance Measurement, Earned Value, and\n\n_Risk Management: An Integrated Diagnostic Tool for Program Management, Program_\nExecutive Office for Air ASW, Assault, and Special Mission Programs (PEO [A]).\n\n9. “Statement of Work (SOW) for Development, Production, Deployment, and Interim\n\nContractor Support of the Minuteman Minimum Essential Emergency Communications\nNetwork (MEECN) Program,” June 28, 2007, MMP Upgrade.\n\n10. Pariseau, R., and I. Oswalt, Spring 1994, “Using Data Types and Scales for Analysis and\n\nDecision Making,” Acquisition Review Quarterly, p. 145.\n\n###### Additional References and Resources\n\nMassachusetts Institute of Technology, International Council on Systems Engineering, Practice\n[Software and Systems Measurement, June 15, 2007, Systems Engineering Leading Indicators](http://lean.mit.edu/downloads/cat_view/94-products/584-systems-engineering-leading-indicators-guide)\n_[Guide, Ver. 1.0.](http://lean.mit.edu/downloads/cat_view/94-products/584-systems-engineering-leading-indicators-guide)_\n\n[NIST Special Publication 500-234, March 29, 1996, Reference Information for the Software](http://hissa.nist.gov/HHRFdata/Artifacts/ITLdoc/234/val-proc.html)\n[Verification and Validation Process, (Appendix A.1, Metrics).](http://hissa.nist.gov/HHRFdata/Artifacts/ITLdoc/234/val-proc.html)\n\n\n-----\n\nDefinition: Assessing the matu­\n\n_rity of a particular technology_\n\n_involves determining its readi­_\n\n_ness for operations across a_\n\n_spectrum of environments with_\n\n_a final objective of transition­_\n\n_ing it to the user. Application_\n\n_to an acquisition program also_\n\n_includes determining the fitness_\n\n_of a particular technology to_\n\n_meet the customer’s require­_\n\n_ments and desired outcome for_\n\n_operations._\n\nKeywords: disruptive technol­\n\n_ogy, emerging technology,_\n\n_mature technology, revolution­_\n\n_ary technology, sustaining_\n\n_technologies, technological_\n\n_innovation, technology assess­_\n\n_ment, technology insertion,_\n\n_technology readiness, TRL_\n\n\nACQUISITION PROGRAM PLANNING\n###### Assessing Technical Maturity\n\n**MITRE SE Roles and Expectations: Systems**\n\nengineers (SEs) are expected to anticipate future\n\ntechnology needs and changes based on a\n\nbroad understanding of the systems context and\n\nenvironment, recommend long-term technology\n\nstrategies that achieve business/mission objec­\n\ntives, and exploit innovation. As part of acquisition\n\nplanning, the ability to successfully procure new\n\ntechnology and systems involves assessing cur­\n\nrent technology to support the program require­\n\nments. Understanding how to assess technology\n\nreadiness, apply technologies to a program, and\n\nmature technologies for insertion is an impor­\n\ntant part of what MITRE SEs are expected to\n\nprovide our customers. And because MITRE\n\nserves a role independent from commercial\n\nindustry, MITRE is often asked to independently\n\nassess a particular technology for “readiness.”\n\n\n-----\n\nWhether assessing the usefulness of a particular technology or research program, or\nassessing the ability to meet a set of new requirements with mature technology, it is best to\nfirst understand the typical cycle technology developments follow and the methodologies to\nconsider for selecting the appropriate path for your program.\n\n###### Best Practices and Lessons Learned\n\n\nTechnology hype cycle. One way to look at\n\ntechnology maturity is through a Gartner hype\n\n\ncycle [1]: a graphic representation of the maturity,\n\nadoption, and business application of spe­\n\n\ncific technologies. Gartner uses hype cycles to\n\ncharacterize the over-enthusiasm or “hype” and\n\nsubsequent disappointment that typically follow\n\nthe introduction of new technologies. A generic\n\nexample of Gartner hype cycles is shown in Hype\n\n\nMaturity\n\n\nCycles (Figure 1).\n\nA hype cycle in Gartner’s interpretation has five\n\n\nsteps:\n\n1. Technology Trigger: The first phase of a\n\n\nFigure 1. Hype Cycles\n\n4. Slope of Enlightenment: Although the\n\n\nhype cycle is the “technology trigger” or\n\nbreakthrough, product launch, or other\n\nevent that generates significant press and\n\n\nthrough the “slope of enlightenment”\n\nand experiment to understand the\n\n\npress may have stopped covering the\n\ntechnology, some businesses continue\n\n\ninterest.\n\n\n2. Peak of Inflated Expectations: In the\n\nnext phase, a frenzy of publicity typically\n\ngenerates over-enthusiasm and unreal­\n\n\nbenefits and practical application of the\n\ntechnology.\n\n\nistic expectations. There may be some\n\nsuccessful applications of a technology,\n\n\n5. Plateau of Productivity: Mainstream\n\nadoption starts to take off. Criteria for\n\n\nassessing provider viability are more\n\nclearly defined. The technology’s broad\n\n\nbut there are typically more failures.\n\n\n3. Trough of Disillusionment: Technolo­\n\ngies enter the “trough of disillusionment”\n\n\nmarket applicability and relevance are\n\n\nbecause they fail to meet expectations\n\nand quickly become unfashionable. Con­\n\n\nclearly paying off.\n\nAlthough Gartner references the “press” above,\n\n\nsequently, the press usually abandons the\n\ntopic and the technology.\n\n\ntechnology hype can and does occur throughout\n\ndifferent organizations. It can often result in sig­\n\n\nnificant program investment funding being applied\n\n\n-----\n\nto technologies that may not be suitable for\n\nthe intended system or user, but were deemed\n\n\npromising by program stakeholders. The preced­\n\ning steps are applicable to all MITRE sponsors\n\nand customers to which technology programs\n\n\nare marketed. When significant attention is given\n\nby program stakeholders to a new research,\n\n\ntechnology, technology development program, or\n\ndemonstration, the targeted technology should\n\nbe objectively evaluated and assessed for matu­\n\n\n_Performance Limit_\n\nImproving Mature\n\nResources Expended or Time\n\n|Performance Limit|Col2|Col3|Col4|\n|---|---|---|---|\n|||||\n|New|Improving|Mature|Aging|\n\n\nrity as soon as possible before committing any\n\nsignificant program investment funding.\n\n\nTechnology maturity. A generic depiction of\n\ntechnology maturity is shown by the s-curve in\n\n\nFigure 2. Technology Maturity\n\n“relevant environment,” and “operational mission\n\n\nFigure 2. In general, technology can be defined as\n\nfollows: new technology has not reached the first\n\ntipping point in the s-curve of technology matu­\n\nrity; improving or emerging technology is within\n\nthe exponential development stage of the curve\n\n\nconditions,” which must be interpreted in the con­\n\ntext of the system or capability under develop­\n\n\nment. Close communication among the program\n\noffice, operational users, and the developer on\n\n\nafter the first tipping point and before the second\n\ntipping point; mature technology follows the sec­\n\n\nthese terms is needed to ensure an accurate\n\nassessment. One factor the current TRL scale\n\n\ndoes not address is how well the developed tech­\n\nnology fits into the architecture and system struc­\n\nture of the program absorbing it. This is an integral\n\npart of the systems engineering job and critical to\n\nthe success of the technology transition.\n\n\nond tipping point before the curve starts down,\n\nand aging technology is on the downward tail.\n\n\nThe most universally accepted methodology for\n\nassessing the upward slope of this curve is the\n\n\nTechnology Readiness Level (TRL) scale [2]. There\n\nare actually several versions of the original NASA\ndeveloped TRL scale depending on the applica­\n\ntion (software, manufacturing, etc.), but all rate a\n\n\nSelecting technology alternatives. For assess­\n\ning which technology to employ to satisfy new\n\n\nrequirements, various fitness criteria can be used\n\nto select which alternative will best realize cus­\n\n\ntechnology based on the amount of development\n\ncompleted, prototyping, and testing within a range\n\nof environments from lab (or “breadboard”) to\n\n\ntomer desired outcomes from the total spectrum\n\nof technologies available. Criteria that consider\n\n\nboth the technology and the customer’s ability to\n\nassimilate it are more likely to succeed than those\n\nthat consider only the technology (as seen above\n\nin the use of TRLs). Moore [3] identifies types of\n\ncustomers as: innovators, early adopters, early\n\n\noperationally relevant. It is critical to get a com­\n\nmon and detailed understanding of the TRL\n\n\nscale among program stakeholders, particularly\n\nconcerning terms like “simulated environment,”\n\n\n-----\n\n|Innovators 2.5% Early Adopters Early Majority 13.5% 34%|Late Majority Laggards 34% 16%|\n|---|---|\n\n\nFigure 3. Roger’s Bell Curve\n\n\nmajority, late majority, and laggards. The curve\n\ndepicted in Figure 3 is referred to as the technol­\n\nogy adoption life cycle, or Roger’s Bell Curve [4].\n\nAs the names suggest, each customer type\n\nhas its own tolerance for change and novelty.\n\nTechnology assessment considers the customer’s\n\ntolerance for disruptive change as well as new\n\nor old technologies. For example, it would not be\n\nappropriate to recommend new technology to\n\n“late majority” customers, nor mature technology\n\nto “innovators.”\n\nDepartment of Defense acquisition programs are\n\nrequired to assess all threshold capabilities in the\n\nCapabilities Description Document for maturity;\n\nthose deemed to be met with immature technol­\n\nogy (a TRL of less than six) will not be considered\n\nfurther as “threshold” and may jeopardize the\n\nprogram milestone decision. Programs structured\n\nto inject developing technologies could be more\n\nreceptive to innovation and less mature technolo­\n\ngies, but in this case be sure to carefully evaluate\n\n\nthe risks involved (for further reference, see the\n\nSEG’s Risk Management topic).\n\nABC alternatives. Another dimension of the\n\nselection criteria considers the capabilities of\n\ntechnology providers. Former Director of the\n\nDefense Information Systems Agency, Lt Gen\n\nCharles Croom, devised a new philosophy for\n\nacquisition called ABC [5]. In the “ABC” concept,\n\n“A” stands for adopt existing technology, “B” is\n\nbuy it, and “C” is create it yourself. Adopt may\n\nseem an obvious decision if the technology fits\n\nthe purpose, but both the technology and the\n\nprovider should be evaluated for reliability and\n\nsustainability. With the buy alternative, vendor\n\nresponsiveness and capability are concerns\n\n(for further reference, see the SEG’s Integrated\n\nLogistics Support topic). Create is the choice of\n\nlast resort, but it may be the best alternative in\n\ncertain circumstances.\n\n\n-----\n\n###### References and Resources\n\n[1. Gartner Hype Cycles, http://www.gartner.com/technology/research/methodologies/hype-](http://www.gartner.com/technology/research/methodologies/hype-cycle.jsp)\n\n[cycle.jsp, accessed February 4, 2014.](http://www.gartner.com/technology/research/methodologies/hype-cycle.jsp)\n\n2. “Technology Readiness Level,” Wikipedia, accessed February 22, 2010.\n\n3. Moore, G. A., 1998. Crossing the Chasm, Capstone Publishing Limited.\n\n4. Rogers, E., Diffusion of Innovation Model, “Technology Adoption Lifecycle,” Wikipedia,\n\naccessed February 22, 2010.\n\n5. Gallagher, S., March 20, 2008, “Croom: Acquisition Done Better, Faster, Cheaper,” Federal\n\n_Computer Week._\n\n\n-----\n\nDefinition: Technology Planning\n\n_is the process of planning_\n\n_the technical evolution of a_\n\n_program or system to achieve_\n\n_its future vision or end-state._\n\n_Technology planning may_\n\n_include desired customer_\n\n_outcomes, technology fore­_\n\n_casting and schedule projec­_\n\n_tions, technology maturation_\n\n_requirements and planning, and_\n\n_technology insertion points._\n\n_The goal is a defined technical_\n\n_end-state enabled by technol­_\n\n_ogy insertion over time. Note_\n\n_that sometimes this is referred_\n\n_to as “strategic technical_\n\n_planning” (STP) applied at a_\n\n_program level, although the_\n\n_preferred use of the STP term_\n\n_is at the enterprise or portfolio_\n\n_level [1]._\n\nKeywords: technology evalu­\n\n_ation, technology plan, tech­_\n\n_nology planning, technology_\n\n_roadmap_\n\n\nACQUISITION PROGRAM PLANNING\n###### Technology Planning\n\n**MITRE SE Roles and Expectations: MITRE’s role**\n\nas a strategic partner with our customers requires\n\nus to focus on the up-front planning stages of\n\ncustomer programs, including defining the techni­\n\ncal direction of a particular program. MITRE sys­\n\ntems engineers (SEs) working on technical strat­\n\negy and planning are expected to understand the\n\nfuture vision and mission being addressed by that\n\nplan and how technology can be brought to bear\n\non solutions to meet that future vision. MITRE\n\nSEs are also expected to acquire and maintain\n\ninsight into developing technology to provide a\n\ntimely “honest broker” perspective in technol­\n\nogy planning. MITRE SEs are also expected to\n\nbridge user and research communities to bet­\n\nter align government research investment and\n\ndirection with future operational mission needs.\n\n\n-----\n\n###### What Is Technology Planning?\n\nFor a particular acquisition program, the\n\nBusiness/Mission\n\nfuture technical direction may be defined\n\nObjectives & Gaps\n\nby generating a program-level technical\nstrategy documented in a technology plan\nor roadmap. Given the current state and Drives Technology Informs\nconstant change of technology, without Plan\ncommon guidance, individual organiza­\ntions may use their own methods and Technology\n\nEvaluations\n\ntechnologies in ways that can actually hin­\nder adaptation to the future. A technology\nplan provides the guidance to evolve and\nmature relevant technologies to address\nfuture mission needs, communicate vital Figure 1. Technology Planning Bridges Business,\n\nMission, and Technology Domains\n\ninformation to stakeholders, provide the\ntechnical portion of the overall program\nplan (cost and schedule), and gain strong\nexecutive support. It should be a “living” document that is a basis for an ongoing technology\ndialog between the customer and the systems developers.\n\n\nStrategic technical planning embraces a wider scope and can cover a wide range of topics.\nIt can be organizationally dependent, portfolio focused, enterprise-wide, and system focused.\nA program’s technology plan may be linked to an organizational or enterprise “strategic tech­\nnical plan” [2, 3]. It should also serve as the companion to the program’s business or mission\nobjectives because business or mission needs and gaps drive the technology needs. At the\nsame time, technology evaluations inform the technical planning activity of technologies to\nachieve the future technical vision or end-state. The resulting technology plan serves as the\nroadmap for satisfying the gaps over time to achieve the end-state. These relationships are\ndepicted in Figure 1.\n\n###### Technology Plan Components\n\nA technology plan is a key enabler for the systems engineering function. Based on the future\nmission or business needs, it defines a desired technical end-state to evolve toward. Because\nthat end-state may not be achievable with current technology, it is important to determine\nwhich technologies are available now, which technologies are in development, including their\nmaturity levels, and which technologies do not yet exist. This helps influence an investment\nstrategy that can focus and push the “state of the art,” and it helps define requirements that\nare not achievable at all or may be cost prohibitive.\n\n\n-----\n\nTechnologies requiring further investment and maturation should be assessed as part of\nthe technical planning process. Appropriate risk should be assigned to technologies assessed\nas immature, with the need for concomitant mitigation plans. Technologies that have been in\nthe research and development (R&D) phase for an extended period (over five years) should be\nassessed for the maturation trend to determine if additional investment would significantly\nimprove the maturity.\n\nAt a minimum, the plan should include identification of all technology being brought to\nbear for the solutions, the maturation and trend of applicable technologies (forecast), insertion\npoints, required investments, and dependencies.\n\n###### Best Practices and Lessons Learned\n\n\nThe process of developing and implementing\n\na technology plan should include the following\n\nactivities [4]:\n\nEvaluate the environment for innovative uses of\n\ntechnology. What is changing in the environment\n\nthat needs to be taken into account or can be\n\nexploited? Where is industry headed and what are\n\nits technology roadmaps?\n\nDefine desired results. Where does the organiza­\n\ntion want to be within a planning horizon, usually\n\n5–10 years? Envision the future as if it were today,\n\nand then work back to the present.\n\nIdentify the core technologies needed for\n\nmeeting the vision and focus on those first.\n\nAssess the risks for maturation and focus on\n\ninvestment and mitigation. If the risk is very high,\n\nthe choice is to wait and depend on the “bleed­\n\ning edge,” or embark on a serious investment\n\nprogram. The criticality of the technology and/or\n\nmission will drive this choice. If it is indeed a core\n\ntechnology and critical to the success of achiev­\n\ning the end-state, significant investment will need\n\nto be applied to buy down the risk. One example\n\nof this is the government choosing to invest heav­\n\nily in cyber security.\n\n\nIdentify the remaining technologies applicable\n\nto the mission or business area end-state. But,\n\ndon’t become enamored with technology for\n\ntechnology’s sake! Keep it simple and focused on\n\nthe end-state.\n\nEstablish a quantifiable feedback system\n\nto measure progress. Define what must be\n\ndone and how it will be measured to determine\n\nprogress. Define measures of success to gauge\n\nwhether the implementation of the plan is pro­\n\ngressing successfully. Adjust the plan accordingly.\n\nMeasuring return on investment for those tech­\n\nnologies requiring maturation can be challenging;\n\nmake allowances for failures depending on the\n\nassessed risk.\n\nAssess the current state of the organization\n\nimplementing the plan. Are resources (staff,\n\nfunding) and processes in place to effectively\n\nimplement the plan? Are the required skills\n\navailable?\n\nDevelop tactical plans with measureable goals\n\nto implement the strategy.\n\nForm the roadmap. Develop the phasing, inser­\n\ntion points, associated R&D investments, work\n\n\n-----\n\nplans or packages, and sequence the activities\n\nwithin each functional and major program area in\n\nthe tactical plan to form the roadmap. Allocate\n\nresources and tasks and set priorities for action\n\nduring the current year.\n\nAssess the life-cycle costs of technology.\n\nTry not to underestimate the life-cycle cost of\n\ntechnology. This can be difficult. Industry invest­\n\nments in new technology tend to be closely held\n\nand proprietary. Often, the full product or proto­\n\ntype is not made visible to the customer until it’s\n\nready for sale or deployment. So, usually there is a\n\nlack of technical detail and understanding of the\n\nwhole product or technology and its application\n\nto the mission area. The result can be increased\n\ncosts for integration, maintenance, and licensing.\n\nLicensing for proprietary special-purpose (non\ncommercial-off-the-shelf) technology can be\n\nparticularly costly. An alternative is a sole-source\n\nrelationship.\n\n###### References and Resources\n\n\nEducate the organization and stakeholders on\n\nthe plan and its implementation. Communicate\n\nwith stakeholders and users using their opera­\n\ntional terminology and non-technical language.\n\nUsers won’t support what they can’t under­\n\nstand and can’t clearly link to their mission.\n\nCommunicate the plan to outside industry, labs,\n\nand associated R&D activities to ensure under­\n\nstanding and form or solidify relationships. The\n\ntechnology plan can be a tool to collaborate with\n\nindustry, labs, and other organizations on shared\n\ninvestment strategies toward achieving common\n\ngoals.\n\nImplement the technology plan. Monitor, track,\n\nand make adjustments to the plan according to\n\nperiodic reviews.\n\nReview the technology plan. Review annually\n\nor in other agreed period by iterating the above\n\nprocess.\n\n\n1. Swarz, R., and J. DeRosa, 2006, A Framework for Enterprise Systems Engineering\n\nProcesses, The MITRE Corporation.\n\n2. Byrne, R., June 2, 2005, A Netcentric Strategic Technical Plan (STP), The MITRE\n\nCorporation.\n\n3. MITRE Mission Planning Team, November 2006, JMPS Strategic Technical Plan, Ver. 3.0.\n\n4. CC2SG Technology Planning Team, October 5, 2005, COCOM C2 Systems Group Technical\n\nPlanning Process.\n\n\n-----\n\nDefinition: Cost analysis is\n\n_“the process of collecting and_\n\n_analyzing historical data and_\n\n_applying quantitative models,_\n\n_techniques, tools, and data­_\n\n_bases to predict the future cost_\n\n_of an item, product, program or_\n\n_task.” Cost estimates “translate_\n\n_system/functional requirements_\n\n_associated with programs, proj­_\n\n_ects, proposals, or processes_\n\n_into budget requirements, and_\n\n_determine and communicate a_\n\n_realistic view of the likely cost_\n\n_outcome, which can form the_\n\n_basis of the plan for executing_\n\n_the work.”[1]_\n\nKeywords: budget, cost\n\n_analysis, cost benefit, cost_\n\n_estimation, regression analysis,_\n\n_trade-offs_\n\n\nACQUISITION PROGRAM PLANNING\n###### Life-Cycle Cost Estimation\n\n**MITRE SE Roles and Expectations: Systems**\n\nengineers (SEs) are expected to use cost analysis\n\nto identify and quantify risks and to evaluate\n\ncompeting systems/initiatives, proposals, and\n\ntrade-offs. They are expected to collaborate\n\nwith the cost/benefit analyst and sponsor/\n\ncustomer to define the approach, scope,\n\nproducts, key parameters, and trade-offs of\n\nthe analysis. SEs support and provide direction\n\nto the analyst, review results, guide and evalu­\n\nate the sensitivity of the analysis, and provide\n\ntechnical, programmatic, and enterprise-wide\n\nperspectives and context for the analyst.\n\nCost analysis is an often misunderstood and\n\nfrequently overlooked practice that encompasses\n\nmany areas of a program’s business management.\n\nIt combines the knowledge of many ­different\n\n\n-----\n\ndisciplines and produces results that have far-reaching impacts on a program and its success.\nIn many cases, the analyst who built a program’s life-cycle cost estimate (LCCE) will have\nmore knowledge and understanding of the program than any other member of the program\nteam.\n\n###### Cost Estimate Development Overview\n\nCost estimation methodologies and techniques vary widely depending on the customer and\nprogram. These variations are based on several factors. What is being estimated, the extent\nof available data, existence of an agreed-on work breakdown structure (WBS), regulatory\nrequirements, agency requirements, and industry best practices all influence the methodolo­\ngies and techniques that may be applied when creating a cost estimate. For example, the\nLCCE for a Department of Defense weapon system will be conducted differently and look very\ndifferent from an estimate for a data center or the development of a computer application for a\ncivilian agency. Also, the type of estimate will influence the methodology and approach used.\nA much more rigorous process is required for a budgetary estimate or a full LCCE than for\na rough order of magnitude or “back of the envelope” type of estimate. Although there is no\n“cookie-cutter” approach to developing a cost estimate, Figure 1 depicts a generic cost estimat­\ning process.\n\n###### �Define Cost Estimate Scope: The initial step is to define the possible scope of the cost\n\nmodel. The scope will determine the content of the cost elements that must be included\nin the model. Sources for scope definition of a program include the project management\nplan, the scope statement, the WBS, and any requirements documentation, etc.\n###### �Identify Assumptions and Constraints: Assumptions are statements that are used to\n\nlimit the scope of the model. They are a “given” as opposed to a “fact.” They usually\nrelate to a future occurrence and therefore contain uncertainty. Assumptions must be\nevaluated during sensitivity analysis. Constraints are usually fixed, externally imposed\nboundaries such as schedule, policies, and physical limitations.\n###### �Develop Cost Element Structure: The cost element structure can also be thought of as a\n\nchart of accounts. It is a listing of the possible categories of cost contained in the model.\nEach element must be defined so that all costs are covered, and there are no duplica­\ntions of costs within the structure.\n###### �Collect and Normalize Data: Cost data is collected for all of the elements within the\n\nmodel. Information from benchmark research and actual cost experience is used.\nNormalization is the process for ensuring that cost data are comparable.\n###### �Develop Cost Estimating Relationships: The cost data is used to develop equations that\n\nwill be entered into the cost model. The equations will be the basis for estimating costs\nas a function of system capacity and service level.\n\n\n-----\n\n|Col1|• D • M • Ri • R|\n|---|---|\n||Yes|\n\n\nFigure 1. General Depiction of Cost Estimating Process\n\n###### �Document Approach: Documentation is provided for each cost element that indicates\n\nthe sources of data, assumptions used, and any equations that have been used in the\ncalculations.\n###### �Customer Review: A walkthrough on the model and results is conducted with the spon­\n\nsor to ensure that all expected costs have been adequately represented and to achieve\nacceptance of the estimate.\n\n###### Best Practices and Lessons Learned\n\n\nThe three Rs. For an LCCE to be credible and\n\neffective, it must meet three basic requirements,\n\n\nalso known as the three Rs of cost estimation:\n\nReplication, Rationale, and Risk.\n\n\n-----\n\n###### � [Replication:][ The estimator must provide ]\n\nan audit trail that is sufficiently detailed,\n\nincluding clearly stated assumptions for\n\neach cost element, to allow for an inde­\n\npendent replication of the estimate by a\n\nthird or external party.\n###### � [Rationale:][ The estimator must provide a ]\n\nconvincing and justifiable rationale for the\n\nselection of key parameter values, labor\n\nestimates, cost factors, assumptions, and\n\nall underlying inputs to the estimate. These\n\ncan come from early project experience,\n\nother similar projects, parametric models,\n\nand documented engineering judgments.\n###### � [Risk:][ The estimator must conduct risk/]\n\nsensitivity analysis to assess the impact\n\nof the inherent uncertainty in input values.\n\nRegression analysis is the most frequently\n\nused method of conducting sensitivity\n\nanalysis in this area.\n\nUtility to the program. Investments require clear\n\nidentification of benefits, which can be catego­\n\nrized as either tangible or intangible. The benefits\n\ncan be packaged in a program that will most likely\n\nyield desirable outcomes. As a cautionary step,\n\none must consider the cost to stand up the pro­\n\ngram, the cost to incur the chain of activities for\n\nthe identified investment such as implementation,\n\noperation, and maintenance from both quantita­\n\ntive and qualitative perspectives. When properly\n\ndone, cost analysis provides the following utility to\n\nthe program:\n\n###### � [Supports budgeting process by: ]\n\nyy Integrating the requirements and bud­\n\ngeting processes.\n\n\nyy Assessing affordability and reasonable­\n\nness of program budgets.\n\nyy Providing basis for defending budgets\n\nto oversight organizations.\n\nyy Quickly/accurately determining impacts\n\nof budget cuts on program baselines\nand associated functionality.\n###### � [Enables early identification of potential ]\n\npitfalls such as cost growth and schedule\n\nslips.\n###### � [Enables identification of future cost ]\n\nimprovement initiatives.\n###### � [Provides for the identification and objec­]\n\ntive quantification of the impact of pro­\n\ngram risks (technical and schedule risks).\n###### � [Provides a basis for evaluating competing ]\n\nsystems/initiatives (cost/benefit analyses\n\nand analysis of alternatives [AoA]).\n###### � [Enables proposal pricing and evaluation of ]\n\nproposals for cost reasonableness (inde­\n\npendent government cost estimates).\n###### � [Captures cost impacts of design deci­]\n\nsions to facilitate trade-offs in cost as an\n\nindependent variable/design to cost/tar­\n\nget costing.\n###### � [Facilitates evaluation of the impact of new ]\n\nways of doing business (e.g., in-sourcing\n\nvs. outsourcing, commercial off-the-shelf\n\nvs. custom software).\n\nAn art, not a science. As with any discipline, the\n\nactual application and practice of cost analysis is\n\nmore difficult than the academic description. It is\n\nseldom the case that the process outlined above\n\ncan be applied with complete precision. In most\n\ncases many factors conspire to force the systems\n\n\n-----\n\nengineer and the cost estimator to step “outside\n\nthe box” in completing a cost estimate.\n\nWhen data is unavailable. Oftentimes data\n\nto support the estimate is not readily available\n\nthrough the customer organization. Finding sup­\n\nportable data will often require creative thinking\n\nand problem solving on the part of the systems\n\nengineer and cost estimator. An example is an\n\nAoA in which one of the alternatives was to build\n\nroads. The agency in question did not possess\n\nany in-house knowledge on road construction,\n\nancillary costs (such as drainage ditches and\n\neasements), or permit and legal requirements for\n\nconstruction of several hundred miles of access\n\nroad. The situation required reaching out to the\n\ncivil engineering community and several state\n\ndepartments of transportation in order to bridge\n\nthe knowledge gap and obtain the information in\n\nquestion. This resulted in a detailed and support­\n\nable estimate that the customer was able to use\n\nin justifying managerial decisions.\n\nAdaptability is key. As stated in the cost esti­\n\nmation development discussion of this article,\n\n###### References and Resources\n\n\nthere is no single way to construct a cost esti­\n\nmate—too much depends on the details of the\n\ncircumstances at hand. An estimator cannot do a\n\nparametric estimate, for example, if the data and\n\nsituation do not support that approach. Another\n\nAoA provides an example of this. When tasked\n\nto provide an AoA for an outsourcing or internal\n\ndevelopment of a customer’s financial manage­\n\nment system, the estimator predetermined that\n\nan engineering build-up based on engineering\n\nknowledge of the problem set would be per­\n\nformed. Unfortunately the customer organization\n\nhad no internal engineering expertise in this area.\n\nThe estimator was forced to change its approach\n\nand build an estimate based on analogy of similar\n\nsystems and industry benchmark studies.\n\nKeep program needs in sight. Overall, the most\n\nimportant perspective on cost estimating is to\n\nkeep the process in context. Remember that the\n\ncost estimate is not an end in itself but a means\n\nto an end. Understand what the program needs\n\nto accomplish through cost estimation and work\n\nwith the cost estimator to tailor your product\n\naccordingly.\n\n\n1. The International Society of Parametric Analysts and The Society of Cost Estimating and\n\nAnalysis (ISPA/SCEA) Professional Development and Training Workshop proceedings,\nJune 2–5, 2009, training presentation on Cost Estimating Basics [slide 5].\n\n###### Additional References and Resources\n\n[Army Financial Management Home Page, http://www.asafm.army.mil/, accessed February 4,](http://www.asafm.army.mil/)\n2014.\n\nGAO, March 2009, GAO Cost Estimating and Assessment Guide, “Best Practices for Developing\nand Managing Capital Program Costs.”\n\n\n-----\n\n[Software Engineering Institute, http://www.sei.cmu.edu/, accessed February 4, 2014.](http://www.sei.cmu.edu/)\n\n[The Data and Analysis Center for Software, https://www.thecsiac.com/, accessed February 4,](https://www.thecsiac.com/)\n2014.\n\nThe Project Management Institute, http://www.pmi.org/, accessed February 4, 2014.\n\n[The Society of Cost Estimating and Analysis, https://www.iceaaonline.org/, accessed February](https://www.iceaaonline.org/)\n4, 2014.\n\n\n-----\n\nDefinition: The IMP is com­\n\n_prised of a hierarchy of pro­_\n\n_gram events, in which each_\n\n_event is supported by specific_\n\n_accomplishments, and each_\n\n_accomplishment is based on_\n\n_satisfying specific criteria to_\n\n_be considered complete. The_\n\n_IMS is an integrated, networked_\n\n_schedule containing all the_\n\n_detailed discrete work pack­_\n\n_ages and planning packages_\n\n_(or lower level tasks of activi­_\n\n_ties) necessary to support the_\n\n_events, accomplishments, and_\n\n_criteria of the IMP._\n\nKeywords: earned value man­\n\n_agement, EVMS, integrated_\n\n_master plan, integrated master_\n\n_schedule, program plan, work_\n\n_breakdown structure, WBS_\n\n\nACQUISITION PROGRAM PLANNING\n###### Integrated Master Schedule (IMS)/Integrated Master Plan (IMP) Application\n\n**MITRE SE Roles and Expectations: The IMS**\n\nand IMP form a critical part of effectively provid­\n\ning acquisition support. MITRE systems engineers\n\n(SEs) should understand the use and imple­\n\nmentation of these tools and how they can be\n\nused to effectively monitor program execution.\n\n\n-----\n\n###### What We Know About the IMS and IMP\n\nProgram planning involves developing and maintaining plans for all program processes,\nincluding those required for effective program office-contractor interaction. Once the contract\nis signed and schedule, costs, and resources from the contractor are established, the program\nplan takes into account, at an appropriate level of detail, the contractor’s estimations for the\nprogram. Together, the IMP and IMS should clearly demonstrate that the program is struc­\ntured and executable within schedule and cost constraints with an acceptable level of risk.\nDuring the proposal evaluation and source selection phases, the IMP and IMS are critical\ncomponents of the offeror’s proposal; they identify the offeror’s ability to partition a program\ninto tasks and phases that can be successfully executed to deliver the proposed capability.\nAfter contract award, the contractor and/or the government use the IMP and IMS as the dayto-day tools for executing the program and tracking program technical and schedule status,\nincluding all significant risk mitigation efforts.\n\nThe IMP and IMS are business tools to manage and provide oversight of acquisition,\nmodification, and sustainment programs. They provide a systematic approach to program\nplanning, scheduling, and execution. They are equally applicable to competitive and sole\nsource procurements with industry, as well as to government-only, in-house efforts. They help\ndevelop and support program/project budgeting, and can be used to perform “what-if” exer­\ncises and to identify and assess candidate problem workarounds. Finally, use of the IMP/IMS\nfocuses and strengthens the interaction between the government and contractor teams with\nrespect to program execution.\n\n###### Best Practices and Lessons Learned\n\n\nRight type and level of detail. The IMP should\n\nprovide sufficient definition to track the step-by\nstep completion of the required accomplishments\n\nfor each event, and to demonstrate satisfaction of\n\nthe completion criteria for each accomplishment.\n\nEvents in the IMP are not tied to calendar dates;\n\nthey are tied to the accomplishment of a task or\n\nwork package as evidenced by the satisfaction\n\nof the specified criteria for that accomplish­\n\nment. The IMS should be defined to the level of\n\ndetail necessary for day-to-day execution of the\n\nprogram.\n\n\nTo build a reasonable IMP and IMS, you need to\n\nestimate the attributes of work products and\n\ntasks, determine the resources needed, estimate\n\na schedule, and identify and analyze program\n\nrisks. Accomplishments in the IMP should have\n\ncriteria for determining completion with clear\n\nevidence so that the entire program team can\n\nunderstand the progress. The IMS and IMP\n\nshould be traceable to the work breakdown\n\nstructure (WBS) and be linked to the state­\n\nment of work and ultimately to the earned value\n\nmanagement system (EVMS). The WBS speci­\n\nfies the breakout of work tasks that the IMP and\n\n\n-----\n\nIMS should be built on and the EVMS should\n\nreport on. A good WBS includes key work efforts\n\npartitioned into discrete elements that result\n\nin a product (i.e., document, software item, test\n\ncompletion, integrated product) or in measur­\n\nable progress (percent complete is not recom­\n\nmended when the end-state is not completely\n\nquantifiable—an issue in software development,\n\ntest procedures, or training materials). With a\n\ngood WBS foundation, both the IMP and IMS can\n\nbe more useful tools; with the IMP integrating\n\nall work efforts into a defined program plan, and\n\nthe IMS summarizing the detailed schedule for\n\nperforming those work efforts. The IMP is placed\n\non contract and becomes the baseline execu­\n\ntion plan for the program/project. Although fairly\n\ndetailed, the IMP is a relatively top-level docu­\n\nment compared to the IMS. The IMS should not\n\nbe placed on contract; it is normally a contract\n\ndeliverable.\n\nFor evaluating a proposed IMS, focus on realistic\n\ntask durations, predecessor/successor relation­\n\nships, and identification of critical path tasks with\n\nviable risk mitigation and contingency plans. An\n\nIMS summarized at too high a level often results\n\nin obscuring critical execution elements and\n\ncontributing to the EVMS’s failure to accurately\n\nreport progress (for more on EVMS, see the\n\nSEG’s “Acquisition Management Metrics” article).\n\nA high-level IMS may also fail to show related risk\n\nmanagement approaches being used, which often\n\nresults in long-duration tasks and artificial linkages\n\nmasking the true critical path.\n\nAn example of this is an IMS with several con­\n\ncurrent activities progressing in parallel and\n\nshowing a critical path along one activity that\n\n\nlater links and transitions to another activity. A\n\nthird activity also shows a dependency to the\n\nfirst, but it is not considered to be on the criti­\n\ncal path. If the IMS does not have the detail to\n\ndetermine the progress point at which the criti­\n\ncal path transitions to the second activity, the\n\nreal critical path could be along the dependency\n\nof the third activity. Conversely an IMS that is\n\ntoo detailed may result in similar problems; the\n\ncritical path is too hard to identify (looking in the\n\nweeds not up at the trees). The IMS’s physical\n\nmaintenance becomes tedious and linkages\n\ncould be missed in the details. An IMS can be\n\nineffective on a program when it is either too\n\nhigh level or too detailed.\n\nIn general, the IMP can be thought of as the top\ndown planning tool and the IMS as the bottom-up\n\nexecution tool for those plans. Note, however,\n\nthat the IMS is a scheduling tool for management\n\ncontrol of program progression, not for cost col­\n\nlection purposes.\n\nMeasurable criteria. Criteria established for IMP\n\naccomplishments should be measurable (i.e.,\n\nsatisfactory completion of a test event, approval\n\nof a study report, or verification of an activity\n\nor test). Consider including accomplishment of\n\ncritical performance requirements (key perfor­\n\nmance parameters or technical performance\n\nmetrics). For these, it important to link criteria to\n\nthe specification versus the actual performance\n\nrequirement embedded in the criteria so require­\n\nments do not have to be maintained in more than\n\none document.\n\nMultiple delivery/increment programs. On\n\nprograms with multiple deliveries and/or mul­\n\ntiple increments, ensure that the IMS includes\n\n\n-----\n\ncross-delivery order and cross-increment\n\nrelationships. This is valuable when conducting\n\ncritical path analyses on the IMS. These relation­\n\nships sometimes drive “ripple effects” across the\n\ndelivery orders and work tasks, and when analyz­\n\ning a critical path or estimating a “what if” or\n\ntotal cost for a modification, this is an extremely\n\nvaluable factor.\n\nStakeholder involvement. Relevant stakeholders\n\n(including user organizations, financial manag­\n\ners, and sustainment organizations) should be\n\n###### References and Resources\n\n\ninvolved in the planning process from all life-cycle\n\nphases to ensure that all technical and support\n\nactivities are adequately addressed in program\n\nplans such as the IMP and IMS.\n\nCommunicating via IMS. The IMS can be used\n\nto communicate with stakeholders on a regular\n\nbasis. For enterprise systems with large numbers\n\nof external interfaces and programs, the IMS can\n\nbe used as the integration tool to indicate and\n\ntrack milestones relevant to the other programs.\n\n\nAcquisition Community Connection, Integrated Master Plan (IMP)/Integrated Master Schedule\n(IMS).\n\nAFMC Pamphlet 63-5, November 11, 2004, Integrated Master Plan and Schedule Guide.\n\nDepartment of Defense, October 21, 2005, Integrated Master Plan and Integrated Master\n_Schedule: Preparation and Use Guide, ver. 0.9._\n\n\n-----\n\nDefinition: Performance engi­\n\n_neering is a specialty systems_\n\n_engineering discipline that_\n\n_encompasses the practices,_\n\n_techniques, and activities_\n\n_required during each phase of_\n\n_the Systems Development Life_\n\n_Cycle [1] to ensure that a pro­_\n\n_posed or existing solution will_\n\n_meet its nonfunctional require­_\n\n_ments. Nonfunctional require­_\n\n_ments specify the criteria used_\n\n_to judge the operation of a_\n\n_system rather than its specific_\n\n_behaviors or functions._\n\nKeywords: capacity planning,\n\n_design validation, feasibility,_\n\n_instrumentation, load testing,_\n\n_measurement, modeling and_\n\n_simulation, monitoring, require­_\n\n_ments validation, response_\n\n_time, scalability, stress testing,_\n\n_throughput_\n\n\nACQUISITION PROGRAM PLANNING\n###### Performance Engineering\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to under­\n\nstand the purpose and role of performance\n\nengineering in the acquisition process, where it\n\noccurs in systems development, and the benefits\n\nof employing it. MITRE SEs are also expected to\n\nunderstand and recommend when performance\n\nengineering is appropriate to a situation. Some\n\naspects of performance engineering are often\n\nassociated with specialty engineering disciplines.\n\nOthers, however, are the purview of mainstream\n\nsystems and design engineering (e.g., many of the\n\ndimensions of usability). MITRE SEs are expected\n\nto monitor and evaluate performance engineering\n\ntechnical efforts and the acquisition program’s\n\noverall performance engineering activities and\n\nrecommend changes when warranted, including\n\nthe need to apply specialty engineering expertise.\n\n\n-----\n\n###### Performance Engineering Scope\n\nPerformance engineering focuses on the ability of systems to meet their nonfunctional\nrequirements. A nonfunctional requirement is a requirement that specifies criteria that can be\nused to judge the operation of a system, rather than specific behaviors. It may address a prop­\nerty the end product must possess, the standards by which it must be created, or the environ­\nment in which it must exist. Examples are usability, maintainability, extensibility, scalability,\nreusability, security and transportability. Performance engineering activities occur in each\nphase of the Systems Development Life Cycle. It includes defining nonfunctional require­\nments; assessing alternative architectures; developing test plans, procedures, and scripts to\nsupport load and stress testing; conducting benchmarking and prototyping activities; incor­\nporating performance into software development; monitoring production systems; performing\nroot cause analysis; and supporting capacity planning activities. The performance engineer­\ning discipline is grounded in expertise in modeling and simulation, measurement techniques,\nand statistical methods.\n\nTraditionally, much of performance engineering has been concerned with the perfor­\nmance of hardware and software systems, focusing on measurable items such as through­\nput, response time, and utilization, as well as some of the “-ilities”—availability, reliability,\nscalability, and usability. Tying the performance of hardware and software components to\nthe mission or objectives of the enterprise should be the goal when conducting performance\nengineering activities. This presents performance results to stakeholders in a more meaning­\nful way.\n\nAlthough performance engineering activities are most often associated with hardware\nand software elements of a system, its principles and techniques can be applied to other\naspects of systems that can be measured in some meaningful way, including, for example,\nbusiness processes. In the most simplistic sense, a system accepts an input and produces an\noutput. Therefore, performance engineering is applicable to not only systems but networks of\nsystems, enterprises, and other examples of complex systems.\n\nAs an example, given the critical nature of air traffic control systems, their ability to\nmeet nonfunctional requirements, such as response time and availability, is vital to National\nAirspace System (NAS) operations. Though there are many air traffic control systems within\nthe NAS, the NAS itself is an example of an enterprise comprising people, processes, hard­\nware, and software, among other things. At any given time, the NAS has a finite capacity;\nhowever, an opportunity exists to increase that capacity through more efficient processes or\nnew technology. The NAS is an example of a non-IT system to which performance engineer­\ning techniques can be applied.\n\n\n-----\n\n###### Performance Engineering Across the Systems Engineering Life Cycle\n\nAs illustrated in Figure 1, the activities associated with performance engineering span the\nentire systems life cycle—from Pre-Systems Acquisition through Sustainment. Although\nperformance engineering is recognized as fundamental in manufacturing and production, its\nactivities should begin earlier in the system life cycle when an opportunity exists to influ­\nence the concept or design to ensure that performance requirements can be met. Performance\nengineering techniques can be used to determine the feasibility of a particular solution or to\nvalidate the concept or requirements in the Pre-Systems Acquisition stage of the life cycle.\nLikewise, performance engineering techniques can be used to conduct design validation\nas well.\n\n###### Performance Engineering Activities\n\nPerformance engineering includes various risk reduction activities that ensure that a system\ncan meet its nonfunctional requirements. Performance engineering techniques can be used to\nvalidate various aspects of a planned system (whether new or evolving). For instance, perfor­\nmance engineering is concerned with validating that the nonfunctional requirements for a\nparticular system are feasible even before a design for that system is in place. In this regard,\n\nCompetitive Prototyping\nperformed here = Decision Point = Milestone Review\n\nInitial Operational Full Operational\n\nA B (Program initiation) C Capability Capability\n\nMateriel Technology Engineering and Production and Operations\nSolution Development Manufacturing Deployment and\nAnalysis Development Support\n\nMateriel FRP\nDevelopment Decision\nDecision Review\n\nPre-Systems Acquisition Systems Acquisition Sustainment\n\n\n\n- Modeling and Simulation\n\n- Feasibility Studies\n\n- Trade-off Analysis\n\n- Concept and\nRequirements Validation\n\n- Technology Assessments\n\n- Design Validation\n\n\n\n- Modeling and Simulation\n\n- Component and System\nInstrumentation\n\n- Component and System\nLoad Testing\n\n- Bottleneck Identification\n\n- Capacity Planning\n\n\n\n- Modeling and Simulation\n\n- System Instrumentation\n\n- System Load Testing\n\n- Bottleneck Identification\n\n- Capacity Planning\n\n- System Monitoring\n\n\nFigure 1. Performance Engineering in the System Life Cycle\n\n\n-----\n\nrequirements validation ensures that the nonfunctional requirements, as written, can be met\nusing a reasonable architecture, design, and existing technology.\n\nOnce a design is in place, performance engineering techniques can be used to ensure\nthat the particular design will continue to meet the nonfunctional requirements prior to\nactually building that system. Design validation is a form of feasibility study used to deter­\nmine whether the design is feasible with respect to meeting the nonfunctional requirements.\nLikewise, performance engineering activities can be used, as part of a technology assessment,\nto assess a particular high-risk aspect of a design.\n\nFinally, trade-off analysis is related to all of the activities mentioned previously in that\nperformance engineering stresses the importance of conducting a what-if analysis—an itera­\ntive exploration in which various aspects of an architecture or design are traded off to assess\nthe impact. Performance modeling and simulation as well as other quantitative analysis tech­\nniques are often used to conduct design validation as well as trade-off, or what-if, analyses.\n\nOnce a system is deployed, it is important to monitor and measure function and perfor­\nmance to ensure that problems are alleviated or avoided. Monitoring a system means being\naware of the system’s state in order to respond to potential problems. There are different levels\nof monitoring. At a minimum, monitoring should reveal whether a particular system compo­\nnent is available for use. Monitoring may also include the collection of various measurements\nsuch as the system load and resource utilization over time. Ideally availability and measure­\nment data collected as part of the monitoring process are archived in order to support perfor­\nmance analysis and to track trends, which can be used to make predictions about the future.\nIf a permanent measuring and monitoring capability is to be built into a system, its impacts\non the overall performance must be taken into consideration during the design and implemen­\ntation of that system. This is characterized as measurement overhead and should be factored\ninto the overall performance measurement of the system.\n\nSystem instrumentation is concerned with the measurement of a system, under controlled\nconditions, to determine how that system will respond under those conditions. Load test­\ning is a form of system instrumentation in which an artificial load is injected into the system\nto determine how the system will respond under that load. Understanding how the system\nresponds under a particular load implies that additional measurements, such as response\ntimes and resource utilizations, must be collected during the load test activity as well. If the\nsystem is unable to handle the load such that the response times or utilization of resources\nincreases to an unacceptable level or shows an unhealthy upward trend, it may be necessary\nto identify the system bottleneck. A system bottleneck is a component that limits the through­\nput of the system and often impacts its scalability. A scalable system is one whose throughput\nincreases proportionally to the capacity of the hardware when hardware is added. Note that\nelements like load balancing components can affect the proportion by which capacity can\n\n\n-----\n\nbe increased. Careful planning is necessary to ensure that analysis of the collected data will\nreveal meaningful information.\n\nFinally, capacity planning is a performance engineering activity that determines whether\na system is capable of handling increased load that is predicted in the future. Capacity\nplanning is related to all the activities mentioned previously—the ability to respond to pre­\ndicted load and still meet nonfunctional requirements is a cornerstone of capacity planning.\nFurthermore, measurements and instrumentation are necessary elements of capacity plan­\nning. Likewise, because bottlenecks and nonscalable systems limit the capacity of a system,\nthe activities associated with identifying bottlenecks and scalability are closely related to\ncapacity planning as well.\n\n\n###### Best Practices and Lessons Learned\n\nSystem vs. mission performance. The ability to\n\ntie the performance of hardware or software or\n\n\nnetwork components to the mission or objectives\n\nof the enterprise should be the goal. This allows\n\n\nthe results of performance engineering studies to\n\nbe presented to stakeholders in a more meaning­\n\nful way. It also serves to focus testing on out­\n\n\ncomes that are meaningful. For example, central\n\n\nWorst Time\nfor Change\n\n\nIdeal Time\nfor Change\n\n\nConcept Definition Design Development\n\n_“Concurrent Engineering,” J.R. Hartley, Productivity Press_\n\nFigure 2. The Cost of Change\n\n\nValidation Production\n\n\n-----\n\nprocessing unit utilization by itself is not meaning­\n\nful unless it is the cause of a mission failure or a\n\nsignificant delay in processing critical real-time\n\ninformation.\n\nEarly life-cycle performance engineering. Too\n\noften, systems are designed and built without\n\ndoing the early performance engineering analy­\n\nsis associated with the Pre-Systems Acquisition\n\nstage shown in Figure 1. When performance\n\nengineering is bypassed, stakeholders are often\n\ndisappointed and the system may even be\n\ndeemed unusable. Although it is common prac­\n\ntice to optimize the system after it’s built, the cost\n\nassociated with implementing changes to accom­\n\nmodate poor performance increases with each\n\nphase of the system’s life cycle, as shown in Figure\n\n2. Performance engineering activities should begin\n\nearly in the system’s life cycle when an opportu­\n\nnity exists to influence the concept or design of\n\nthe system in a way that ensures performance\n\nrequirements can be met.\n\nRisk reduction. Performance engineering activi­\n\nties are used to validate that the nonfunctional\n\nrequirements for a particular system are feasible\n\neven before a design for that system is in place,\n\nand especially to assess a particular high-risk\n\naspect of a design in the form of a technology\n\nassessment. Without proper analysis, it is difficult\n\nto identify and address potential performance\n\nproblems that may be inherent to a system design\n\nbefore that system is built. Waiting until system\n\nintegration and test phases to identify and resolve\n\nsystem bottlenecks is too late.\n\nTrade-off analysis. Performance engineering\n\nstresses the importance of conducting a trade\noff, or what-if, analysis—an iterative analysis in\n\n\nwhich various aspects of an architecture or design\n\nare traded off to assess the impact.\n\nTest-driven design. Under agile development\n\nmethodologies, such as test-driven design, per­\n\nformance requirements should be a part of the\n\nguiding test set. This ensures that the nonfunc­\n\ntional requirements are taken into consideration\n\nat all phases of the engineering life cycle and not\n\noverlooked.\n\nMonitoring, measurement, and instrumentation.\n\nSystem instrumentation is a critical performance\n\nengineering activity. Careful planning is neces­\n\nsary to ensure that useful metrics are specified,\n\nthat the right monitoring tools are put in place to\n\ncollect those metrics, and that analysis of the col­\n\nlected data will reveal meaningful information.\n\nPerformance challenges in integrated systems.\n\nProjects that involve off-the-shelf components\n\nor systems of systems introduce special chal­\n\nlenges for performance engineering. Modeling and\n\nsimulation may be useful in trying to anticipate the\n\nproblems that arise in such contexts and to sup­\n\nport root cause analysis should issues emerge/\n\nmaterialize. System instrumentation and analysis\n\nof the resulting measurements may become more\n\ncomplex, especially if various subsystems operate\n\non incompatible platforms. Isolating performance\n\nproblems and bottlenecks may become more\n\ndifficult as a problem initiated in one system or\n\nsubsystem may emerge as a performance issue\n\nin a different component. Resolving performance\n\nengineering issues may require cooperation\n\namong different organizations, including hardware,\n\nsoftware, and network vendors.\n\n\n-----\n\nPredicting usage trends. Performance data col­\n\nlected as part of the monitoring process should\n\nbe archived and analyzed on a regular basis in\n\n###### References and Resources\n\n\norder to track trends, which can be used to make\n\npredictions about the future.\n\n\n1. Systems Development Life Cycle, Wikipedia, accessed February 5, 2014.\n\n###### Additional References and Resources\n\nProfessional Bodies\n\n[Association for Computing Machinery (ACM) [SIGSIM and SIGMETRICS]. Contains special](http://www.acm.org/)\ninterest groups in both simulation and measurement.\n\nComputer Measurement Group (CMG). A professional organization of performance profession­\nals and practitioners. The CMG holds a yearly conference and publishes a quarterly newsletter.\n\n[International Council on Systems Engineering (INCOSE). Recognizes performance engineer­](http://www.incose.org/)\ning as part of the system life cycle.\n\n[The Society for Modeling and Simulation International (SCS). Expertise in modeling and simu­](http://www.scs.org/)\nlation, which is used extensively in performance engineering activities.\n\nPerformance-Related Frameworks\n\n[Capability Maturity Model Integration (CMMI). CMMI is a process improvement approach that](http://www.sei.cmu.edu/cmmi/)\nprovides organizations with the essential elements of effective processes.\n\n[Federal Enterprise Architecture (FEA) Performance Reference Model. The PRM is a “reference](http://www.whitehouse.gov/sites/default/files/omb/assets/fea_docs/FEA_CRM_v23_Final_Oct_2007_Revised.pdf)\nmodel” or standardized framework to measure the performance of major IT investments and\ntheir contribution to program performance.\n\n[Information Technology Infrastructure Library (ITIL). Industry standard for IT service man­](http://www.itil-officialsite.com/)\nagement (includes aspects of performance engineering).\n\nStandards Organizations\n\n[Object Management Group (MARTE Profile). International, not-for-profit, computer indus­](http://www.omg.org/)\ntry consortium focused on the development of enterprise integration standards. MARTE is\n“Modeling and Analysis of Real-time and Embedded Systems.”\n\n[The Standard Performance Evaluation Corporation (SPEC). A standards body for performance](http://www.spec.org/)\nbenchmarks. SPEC is an umbrella organization encompassing the efforts of the Open Systems\nGroup.\n\n\n-----\n\n[Transaction Processing Performance Council (TPC). The Transaction Processing Performance](http://www.tpc.org/)\nCouncil defines transaction processing and database benchmarks and delivers trusted results\nto the industry.\n\nAuthors\n\n[Gunther, N. Author of several performance engineering books.](http://www.perfdynamics.com/)\n\n[Jain, R. Professor at Washington University in St. Louis. Author of several performance engi­](http://www1.cse.wustl.edu/~jain/)\nneering books and articles.\n\n[Maddox, M. 2005, A Performance Process Maturity Model, MeasureIT, Issue 3.06.](http://www.cmg.org/measureit/issues/mit23/m_23_3.html)\n\n[Menasce, D. A. Professor at George Mason University. Author of several performance engi­](http://www.cs.gmu.edu/faculty/menasce.html)\nneering books.\n\nSmith, C. U., and L. G. Williams. Creators of the well-known Software Performance\n[Engineering (SPE) process and associated tool. Authors of “Performance Solutions” as well as](http://www.perfeng.com/paperndx.htm)\nnumerous white papers.\n\n\n-----\n\nDefinition: Investment analy­\n\n_ses take many different forms_\n\n_within the federal government_\n\n_and are applied to support_\n\n_a number of key decisions_\n\n_throughout the investment,_\n\n_acquisition, and program life_\n\n_cycles. Examples include_\n\n_Analysis of Alternatives,_\n\n_Business Case Analysis, Cost-_\n\n_Benefit Analysis, and Economic_\n\n_Analysis._\n\nKeywords: analysis of alterna­\n\n_tives, business case analysis,_\n\n_cost-benefit analysis, economic_\n\n_analysis, financial analysis,_\n\n_investment analysis_\n\n\nACQUISITION PROGRAM PLANNING\n###### Comparison of Investment Analyses\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand key aspects of investment analyses\n\nto be performed at various stages of the life\n\ncycle, including decisions supported, primary\n\nobjectives, and general approaches that can be\n\napplied. SEs frequently team with economic/\n\ncost analysts to perform investment analyses,\n\nand their familiarity with key aspects of prevalent\n\nanalytic approaches can improve the overall\n\nquality of completed analyses and the effi­\n\nciency of supporting activities performed.\n\n\n-----\n\n###### Background\n\nSeveral different analysis approaches are designed to inform sponsor funding and expendi­\nture decisions. These include Analysis of Alternatives, Business Case Analysis, Cost Benefit\nAnalysis, and Economic Analysis. This article provides an overview of the commonalities,\noverlaps, and differences among prevalent business, investment, and economic analysis\napproaches and of which analyses are required by different government organizations.\n\n###### Comparison of Key Investment Analysis Types\n �Analysis of Alternatives (AoA): An AoA is a technical assessment using distinct metrics\n\nand various decision criteria to objectively evaluate different potential courses of action\n(or alternatives). Typically the focus is on an analysis of alternative technical approaches\nfor meeting a given set of functional requirements or mission needs. Alternatives can, in\nspecial cases, be distinguished by different acquisition-related approaches if a real range\nof feasible technical alternatives does not exist. The assessment also includes a lifecycle cost estimate (LCCE) and risk assessment for each alternative and a preliminary\nrecommendation(s).\n###### �Business Case Analysis (BCA): A BCA is used to determine if a new approach and\n\noverall acquisition should be undertaken. A BCA provides justification for investment\n(either to invest or not) based on a comparison of life-cycle costs, benefits, and the\nresults of applying financially oriented calculations such as return on investment (ROI),\nnet present value (NPV), and discounted payback period (DPP) for each alternative. An\nAoA establishes the alternatives to be evaluated. Based on the results of the financial\nanalysis, a BCA helps determine if a potential new acquisition is warranted and if the\neffort should proceed.\n###### �Cost Benefit Analysis (CBA): The primary objective of a CBA is to identify and obtain\n\napproval for the optimum way to solve a specific problem or capitalize on a specific\nimprovement opportunity. A CBA documents the predicted effects (financial, opera­\ntional, quantitative, and qualitative) of actions under consideration and describes\nthe potential financial impacts and other business benefits to support a a decision\nregarding adopting a particular solution before significant funds are invested. Agencyspecific policies may differ, and CBAs within some organization may need to include\na detailed budget estimate and identification of funding sources for the preferred alter­\nnative. Many organizations consider a CBA a living document that should be updated\nas needed.\n###### �Economic Analysis (EA): An EA is a systematic approach to identifying, analyzing, and\n\ncomparing costs and benefits of alternative courses of action. An EA should empha­\nsize an identification of key variables that drive cost, benefit, risk, and uncertainty\n\n\n-----\n\nassessment results across competing alternatives. Depending on agency policy, an EA\nmay be very similar to a CBA. In some government organizations, EAs will include postinvestment management considerations, including the identification of key performance\nmeasures for monitoring and evaluating whether initiatives ultimately achieve expected\nor desired results.\nAoAs and BCAs are typically conducted early in the acquisition process for a new\ninformation technology (IT) initiative. For example, the Department of Defense (DoD) Joint\nCapabilities Integration and Development System (JCIDS) process identifies that they should\nbe conducted prior to Milestone A. For each subsequent year, they should be validated against\nthe baseline as costs, schedule, etc., may change.\n\nIn the past, many civilian agencies were not necessarily required to prepare distinct AoAs\nor BCAs but would, instead, rely on the Alternatives Analysis that was prepared in support\nof an Office of Management and Budget (OMB) Exhibit 300 submission, as required by OMB\nCircular A-11, Part 7. Although the E-300 is a fairly comprehensive document, a typical AoA or\nBCA will be more detailed and rigorous.\n\nThe DoD requires distinct and more comprehensive AoAs and BCAs primarily because\nDoD initiatives are often larger dollar investments than civilian IT initiatives. Civilian agen­\ncies must periodically check progress against the baseline and, as necessary, revise the\nE-300 annually. Most recently, a number of civilian agencies (e.g., Department of Homeland\nSecurity) have produced guidance for preparing AoAs or BCAs to support investment decision\nmaking.\n\nAoAs and BCAs are often well understood among IT acquisition professionals. The under­\nstanding of CBAs and EAs is often less clear. The terminology of these analyses is often used\ninterchangeably depending on a given agency and individual’s opinion. CBA and EA are basi­\ncally similar analysis approaches.\n\nTable 1 summarizes the general similarities and differences of the analytical approaches\nacross various agencies/communities.\n\n###### Best Practices and Lessons Learned\n\n\nMany identified best practices and lessons\n\nlearned for particular investment analyses will also\n\nbe relevant for other investment analysis types.\n\nAnalysis of Alternatives (AoA)\n\nDevelop viable alternatives. Alternative courses\n\nof action that could feasibly be pursued to achieve\n\n\nthe mission should be developed. An analysis\n\nshould not use “throwaway” alternatives to justify\n\na preconceived course of action.\n\nExamine three or more alternatives in addi­\n\ntion to any Status Quo (SQ) baseline. For\n\nexample, new alternatives for traditional soft­\n\nware development initiatives might include\n\n\n-----\n\nTable 1. Comparison of Analytical Approaches\n\n|Element|Alternatives Analysis|AoA|BCA|CBA|EA|\n|---|---|---|---|---|---|\n|To address a gap, should I invest or not?||x|x|||\n|I’m going to invest to address a gap. So how should I invest?|x|x|x|x|x|\n|Operational effectiveness||x|x|x|x|\n|LCCE|x|x|x|x|x|\n|Qualitative cost assessment||x|x|x|x|\n|Quantitative benefits assessment|x||x|x|x|\n|Qualitative benefits assessment|||x|x|x|\n|ROI calculation|x||x|x|x|\n|Uncertainty analysis|x||x|x|x|\n|Risk analysis||x|x|x|x|\n|Sensitivity analysis|||x|x|x|\n|Implementation description||x|x|||\n\n\ncommercial-off-the-shelf (COTS), custom\n\nsoftware development, and COTS/software (SW)\n\ndevelopment hybrid. To meet certain require­\n\nments, it may be impractical to use all COTS or\n\nto solely embark on a custom software devel­\n\nopment. A well-developed AoA should include\n\ntechnically distinct, realistic alternatives. The SQ\n\nalternative should be viable; in particular, SQ does\n\nnot necessarily mean “do nothing.” For example,\n\ninclusion of technology refresh in the future\n\nmay be necessary if it is essential to maintaining\n\nrequired functionality.\n\nAlternatives need to be developed in collabo­\n\nration with technical subject matter experts\n\n(SMEs). An AoA/BCA author should not invent\n\nalternatives for the purpose of checking a box and\n\nmeeting a capital planning requirement. Technical\n\nSMEs should be consulted to think through the\n\nimplications of how to best fulfill mission needs.\n\n\nAlternatives can be differentiated by acquisition\n\napproaches. In certain cases, there may be few\n\ndistinct technical approaches for fulfilling mission\n\nneeds.\n\nEstablish evaluation criteria and scoring. Identify\n\nmultiple, independent metrics and compare how\n\neach alternative impacts them. Metrics may be\n\nquantitative or qualitative. Every metric does not\n\nneed to be equally important. Weightings can be\n\napplied to reflect priorities.\n\nIdentify technical and cost risks. Risks with each\n\nalternative should be identified, including ease of\n\nimplementation, relative difficulty to meet certain\n\nchallenging requirements, as well as an assess­\n\nment of the probability that an alternative can\n\nmeet cost and schedule goals.\n\nLife-cycle cost estimating. An AoA should\n\ninclude a multiyear life-cycle estimate generally\n\nbroken out by acquisition, operations and support\n\n\n-----\n\n(O&S), and SQ phase-out costs. A typical DoD\n\napproach is to analyze O&S costs after 10 years\n\npost-system full operational capability. This would\n\ninclude at least one tech refresh cycle and enable\n\nthe analysis to calculate financial metrics and\n\nbenefits for each alternative over a longer time\n\nperiod.\n\nRecommendations that consider environmen­\n\ntal changes. It is useful to recommend a specific\n\nalternative to pursue, and this can be done in\n\nconjunction with a subsequent BCA, as needed. In\n\ncertain cases, it is useful to present a few recom­\n\nmendations that take into account likely changes\n\nto the surrounding environment. For example,\n\nAlternative 1 may be the preferred recommenda­\n\ntion if the program receives adequate funding\n\nbut not if its funding is cut. Recommendations\n\nfor specific alternatives can be tied to potential\n\nfuture situations that may arise (which should\n\nalso be documented in the ground rules and\n\nassumptions).\n\nGovernment/sponsor buy-in. An AoA is some­\n\ntimes viewed as a required check-the-box activity\n\nthat is a necessary part of capital planning. For\n\nprograms requiring large expenditures of funds,\n\nhowever, an AoA is a useful means for the pro­\n\ngram office to fully consider what must be done to\n\nmove forward in an effective and efficient manner.\n\nThe approach requires a degree of rigor that will\n\nhelp the program succeed in the long term.\n\nBusiness Case Analysis (BCA)\n\nIdentify potential benefits for each alternative.\n\nIn contrast to an AoA, a BCA identifies measur­\n\nable benefits for each alternative. An example of\n\na measurable, or quantitative, benefit includes\n\n\nan increase in productivity. Although qualita­\n\ntive benefits such as morale, better information\n\nsharing, and improved security cannot be readily\n\nmeasured, they may still be important to alterna­\n\ntive selection.\n\nDevelop ROI statistics. Key financial metrics\n\nhelp gauge the ROI of alternative courses of\n\naction relative to the SQ alternative. No single\n\ncalculation can fully describe ROI, and finan­\n\ncial metrics typically calculated to support this\n\ndescription include net present value (NPV),\n\nrate of return (ROR), and discounted payback\n\nperiod (DPP). These three metrics take into\n\nconsideration the “time value of money” (i.e., a\n\ndollar received today is worth more than a dollar\n\nreceived one year from now due to factors such\n\nas inflation). All calculations, except NPV, are\n\ntypically performed relative to the SQ alterna­\n\ntive. In other words, the calculations take into\n\naccount how significantly an alternative course\n\nof action increases or decreases costs and ben­\n\nefits relative to the SQ. The NPV for an alterna­\n\ntive describes current and future net expected\n\nbenefits (i.e., expected benefits less expected\n\ncosts) over the life cycle for that alternative. ROR,\n\nstated as a percentage, describes the discount\n\nrate at which the incremental current and future\n\nbenefits of an alternative in comparison to the\n\nSQ equal the incremental costs of that alterna­\n\ntive relative to the SQ baseline. DPP, stated in\n\nunits of time, describes how much time it will\n\ntake for the cumulative incremental benefits of\n\nan alternative course of action relative to the SQ\n\nalternative to exceed the cumulative incremental\n\ncosts of an alternative course of action.\n\n\n-----\n\nClarify the acquisition/implementation timeline.\n\nA clear timeline is important to see how costs are\n\nphased in over an investment and O&S period.\n\nPrograms typically have from two- to five-year\n\ninvestment periods depending on the size of the\n\ninitiative and the type of acquisition approach.\n\nIdentify the risks and potential benefit impact.\n\nThis pertains specifically to the risks that the\n\nLCCE will deviate from the baseline estimate.\n\nSpecific major cost drivers that are risky should be\n\ncalled out. Software development labor is particu­\n\nlarly prone to potential risk of under-estimation.\n\nThere are others. The risks that certain benefits\n\nwill not be realized should also be identified. This\n\nis similar in concept to identifying cost risks.\n\nCost Benefit Analysis (CBA)\n\nAlternatives may differ from those in the\n\nAoA and BCA and be analyzed in more detail.\n\nDepending on how much detail was provided\n\nin the AoA and BCA, a CBA should break down\n\nthe preferred alternative into more specificity. A\n\nCBA may focus on how best to implement the\n\npreferred alternative, which requirements may\n\nyield the biggest “bang for the buck” in higher ROI\n\nearlier in the life cycle, how different contracting\n\napproaches may reduce risk, etc.\n\nCan analyze multiple alternatives. The standard\n\nrule of thumb for a BCA is three new alternatives\n\nplus the SQ (policies on the number of alterna­\n\ntives to evaluate may vary, and the number may\n\nbe predicated on the problem being addressed). A\n\nCBA can offer many more, if necessary, with minor\n\nvariations among each to help determine the best\n\nspecific approach forward. It will still follow the\n\nbasic format of a BCA, although the analysis for\n\n\neach alternative approach may not be as compre­\n\nhensive as in the AoA/BCA.\n\nAllows for incremental ROI results. A CBA is\n\nparticularly useful for demonstrating the quantita­\n\ntive benefits that specific actions within a work\n\nbreakdown structure (WBS) may yield. Therefore,\n\na CBA is well suited to analyze if one alternative\n\noffers a greater ROI than a competing alternative.\n\nIncremental financial analysis helps decision mak­\n\ners move forward by considering which alterna­\n\ntive approach at a given point along an integrated\n\nmaster schedule (IMS) will yield a greater “bang\n\nfor the buck” to justify an investment decision at a\n\nparticular acquisition milestone or increment.\n\nEconomic Analysis (EA)\n\nPerform economic sensitivity analyses for\n\nkey variables. EAs are often evaluated over the\n\ninvestment life cycle, and it is often unreason­\n\nable to assume perfect knowledge of what\n\ncosts, benefits, and risks will be at all points in\n\nthe future. For analysis variables that significantly\n\ndrive analysis results and for which there is either\n\n(a) considerable uncertainty or (b) consider­\n\nable impact if unlikely conditions actually occur,\n\nsensitivity analyses or uncertainty-weighted\n\nassessments should be performed to determine\n\nthe potential impact of uncertainty on analysis\n\nresults.\n\nEvaluate all significant economic implications,\n\nbut not necessarily in monetary terms. In the\n\nfederal government, many investment costs,\n\nbenefits, and risks are not readily translated into\n\nmonetary terms. Regardless, if these implications\n\nare critically important to informing investment\n\ndecisions, they should be evaluated. Analysts\n\n\n-----\n\nshould consider whether other quantitative\n\nmethods (e.g., comparison of numeric ratings for\n\naspects of investment options) can be applied to\n\n###### References and Resources\n\n\nassess the relative strengths and weaknesses of\n\ninvestment options.\n\n\nAFMAN 65-506, “Economic Analysis,” August 29, 2011, U.S. Air Force.\n\nArmy Cost Benefit Guide, April 8, 2011, Office of the Deputy Assistant Secretary of the Army.\n\nBuck, K. “Conducting a Business Case Analysis,” The MITRE Corporation.\n\nBuck, K. “Economic Analysis Documentation,” The MITRE Corporation.\n\nOMB Circular A-11, Part 7 (“Planning, Budgeting, Acquisition, and Management of Capital\nAssets”), Exhibit 300.\n\n\n-----\n\n##### Source Selection Preparation\n and Evaluation\n\nDefinition: Source selection is a critical phase of the pre-award procurement\n\n_process. It has been thoroughly discussed in regulations and procurement litera­_\n\n_ture. Source selection is often thought of as making trade-offs among offerors’_\n\n_proposals to determine the best value offer._\n\nKeywords: advisory multi-step process, best value determination, down-select,\n\n_evaluation, proposal evaluation, source selection, technical evaluation_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to create technical and\n\nengineering portions of request for proposal (RFP) documentation\n\n(requirements documents, statement of work, evaluation criteria), assist\n\nin developing the technical portions of source selection plans, and assist\n\nin the technical evaluation of bidders. MITRE SEs also are expected to\n\nencourage agency program and acquisition managers to build effec­\n\ntive processes into their acquisition strategies. Increasing the program\n\noffice’s likelihood of success often requires acting as an intermediary\n\nbetween the government and contractors to objectively and indepen­\n\ndently assess the degree to which proposed solutions or courses of\n\naction will provide the capabilities needed to meet the government’s\n\nneeds. This includes conducting assessments of the risk inherent in\n\nproposed solutions—including strategies for acquiring (or implementing)\n\nthem—and identifying actionable options for mitigating those risks.\n\n\n-----\n\n###### Background\n\nSource selection has been thoroughly discussed in regulations and procurement literature.\nOne definition would not give it justice. Here are two of the widely used definitions:\n\nAccording to Federal Acquisition Regulation (FAR) 15.3, source selection is the “selec­\ntion of a source or sources in competitive negotiated acquisitions...The objective of\nsource selection is to select the proposal that represents the best value [1].”\n\nThe Department of Homeland Security (DHS) Guide to Source Selection defines source\nselection as “the process used in competitive, negotiated contracting to select the\nproposal expected to result in the best value to the Government. By definition, negotia­\ntion is contracting without using sealed bidding procedures. It is accomplished through\nsolicitation and receipt of proposals from offerors; it permits discussions, persuasion,\nalteration of initial positions, may afford offerors an opportunity to review their offers\nbefore award, and results in award to the proposal representing the best value to the\nGovernment.”\n\nSource selection is not an isolated aspect of the acquisition life cycle; instead, it is a key\nphase of the life cycle shown in Figure 1. In order for source selection to be successful, the\nprecursor phases of the life cycle (need identification, market research, requirements defini­\ntion, strong acquisition planning, solicitation development, and proposal solicitation) must be\ncompleted effectively.\n\nThe source selection approach should be captured in a source selection plan. The plan\nshould include the proposal evaluation criteria. Selecting appropriate evaluation factors is\none of the most important steps in the entire source selection process. The source selection\nplan explains how proposals are to be solicited and evaluated to make selection decisions. It\ndefines the roles of the source selection team members. A realistic schedule also should be\nincluded in the plan.\n\nThe article “Picking the Right Contractor” describes best practices and lessons learned\nin the pre-proposal and selection process, including ways to involve Industry to improve the\nlikelihood of a better source selection outcome. The article “RFP Preparation and Source\nSelection” will walk you through the RFP process, typical MITRE systems engineering roles,\nand the important points of the selection process. Both articles contain best practices and les­\nsons learned for the preparation and evaluation processes.\n\n###### Best Practices and Lessons Learned\n\n\nAdvocate the right definition of success. Some\n\norganizations define “acquisition success” as the\n\n\nawarding of the contract. Once the contract is\n\nawarded (without a protest), victory is declared.\n\n\n-----\n\nFigure 1. Key Phases of the Acquisition Life Cycle\n\n\nAlthough contract award is one of several\n\nimportant milestones, this limited view of acqui­\n\nsition tends to overlook the need to adequately\n\nconsider what it will take to successfully execute\n\nthe acquisition effort in a way that achieves the\n\ndesired outcomes. It leads to a “ready, fire, aim”\n\napproach to acquisition planning. Advocate for a\n\nbroader view of acquisition success, one that bal­\n\nances the desire to award a contract quickly with\n\nadequate planning, program management, and\n\nsystems engineering across the entire system or\n\ncapability life cycle.\n\nThe importance of planning. The importance of\n\nconducting adequate acquisition planning before\n\nrelease of the RFP cannot be overstated. This\n\nincludes encouraging clients to take the time to\n\nconduct market research and have dialog with\n\nindustry so the government becomes a smart\n\nbuyer that recognizes what is available in the\n\nmarketplace, including the risks and opportuni­\n\nties associated with being able acquire solutions\n\nthat meet their needs. This insight allows the\n\n\ngovernment to develop a more effective source\n\nselection strategy, which includes choosing more\n\nmeaningful evaluation factors (or criteria) that\n\nfocus on key discriminators, linked to outcome\n\nmetrics. Concentrating on a few key differenti­\n\nating factors can also translate into a need for\n\nless proposal information instead of asking for\n\n“everything,” which tends to occur when not\n\ncertain what is important. Adequate acquisition\n\nplanning helps ensure that the source selection\n\nprocess will go smoothly, increases the probability\n\nof selecting the best solution, and reduces the risk\n\nof protest.\n\nMaintain the right focus. Focusing on mission/\n\nbusiness outcomes instead of detailed techni­\n\ncal specifications broadens the trade space\n\nof potential innovative solutions that industry\n\n(potential contractors) may offer. It can increase\n\nindustry’s ability to use commercial items and/\n\nor non-developmental items to fulfill government\n\nneeds.\n\n\n-----\n\nFollow your process. The evaluation documen­\n\ntation must provide a strong rationale for the\n\nselection decision. During the proposal evalua­\n\ntion phase, a critical lesson is to ensure that the\n\nevaluation team does not deviate from the stated\n\nRFP evaluation factors. General Accounting Office\n\ndecisions clearly indicate that use of factors other\n\nthan those published in the RFP almost guaran­\n\ntees that a bid protest will be sustained. At a mini­\n\nmum, the source selection documentation must\n\nidentify weaknesses, significant weaknesses, and\n\ndeficiencies as defined by FAR 15.001 Definitions\n\n[1]. Good documentation also identifies strengths\n\nand risks.\n\nThe importance of industry exchanges.\n\nIncreased communication with industry through\n\npresolicitation notices, information exchanges,\n\nand draft RFPs makes the acquisition process\n\nmore transparent and may lower the likelihood\n\nof a protest. These techniques can be an effec­\n\ntive way to increase competition, especially when\n\nthere is a strong incumbent. Exchanges with\n\nindustry are especially important when the pro­\n\ncurement requirements are complex.\n\nHandling sensitive proposal information—a\n\ncritical requirement. To maintain the integrity of\n\n###### References and Resources\n\n\nprocurement, sensitive source selection infor­\n\nmation must be handled with discretion to avoid\n\ncompromise. All government team participants\n\nshare the critical responsibility to ensure that\n\nsource selection and proprietary information is\n\nnot disclosed. There is no room for error. Any\n\nlapses by MITRE individuals not only could com­\n\npromise the integrity of a federal procurement but\n\nalso could damage MITRE’s relationship with the\n\ngovernment.\n\nClarity of evaluation factors. It is not unusual for\n\nthe government to ask MITRE SEs to help draft\n\nproposal evaluation factors (Section M) for a\n\nsolicitation. The focus should be on the key dis­\n\ncriminators that will help distinguish one proposal\n\nfrom another. Cost must always be one of the\n\nfactors, along with such factors as mission capa­\n\nbility, similar experience, past performance, and\n\nkey personnel. Many solicitations are often vague\n\nabout the relative weights among such evaluation\n\nfactors as cost. These ambiguities often lead to\n\nsuccessful protests. It is important to do every­\n\nthing possible to ensure that the relative weights\n\nof the factors are as clear as possible in the minds\n\nof the potential offerors and the government\n\nevaluation team.\n\n\n[1. Federal Acquisition Regulation (FAR), http://www.acquisition.gov/far/, accessed](http://www.acquisition.gov/far/)\n\nFebruary 5, 2014.\nFAR 15.001 Definitions\nFAR 15.1 Source Selection Processes and Techniques\nFAR 15.202 Advisory Multi-step Process\nFAR 15.3 Source Selection\n\n\n-----\n\n###### Additional References and Resources\n\nAsset Reuse (Procurement): The Use of Industry Exchange to Increase Competition.\n\nMITRE P&P CR 3.2 Support to Sponsors’ Source Selection Proceedings.\n\nMITRE’s Input to the Data Access and Dissemination Systems (DADS) Cost Evaluation Lessons\nLearned, September 25, 2009.\n\nNCMA World Congress, April 22–25, 2007, Source Selection: Best Practices in Streamlining the\nProcess.\n\n\n-----\n\nDefinition: The Advisory Multi\n_step Process is a presolicitation_\n\n_process that can help to help_\n\n_define requirements, streamline_\n\n_competition, increase competi­_\n\n_tion, and reduce the likelihood_\n\n_of a protest._\n\nKeywords: advisory multi-step\n\n_process, down-select, draft_\n\n\nSOURCE SELECTION PREPARATION AND\nEVALUATION\n###### Picking the Right Contractor\n\n\n_RFP, exchanges with industry_ **MITRE SE Roles and Expectations: In the**\n\nearly planning stages of a major acquisition,\n\nMITRE systems engineers (SEs) and acquisi­\n\ntion experts are expected to encourage agency\n\nprogram and acquisition managers to build\n\ninto their acquisition strategies sufficient time\n\nto employ the Advisory Multi-step Process in\n\nconjunction with Industry Exchanges and draft\n\nRFPs. These presolicitation techniques are\n\npowerful tools that can help define require­\n\nments, streamline competition, increase com­\n\npetition, and reduce the likelihood of a protest.\n\nThese techniques can be used to increase the\n\nprobability of a successful source selection.\n\n\n-----\n\n###### How the Advisory Multi-step Process Works\n\nThe Advisory Multi-step Process is described in the FAR at 15.202 [1]. Generally, in an advi­\nsory multi-step process, agencies issue a presolicitation notice inviting potential offerors to\nsubmit sufficient information to allow the government to judge whether the offeror could be a\nviable competitor. The presolicitation notice should identify the information that must be sub­\nmitted and the criteria that will be used in making the evaluation. Some examples of informa­\ntion include a statement of qualifications, proposed technical concept, past performance, and\nlimited pricing information. The presolicitation notice should contain sufficient information\nto permit a potential offeror to make an informed decision about whether to participate in the\nacquisition. Examples include asking for information on specific existing technologies of a\nmaturity to be demonstrable in a laboratory environment, or asking for experience with a new\ndevelopment technique or technology. The information should allow for discrete differentia­\ntion between industry respondents that can clearly inform industry of their ability to compete.\n\nThe agency must evaluate all responses in accordance with the criteria stated in the\nnotice and must advise each respondent in writing whether it will be invited to participate\nin the resultant acquisition or, based on the information submitted, that it is unlikely to be a\nviable competitor. The agency must advise respondents considered not to be viable competi­\ntors of the general basis for that opinion. Notwithstanding the results of this initial evaluation,\nall respondents may participate in the resultant acquisition.\n\n###### Best Practices and Lessons Learned\n\n\nA more manageable and efficient source\n\nselection process. This multi-step technique has\n\nbeen used to produce a more manageable and\n\nefficient source selection process; potential offer­\n\nors learn early in the process that they may not\n\nbe able to compete effectively. Industry benefits\n\nby avoiding expenditure of unnecessary business\n\ndevelopment resources. Government benefits\n\nby avoiding the expenditure of scarce evaluation\n\nresources on weak proposals. This technique\n\ncan be used to make the evaluation process\n\nmore manageable and streamlined in a situation\n\nwhere an agency is expecting to receive a large\n\nnumber of offers. On the other hand, the tech­\n\nnique has also been used successfully to increase\n\n\ncompetition when a strong incumbent may be\n\ndiscouraging competition.\n\nIn both cases, the likelihood of a protest is\n\nreduced when there is clear, frequent, and fair\n\ncommunication with Industry.\n\nMore open communication with industry.\n\nA logical follow-on to the Advisory Multi-step\n\nProcess is to conduct a series of information\n\nexchanges with those respondents who have\n\nbeen determined to be viable competitors. Before\n\nreceipt of proposals, the FAR allows for one\non-one interaction with Industry. FAR 15.201(b)\n\nstates: “The purpose of exchanging information\n\nis to improve the understanding of Government\n\n\n-----\n\nrequirements and industry capabilities.” These\n\none-on-one sessions can be powerful opportu­\n\nnities for open communication. Experience has\n\nshown that open forums serve to inform Industry,\n\nbut do not inform the government of industry\n\ncapabilities. The normal hesitation to ask the\n\nimportant questions in a large forum evaporates in\n\nprivate sessions when competitors are not in the\n\nroom. Moreover, the benefits to the government\n\nfrom these sessions include better understanding\n\nof requirements and an improved solicitation.\n\nA 2007 NCMA World Congress presentation\n\ncalls exchanges with industry a best practice for\n\nthe solicitation phase. One of the critical lessons\n\nlearned on a major civilian agency acquisition con­\n\ncerned exchanges with industry: “The technical\n\nexchange...led to RFP modifications improving the\n\nsolicitation, and to a better understanding of the\n\nGovernment’s requirements [2].”\n\nChanging the competitive environment with\n\na draft RFP. When there is sufficient time in the\n\npresolicitation phase of a procurement, agen­\n\ncies should be encouraged to issue draft RFPs\n\nas a key attachment to the presolicitation notice\n\nissued under the Advisory Multi-step Process.\n\nDraft RFPs provide the information needed to\n\nhelp potential respondents make decisions about\n\n###### References and Resources\n\n\nwhether to participate. In addition, draft RFPs\n\ninform the industry exchanges described earlier.\n\nLessons learned from multiple agency procure­\n\nments indicate that draft RFPs inform industry of\n\nthe desire for open competition and increase the\n\ncompetitive field, thereby allowing for stronger\n\ntechnical and cost competition and providing\n\nclearer direction and requirements for the RFP.\n\nDraft RFPs can change the competitive environ­\n\nment of an acquisition, especially when there has\n\nbeen a long-term incumbent.\n\n###### � [Industry exchanges should be used to ]\n\nhelp the government communicate more\n\nclearly, especially when the requirements\n\nare complex.\n###### � [Multi-step techniques can increase tech ]\n\nnical and cost competition.\n###### � [Multi-step techniques are an effective way ]\n\nto increase competition when there is a\n\nstrong incumbent.\n###### � [Pre-solicitation planning has a high payoff ]\n\nfor all participants.\n###### � [Increasing communication with industry ]\n\nthrough presolicitation notices, informa­\n\ntion exchanges, and draft RFPs makes the\n\nacquisition process more transparent and\n\nlowers the likelihood of a protest.\n\n\n1. Federal Acquisition Regulation (FAR) 15.202 Advisory Multi-step Process, effective\n\nNovember 13, 2009.\n\n2. Zwiselsberger, D., A. Holahan, and J. Latka, April 25, 2007, “Source Selection: Best\n\nPractices in Streamlining the Process,” NCMA World Congress, April 22–25, 2007.\n\n\n-----\n\nDefinition: RFP preparation and\n\n_source selection are the actions_\n\n_necessary to prepare for a_\n\n_government solicitation and to_\n\n_select one or more contrac­_\n\n_tors for delivery of a product or_\n\n_service._\n\nKeywords: acquisitions,\n\n_competitive procurement,_\n\n_non-competitive procurement,_\n\n_proposal, RFP, RFP develop­_\n\n_ment, source selection, strategy_\n\n\nSOURCE SELECTION PREPARATION AND\nEVALUATION\n###### RFP Preparation and Source Selection\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to cre­\n\nate technical and engineering portions of\n\nRequest for Proposal (RFP) documentation\n\n(requirements documents, statement of work,\n\nevaluation criteria) and to assist in the technical\n\nevaluation of bidders during source selection.\n\n\n-----\n\n###### Background\n\nA program’s acquisition strategy addresses the objectives for the acquisition, the constraints,\navailability of resources and technologies, consideration of acquisition methods, types of con­\ntracts, terms and conditions of the contracts, management considerations, risk, and the logis­\ntics considerations for the resulting products. The acquisition strategy identifies the context for\ndevelopment of the RFP and source selection as either a “competitive” or “non-competitive”\nprocurement. The requirements contained in the RFP, and the contractor(s) selected during\na procurement, can often determine the success or failure of a system for the duration of a\nprogram. Using a process to develop the RFP and conduct a source selection can significantly\nimprove the likelihood of success. Doing it right the first time is critical—rarely does a pro­\ngram have a chance to do it again.\n\n###### Competitive Procurement\n\nIn a competitive procurement, two or more contractors, acting independently, are solicited\nto respond to the government RFP. Their proposals are evaluated during source selection,\n\nRequirements & Requirements & Risk\nProgram Planning Program Planning Management\nProcesses Processes Process\n\nPrepare for Develop Conduct Implement Develop\n\nIdentify RFP dev. & technical market risk acquisition\nuser needs source requirements research management plan\n\nselection\n\nNo\n\nDevelop & Source Develop Develop\n\nScreen Multi\nrelease selection evaluation solicitation\n\nofferors step?\n\nRFP Yes planning criteria documents\n\nWithout discussion\n\nWith\n\nType disc.\n\nevaluationInitial selectionsource disussionsEvaluationwith Evaluationproposalof split Award debriefingsConduct\n\n?\n\nSplit proposal\n\nFigure 1. Competitive Procurement Actions\n\n\n-----\n\nand the contract is awarded to the contractor(s) who offers the most favorable terms to the\ngovernment.\n\nCompetitive procurement activities include the preparation steps that lead to development\nof the acquisition strategy, which as noted above, provides the basis for developing the RFP\nand conducting the source selection. Leading up to this are the development of solicitation\ndocuments, evaluation criteria, and source selection approach. If the program office wants to\nreduce the number of contractors proposing, it can conduct a multi-step competitive procure­\nment. One example of this is to use competitive prototyping as a step to further evaluate the\nqualifications of competing contractors under more representative conditions. Depending on\nthe amount of development required for the program (and under DoD 5000.02), competitive\nprototyping should not only be recommended but required. The overall activities in a competi­\ntive procurement are illustrated in Figure 1.\n\n**RFP Development Process**\n\nRFP development is part of the overall procurement process. The actions necessary for devel­\nopment and release of the RFP are shown in Figure 2.\n\nThe acquisition strategy provides the overall guidance for development of the RFP, and\nthe work breakdown structure (WBS) provides the definition of the program and guides the\ncontractor in creating the contract WBS. The specifications or technical/system requirements\ndocument (TRD/SRD), the statement of objectives or work (SOO/SOW), and the contract data\n\n\nWork\nBreakdown\nStructure\n(WBS)\n\n\nRequest for\nProposal\n(RFP)\nRelease\n\n\nAcquisition\nStrategy\n\n|Col1|Specifications or TRD/SRD Statement of Objectives/ Work (SOO/SOW) Contract Data Requirements List (CDRL)|Source Selection Evaluation Criteria & IFPP Draft Solicitation Request for Review|\n|---|---|---|\n||||\n|||Proposal Board (DRFP) Contract Documents (Sec A–J)|\n||||\n\n\nFigure 2. RFP Development Actions\n\n\n-----\n\nrequirements list (CDRL) form the technical basis of the RFP. These are usually the focus of\nMITRE SEs on RFPs.\n\nAn RFP is divided into sections A–M. Sections A–J are primarily contract documents,\nexcept for section C, which is the SOO or SOW. Section K contains attachments like the TRD/\nSRD, section L is the Instructions For Proposal Preparation, and section M is the evalua­\ntion criteria. MITRE is often asked to participate in the construction of sections L and M.\nEvaluation criteria are another critical component of the RFP. Technical criteria need to be\nincluded and need to address areas of technical risk and complexity.\n\n**Source Selection**\n\nIn a competitive procurement of a system/project, source selection is the process wherein\nproposals are examined against the requirements, facts, recommendations, and government\npolicy relevant to an award decision, and, in general, the best value proposal is selected. The\nactions shown in Figure 3 are those generally conducted during source selection. The focus of\nMITRE’s participation in source selections is the evaluation of the technical proposal and the\nresulting risk assessment.\n\n###### Non-Competitive Procurement\n\nAlthough it is not a common initial procurement approach, on occasion non-competitive\nprocurement is necessary to meet government needs for certain critical procurements. This\napproach is more commonly used with a contractor who is already on contract with the\ngovernment (but not necessarily the same organization doing the procurement) providing a\nsimilar capability, or when it is clearly advantageous to use the non-competitive approach in\nsubsequent contract changes or new solicitations for an existing program.\n\nAs with competitive procurement, the actions taken in a non-competitive procurement\ninclude the preparation steps that lead to development of the acquisition strategy. Prior to\ndevelopment of the solicitation documents that constitute the RFP, the program office must\nsubmit Justification & Approval (J&A) documentation to the appropriate agency office to\nreceive approval for the non-competitive procurement. Occasionally there is a technical reason\nfor using a particular contractor, and MITRE is involved with generating the J&A. With this\napproval, the program office can develop the solicitation documents and enter into collabora­\ntive contract development with the contractor. On completion of the collaborative contract\ndevelopment, the program office evaluates, negotiates, and awards a contract with many of\nthe steps indicated above. MITRE is most often used to evaluate the proposal for technical\napproach and resources/engineering hours.\n\n\n-----\n\n|Evaluation worksheets Database|Col2|\n|---|---|\n|||\n\n|S mDuaorb rkeef ira m spchoat rdo tea nr--ts Factors|Subfactor 1|Subfactor 2|Subfactor 3 Subfactor 4|Subfactor 5|Subfactor 1|Subfactor 2 Subfactor 3|Subfactor 4 Subfactor 5|\n|---|---|---|---|---|---|---|---|\n|HSCiioggnnhfi ifCdiceoanPnncftie daCeosnnctfei dPenceerforLNmiott leCa oCnnofnidcfeidenecnece|Confidence||||CSoignnfiidfiecnacnet|||\n|BGlrueMee n(E i(x (As HcPcesc )p iergtipiooo htan pnba (ll Me) o) )C osda ae rp laYR ta Re eeld,lo b (i(w LUsi )n(l koMiat wcaycrgeipntaalb)le),|G M|G M|YB ML|G L|G Y LM|G L|G G ML|\n\n|A B|Col2|Col3|Col4|\n|---|---|---|---|\n||B|||\n|A||B||\n\n|S mDuaorb rkeef ira m spchoat rdo tea nr--ts Factors|Subfactor 1 Subfactor 2 Subfactor 3|Subfactor 4 Subfactor 5 Subfactor 1|Subfactor 2 Subfactor 3|Subfactor 4|Subfactor 5|Col7|\n|---|---|---|---|---|---|---|\n|iioggnnhfi ifCdiceoanPnncftie daCeosnnctfei dPenceerforLNmiott leCa oCnnofnidcfeidenecnece|Confiden|ce CSoignnfiidfiecancnet|||||\n|BGlrueMee n(E i(xAsccescpetipiootannballe) )CapYRaeeldlob (wUi n(lMiatcaycrgeipntaalb)le)|G G Y|B GG Y|G|G|G||\n|(HP)irgoh, p(Mo)osdaerla tRe, (iLs)kow|MMM M|LLLM PC $ Propose|L d M|M PC $|L||\n\n\nFigure 3. Source Selection Actions\n\n###### Best Practices and Lessons Learned\n\n\nGetting the most bang for your bucks—­\n\nmarket research and competitive prototyping.\n\nAlthough it is time-consuming, spending time\n\nresearching the state of the art and visiting with\n\ncontractors and vendors will give you a good\n\nsense of what’s achievable for program require­\n\nments. In competitive procurements, solicita­\n\ntions are very helpful in determining the range of\n\navailable developers/suppliers. Solicitations may\n\nalso be used to perform work toward the acquisi­\n\ntion; meaning asking industry to submit papers\n\n\nand demonstrations prior to the release of an RFP.\n\nMITRE, as an operator of FFRDCs, may review\n\nthis kind of proprietary information and use it as\n\na basis for validating technology or assumptions\n\nabout requirements. Such feedback from industry\n\nmay also be useful for refining the evaluation\n\ncriteria for the RFP.\n\nCompetitive prototyping can be used to require\n\ncompeting developers to demonstrate applicable\n\ntechnology or services, along with engineering\n\nprocess and documentation (as examples) to\n\n\n-----\n\nenable better evaluation of their overall abilities to\n\ndeliver the full program. It may also be used as a\n\ntechnique to reduce risk in complex or unproven\n\ntechnical areas. For more information on com­\n\npetitive prototyping, see the article “Competitive\n\nPrototyping” under the SEG’s Contractor\n\nEvaluation topic.\n\nThe right level of detail for a WBS. The WBS is\n\noften the foundation used for determining con­\n\ntractor progress and earned value during the pro­\n\ngram development and deployment phases. As\n\nsuch, it needs to be structured to provide enough\n\ndetail to judge sufficient progress. WBS elements\n\nshould be broken down into efforts no larger\n\nthan 60 days per unit. At least 90+ percent of the\n\nWBS elements must be measureable in dura­\n\ntions of 60 days or less. This allows you to track\n\nWBS completion on a quarterly basis and get a\n\ngood idea of progress on a monthly basis. Each\n\nWBS item should only have three reporting states:\n\nzero percent complete (not started), 50 percent\n\ncomplete (at least 50 percent complete), 100 per­\n\ncent complete (done). This allows you to track the\n\nWBS status without overestimating the percent\n\ncomplete. It is possible if the development effort\n\nis quite large that this level of detail may result in a\n\nvery large integrated schedule and plan later (see\n\nthe article “Integrated Master Schedule (IMS)/\n\nIntegrated Master Plan (IMP) Application”), but you\n\nwant the WBS foundation to allow for as much or\n\nas little detail as may be applied later on. Keeping\n\nthe WBS at a high level will definitely cause an\n\ninability to judge accurate progress during pro­\n\ngram execution, and lead to late identification of\n\ncost and schedule risks.\n\n\nWhat matters in the RFP. Depending on the\n\nacquisition strategy chosen, the completeness\n\nof the TRD/SRD is critical. Programs expecting to\n\nevolve over time through “Agile” or “Evolutionary”\n\nacquisition strategies will need to have carefully\n\nchosen threshold requirements specified for the\n\ninitial delivery of capability; ones that are achiev­\n\nable within the allotted schedule for that first\n\ndelivery. Requirements to be satisfied in a later\n\ndelivery of capability may be less stringent if they\n\nare apt to change before being contracted for\n\ndevelopment. In a more traditional acquisition\n\nstrategy where all requirements are to be satisfied\n\nin one or two deliveries of capability, the TRD/SRD\n\nmust be complete.\n\nAnother point to remember is that TRD/SRDs and\n\nSOO/SOW form the basis of testing—both sets\n\nof documents need to be written with a focus on\n\nperformance and test. Waiting until test prepara­\n\ntion is too late to discover that requirements were\n\nnot stated in a manner that is quantifiable or test­\n\nable. For more information on requirements and\n\ntesting, see the System Design and Development\n\nand Test and Evaluation topics in the SEG’s SE\n\nLife-Cycle Building Blocks section.\n\nEvaluation criteria need to be comprehensive\n\nand specific enough to allow clear differentia­\n\ntion between offerors, especially on those areas\n\nof requirements of critical importance to the\n\nsuccess of the program—try a sample proposal\n\nagainst the criteria to see if they are in fact selec­\n\ntive enough. There have been cases where the\n\ncriteria have not been expansive enough and\n\ndifferentiating technical information found in the\n\nproposals to be relevant to selection could not be\n\nconsidered for evaluation. Beyond just the written\n\n\n-----\n\ncriteria, consider requiring the offerors to provide\n\nand follow their risk management process or\n\nsoftware development plan as part of an exercise\n\nor demonstration.\n\nSource selection—be prepared. MITRE engi­\n\nneers responsible for evaluating technical propos­\n\nals need to be well versed in applicable current\n\ntechnology for the program. If a proposal contains\n\na new technical approach that is unfamiliar, per­\n\nform the research to determine the viability. Do\n\nnot assume the approach is low risk or common­\n\nplace; do the research to determine feasibility, risk\n\n,and the proposing contractor’s familiarity with it.\n\nConsult with MITRE experts in our “tech centers”\n\nto provide expertise in areas where program staff\n\nare limited in depth of knowledge; getting the right\n\nassistance in source selection is critical to choos­\n\ning the right contractor. You don’t get another\n\nchance!\n\nThe danger of “leveling.” During the source\n\nselection, offerors are often asked clarifica­\n\ntion questions in writing, or asked to provide oral\n\nproposals and questions/answer sessions. The\n\nresult of several iterations of these information\n\nexchanges among the offerors can result in or\n\nlook like “leveling;” when the government team has\n\neffectively obtained similar information and tech­\n\nnical approaches across the offerors, resulting\n\nin similar risk and allowing a final selection based\n\npurely on cost. Beware that this is usually not the\n\n###### References and Resources\n\n\nfactual result but a perception and result of itera­\n\ntive clarification calls: that all the offerors have\n\nprovided adequate detail and reasonable techni­\n\ncal approaches. As most MITRE SEs who have\n\nparticipated in multiple source selections will tell\n\nyou, the offerors are not likely to be even in terms\n\nof technical risk, past experience/expertise, or in\n\narchitectural approach. It is up to the engineering\n\nteam to clarify the differences so that “leveling”\n\ndoes not occur. This means careful consider­\n\nation of the evaluation criteria for differentiation,\n\nfocusing on the critical areas needed for success\n\non the program. If the offeror has not demon­\n\nstrated a consistent approach throughout the\n\nproposal process, this in itself may be a legitimate\n\nweakness.\n\nLeverage in a sole-source environment. It is\n\na best practice to judge a proposed effort on a\n\nsole-source contract against similar past efforts\n\nalready expensed and for which hours are actual.\n\nAgain, the ability to do this is dependent on\n\nthe initial contract (specifically the WBS) being\n\nstructured to capture progress at an appropriate\n\nlevel to accrue cost and schedule for indepen­\n\ndent efforts. Lacking a reasonable facsimile for\n\nthe proposed effort will require either experience\n\nin the contractor’s development methodology\n\nenough to estimate hours or research into other\n\nprograms and developments to compare against\n\n(both inherently less helpful for negotiating).\n\n\nAir Force Materiel Command (AFMC), November 2004, AFMC Integrated Master Plan and\nSchedule (IMP/IMS) Guide.\n\n\n-----\n\nAir Force Materiel Command (AFMC), April 2005, HQ AFMC Justification and Approval\nPreparation Guide and Template.\n\nBloom, M., and J. Duquette, July 2006, Systems Engineering in RFP Prep and Source Selection\nProcess V3.0, The MITRE Corporation.\n\nDefense Federal Acquisition Regulation Supplement and Procedures, Guidance, and\nInformation, Contracting by Negotiation, DFAR Subpart 215.\n\nDepartment of Defense, April 3, 1996, MIL-HDBK-245D DoD Handbook for Preparation of\nStatement of Work (SOW).\n\nDepartment of Defense, August 1, 2003, MIL-STD-961E DoD Standard Practice Defense and\nProgram-Unique Specifications Format and Content.\n\nDepartment of Defense, July 30, 2005, MIL-HDBK-881A DoD Handbook Work Breakdown\nStructure, Revision A.\n\nFederal Acquisition Regulation, Contracting by Negotiation, FAR Part 15.\n\n[IEEE, 1998, IEEE STD 1233 IEEE Guide for Developing System Requirements Specifications.](http://ieeexplore.ieee.org/iel4/5982/16016/00741940.pdf)\n\nMITRE SEPO, RFP Preparation and Source Selection Toolkit.\n\nOC-ALC/AE (ACE), June 20, 2003, Statement of Objectives (SOO) Information Guide.\n\nU.S. Air Force, January 2004, Other Than Full and Open Competition, AFFARS (Air Force\n_Federal Acquisition Regulation Supplement) MP5306.3._\n\n\n-----\n\n##### Program Acquisition Strategy Formulation\n\nDefinition: Developing a comprehensive, integrated acquisition strategy is an\n\n_acquisition planning activity. It describes business, technical, and support strate­_\n\n_gies to manage risks and meet objectives. It guides acquisition program execution_\n\n_across the life cycle. It defines acquisition phase and work effort relationships and_\n\n_key program events (decision points, reviews, contract awards, test activities, etc.)._\n\n_It evolves over time and should continuously reflect the program’s current status_\n\n_and desired end point. [1]._\n\nKeywords: acquisition, acquisition strategy, agile acquisition, “big bang” acquisi­\n\n_tion, contracting, evolutionary acquisition, information technology, software_\n\n_engineering, spirals, systems engineering_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to help our clients/\n\ncustomers craft realistic, robust, and executable acquisition strategies.\n\nThis means helping articulate what the government needs, translating\n\nthose needs into mission/outcome-oriented procurement/solicita­\n\ntion requirements, and adequately identifying the issues, risks, and\n\nopportunities that shape and influence the soundness of the acquisition\n\nstrategy. MITRE SEs help agencies achieve what the Federal Acquisition\n\nRegulation (FAR) characterizes as “mission-oriented solicitations [2].”\n\n\n-----\n\n###### Acquisition Strategy: Answering the Question “How to Acquire?”\n\nDeveloping and executing an effective acquisition strategy requires “systems thinking” to\nensure that the various elements of the strategy are integrated and that interdependencies are\nunderstood and accounted for during execution of the strategy. The acquisition strategy is\ndynamic in that it must reflect changes that occur often during execution. Cost, schedule, and\nsystem performance (or capability) trade-offs may also be required, and program managers\nwill need the insight to make informed decisions based on understanding the risks involved\nin achieving desired outcomes. Therefore, systems engineering plays a key role in both plan­\nning and executing this strategy.\n\nHow agencies acquire the products and services they need to perform their operations\n(or execute their mission) varies depending on the criticality or urgency of the product or\nservice to the agency’s mission. Another factor is whether the acquisition entails investments\nor allocation of relatively large amounts of agency resources. Acquisition planning for major,\ncomplex efforts requires greater detail and formality, and a greater, earlier need for systems\nengineering. Acquisition management at this level often requires balancing the equities of\nmultiple stakeholders by establishing and executing a governance process that strikes a bal­\nance between the desire for consensus and the agility needed to make trade-offs among cost,\nschedule, and the capabilities delivered to address changing needs and priorities. Because of\nthe challenges inherent in such an acquisition environment—political, organizational/opera­\ntional, economic, and technical—the FAR requires program managers to develop and docu­\nment an acquisition strategy to articulate the business and technical management concepts\nfor achieving program objectives within imposed resource constraints. MITRE SEs are often\ncalled on to help develop and execute this strategy.\n\n###### Best Practices and Lessons Learned\n\n\nFocus on total strategy. Avoid the tempta­\n\ntion to only focus on the contracting aspects of\n\nan acquisition strategy. In many agencies, the\n\nterm “acquisition strategy” typically refers to the\n\ncontracting aspects of an acquisition effort. This\n\nview tends to ignore other factors that influence\n\na successful outcome, including technical, cost\n\nor schedule, which often have interdependencies\n\nthat must be considered to determine how to\n\nacquire needed capabilities. The overall strategy\n\nfor doing so will be shaped and influenced by a\n\n\nvariety of factors that must be considered indi­\n\nvidually and collectively in planning and executing\n\nthe acquisition effort.\n\nWrite it down. The FAR requires program manag­\n\ners to develop an acquisition strategy tailored to\n\nthe particulars of their program. It further defines\n\nacquisition strategy as “the program manager’s\n\noverall plan for satisfying the mission need in the\n\nmost effective, economical, and timely manner\n\n[3].” An acquisition strategy is not required (or\n\n\n-----\n\nwarranted) for all acquisition efforts. However,\n\nmost (if not all) acquisition efforts for which\n\nMITRE provides systems engineering support\n\nrequire acquisition planning and a written plan\n\n(acquisition plan) documenting the ground rules,\n\nassumptions, and other factors that will guide the\n\nacquisition effort. For major system acquisition\n\nprograms, a written (i.e., documented) acquisition\n\nstrategy may be used to satisfy FAR requirements\n\nfor an acquisition plan [4].\n\nApply early systems engineering. In essence,\n\nthe FAR requires federal agencies to have increas­\n\ningly greater detail and formality in the planning\n\nprocess for acquisitions that are more complex\n\nand costly. These major or large-scale federal\n\nacquisition efforts also have a greater need\n\nfor application of sound systems engineering\n\nprinciples and practices, both within the govern­\n\nment and among the suppliers (industrial base) of\n\nthe products and services. A well-known axiom\n\nof program/project management is that most\n\nprograms fail at the beginning [5]. In part this can\n\nbe attributed to inadequate systems engineering,\n\nwhich when done effectively articulates what the\n\ngovernment needs, translates those needs into\n\nprocurement/solicitation requirements, and iden­\n\ntifies issues, risks, and opportunities that shape\n\nand influence the soundness of the acquisition\n\nstrategy.\n\nAn acquisition strategy is not a single entity.\n\nIt typically includes several component parts\n\n(or strategy elements) that collectively combine\n\nto form the overall strategy (or approach) for\n\nacquiring via contract(s) the products/supplies\n\nor services needed to fulfill an agency’s needs.\n\nThese elements tend to differ depending on the\n\n\nnature of the acquisition effort, whether or not\n\nthe effort is formally managed as a “program,” and\n\nthe acquisition policies, procedures, and gov­\n\nernance policies and regulations of the agency\n\nbeing supported. However, a common element\n\nof most acquisition (program) strategies includes\n\nstructuring the program in terms of how and when\n\nneeded capabilities will be developed, tested, and\n\ndelivered to the end user. In general, there are\n\ntwo basic approaches: delivering the capability all\n\nat once, in a single step (sometimes referred to\n\nas “grand design” or “big bang”) to fulfill a well\ndefined, unchanging need; or delivering capability\n\nincrementally in a series of steps (or spirals) to\n\naccommodate changes and updates to needs\n\nbased on feedback from incremental delivery of\n\ncapability. This latter approach is often referred\n\nto as evolutionary or agile acquisition (actual\n\ndefinitions may vary). For more details on each of\n\nthese strategies, see the articles “Agile Acquisition\n\nStrategy,” “Evolutionary Acquisition,” and “‘Big\nBang’ Acquisition.”\n\nPicking the appropriate strategy. Selecting\n\nwhich capability delivery approach to use depends\n\non several factors, including what is being\n\nacquired, the enabling technology’s level of matu­\n\nrity, the rate at which technology changes, and the\n\nstability of the requirements or evolving nature of\n\nthe need that the acquisition effort is address­\n\ning. For most information technology acquisition\n\nefforts, experience has shown that an evolution­\n\nary/incremental/agile approach is preferred to\n\naccommodate and plan for change inherent in\n\nsoftware-intensive systems. However, incorporat­\n\ning an evolutionary or agile approach to delivery\n\nof capability is no guarantee that an acquisition\n\n\n-----\n\nstrategy is sound. Other factors also determine\n\nthe effectiveness of a given strategy [5]. These\n\ninclude aspects such as whether the strategy is\n\nbased on an authoritative assessment of the risk\n\ninvolved in achieving the objectives of the acquisi­\n\ntion effort; a realistic test and verification strategy\n\n###### References and Resources\n\n\nand meaningful event or knowledge-driven\n\nmilestone reviews to assess progress toward\n\nachieving acquisition objectives; and realistic cost,\n\nschedule, and performance (delivered capability)\n\nbaselines.\n\n\n1. Defense Acquisition Guide Book, accessed February 5, 2014.\n\n2. Federal Acquisition Regulation, Subpart 34.005-2.\n\n3. Federal Acquisition Regulation, Part 34.\n\n4. Federal Acquisition Regulation, Subpart 34.004.\n\n5. S. Meier, Best project management and systems engineering practices for large-scale fed­\n\neral acquisition programs, Acquisition Community Connection Practice Center, DAU.\n\n\n-----\n\nDefinition: Agile acquisition is a\n\n_strategy for providing multiple,_\n\n_rapid deliveries of incremental_\n\n_capabilities for operational_\n\n_use and evaluation. Deliveries_\n\n_(called spirals, spins, or sprints)_\n\n_can be a few weeks or months_\n\n_to develop and are built with_\n\n_continuous user participation_\n\n_and feedback. This strategy_\n\n_assumes a predominately_\n\n_software-based development._\n\n_Do not confuse agile acquisition_\n\n_with agile development, which_\n\n_is “a group of software develop­_\n\n_ment methodologies based on_\n\n_iterative development, where_\n\n_requirements and solutions_\n\n_evolve through collaboration_\n\n_between self-organizing cross-_\n\n_functional teams.”[1]_\n\nKeywords: acquisition strategy,\n\n_agile, agile acquisition, agile_\n\n_development, increment, incre­_\n\n_mental, uncertainty_\n\n\nPROGRAM ACQUISITION STRATEGY\nFORMULATION\n###### Agile Acquisition Strategy\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the conditions under which an\n\nagile acquisition strategy will minimize program\n\nrisk and provide useful capability to customers\n\ncompared to other strategies. They are expected\n\nto recommend agile acquisition for the appro­\n\npriate situation, and develop and recommend\n\ntechnical requirements, strategies, and pro­\n\ncesses that facilitate its implementation. MITRE\n\nsystems engineers are expected to monitor\n\nand evaluate program agile acquisition efforts\n\nand recommend changes when warranted.\n\n\n-----\n\n###### Background\n\nMITRE SEs often are involved in the planning stages of a new program or major modifications\nto existing programs. As members of the planning team, they must be well acquainted with\nvarious acquisition strategies and factors that should be considered when choosing a strategy.\nTwo fundamental reasons for moving toward “agile” processes are:\n\n###### �To respond quickly to an immediate and pressing need. �To engage with users throughout the development process and ensure that what is\n\ndeveloped meets their needs.\nRegardless of whether the final system requirements are clear, agile acquisition strategy is\nfocused on getting capabilities to the user quickly rather than waiting for the final system [2].\n\nAgile acquisition is generally appropriate for two types of system development: 1) enter­\nprise systems with a high degree of functional requirements uncertainty, even if the purpose\nand intent of the system is known; and 2) small, tactical systems that may have a short life,\nbut an immediate and pressing need. These are cases where the fielding of blocks of an entire\nsystem is not feasible because total system requirements cannot be defined at the beginning,\nbut immediate user needs can be partially satisfied with rapidly deployed incremental capa­\nbilities. The keys to success of this strategy are close alignment with stakeholders on expecta­\ntions, and agreement with a limited set of users for providing continuous, rapid user feedback\nin response to each capability increment.\n\nAdvantages of this strategy demonstrate that: 1) development can begin immediately,\nwithout the time and expense needed for development, refinement, and approval of functional\nrequirements; and 2) significant user involvement during development guarantees that the\ncapabilities delivered will meet the user’s needs and will be used effectively.\n\nDisadvantages are that agile methodologies: 1) usually require stable infrastructure; 2)\nrequire significant management and technical oversight to ensure compatibility of ensu­\ning releases; and 3) usually sacrifice documentation and logistics concerns in favor of rapid\nreleases for fielding.\n\n###### Best Practices and Lessons Learned\n\n\nWhen to use agile. The use of agile methods\n\nshould not be considered for all development\n\nefforts. MITRE SEs need to determine if agile\n\nmethodology is appropriate for the program. It is\n\ncritical to create evaluation criteria and processes\n\nby which any proposed effort can be vetted, to\n\n\ndetermine if the agile approach is warranted. The\n\nminimum criteria are:\n\n###### � [Need for quick initial capability] � [High degree of uncertainty in final set of ]\n\nfunctional requirements\n\n\n-----\n\n###### � [Fluctuation and complexity due to fre­]\n\nquent changes in enterprise business\n\nmodel, data, interfaces, or technology\n###### � [Initial architecture or infrastructure that ]\n\nallows for small incremental developments\n\n(60-90 day cycles)\n###### � [Close cooperation and collaboration with ]\n\nusers; identified set of users for continu­\n\nous interaction with development team\n\nWhen a contractor bids agile development\n\nmethodology. If an agile development methodol­\n\nogy is proposed by a bidding contractor, deter­\n\nmine if the proposed effort is appropriate and\n\nhas adequate resources in both the contractor\n\nand Program Office organizations. For smaller\n\nprograms, eXtreme Programming techniques\n\nmay work; for larger efforts requiring scalability,\n\nanother methodology such as Scrum may work\n\nbetter. In addition, the developing contractor’s\n\n(and subcontractor’s) organization needs to be\n\nset up and resourced with the relevant develop­\n\nment tools, collaborative processes, and experi­\n\nenced personnel for whichever methodology is to\n\nbe applied. Government oversight and manage­\n\nment of a program using agile methods require\n\nstaff who are experienced in the methodology\n\nand its highly interactive, collaborative manage­\n\nment style. In-plant participation by MITRE engi­\n\nneers on a continuous basis should be strongly\n\nconsidered.\n\nArchitecture implications. There are differing\n\nopinions on the level of effort required to define\n\nthe overall architecture of an agile activity [2].\n\nSome of this difference is due to the particular\n\ndevelopment methodology applied. In general, the\n\n\nlarger and more complex the system, the greater\n\nthe effort placed on architecture up front.\n\nRather than detailing an architecture before\n\ndevelopment, consider using the first few spirals/\n\nsprints to evolve the architecture as more about\n\nthe system becomes known. This allows the\n\narchitecture to evolve, based on the needs of the\n\ndeveloping system.\n\nThe architecture of the system should be\n\ndesigned with enough flexibility so that capabili­\n\nties can be designed, added, or modified with\n\nfunctional independence. That is, the system is\n\narchitected and designed so that capabilities\n\nscheduled for delivery can interoperate with the\n\ncomponents that have been delivered, and do not\n\nhave critical operational dependencies on capa­\n\nbilities that have not yet been delivered. Layered\n\narchitectures lend themselves to this application.\n\nComponents may be developed for different lay­\n\ners; concentrating on the most commonly used/\n\nreused components first. An initial capability for\n\ndelivery could be a component that crosses layers\n\nand provides utility to the user (“user-facing”).\n\nLessons learned:\n\n###### � [For larger projects, recognize the need to ]\n\ndefine business (organization, people, and\n\nprocess) and technical architectures.\n###### � [Be aware of agile developers who do not ]\n\nhave the skills for designing the architec­\n\nture, and bring in systems engineers and\n\narchitects to help.\n###### � [Architecturally partition the system into ]\n\nlayers and components for clean inter­\n\nfaces and cleanly divided work among\n\n\n-----\n\nteams. Agile iterations need separable\n\ndesign pieces.\n###### � [Identify nonfunctional requirements to ]\n\nbuild a scalable infrastructure. Large\n\nprojects will have multiple iterations being\n\nworked in parallel. Parallel implementa­\n\ntions imply:\n\nyy Discrete implementation units\n\nyy Dependencies between implementa­\n\ntion units must be recognized and\nconsidered.\n###### � [Features delivered for fielding must aggre­]\n\ngate into a workable baseline. Dependen­\n\ncies between Configuration Items must be\n\nrecognized and planned to achieve stable\n\nupgrades [3, 4, 5].\n###### � [The first few months of an agile develop­]\n\nment are the most critical time for involve­\n\nment by MITRE SEs:\n\nyy Lots of collaboration—requires senior\n\npeople with right skills to provide guid­\nance to developers on priorities and\nrequirements (“stories” or “threads”) and\nto involve the users\n\nyy Skills and levels of expertise may\n\nchange as different applications and\ncomponents are being developed.\nRevisit, as needed.\n\nInfrastructure implications. When agile method­\n\nologies are used for developing software systems,\n\na stable infrastructure must be available to the\n\ndevelopment team and users. A traditionally\n\ndeveloped infrastructure provides a stable inter­\n\nface for developers, and allows the agile devel­\n\nopment team freedom to focus on their section\n\nof functionality and respond quickly to changing\n\nrequirements. If the infrastructure changes often,\n\n\ndevelopers waste time reworking their code to\n\nadapt to the new architecture without adding new\n\nfunctionality. Examples include stable operating\n\nsystem interfaces for agile desktop application\n\ndevelopment and a stable Java Web container\n\narchitecture for “plug and play” filter implementa­\n\ntions [2].\n\nTeaming is key. Constant collaboration between\n\nthe users and the developers, and among the\n\nProgram Office, developing organization, and\n\nstakeholders, is absolutely critical. Agile methods\n\ndo not work without this. It is essential that the\n\nProgram Manager secure user agreement to pro­\n\nvide evaluation feedback. It is also necessary that\n\nstakeholders be educated on the strategy and\n\nbenefits of agile methods because agile strate­\n\ngies can be new to government procurement\n\norganizations.\n\nConstant communication across the team is\n\nessential. Most agile methodologies require daily\n\nmeetings for status, direction, feedback, and\n\nassignments. Meetings are at different levels of\n\nthe organization—from program management\n\nto user feedback sessions. Meetings need to be\n\nforums for open communication; problems and\n\nlimitations need to be identified and addressed\n\nopenly. Transparency is necessary in planning\n\nand development decisions. Keep user evaluation\n\nand feedback sessions small, thereby allowing for\n\nfocused and open communication.\n\n_“Quality is job one.”_ Because agile method­\n\nologies are about flexibility and change, there\n\ncan be a fear that quality will suffer in the race\n\nto deliver. However, there is an inherent quality\n\ndriver in agile development if conducted properly.\n\nThe operational user, who should be constantly\n\n\n-----\n\nevaluating and providing feedback on the product,\n\nis an important bellwether of quality. Adherence\n\nto development standards, defining segregable\n\ncapabilities that enable successfully delivered\n\nfunctional increments, constant iterative testing,\n\nand constant user feedback all bring quality for­\n\nward as a critical measurement for agile success.\n\n“Agile” does not mean less disciplined. The\n\nvarious agile development methodologies are\n\nquite disciplined. The development teams need\n\nto work in parallel within the same development\n\nenvironment without interfering with each other.\n\n###### References and Resources\n\n\nThis requires a disciplined process enforced with\n\nconstant open communication, good configura­\n\ntion management practices, and quality code to\n\nensure interoperability of the deliveries on the\n\ndeveloping baseline. Critical to success is experi­\n\nence of the development team with the agile pro­\n\ncess and methods. Ensure there are processes in\n\nplace to enable the communications and tools to\n\nsupport collaboration and parallel development\n\nand testing. Quality measurements should be\n\ncaptured and monitored, to include testing suc­\n\ncesses/failures, defect rates, user comments, and\n\nfeedback (negative/positive).\n\n\n1. Wikipedia contributors, “Agile Software Development,” January 13, 2010.\n\n2. Dobbins, J. H., S. Luke, A. Epps, R. Case, and J. Wheeler, September 1, 2007, “Agile\n\nAcquisition Strategies for NSA Programs,” The MITRE Corporation.\n\n3. Doughty, J., September 4, 2007, “Introduction to Agile Development,” The MITRE\n\nCorporation.\n\n4. Hagan, P., September 4, 2007, “Agile Methods Overview and Implications for\n\nArchitecture,” The MITRE Corporation.\n\n5. Morgan, N., September 4, 2007, “Sponsor Perspective,” The MITRE Corporation.\n\n###### Additional References and Resources\n\n“Agile Acquisition,” MITRE Project Leadership Handbook, accessed January 15, 2010.\n\nColdewey, J., January 19, 2009, The 31 Square-Foot Architecture, Cutter Consortium.\n\n\n-----\n\nDefinition: Evolutionary acquisi­\n\n_tion is an acquisition strategy_\n\n_structured to deliver capability_\n\n_in increments, recognizing,_\n\n_up front, the need for future_\n\n_capability improvements. The_\n\n_objective is to balance needs_\n\n_and available capability with_\n\n_resources, and to put capabil­_\n\n_ity into the hands of the user_\n\n_quickly. The success of the_\n\n_strategy depends on phased_\n\n_definition of capability needs_\n\n_and system requirements, and_\n\n_the maturation of technologies_\n\n_that lead to disciplined devel­_\n\n_opment and production of_\n\n_systems that provide increasing_\n\n_capability over time [1]._\n\nKeywords: acquisition strategy,\n\n_capability increments, evolu­_\n\n_tionary acquisition, operation­_\n\n_ally useful_\n\n\nPROGRAM ACQUISITION STRATEGY\nFORMULATION\n###### Evolutionary Acquisition\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the principles of evolutionary\n\nacquisition as it applies to the programs they\n\nsupport. They are expected to evaluate the\n\nintended purpose of a proposed acquisition,\n\nadvise the program manager on the use of an\n\nevolutionary acquisition strategy compared\n\nto other strategies, and implement that strat­\n\negy for program planning and execution.\n\n\n-----\n\n###### Background\n\nAn acquisition strategy is a high-level business and technical management approach that is\ndesigned to achieve program objectives within specified resource constraints. It is the frame­\nwork for planning, organizing, staffing, controlling, and leading a program. It provides a mas­\nter schedule for research, development, test, production, fielding, and other activities essential\nfor program success. It also provides a master schedule for formulating functional strategies\nand plans. A primary goal in developing an acquisition strategy is to minimize the time and\ncost of satisfying an identified, validated need. This goal is consistent with common sense,\nsound business practices, and the basic policies established by sponsors.\n\nAn evolutionary approach delivers capability in increments, recognizing, up front, the\nneed for future capability improvements. The objective is to balance needs and available\ncapability with resources, and to put capability into the hands of the user quickly. The suc­\ncess of the strategy depends on consistent and continuous definition of requirements, and the\nmaturation of technologies that lead to disciplined development and production of systems\nthat provide increasing capability toward a materiel concept [1].\n\nEvolutionary acquisition is a method intended to reduce cycle time and speed the delivery\nof advanced capability to users. The approach is designed to develop and field mature technol­\nogies for hardware and software in manageable pieces. It is focused on providing the opera­\ntional user with an initial capability that may be less than the full requirement as a trade-off\nfor speed, agility, and affordability. Evolutionary acquisition also allows insertion of new\ntechnologies (when they become sufficiently matured) and capabilities over time. In principle,\nthe approach provides the best means of quickly providing advanced technologies to the users\nwhile providing the flexibility to improve that capability over time.\n\nThere are two approaches to evolutionary acquisition. In the first, the ultimate functional­\nity is defined at the beginning of the program, with the content of each deployable increment\ndetermined by the maturation of key technologies. In the second, the ultimate functional­\nity cannot be defined at the beginning of the program, and each increment of capability is\ndefined by the maturation of the technologies matched with the evolving user needs.\n\n###### Best Practices and Lessons Learned\n\n\nArchitecture is key. This is the most important\n\nlesson of this article. To effectively implement\n\nevolutionary acquisition, the architecture of the\n\nsystem or capability must be developed first.\n\nEmploy use-case and similar methodologies to\n\ndefine an “operational architecture” or business\n\n\nprocess model. Once the operations of the sys­\n\ntem are understood, a notional system architec­\n\nture can be generated. The architecture will need\n\nto support an evolutionary methodology and\n\nshould form the basis of the program plan. The\n\nfirst delivered increment should include the target\n\n\n-----\n\nsystem architecture from which subsequent\n\nincrements can build. Modeling and simulation\n\nmay be used to validate architecture assumptions\n\nand information flow.\n\nManage stakeholder expectations and yours,\n\ntoo. Managing expectations ranks high in impor­\n\ntance. Whether you define the ultimate function­\n\nality up front (not recommended for commercial\n\noff-the-self [COTS]-based information technol­\n\nogy [IT] systems) or by increment, realistic expec­\n\ntations within the program office and community\n\nstakeholders are a must for success. Expect\n\nchange and expect cost growth. Do not believe\n\nyou can define the content of all the increments\n\naccurately in advance. Change will happen and\n\nyou need to be prepared. Do not confuse this with\n\n“requirements creep;” recognize it as a normal\n\npart of the evolutionary process. For more infor­\n\nmation on managing stakeholder expectations,\n\nsee the article “Stakeholder Assessment and\n\nManagement” under the SEG’s Transformation\n\nPlanning and Organizational Change topic.\n\nUnderstand the operational users’ context:\n\nAlong with managing stakeholder expectations,\n\nclose coordination and communication with\n\nstakeholders is important. User requirements\n\nshould drive the content and priority of incre­\n\nments, as long as the users understand the end\n\nstate of the program. This should to be balanced\n\nwith effective contract execution. Understand the\n\nintended mission and operations of the system\n\nbeing acquired; systems engineering in an opera­\n\ntional vacuum is not effective. Get acquainted\n\nwith the operations and the users. Recognize that\n\nthey have different priorities and a different per­\n\nspective from a program office. Program offices\n\n\nplan and execute a program; users conduct a\n\nmission. It is acceptable to have differing opinions\n\nat times. Make sure those differences are handled\n\nby open communication leading to positive\n\nresolutions.\n\nNo technology before its time. Manage tech­\n\nnology, instead of technology managing you. The\n\nprinciple behind evolution is to take advantage\n\nof emerging technology when it is mature. Stay\n\nabreast of new products that can contribute to\n\nmission effectiveness, and incorporate routine\n\nmethods for tracking new technology (e.g., via the\n\nannual cycles in the Department of Defense [DoD]\n\nfor government research laboratories Advanced\n\nTechnology Demonstrations and Small Business\n\nInnovative Research programs). Be aware that new\n\ntechnology sometimes can bring new operational\n\ncapabilities not previously considered. For exam­\n\nple, users see a new toy, and instead of asking for\n\nthe functionality, they ask for the toy. They see a\n\ncapability in this toy that you are not aware of and\n\nthey cannot articulate. Establishing a good work­\n\ning relationship with the stakeholders and users\n\ncan help you bring out the capability inherent in\n\nthe toy. This can help mitigate the churn often\n\nseen in COTS-based IT systems. Use evolution­\n\nary acquisition to your advantage (see the articles\n\n“Assessing Technical Maturity” and “Technology\n\nPlanning”). As long as a useful capability is deliv­\n\nered within a short period (approximately 12–18\n\nmonths), the users will respond in kind.\n\nThink “parallel developments.” Often in an\n\nevolutionary model, development of increments\n\nmust occur in parallel to deliver capability on time.\n\nIncrements may vary in time to develop and inte­\n\ngrate. If done serially, they can extend the program\n\n\n-----\n\nschedule and adversely impact the ability to\n\ndeliver capability to the users in a timely manner,\n\nwhich was the purpose of evolutionary acquisition.\n\nManaging parallel development is challenging but\n\nnot unachievable; it should not be avoided. Make\n\nuse of configuration management to control the\n\ndevelopment baselines and track changes. Allow\n\ntime in the increment development schedules\n\nfor the reintegration of a “gold” baseline for final\n\nincorporation of parallel changes prior to test\n\nand fielding. For more information, see the SEG’s\n\nConfiguration Management topic.\n\nThe right contract type. Carefully consider the\n\ncontract type for an evolutionary acquisition. As\n\n###### References and Resources\n\n\nevidenced by these lessons learned, changes are\n\nfrequent and should be part of the plan. Time\nand-materials, cost-reimbursement, product\ndriven payments, or awards can allow for flex­\n\nibility without severe cost implications. Focus\n\nshould be on delivery of a useful product, not\n\nprocesses. Indefinite Delivery/Indefinite Quantity\n\nstyle contracts should be considered, but need\n\nto be structured so that each delivery order is not\n\ntreated as an independent entity from the total\n\nprogram (this was seen on a DoD program and\n\nwas quite painful, since delivery orders can be\n\ninterdependent and cause critical path analysis\n\nand integrated master schedule obscuration if not\n\nmanaged and reported as a single program).\n\n\n1. Department of Defense, December 2, 2008, Defense Acquisition Guidebook, Operations of\n\nthe Defense Acquisition System, DoD Instruction Number 5000.02.\n\n###### Additional References and Resources\n\nBoehm, B., July 2000, Spiral Development: Experience, Principles, and Refinements, Spiral\nDevelopment Workshop, February 9, 2000, CMU/SEI-2000-SR-008.\n\nDobbins, J., M. Kelley, and P. Sherlock, October 2009, DoD Acquisition Assurance for IT\nSystems, The MITRE Corporation.\n\nHansen, W.J., et al., July 2000, Spiral Development—Building the Culture, A Report on the\nCSE-SEI Workshop, February 2000, CMU/SEI-2000-SR-006.\n\nHansen, W.J., et al., May 2001, Spiral Development and Evolutionary Acquisition, The SEI-CSE\nWorkshop, September 2000, CMU/SEI-2001-SR-005.\n\n\n-----\n\nDefinition: An acquisition\n\n_strategy is a high-level business_\n\n_and technical management_\n\n_approach designed to achieve_\n\n_program objectives within_\n\n_specified resource constraints._\n\n_It is the framework for planning,_\n\n_organizing, staffing, control­_\n\n_ling, and leading a program._\n\n_The traditional strategy for_\n\n_system acquisition—also called_\n\n_“Big-Bang,” “Grand Design,” or_\n\n_“One-Shot”—involves a single_\n\n_pass through the organization’s_\n\n_acquisition life cycle._\n\nKeywords: acquisition, big bang,\n\n_grand design, one pass, one_\n\n_shot_\n\n\nPROGRAM ACQUISITION STRATEGY\nFORMULATION\n###### “Big-Bang” Acquisition\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nunderstand the fundamental conditions and\n\nassumptions under which an acquisition pro­\n\ngram/project can be successfully pre-specified,\n\nplanned, and controlled so that only one pass\n\nthrough the traditional systems engineering life\n\ncycle is necessary to deliver a system or capabil­\n\nity. MITRE SEs are also expected to understand\n\nwhen a big-bang acquisition is not appropriate\n\nto a situation, explain the basis for their assess­\n\nment, and recommend better alternatives.\n\n\n-----\n\n###### Background\n\nMax Wideman says, “What happens in a traditional acquisition process? Simply put, you\nfigure out what you want, describe it in a Request for Proposal, solicit bids, pick a competent\nvendor with the lowest price and fastest delivery, enter into a standard legal contract, wait for\nthe work to be finished, and come back when you can ‘pick up the keys to the front door’[1].”\nThis strategy was used extensively by the Department of Defense (DoD), as well as other\ngovernment agencies and foreign governments, for the acquisition of major systems [2]. In\nthe mid-1990s, it was replaced by evolutionary acquisition as the preferred strategy. (See the\narticle “Evolutionary Acquisition.”)\n\nIncreasingly, government departments and agencies are acquiring information tech­nology\n(IT)-intensive systems and capabilities, and deploying rapidly fielded systems. Recent stud­\nies by the Defense Science Board (DSB) [3, 4] have concluded what many have suspected for\nyears: “The deliberate process through which weapon systems and information technology are\nbeing acquired does not match the speed at which new IT capabilities are being introduced in\ntoday’s information age.” Thus big-bang acquisitions are expected to become less relevant and\nless frequently applied to future government capability developments.\n\n###### Best Practices and Lessons Learned\n\n\nThe big-bang strategy is based on a number of\n\nassumptions (usually unstated), such as:\n\n###### � [The user and the acquisition organiza­]\n\ntion can define and articulate all system\n\nrequirements in the request for proposal.\n###### � [The critical technologies for the system ]\n\nremain static from the time the proposal\n\nis requested until the system is tested and\n\ndelivered.\n###### � [This type of system has been built before, ]\n\nand the development contractor knows\n\nhow to build (or acquire) and integrate the\n\nnecessary subsystems and components.\n###### � [The interfaces with other systems remain ]\n\nstatic from the time the proposal is\n\nrequested until the system is tested and\n\ndelivered.\n\n\n###### � [The user’s operational environment does ]\n\nnot change from initial request through\n\ndelivery of the product.\n###### � [The contractor will deliver the requested ]\n\nproduct without substantial interim review\n\nby the acquisition organization.\n###### � [The government’s original cost estimate ]\n\nwas accurate, product funding remains\n\n“protected” for the life of the contract, and\n\nmanagement reserve is available to handle\n\nknown unknowns.\n###### � [The acquisition organization can coor­]\n\ndinate and integrate the acquisition with\n\nother interfacing systems and parallel\n\ndevelopment efforts.\n\nIn the acquisition of systems where these\n\nassumptions hold true, big-bang acquisition is\n\n\n-----\n\nthe most efficient approach. However, when the\n\nprogram is canceled before final delivery, the cus­\n\ntomer does not receive expended funding to date\n\nand must pay for any termination liability that was\n\nspecified in the contract. When these assump­\n\ntions do not hold and the program has elected\n\n(or been directed) to use a big-bang acquisition\n\nstrategy, the program will require more time and\n\nfunding and will probably deliver less functionality,\n\nperformance, or quantity than originally specified.\n\n\nThe typical response to these shortcomings is to\n\nadd more funding rather than cancel the program\n\n(usually causing a reverse incentive to the devel­\n\noping contractor).\n\nFigure 1 illustrates the General Accountability\n\nOffice’s assessment of the F/A-22 program for\n\nusing a big-bang acquisition strategy [5]. The F/A\n22’s advanced avionics, intelligence, and communi­\n\ncations technologies were not available at the time\n\n\nEvolutionary\n3rd generation\n_Basic stealth platform_\n_Advanced avionics_\n_Advanced intelligence &_\n_communications_\n\nNeeded\ntechnologies\nare mature\n\n\nEvolutionary\n1st generation\n_Basic stealth_\n_platform_\n\nNeeded\ntechnologies\nare mature\n\n\nEvolutionary\n2nd generation\n_Basic stealth platform_\n_Advanced avionics_\n\n|eded hnologies mature|Needed technologies are mature|technologies are mature|\n|---|---|---|\n||||\n||||\n\n\n5 yrs. 10 yrs. 15 yrs.\n\n###### “Big Bang”\n\n\nDelivered capability\n\nBasic capability\n\nEnhanced capability\n\nFull capability\n\n\nSingle step\n1st generation\n_Basic stealth platform_\n_Advanced avionics_\n_Advanced intelligence &_\n_communications_\n\n\nFigure 1. Comparison of “Big-Bang” and Evolutionary Acquisition\n\n\n-----\n\nof initial contract award. Rather than developing\n\nthe basic stealth platform with provisions for later\n\ntechnology insertion, a version of big-bang acquisi­\n\ntion was used in which the entire aircraft delivery\n\nwas delayed while the avionics, intelligence, and\n\ncommunications technologies matured.\n\nPrior to the early or mid-1990s, big-bang acqui­\n\nsition was the normal approach for doing busi­\n\nness in the DoD. With the exception of the SR-71\n\n“Skunkworks” development, there have been few\n\ncases in the last four decades where a govern­\n\nment big-bang development was completed\n\nfast enough for all of the assumptions to hold.\n\nCommercial aircraft and automobile firms have\n\nhad better success with the big-bang strategy\n\nbecause of their shorter development cycles.\n\nWhen it became obvious that long acquisition\n\ntimes for major system acquisitions were caus­\n\ning the assumptions to fail, the department\n\npolicy was changed to favor evolutionary acquisi­\n\ntion. Evolutionary acquisition is an incremental\n\napproach to delivery of capability, providing for\n\nquicker initial (or partial) delivery of capabil­\n\nity, while allowing future increments or spirals\n\nto address the balance of the requirements\n\n###### References and Resources\n\n\nand accommodate changes. (See the article\n\n“Evolutionary Acquisition.”)\n\nThe systems engineering implications of a\n\nbig-bang acquisition are tied closely to the\n\nassumptions listed above. These assumptions\n\nhave been learned through MITRE’s experience\n\nwith traditional big acquisitions and explain why\n\nmany large programs fail despite good traditional\n\nsystems engineering practice. The longer an\n\nacquisition program remains in the development\n\nphase, the more likely there will be changes to the\n\nrequirements, environment or mission, or relevant\n\ntechnology requiring contract modifications and\n\nengineering change proposals, thereby lengthen­\n\ning the cycle for delivery and increasing the cost.\n\nFor some programs, this becomes a vicious cycle\n\n(more changes beget more changes) and the\n\ndevelopment is never completed.\n\nIt is critical for the MITRE SE to point to the\n\nassumptions and stress their importance as indis­\n\npensable to success when using big-bang as the\n\nacquisition strategy for a program. It is more likely\n\nthat this strategy should never be chosen (based\n\non historical lack of success) and an alternate\n\nstrategy should be recommended.\n\n\n1. Max’s Project Management Wisdom website, “Progressive Acquisition and the RUP, Part I:\n\nDefining the Problem and Common Terminology.”\n\n2. Defense Systems Management College (DSMC), 1999, “Acquisition Strategy Guide,” 4th Ed.\n\n3. DSB Task Force on Department of Defense Policies and Procedures for the Acquisition of\n\nInformation Technology, March 2009, “Department of Defense Policies and Procedures for\nthe Acquisition of Information Technology.”\n\n4. DSB Task Force on Future Perspectives, April 2009, “Creating a DoD Strategic Platform.”\n\n5. GAO Testimony, April 11, 2003, “Better Acquisition Outcomes Are Possible If DoD Can\n\nApply Lessons From F/A-22 Program.”\n\n\n-----\n\n##### Contractor Evaluation\n\nDefinition: Contractor evaluation is an activity to assess the contractor’s techni­\n\n_cal and programmatic progress, approaches, and deliverables. The purpose of_\n\n_contractor evaluation is to provide insight into risks and the likelihood of meeting_\n\n_program and contractual requirements [1]._\n\nKeywords: contractor evaluation, contractor performance, milestone reviews\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) support contractor evaluations and\n\nmilestone reviews, influence sponsor/customer decisions during those\n\nreviews, monitor the contractor’s continued performance, and recom­\n\nmend changes based on their performance [1]. MITRE SEs are expected\n\nto apply strong domain and technical expertise and experience and\n\nperform with objectivity consistent with the FFRDC role.\n\n###### Context\n\nContractor evaluation is a component of performance management. It\n\nis a process for making course corrections that uses performance infor­\n\nmation to adjust resources and activities to achieve an organization’s\n\nend goals. The focus of performance management is on the future:\n\nWhat do you need to be able to do, and how can you do things better?\n\nManaging performance is about managing for results.\n\n\n-----\n\nMITRE teams frequently are asked to lead and participate in contractor evaluation\nactivities because the characteristics of federally funded research and development cen­\nters (FFRDCs), as chartered under Federal Acquisition Regulation (FAR) 35.017 [2], promote\nindependence, objectivity, freedom from conflicts of interest, and technical expertise. These\ncharacteristics enable the MITRE team to provide findings and recommendations that might\nreduce risk to the government program and increase the probability of a favorable outcome.\n\nThis topic contains three articles. The article “Data-Driven Contractor Evaluations and\nMilestone Reviews” provides guidance to MITRE SEs who monitor, assess, and recommend\nimprovements to a contractor’s technical and programmatic approaches, work packages,\nprototypes, and deliverables before and during reviews. The other two articles—“Earned\nValue Management” and “Competitive Prototyping”—provide guidance and lessons learned\non key specific techniques for monitoring contractor performance. Earned value manage­\nment (EVM) integrates data on project scope, schedule, and cost to measure progress, and\nis required in many government programs. It gives the MITRE SE insight into potential\nprogram risks and can form the basis for making recommendations to mitigate those risks.\nCompetitive prototyping (CP) is an approach in which two or more competing organizations\ndevelop prototypes during the early stages of a project. In a number of acquisition reform\ninitiatives, the U.S. government has encouraged or required CP to be used as a tool to\nassess technology maturity and reduce program risk. The “Competitive Prototyping” article\nprovides guidance on when to recommend competitive prototyping and offers best practices\nand lessons learned for monitoring and evaluating contractor competitive prototyping tech­\nnical efforts. See related information in articles under the SEG’s MITRE FFRDC Independent\nAssessments topic.\n\n###### Best Practices and Lessons Learned\n\n\nMaintain positive and professional relationships\n\nwith all contractors. The contractor and govern­\n\nment teams will be more receptive to MITRE find­\n\nings and recommendations if they are developed\n\nand presented in a positive, professional atmo­\n\nsphere. In addition, occasionally MITRE SEs find\n\nthemselves working with the same contractor or\n\ngovernment team members on different projects.\n\nIn that situation, the shared experience of a pro­\n\nfessional encounter can prove helpful in making\n\nprogress with the new project.\n\n\nPlanning for roles and activities is essential.\n\nThe government team, MITRE, and the contrac­\n\ntor all have specific roles and responsibilities, and\n\nfrequently MITRE is asked to lead the government\n\nteam efforts in defining the technical compo­\n\nnents. MITRE also usually drafts the technical\n\nevaluation plan. The best practices and lessons\n\nlearned sections in the articles under this topic\n\nprovide guidance on both roles and responsibili­\n\nties and planning.\n\n\n-----\n\n###### References and Resources\n\nFederal Acquisition Regulation (FAR), http://www.acquisition.gov/far/, accessed February 5,\n2010.\n\n[The MITRE Corporation, September 1, 2007, MITRE Systems Engineering (SE) Competency](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n[Model, Ver. 1.13E, pp. 39–40.](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n\n-----\n\nDefinition: Data-driven\n\n_contractor evaluations and_\n\n_milestone reviews provide an_\n\n_objective assessment of con­_\n\n_tractor performance at techni­_\n\n_cal milestone reviews. Technical_\n\n_reviews and the content to be_\n\n_addressed are typically pre­_\n\n_scribed by government agency_\n\n_or department mandates avail­_\n\n_able to MITRE staff and other_\n\n_project members prior to the_\n\n_actual milestone._\n\nKeywords: empirical data,\n\n_independent technical assess­_\n\n_ments, metrics, milestone_\n\n_reviews, performance assess­_\n\n_ments, technical reviews_\n\n\nCONTRACTOR EVALUATION\n###### Data-Driven Contractor Evaluations and Milestone Reviews\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to provide\n\ntechnical thought leadership and assessment\n\nthroughout an entire government program life\n\ncycle. While ongoing insight is needed to quickly\n\ngrasp and respond to program risks and opportu­\n\nnities, its importance peaks at event-driven mile­\n\nstones when key government decisions are made.\n\nAt those times, MITRE SEs are expected to lead\n\nand participate in teams reviewing the contractor\n\nproposed technical approach. MITRE SEs analyze\n\ndesign review content against milestone entry\n\nand exit criteria to ensure that the contractor\n\ndelivers quality products on time and within bud­\n\nget. They are expected to assess the contractor’s\n\ntechnical and programmatic approaches, work\n\npackages, prototypes, and deliverables before\n\n\n-----\n\nand during reviews to identify issues and ensure that decision makers are provided with datadriven recommendations during technical and program milestone reviews [1].\n\n###### Introduction\n\nMITRE SEs can assume many roles at technical milestone reviews. Depending on the size and\ncomplexity of the program, many MITRE staff may be supporting the same technical review\nor, on some programs, only one or two. Staff typically perform as subject matter experts\n(SMEs) for specific technical areas (e.g., adequacy of requirements capture, maturity of the\narchitecture) to be reviewed; they provide informal and formal assessments to the govern­\nment sponsor. It is also not uncommon for MITRE to develop an overall assessment of the\nentire technical review. This assessment may include aggregating the input from MITRE staff\nand other program office contractor support. Whatever the scope, focus, or size of the MITRE\nreview effort, the overall assessment must be based largely on empirical data, metrics, the\ntrends they indicate, and demonstrated system performance. During reviews, MITRE staff\nneed to be prepared, inquisitive, confident, technically competent, thorough, current with pro­\ngram progress, tactful in dealing with the contractor, and convincing in their overall assess­\nments. Finally, MITRE’s assessment of and recommendation on whether the technical review\n“passed” or “failed” can have a significant impact on whether the program meets its schedule\nor experiences long and costly delays.\n\n###### Government Interest and Use\n\nThe government has myriad guidelines and mandates that define how systems should be\nacquired, developed, delivered, and sustained. In attempts to track the progress of a system\ndevelopment, the government has also defined a set of technical reviews to be conducted at\nvarious phases of development. Conducting these reviews successfully requires insight into\ncontractor progress. Although it is a government responsibility to formally sign off on the final\nassessment of a technical review, MITRE is relied on heavily to provide convincing and cred­\nible technical evidence to support the assessment.\n\nIndependent, fact-based engineering analysis is essential to government program man­\nagers (PMs) in making their assessment of whether a program meets its technical review\ncriteria.\n\nAmong the most critical times for MITRE to provide unbiased and technically substanti­\nated assessments on a program is when supporting technical milestone reviews. We need to\nwork with the contractor to ensure that the government PM is presented with empirical data\nand metrics that characterize system progress and performance as accurately as possible. That\nincreases the likelihood that the government PM will make the right decision because it is\nbased on objective data that supports the overall assessment.\n\n\n-----\n\nIt is important to ensure that technical recommendations are not influenced by the natu­\nral, collective desire of program stakeholders for the program to be viewed as a success and to\nmove forward. Because of program pressures to succeed, technical assessments that indicate\nprogram problems may not be immediately embraced. In rare cases, it may be necessary to\nprovide a formal, independent message of record to the PM documenting the technical assess­\nment, the rationale for the perceived risk to the program (i.e., the likelihood of not meeting\ntechnical objectives, schedule, or cost and the impact), what may happen if the situation is\nnot addressed, and recommended steps to mitigate the risk. The PM should be made aware of\nsuch a message and its contents personally before it is issued. While such a communication\nmay not be welcomed in the short term, in the long run, it maintains the high standard that\nour customers expect of us.\n\n###### Best Practices and Lessons Learned\n\n\nEnsure consensus-based entry/exit criteria.\n\nThe name, purpose, and general requirements\n\nof each technical review in standard acquisition\n\nprocesses are usually well defined in department\n\nor agency regulations [2]. What is often not done,\n\nbut is essential for conducting a coordinated and\n\nsuccessful technical review, is to ensure that the\n\ngovernment team and contractor have docu­\n\nmented formal entry and exit criteria, and that\n\nconsensus has been reached on their content. If\n\nthese do not exist, it is important to ensure that\n\nthey are created and, if required, for MITRE staff\n\nto take responsibility for ensuring that they are\n\ndefined. The entry/exit criteria should be tailored\n\nto meet the needs of each program. This is an\n\narea where MITRE can contribute—by emphasiz­\n\ning criteria (e.g., data, prototypes, metrics) that can\n\nbe objectively assessed. Sample entry/exit criteria\n\nfor many reviews are contained in the Mission\n\nPlanning Technical Reviews [3].\n\nPrepare, prepare, prepare. The backgrounds, skill\n\nsets, and experiences of the systems engineering\n\nteam supporting the government at a technical\n\n\nreview can vary widely. Depending on our role in\n\nthe supported program, MITRE can and should\n\ninstigate and lead government preparation meet­\n\nings to ensure that entry/exit criteria are known,\n\nresponsibilities of each SME are defined ahead\n\nof time, there is a pre-review artifacts/contract\n\ndata requirements lists and government leader­\n\nship attending have been “prepped” on strengths/\n\nweaknesses of the contractor and where they\n\nshould weigh in. It is also beneficial to conduct\n\ntechnical review “dry runs” with the contractor\n\nprior to the review. At the same time, be sensi­\n\ntive to the demands that dry runs place on the\n\ncontractor. Structure them to be less formal and\n\nintrusive while achieving the insight they provide.\n\nThe benefits of these dry runs are:\n\n###### � [They require the contractor to prepare for ]\n\nthe review earlier and reduce the pos­\n\nsibility of them creating “just-in-time”\n\ncharts for the major review that may have\n\ndisappointing content from the govern­\n\nment perspective. If the content falls short\n\n\n-----\n\nof expectations, there is time for them to\n\ncorrect it.\n###### � [They allow more people to attend a ver­]\n\nsion of the review and have their questions\n\nanswered, since meetings will be smaller.\n\nThough key PM and technical team mem­\n\nbers will attend both the dry run and final\n\nreview, others are likely to attend only one.\n###### � [They allow a graceful way to reschedule ]\n\nthe review if the contractor is not ready\n\nby dry run. This is especially important\n\nfor programs that are under substantial\n\nscrutiny.\n\nDivide and conquer. No one can know all aspects\n\nof a contractor’s effort, regardless of how able the\n\nstaff is, how long they have been on the program,\n\nor how technically competent they are. It may also\n\nhappen that a program’s systems engineering\n\nstaff resources may be weighted in a particular\n\ndiscipline (e.g., software engineers, radar engi­\n\nneers, network specialists). Program technical\n\nreviews are all-encompassing. They must address\n\nuser requirements, risk identification and mitiga­\n\ntion, performance, architecture, security, testing,\n\nintegration, and more. If staff resources are lim­\n\nited, it is advisable to assign SMEs who are strong\n\nin one discipline (e.g., software engineering) the\n\nsecondary responsibility of another discipline (e.g.,\n\nrisk identification) at the technical review. This\n\nhas the benefit of ensuring that all disciplines are\n\ncovered at some level during the review and pro­\n\nvides the opportunity to train staff in secondary\n\nsystems engineering disciplines that broaden their\n\nskill set and help the government in the long run.\n\nGauge “ground truth” for yourself. Be aware\n\nof the true program progress well ahead of the\n\n\nreview. Know the “real” workers responsible for\n\nday-to-day development, who may be differ­\n\nent from those presenting progress reports at\n\nreviews. This will allow you to more accurately\n\ngauge progress. This requires advanced prepa­\n\nration, including meetings with programmers,\n\nattending contractor in-house peer reviews,\n\nreviewing development metrics, witnessing early\n\nprototype results, observing in-house testing, and\n\nspending time in the contractor’s facility to know\n\nfact from fiction.\n\nAssess when fresh. Recognize that technical\n\nreviews can be long, tedious, information packed,\n\nand physically and mentally draining events.\n\nAs difficult as it may be, attempt to conduct a\n\ngovernment team caucus at the end of each day\n\nto review what was accomplished and to gain\n\npreliminary team feedback. Meetings do not\n\nhave to be long; a half hour can be sufficient. It is\n\nadvantageous to gather the impressions of team\n\nmembers, since it can quickly confirm the review’s\n\nformal presentations or uncover differences. Use\n\nthe entry/exit criteria to voice what was “satisfac­\n\ntory” and what was not. Finally, when it is time to\n\naggregate all input for the entire review, it is valu­\n\nable to have the daily reviews to streamline the\n\nassembly of the formal assessment.\n\nUse mostly data, part “gut feeling.” Though it\n\nis desirable for the technical reviews to be civil,\n\n“just the facts” affairs, there may be times when\n\nexchanges become contentious and relation­\n\nships between government and contractor\n\nrepresentatives become strained. Personalities\n\ncan get involved and accusations may be made,\n\nwhich are driven more by defensive instincts than\n\nimpartial assessment of data. This is the time to\n\n\n-----\n\nmake maximum use of objective data to assess\n\ncontractor progress and solution development\n\nmaturity, while refraining from over-reliance on\n\nanecdotal information and subjective assertions.\n\nMetrics and the trends they illuminate should be\n\nused as the basis for questions during the review\n\nand assessments after the review. Metrics to\n\ndemonstrate software size, progress, and qual­\n\nity, should be assessed. (For software-intensive\n\nsystems, it may be advisable to compare produc­\n\ntivity/defect rates to other industries [4], other\n\nmilitary systems [5], or CMMI maturity level stan­\n\ndards [6].) Preliminary data to indicate system per­\n\nformance, reliability, and user satisfaction should\n\nbe examined and challenged if necessary. Staffing\n\nmetrics can be used to corroborate sufficiency\n\nof assigned resources. Testing metrics should be\n\nreviewed, as well. Don’t ignore “gut feelings,” but\n\nuse them selectively. When the data says one\n\nthing and your intuition says another, intensify your\n\nefforts to obtain additional fact-based evidence\n\nto reconcile the disparity.\n\nSearch for independence. Regardless of how\n\nknowledgeable organic project staff is on all\n\nphases of your acquisition and the technologies\n\nresponsible for the most prominent program risks,\n\nit is advisable to call on independent SMEs for\n\nselected technical reviews. In fact, Department of\n\n###### References and Resources\n\n\nDefense (DoD) guidance for the development of\n\nsystems engineering plans, as well as the Defense\n\nAcquisition Guide (DAG), call out the need for\n\nindependent SMEs. This is excellent advice. For\n\nlarge, critical, and high-visibility programs under­\n\ngoing oversight by their respective department\n\nor agency acquisition authority, conducting an\n\nIndependent Technical Assessment (ITA) to\n\nassess the maturity of the program at a major\n\ntechnical review (e.g., PDR, CDR) can help develop\n\nobjective evidence to inform the final assessment.\n\nIt may also be advisable to include an SME from a\n\nlarge, respected technical organization on the ITA\n\nto provide advice in their areas of special exper­\n\ntise (e.g., Carnegie Mellon Software Engineering\n\nInstitute [SEI] on Capability Maturity Model\n\nissues). It may be advantageous to use a quali­\n\nfied, senior-level MITRE technical SME to lead the\n\nITA, as a way of bringing the corporation to bear.\n\nIt is also advisable to include a senior manager\n\nfrom the prime contractor being reviewed, as long\n\nas this person is not in the direct management\n\nchain of the program leadership. This can open\n\nmany doors with the prime contractor that may\n\nhave seemed closed in the past. Recognize that\n\nbringing on independent SMEs for a review has a\n\ndistinct cost (e.g., organic staff resources will need\n\nto bring SME members up to speed). However,\n\njudiciously done, it can be worthwhile.\n\n\n1. MITRE Systems Engineering (SE) Competency Model, Ver. 1, September 1, 2007, p. 38.\n\n2. Defense Acquisition Guidebook, Chapter 4.\n\n3. 951st Electronic Systems Group, April 2007, “Mission Planning Technical Reviews.”\n\n4. Jones, C., April 2008, Applied Software Measurement: Global Analysis of Productivity and\n\n_Quality, 3rd Ed., McGraw-Hill Osborne Media._\n\n\n-----\n\n5. Reifer, J., July 2004, “Industry Software Cost, Quality and Productivity Benchmarks,” The\n\n_DoD Software Tech News, Vol. 7, Number 2._\n\n6. Croxford, M., and R. Chapman, May 2005, “Correctness by Construction: A Manifesto for\n\nHigh-Integrity Software,” Crosstalk: The Journal of Defense Software Engineering.\n\n###### Additional References and Resources\n\nJones, C., May 2000, Software Assessments, Benchmarks, and Best Practices, Addison-Wesley\nProfessional.\n\nNeugent, B., August 2006, “How to Do Independent Program Assessments,” The MITRE\nCorporation.\n\n\n-----\n\nDefinition: Earned value man­\n\n_agement (EVM) is a technique_\n\n_for measuring project progress_\n\n_in an objective manner. It inte­_\n\n_grates technical scope, sched­_\n\n_ule, and cost for definitized_\n\n_contract work [1]._\n\nKeywords: contractor perfor­\n\n_mance, earned value, earned_\n\n_value management system,_\n\n_performance-based earned_\n\n_value, performance measure­_\n\n_ment, planned value_\n\n\nCONTRACTOR EVALUATION\n###### Earned Value Management\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to\n\nsufficiently understand the principles and\n\nelements of EVM to monitor and assess con­\n\ntractor performance, and use its results as the\n\nbasis for recommending program changes.\n\n\n-----\n\n###### Background\n\nMonitoring contractor performance consists of measuring and evaluating the contractor’s\nprogress to assess the likelihood of meeting program and contractual requirements for cost,\nschedule, and technical viability [2]. Performance measurement is part of performance\nmanagement, a process for making course corrections to achieve an organization’s goals by\nusing performance information to adjust resources and activities. The focus of performance\nmanagement is the future: What do you need to be able to do, and how can you do things\nbetter? Managing performance is about managing for results [3].\n\nA widely used practice for monitoring contractor performance by managers and SEs is\nreviewing and assessing earned value management (EVM) results. In the 1990s, many U.S.\ngovernment regulations were eliminated or streamlined. However, EVM not only survived the\nstreamlining, but emerged as a financial analysis specialty that has since become a significant\nbranch of project management and cost engineering for both government and industry. Today,\nmore and more civilian agencies are requiring EVM in their contracts to better manage and\nensure successful outcomes. In accordance with OMB Circular A-11, Part 7, agencies must use\na performance-based acquisition management system, based on ANSI/EIA Standard 748, to\nmeasure achievement of the cost, schedule, and performance goals [4].\n\n**Basic Concepts of Earned Value Management**\n\nEVM is a technique used to track the progress and status of a project and forecast the likely\nfuture performance of the project. EVM integrates technical scope with the time-phased cost\nor budget required to complete the scope to facilitate integrated management of program plan­\nning and execution [1]. It can result in meeting the technical scope within cost and schedule\nparameters, reducing or eliminating schedule delays, and reducing or eliminating cost over­\nruns [1]. The basic elements of the EV are depicted in Figure 1 [5]. Specifically, earned value\n(EV) consists of three dimensions: (1) The plan or budgeted cost of work scheduled (BCWS),\n(2) The performance or budgeted cost of work performed (BCWP), and (3) The cost of perfor­\nmance or actual cost of work performed (ACWP). These three data elements are used as the\nbasis for computing and analyzing project performance.\n\n###### Monitoring Contractor Performance Using EVM\n\nFor purposes of monitoring contractor performance, EVM is useful because it provides quanti­\ntative or earned value data that can be used to assess how well the contractor is performing. It\nalso provides an early warning of performance problems.\n\nEarned value data such as schedule and cost variances stem from a comparison of:\n\n###### �The planned budget and the amount of budget earned for work accomplished �The budget earned with the actual direct costs for the same work.\n\n\n-----\n\n|Col1|Target Cost TIME Scheduled (or contract budget baseline) NOW Completion|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|Col21|Col22|Col23|Col24|Col25|Col26|Col27|Col28|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n||The PLAN, or the target costof the work that should have been performed to date (commonly referred to as BCWS) The actual cost to perform the work accomplished thus far (commonly referred to as ACWP) “Earned Value,” or the planned cost of the work completed to date (commonly referred to as BCWP)|||||||||||||||The remaining planned effort The target cost of the work scheduled to have been accomplished thus far which has not yet been accomplished (commonly referred to as schedule variance but measured in terms of the value of work not done, rather than time) The overrun on effort completed thus far (commonly referred to as cost variance)||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n|||||||||||||||||||||||||||||\n\n\nTime\n\nFigure 1. Basic EVM Concept of Total Contract Performance\n\n\nSchedule and cost indices provide information such as cost over/underruns, schedule\ndelays/or ahead of schedule, etc. Other EV data elements that can be used to measure/monitor\ncontractor performance are the schedule performance index (SPI) and the cost performance\nindex (CPI). The SPI is defined as:\n\n_BCWP_\n_SPI =_\n\n_BCWS_\n\n\nor the ratio of the BCWP over the BCWS. The SPI is a pure dimensionless quantity, insensi­\ntive to inflation, that measures schedule efficiency. A value above 1.00 indicates that the work\nperformed to date is ahead of schedule. In other words, the SPI is an index measuring the\nefficiency of time utilization.\n\nCPI is an index showing the efficiency of resource utilization and is defined as [6]:\n\n\n-----\n\n_BCWP_\n_CPI =_\n_ACWP_\n\nor the ratio of BCWP to ACWP. CPI is an earned-value metric that measures cost efficiency\nwhere a value above 1.00 indicates that the work performed to date cost less than originally\nplanned. A possible reason for CPI > 1 may be a high employee turnover rate. High turnover\nslows the pace of projects and the associated labor costs because there are gaps in the work\nstream due to employee departures and training issues.\n\nThe CPI and SPI are statistically and mathematically related because they share one vari­\nable in common: BCWP.\n\n_SPI = [(ACWP)(CPI)]_\n_BCWS_\n\nThus, if all of the variation in the data is dominated by BCWP, then the SPI and CPI will\nexhibit high correlation.\n\nThe SPI and CPI performance metrics compare baselines to actuals.\n\n###### EVM Case Scenario\n\nThe best way to illustrate EVM is through an example [4]:\n\nScenario: Building an aquarium budgeted at $5,500 by the end of November. At the\nend of November, spent $5,600 and only accomplished $4,900 worth of work.\n\nEV calculation: BCWS = $5,500, BCWP = $4,900, ACWP = $5,600; Cost Variance =\n-$700; Schedule Variance = -$600\n\nEV Analysis: In the month of November, spent $5,600 but only accomplished $4,900\nworth of work; therefore there is a cost overrun of $700 as well as a delay in schedule\nof $600.\nThe preceding EV data elements can be used to compute the schedule and cost variance\nas follows:\n\nSchedule variance = BCWP – BCWS\n\nCost variance = BCWP – ACWP\n\nThis example is depicted in Figure 2.\n\nThe next step for the MITRE SE would be to review these results and determine whether\naction is needed. Because EVM assesses the combined interaction of scope, schedule, and\ncost, an unfavorable EVM report tells the SE that if these three variables remain fixed and\nthere is no change in how project performance is achieved, the project is at risk to achieve the\nthree objectives. Remember the balloon analogy—if you push in one spot, another must give.\n\n\n-----\n\n120,000\n\n100,000\n\n\n80,000\n\n60,000\n\n\n40,000\n\n20,000\n\n\n0\n\n|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|||||||||||||||\n||56000|||||||||||||\n||55000 49000 BCWS|||||||||||||\n||BCWP|||||||||||||\n|ACWP|ACWP|||||||||||||\n|||||||||||||||\n\n\nFigure 2. EVM Example\n\nIf the EVM results indicate action is needed, as it is in this example, the SE should review\noptions to discuss with the government program manager. These options include:\n\n###### �Accept the schedule delay and find the additional funding to cover the cost overrun and\n\n\nschedule delay.\n###### �Recommend changing either the scope, schedule, or budget for the program or project.\n\nWhen this option is chosen, it is frequently for expediency. This is one of the reasons\nwhy we often see projects delivered on time and within budget with a reduction in the\noriginally specified functionality (scope).\n###### �Recommend changing an underlying condition that is causing the unfavorable EVM\n\n\nresults. These include:\n\n**•** **Scope: Requirements have increased beyond those manageable by the allocated**\n\n\nschedule and cost. Suggest returning to the original scope or adjusting the cost and\nschedule accordingly.\n\n\n\n**•** **Schedule: Consider how the schedule was determined. Was it by an engineering**\n\nanalysis and a work breakdown structure, or, as sometimes happens, was it deter­\nmined by an imposed date? Suggest structuring a realistic schedule or adjusting the\nscope and cost accordingly.\n\n\n\n**•** **Cost: Consider the productivity of the team and whether unrealistic assumptions**\n\nwere made in the original plans. Consider adding experienced, exceptionally capable\nstaff. However, keep in mind that, in general, increasing staff significantly usually\n\n\n-----\n\nwill not erase lost productivity. Suggest adjusting the schedule and cost to accommo­\ndate actual productivity, or reduce scope to match the productivity.\nSituations vary and any of these three approaches might be appropriate. For example,\nsometimes the solution must be implemented quickly, and expediency can override the option\nof digging into the underlying conditions of the unfavorable EVM results.\n\n**Value of EVM**\n\nA sound EVM implementation provides the following contractor performance data: [7]\n\n###### �Relates time-phased budgets to specific contract tasks and/or statements of work �Objectively measures work progress �Properly relates cost, schedule, and technical accomplishment �Allows for informed decision making and corrective action �Is valid, timely, and can be audited �Allows for statistical estimation of future costs �Supplies managers at all levels with status information at the appropriate level �Derives from the same EVM system used by the contractor to manage the contract.\nThe EV technique enhances the cost performance analysis of a project. Traditional cost\nanalysis centers on the actual cost of the completed work. Therefore, much progress has been\nmade to collect the actual costs through time charge and accounting systems that exist on\npractically all projects. What EV brings to the process is a measure of the amount of work that\nhas been done in a unit of measure that is consistent and comparable with costs [8].\n\n###### Best Practices and Lessons Learned\n\n\nEVM does not measure the quality and tech­\n\nnical maturity of the evolving work products.\n\nOther project mechanisms should be used to\n\nassess product quality and maturity as well as the\n\nappropriateness of the scope and conformance\n\nto requirements. For more information, see the\n\narticles “Data-Driven Contractor Evaluations and\n\nMilestone Reviews” and “Planning and Managing\n\nIndependent Assessments.” Performance\nbased earned value (PBEV) is an enhancement\n\nto the EVMS standard for measuring technical\n\nperformance and quality. It is based on stan­\n\ndards and models for systems engineering,\n\n\nsoftware engineering, and project management\n\n[9]. Ultimately, aside from cost and schedule, the\n\nPBEV adds the technical element into the mix.\n\nEVM data is reliable and accurate if and only if\n\nthe following occur [9]:\n\n###### � [The indicated quality of the evolving prod­]\n\nuct is measured\n###### � [The right base measures of technical per­]\n\nformance are selected\n###### � [Progress is objectively assessed. ]\n\nUse EVM to mitigate risk by analyzing the\n\nresults. For example, if targets are not being met,\n\n\n-----\n\nare they realistic? Which activities are making\n\nthe greatest impact? What factors in the project\n\nor organizational culture contribute to results?\n\nPerformance measures tell you what is happen­\n\ning, but they do not tell you why it is happening.\n\nSee the article “How to Develop a Measurement\n\nCapability” and the Risk Management topic.\n\nContractor source selection certification con­\n\nsiderations. EVM certification compliance may\n\nbe contained in the Request for Proposal (RFP).\n\nHowever, not all RFPs require EVM compliance.\n\nDepending on what is written in the RFP regard­\n\ning EVM, the contractor’s EVMS can be evaluated\n\nby ensuring that its EVMS is American National\n\nStandards Institute/Electronic Industries Alliance\n\n(ANSI/EIA) 748 compliant. The ANSI/EIA 748\n\nstandard defines 32 criteria, which are intended to\n\nprovide guidance, structure, and process direc­\n\ntion for the successful operation of EVMS. These\n\n32 criteria provide a basis for evaluating a con­\n\ntractor’s EVMS. The best way to ensure that the\n\ncontractor meets this standard is for the con­\n\ntractor to acquire an EVMS certification from the\n\nDefense Contract Management Agency or from\n\na third-party vendor. In the absence of EVMS\n\ncertification, a contractor’s EVMS can be evalu­\n\nated based on its EVMS plan that includes the\n\n32 ANSI/EIA criteria. See the article “Acquisition\n\nManagement Metrics.”\n\nContractor proposal considerations. The\n\ncontractor’s proposal should demonstrate\n\nunderstanding of EVM by containing knowledge\n\nof industry standards and applying them spe­\n\ncifically to the project. The EVMS should align\n\nwith the integrated master schedule (IMS). The\n\ncontrol account details should represent the\n\n\nwork breakdown structure that drives the discrete\n\ndeliverables that can be associated with cost and\n\nresponsible resources. The EVM training listed\n\non the IMS should specifically be tailored to the\n\nproject. The IMS is the impetus to effective plan­\n\nning and scheduling. It contains planned events\n\nand milestones, accomplishments, exit criteria,\n\nand activities from contract award to the comple­\n\ntion of the contract. It also allows for critical path\n\nanalysis, forecasting, and a baseline plan. This\n\nscheduling aspect has to be linked to EVM in\n\norder for EVM analysis to be accurate and effec­\n\ntive. See the article “Integrated Master Schedule\n\n(IMS)/Integrated Master Plan (IMP) Application.”\n\nEVM is not an effective tool for level-of-effort\n\n(LOE) activities. For non-schedule-based con­\n\ntracts (i.e., contracts composed primarily of LOE),\n\nEVM may not be effectively implemented due\n\nto a lack of measurement on work efforts that\n\ncannot be segmented. With that said, the LOE\n\nmethod can be used for measuring EV; however,\n\nit is primarily reserved for tasks that are time\nrelated rather than task-oriented (i.e., tasks that\n\nhave no measurable output). The LOE method has\n\nno schedule variance; therefore, it should not be\n\nused for any tasks with a schedule that might slip\n\nor be variable [1].\n\nEvaluate the EVMS effectiveness. Ultimately the\n\nmeasurement of a successful EVMS depends\n\non the customer’s ability to use the informa­\n\ntion generated from the system and to evaluate\n\nthe contractor’s ability to manage the project\n\n[9]. The EVMS can be costly to maintain, so it is\n\nimportant to periodically consider its effective­\n\nness and whether changes should be made.\n\nRecent U.S. government policy initiatives have\n\n\n-----\n\nbeen introduced to better facilitate customer\n\ninsight into contractor performance. The reduced\n\nreporting threshold is down to $20 million, and\n\n###### References and Resources\n\n\npolicy revisions related to the integrated baseline\n\nreview will have a significant impact on the admin­\n\nistration and performance of contracts [10].\n\n\n1. Society of Cost Estimating and Analysis (SCEA), 2002, “Tracking cost and schedule per­\n\nformance on project,” Earned Value Management Systems (EVMS).\n\n2. United States Government Accountability Office, May 2005, Performance Measurement\n\nand Evaluation: Definitions and Relationships, GAO-05-739SP.\n\n3. The MITRE Institute, “Introduction to Enterprise Business Strategy,” Performance\n\n_Management._\n\n4. Executive Office of the President, Office of Management and Budget, August 2009,\n\nCircular No. A-11 Preparation, Submission, and Execution of the Budget.\n\n5. The MITRE Corporation, October 11, 2005, Earned Value Management: A Quick Review\n\nfor Busy Executives, Slide 17.\n\n6. Tutorials Point website, Earned Value Management: Cost Variance.\n\n7. Ernst, K. D., October 2006, Earned Value Management Implementation Guide, p. 2.\n\n8. Wilkens, T. T., April 1, 1999, Earned Value, Clear and Simple, p. 4, Los Angeles County\n\nTransit Authority.\n\n9. Solomon, P., August 2005, “Performance-Based Earned Value,” CrossTalk: The Journal of\n\n_Defense Software Engineering._\n\n10. Johnson, C., April 2006, “Implementing an ANSI/EIA-748-Compliant Earned Value\n\nManagement System,” Contract Management.\n\n###### Additional References and Resources\n\n“Earned Value Management,” Wikipedia, accessed October 15, 2009.\n\nMcKinlay, M., April 2006, “Why Not Implement EVM?” International Cost Engineering\nCouncil, ICEC Cost Management Journal.\n\nSeigle, J., May 19, 2006, Earned Value Management Demystified Version 2.0, The MITRE\nCorporation.\n\n[The MITRE Corporation, “Contractor Evaluation,” MITRE Systems Engineering Competency](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n[Model.](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\nUSAID, Earned Value Management.\n\n\n-----\n\nDefinition: Competitive\n\n_Prototyping is an approach in_\n\n_which two or more competing_\n\n_teams (organizations) develop_\n\n_prototypes during the early_\n\n_stages of a project (acquisi­_\n\n_tion or procurement phase)._\n\n_The competing prototypes are_\n\n_compared, and ultimately one_\n\n_is chosen that best addresses_\n\n_the issue(s), problem(s), or_\n\n_challenge(s)._\n\nKeywords: competitive pro­\n\n_totype program, competitive_\n\n_prototype tests, competitive_\n\n_prototyping strategy, competi­_\n\n_tive teams_\n\n\nCONTRACTOR EVALUATION\n###### Competitive Prototyping\n\n**MITRE SE Roles and Expectations: MITRE sys­**\n\ntems engineers (SEs) are expected to understand\n\nthe purpose and role of competitive prototyping\n\n(CP) in the acquisition process, where it occurs in\n\nsystems development, and the benefits and risks\n\nof employing it. MITRE SEs are also expected\n\nto understand and recommend when CP is\n\nappropriate to a situation. They are expected to\n\ndevelop and recommend technical requirements\n\nfor CP efforts as well as strategies and processes\n\nthat encourage and facilitate active participa­\n\ntion of end users and other stakeholders in the\n\nCP process. They are expected to monitor and\n\nevaluate contractor CP technical efforts and\n\nthe acquisition program’s overall CP processes\n\nand recommend changes when warranted.\n\n\n-----\n\n###### Background\n\nPrototyping is a practice in which an early sample or model of a system, capability, or process\nis built to answer specific questions about, give insight into, or reduce uncertainty or risk in\nmany diverse areas. This includes exploring alternative concepts, technology maturity assess­\nments, requirements discovery or refinement, design alternative assessments, and perfor­\nmance or suitability issues. It is part of the SE’s toolkit of techniques for managing uncertainty\nand complexity and mitigating their effects.\n\nThe exact form and focus of prototyping is driven by where it is used in the acquisition\nmanagement system life cycle and the nature of the problem the prototype is intended to\naddress. Prototyping may be used immediately after a decision to pursue a material solution\nto meet an operational need (see Figure 1). In this situation, prototypes are used to exam­\nine alternative concepts as part of the analysis of alternatives leading to a preferred solution\nconcept. Prototyping to explore and evaluate the feasibility of high-level conceptual designs\nmay be performed early in technology development as part of government activities to assess\nand increase technology maturity, discover or refine requirements, or develop a preliminary\ndesign. A prototype may even be developed into a reference implementation and provided to a\ncommercial contractor for production. Competitive prototyping may serve any of the just-cited\npurposes, but it typically involves two or more competing teams, usually from commercial\ncontractors, and is often used as a factor in source selection evaluations leading to a formal\nacquisition program start and contract award. This application of competitive prototyping is\ndepicted in Figure 1.\n\nCompetitive Prototyping\nperformed here = Decision Point = Milestone Review\n\nInitial Operational Full Operational\n\nA B (Program initiation) C Capability Capability\n\nMateriel Technology Engineering and Production and Operations\nSolution Development Manufacturing Deployment and\nAnalysis Development Support\n\nMateriel FRP\nDevelopment Decision\nDecision Review\n\nPre-Systems Acquisition Systems Acquisition Sustainment\n\nFigure 1. The Defense Acquisition Management System\n\n\n-----\n\nHistorically much of competitive prototyping focused on building tangible prototypes\nsuch as weapons, aircraft, and automobiles. The earliest documented modern use of competi­\ntive prototyping dates back to just after the War of 1812, when the United States Army became\ninterested in a breech-loading rifle [1]. Numerous contractors submitted actual hardware\nexamples for the Army’s evaluation. After a hardware design was accepted, the contractor was\ngiven an order for a limited production of these rifles.\n\nThe U.S. aircraft industry used CP extensively throughout the 20th century. Some of the\nearly prototypes were developed by Chanute, the Wright Brothers, Curtiss, and Sikorsky and,\nmore recently, by General Dynamics, Boeing, and McDonnell Douglas. Similarly, the U.S. auto\nindustry uses competitive teams to design concept cars of the future.\n\nThe employment of CP in software-intensive system developments is a relatively recent\nphenomenon.\n\n###### Government Interest and Use\n\nIn several acquisition reform initiatives, the U.S. government encouraged or required competi­\ntive prototyping as a tool to assess technology maturity and reduce program risk. Although\nmentioned less frequently as a primary reason, competitive prototyping can also illuminate\nundiscovered or uncertain requirements before engineering and manufacturing development.\n\nThe Office of Management and Budget has identified competitive prototyping as a risk\nmitigation tool to be used in procurement and cites five advantages for its use [2]:\n\n###### �Proves concepts are sound. �Allows efficient and effective communication (among operational users, procurement\n\nagency, and commercial contractors) to identify the best fit between agency (operational\nuser) needs and marketplace capabilities.\n###### �Provides for competition during the development effort. �Where appropriate, ensures development remains constrained. �Facilitates firm fixed-price contracting for production.\nThe strongest and most recent government support for competitive prototyping comes\nfrom the Department of Defense (DoD), which now requires all programs to formulate acquisi­\ntion strategies and funding that provide for two or more competing teams to produce proto­\ntypes through Milestone B [3], as shown in Figure 1. High-level DoD acquisition officials from\nprevious administrations have also endorsed this required use of competitive prototyping in\ngovernment acquisitions [4].\n\nReasons noted in the DoD decision to require competitive prototyping are to enable\ngovernment and industry teams to discover and solve technical issues before engineering and\nmanufacturing development (EMD), so that EMD can focus on producing detailed manufac­\nturing designs, not on solving myriad technical issues. Other anticipated advantages include:\n\n\n-----\n\nreduced technical risk, validated design, validated cost estimates, insight into contractor man­\nufacturing processes, refined requirements, and reduced time to fielding. But note that not all\nof these advantages can be expected to automatically accrue from all instances of competitive\nprototyping. For example, it is unlikely that substantial insight into contractor manufacturing\nprocesses to be used later for full-scale production will result from a prototyping effort unless\nconsiderable funding is devoted to tooling up for the prototype activity.\n\n###### Best Practices and Lessons Learned [4, 5, 6]\n\n\nWhen size (and skill) matters. Acquisition pro­\n\ngram offices that employ CPs successfully tend to\n\nrequire a larger contingent of government systems\n\nengineers with greater than average technical\n\ncompetence. Although this may appear counter­\n\nintuitive, remember that CPs offer advantages to\n\nprograms that use them, but they must be skillfully\n\nplanned, monitored, and managed by the govern­\n\nment team.\n\nRight-sizing CP requirements. CP is an invest­\n\nment that buys information to reduce uncertainty\n\nand risk. But CP adds up-front costs to a pro­\n\ngram right at a time when funding may be scarce\n\nand support for the program is often weak. A CP\n\nmay run into opposition from the least expected\n\nstakeholders—staunch advocates of a program\n\nwho believe that it must be pushed at great speed\n\nto fill capability gaps. To navigate these external\n\nforces on CP efforts, the program CP require­\n\nments must be right-sized. They must focus on\n\nareas that have substantial risk or offer a high\n\nreward-risk ratio, whatever and wherever those\n\nareas may be—high-level capabilities/levels\nof-service, low-level detailed requirements at\n\nthe subsystem level, or issues in between. It is\n\nalso important to make sure that likely perfor­\n\nmance bottlenecks are identified in the prototype\n\n\nprocess that are measurable and measured as\n\npart of prototype testing.\n\nMake sure your CP learns from antecedent\n\nactivities. One focus of recent government\n\nacquisition reform initiatives is on the importance\n\nof early systems engineering. Some departments\n\nand agencies are strongly recommending or\n\nmandating prototyping in advance of technology\n\ndevelopment, during materiel solution analysis\n\n(see Figure 1). Results or lessons learned from\n\nthese very early prototypes should be used to\n\nshape and inform CP activities.\n\nHave your CP do double duty. The primary pur­\n\npose of CP is to illuminate and eliminate technol­\n\nogy maturity risks. But don’t lose sight of the fact\n\nthat a CP can give important insight into other risk\n\nareas such as contractor manufacturing pro­\n\ncesses (if the CP is resourced appropriately) and\n\nundiscovered operational user requirements. Look\n\nfor and collect information on all issues and areas\n\nthat a CP can illuminate, especially important\n\ndownstream systems engineering activities and\n\nassessments for which CP information can form\n\nthe basis of refined government assessments of\n\nsystem design or estimates of program cost.\n\nEnsure persistent, active engagement of all\n\nstakeholders. CP is not a competition between\n\n\n-----\n\ntwo gladiators in an arena slugging it out until\n\none gives in, at which time everyone else in the\n\ncoliseum looks up and applauds the winner. CP\n\nefforts must be structured to encourage active\n\nparticipation of end users and other stakehold­\n\ners throughout the CP life cycle. To facilitate\n\nthat involvement, CP efforts should empha­\n\nsize frequent demonstrations of progress and\n\nevidence that a prototype can scale. Ideally CPs\n\nshould be developed iteratively or in an evolution­\n\nary fashion and be able to demonstrate interim\n\noperational capabilities. Active operational user\n\nstakeholder engagement is particularly critical to\n\nCPs intended to address requirements discovery\n\nand refinement.\n\nRemember those without “skin in the game.”\n\nImportant stakeholders in the eventual outcome\n\nof a program, like certification and accredita­\n\ntion authorities, are frequently forgotten during\n\nCP. Identify and bring these stakeholders into CP\n\nplanning early so they can advise on “non-start­\n\ners” and be engaged through the entire process.\n\nCommercial competitors are stakeholders, too.\n\nCPs are viewed as investments by commercial\n\nindustry. To attract the best commercial competi­\n\ntors for your program, CP goals must be clearly\n\ndefined and any basis for industry investment\n\n(e.g., internal research and development) must be\n\nconvincing. In particular, the production potential\n\nof the contract must be visible and attractive to\n\nwould-be competitors.\n\nDon’t stop competition too quickly. Make sure\n\nthere is sufficient information to make informed\n\ndecisions before terminating a competition. This is\n\na form of the wisdom, “measure twice, cut once.”\n\nThe Joint Strike Fighter program began with a\n\n\ncompetition involving prototypes built by Boeing\n\nand Lockheed Martin. During the CP phase, it\n\nappeared that both prototypes had been exten­\n\nsively flown before the government chose the\n\nLockheed variant, but rising costs and schedule\n\nslips of the F-35 now suggest that competition\n\nmay have been closed too quickly.\n\nBeware the Potemkin Village. Ensure each com­\n\npetitor is presenting an actual prototype and not\n\nsimply performing a demonstration. A demonstra­\n\ntion can be scripted to perform a few things well\n\nwhen conducted by a knowledgeable person. On\n\nthe other hand, a prototype should be able to be\n\noperated without a script and by a domain expert\n\nwith very little experience with the prototype.\n\nKeep your eyes on the prize. Acquisitions using\n\nCPs can overemphasize the operator functional\n\nfeatures of the prototypes at the expense of other\n\ncritical, less obvious requirements. If competi­\n\ntors hit the mark on “operator functionality” or\n\n“user interface look and feel” more or less the\n\nsame, there may be a risk of eliminating the better\n\ncontractor for production. Carefully evaluate the\n\npotential risks of a prototype becoming the actual\n\nproduct. Prototypes often do not have robust\n\narchitectures, standard designs, a full set of\n\nrequirements, or complete documentation. These\n\nweaknesses may become deployment risks such\n\nas lack of maintainability, scalability, or reproduc­\n\nibility. Also, consider how big a challenge it would\n\nbe for the functionality or look and feel of the\n\nother contractor’s prototype to be modified and\n\napproved, as well as the contractor’s ability and\n\nwillingness to do so.\n\nRetain competitors’ core skills. CPs involve\n\ndown-selects, but the intellectual capital and\n\n\n-----\n\nexperience built up by “losers” need not and\n\nshould not be lost to the program or the govern­\n\nment. During the evaluation interval between\n\neach phase of a CP down-select, the government\n\nshould continue to fund all competitors at a level\n\nof effort sufficient to retain the core engineering\n\nskills of the competing teams. Not doing so risks\n\nlosing key members of competitors’ teams and\n\ncan weaken the overall government acquisition\n\n###### References and Resources\n\n\neffort. One reference [5] suggests that invest­\n\nments in down-selected bidders can be capital­\n\nized on by structuring the ensuing acquisition so\n\nthat unsuccessful bidders are offered “consola­\n\ntion prizes,” such as independent verification and\n\nvalidation contracts. If this tack works at all, it is\n\nmore likely to be attractive to small contractors or\n\nothers attempting to break into the business area\n\nrepresented by the system being acquired.\n\n\n1. Patnode, C.A. Jr., LTC, February 1973, Problems of Managing Competitive Prototype\n\nPrograms Study Report. Defense Systems Management School (Program Management\nCourse Student Study Program), PMC 73-2, p. 3.\n\n2. Office of Management and Budget, June 2006, “Competitive Prototyping,” section II.3.3 of\n\nCapital Programming Guide, V 2.0, Supplement to OMB Circular A-11, Part 7.\n\n3. Office of the Under Secretary of Defense for Acquisition, Technology and Logistics,\n\nSeptember 19, 2007, Memorandum on Prototyping and Competition, Washington, DC:\nPentagon.\n\n4. DuPont, D. G., February 2008, “Proactive Prototypes,” Scientific American.\n\n5. Boehm, B., and D. Ingold, July 2008, Initial Competitive Prototyping Survey Results,\n\nUniversity of Southern California Center for Systems and Software Engineering. Presented\nat July 2008 OUSD/AT&L/SSE—USC/CSSE Workshop on Integrating Systems and Software\nEngineering with the Incremental Commitment Model.\n\n6. Ireland, B., July 2008, Competitive Prototyping: Industry Roundtable, Presented at July\n\n2008 OUSD/AT&L/SSE—USC/CSSE Workshop on Integrating Systems and Software\nEngineering with the Incremental Commitment Model.\n\n\n-----\n\n##### Risk Management\n\nDefinition: Risk is an event that, if it occurs, adversely affects the ability of a\n\n_project to achieve its outcome objectives [1]. Risk management is the process of_\n\n_identifying risk, assessing risk, and taking steps to reduce risk to an acceptable_\n\n_level [2]._\n\nKeywords: opportunity, risk, risk analysis, risk management, uncertainty, uncer­\n\n_tainty analysis_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) working on engineering systems are\n\nexpected to propose, influence, and often design the risk management\n\napproach that enables risk-informed trade-offs and decisions to be\n\nmade throughout a system’s evolution. They are expected to identify,\n\nanalyze, and prioritize risks based on impact, probabilities, dependen­\n\ncies, timeframes, and unknowns. They are expected to prepare and\n\nmonitor risk mitigation plans and strategies, conduct reviews, and\n\nelevate important risks [3].\n\n###### Context\n\nRisk management lies at the intersection of project functions performed\n\nby the systems engineer and the project manager [3]. Historically risk\n\nmanagement focused more on management elements such as sched­\n\nule and cost, and less on technical risks for well-defined or smaller\n\n\n-----\n\nprojects. However, larger and more complex projects and environments have increased the\nuncertainty for the technical aspects of many projects. To increase the likelihood of successful\nproject and program outcomes, the SE and project manager must be actively involved in all\naspects of risk management.\n\nA substantial body of knowledge has developed around risk management. In general, risk\nmanagement includes development of a risk management approach and plan, identification of\ncomponents of the risk management process, and guidance on activities, effective practices,\nand tools for executing each component. One characterization of the risk management process\nis shown in Figure 1 [1].\n\n###### �Step 1. Risk Identification: This is the critical first step of the risk management process.\n\nIts objective is the early and continuous identification of risks, including those within\nand external to the engineering system project.\n\n\nRisk events\nand their\nrelationships\nare defined\n\n\nAssess\nProbability &\nConsequence\n\nWatchlisted\n\nRisk\n\nRisks\n\nTracking\n\nRisk Mitigation\n\nRisk events assessed as\nmedium or high criticality\nmight go into risk\nmitigation planning and\nimplementation; low\ncritical risks might be\ntracked/ monitored on a\nwatch list.\n\n\nProbabilities\nand consequences of\nrisk events\nare assessed\n\nConsequences may\ninclude cost,\n\n2. Risk schedule, technical\nImpact performance impacts,\nAssessment as well as capability or\n\nfunctionality impacts\n\nAssess Risk\nCriticality\n\nDecision-analytic\n\n3. Risk rules applied to\n\nrank-order\n\nPrioritization\n\nidentified risk\n\nAnalysis\n\nevents from “most\nto least” critical\n\n\nFigure 1. Fundamental Steps of Risk Management\n\n\n-----\n\n###### �Step 2. Risk Impact or Consequence Assessment: An assessment is made of the impact\n\neach risk event could have on the engineering system project. Typically this includes\nhow the event could impact cost, schedule, or technical performance objectives.\nImpacts are not limited to only these criteria. Additional criteria such as political or eco­\nnomic consequences may also require consideration. In addition, an assessment is made\nof the probability (chance) each risk event will occur.\n###### �Step 3. Risk Prioritization: The overall set of identified risk events, their impact assess­\n\nments, and their occurrence probabilities are “processed” to derive a most critical to\nleast critical rank-order of identified risks. A major purpose for prioritizing risks is to\nform a basis for allocating critical resources.\n###### �Step 4. Risk Mitigation Planning: This step involves the development of mitigation\n\nplans designed to manage, eliminate, or reduce risk to an acceptable level. Once a plan\nis implemented, it is continually monitored to assess its efficacy with the intent to revise\nthe course of action, if needed.\nTwo other steps are involved in executing risk management: developing the approach and\nplan, and selecting the risk management tools. The risk management approach determines the\nprocesses, techniques, tools, and team roles and responsibilities for a specific project. The risk\nmanagement plan describes how risk management will be structured and performed on the\nproject [4]. Risk management tools support the implementation and execution of program risk\nmanagement in systems engineering programs. In selecting the appropriate tools, the project\nteam considers factors such as program complexity and available resources.\n\nThese six steps are discussed in the five articles under the SEG’s Risk Management topic.\n\n###### Risk Management Principles\n\nMITRE SEs supporting government customers in risk management activities have\nobserved the following elements common to the Department of Defense (DoD) and civilian\nenvironments.\n\nRisk Management Is Fundamental\n\nAn event is uncertain if there is indefiniteness about its outcome [1]. Risk management\nacknowledges the concept of uncertainty, which includes risks (unfavorable outcomes) and\nopportunities (favorable outcomes). Risk management is a formal and disciplined practice for\naddressing risk. In many ways, it is indistinguishable from program management. It includes\nidentifying risks, assessing their probabilities and consequences, developing management\nstrategies, and monitoring their state to maintain situational awareness of changes in potential\nthreats.\n\n\n-----\n\nEvery Project Involves Risk\n\nEvery project is a temporary endeavor undertaken to provide a unique result [3]; it is an\nundertaking that has not been done before. Therefore, all projects involve some level of risk,\neven if similar projects have been completed successfully.\n\nRisk and Opportunity Must Be Balanced\n\nRisk and opportunity management deal with uncertainty that is present throughout the sys­\ntems’ life cycle. The objective is to achieve a proper balance between them, while recognizing\none is not the complement of the other.\n\nTypically more risk and opportunity is involved in decisions that are made early in the\nproject life cycle because those decisions have a more significant impact on project scope,\ncost, and schedule than those made later in the life cycle.\n\n**Risk Is Present in Complicated Relationships**\n\nRisk affects all aspects of engineering a system, and can be present in complicated\nrelationships among project goals. A system may be intended for technical accomplish­\nments near the limits of engineering or the maturity of technology, leading to technical\nrisks. System development may be deployed too early to meet an imminent threat, thus\nresulting in schedule risks. All systems have funding challenges, which lead to cost risks.\nRisk can be introduced by external threats, due to changing social, political, or economic\nlandscapes.\n\n###### References and Resources\n\n1. Garvey, P. R., 2008, Analytical Methods for Risk Management: A Systems Engineering\n\n_Perspective, Chapman-Hall/CRC-Press, Taylor & Francis Group (UK), Boca Raton, London,_\nNew York, ISBN: 1584886374.\n\n2. Stoneburner, G., A. Goguen, and A. Feringa, July 2002, Risk Management Guide for\n\nInformation Technology System, National Institute of Standards and Technology, Special\nPublication 800-30, p. 1.\n\n[3. The MITRE Corporation, September 1, 2007, MITRE Systems Engineering (SE)](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n[Competency Model, Ver. 1, pp. 10, 40–41.](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n4. Project Management Institute, A Guide to the Project Management Body of Knowledge,\n\n_(PMBOK Guide), Fourth Edition, ANSI/PMI 99-001-2008, pp. 273–312._\n\n\n-----\n\n###### Additional References and Resources\n\nIEEE Standard for Software Life Cycle Processes - Risk Management, IEEE Std. 1540-2001.\n\nInternational Council on Systems Engineering (INCOSE), January 2010, INCOSE Systems\n_Engineering Handbook, Version 3.2, INCOSE-TP-2003-002-03.2, p. 213–225._\n\nInternational Organization for Standardization (ISO)/International Electrotechnical\n[Commission (IEC), ISO/IEC Guide 73, ISO/IEC Guide 73, Risk Management Vocabulary](http://www.iso.org/iso/catalogue_detail.htm?csnumber=44651)\n[Guidelines.](http://www.iso.org/iso/catalogue_detail.htm?csnumber=44651)\n\nKerzner, H., 2003, Project Management, Eighth Ed., John Wiley & Sons, Inc., pp. 651–710.\n\nKossiakoff, A. and W. N. Sweet, 2003, Systems Engineering Principles and Practice, John Wiley\nand Sons, Inc., pp. 98–106.\n\nMITRE Systems Engineering Practice Office, Risk Management Toolkit.\n\nMoore, J. W., 2006, The Road Map to Software Engineering, A Standards-Based Guide, IEEE\nComputer Society, pp. 171–172.\n\nMulcahy, R., 2003, Risk Management: Tricks of the Trade for Project Managers, RMC\nPublications.\n\nOMB Circular A-11 E-300, June 2008, “Part 7: Planning, Budgeting, Acquisition and\nManagement of Capital Assets.”\n\nSoftware Engineering Institute CMMI, “Risk and Opportunity Management.”\n\nThayer, R. H., and M. Dorfman (eds.), 2005, Software Engineering Volume 2: The Supporting\n_Processes, Third Ed., IEEE Computer Society._\n\n[The Institute of Risk Management, “The Risk Management Standard.”](http://www.theirm.org)\n\nWoodward, D., and K. Buck, July 2007, “Office of Management and Budget (OMB) Uncertainty\nand Risk Assessment Requirements: A Preliminary MITRE Study (MP #070137).”\n\n\n-----\n\nDefinition: Risk management is\n\n_the process of identifying risk,_\n\n_assessing risk, and taking steps_\n\n_to reduce risk to an acceptable_\n\n_level [1]. The risk management_\n\n_approach determines the pro­_\n\n_cesses, techniques, tools, and_\n\n_team roles and responsibilities_\n\n_for a specific project. The risk_\n\n_management plan describes_\n\n_how risk management will be_\n\n_structured and performed on_\n\n_the project [2]._\n\nKeywords: risk management,\n\n_risk management approach,_\n\n_risk management plan, risk_\n\n_management process_\n\n\nRISK MANAGEMENT\n###### Risk Management Approach and Plan\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) working on govern­\n\nment programs propose and influence, and\n\noften design, the risk management approach.\n\nThey prepare and monitor risk mitigation plans\n\nand strategies for the government project or\n\nprogram office, and they review risk management\n\nplans prepared by government contractors [3].\n\n\n-----\n\n###### Background\n\nAs a management process, risk management is used to identify and avoid the potential cost,\nschedule, and performance/technical risks to a system, take a proactive and structured\napproach to manage negative outcomes, respond to them if they occur, and identify potential\nopportunities that may be hidden in the situation [4]. The risk management approach and\nplan operationalize these management goals.\n\nBecause no two projects are exactly alike, the risk management approach and plan\nshould be tailored to the scope and complexity of individual projects. Other considerations\ninclude the roles, responsibilities, and size of the project team, the risk management processes\nrequired or recommended by the government organization, and the risk management tools\navailable to the project.\n\nRisk occurs across the spectrum of government and its various enterprises, systems\nof systems, and individual systems. At the system level, the risk focus typically centers on\ndevelopment. Risk exists in operations, requirements, design, development, integration, test­\ning, training, fielding, etc. (see the SEG’s SE Life-Cycle Building Blocks section). For systems\nof systems, the dependency risks rise to the top. Working consistency across the system of\nsystems, synchronizing capability development and fielding, considering whether to interface,\ninteroperate, or integrate, and the risks associated with these paths all come to the forefront in\nthe system-of-systems environment. At the enterprise level, governance and complexity risks\nbecome more prominent. Governance risk of different guidance across the enterprise for the\nbenefit of the enterprise will trickle down into the system of systems and individual systems,\nresulting in potentially unanticipated demands and perhaps suboptimal solutions at the low\nlevel that may be beneficial at the enterprise level. Dealing with the unknowns increases, and\nthe risks associated with these—techniques in the SEG’s Enterprise Engineering section such\nas loose couplings, federated architectures, and portfolio management—can help the MITRE\nSE alleviate these risks.\n\n###### Risk Management in System-Level Programs\n\nSystem-level risk management is predominantly the responsibility of the team working to\nprovide capabilities for a particular development effort. Within a system-level risk area, the\nprimary responsibility falls to the system program manager and SE for working risk manage­\nment, and the developers and integrators for helping identify and create approaches to reduce\nrisk. In addition, a key responsibility is with the user community’s decision maker on when to\naccept residual risk after it and its consequences have been identified. The articles in the Risk\nManagement topic area provide guidance for identifying risk (“Risk Identification”), mitigat­\ning risks at the system level with options like control, transfer, and watch (“Risk Mitigation\nPlanning, Implementation, and Progress Monitoring”), and a program risk assessment scale\n\n\n-----\n\nand matrix (“Risk Impact Assessment and Prioritization”). These guidelines, together with\nMITRE SEs using tools such as those identified in the “Risk Management Tools” article, will\nhelp the program team deal with risk management and provide realism to the development\nand implementation of capabilities for the users.\n\n###### Risk Management in System-of-Systems Programs\n\nToday, the body of literature on engineering risk management is largely aimed at address­\ning traditional engineering system projects—those systems designed and engineered against\na set of well-defined user requirements, specifications, and technical standards. In contrast,\nlittle exists on how risk management principles apply to a system whose functionality and\nperformance is governed by the interaction of a set of highly interconnected, yet independent,\ncooperating systems. Such systems may be referred to as systems of systems.\n\nA system of systems can be thought of as a set or arrangement of systems that are related\nor interconnected to provide a given capability that, otherwise, would not be possible. The\nloss of any part of the supporting systems degrades or, in some cases, eliminates the perfor­\nmance or capabilities of the whole.\n\nWhat makes risk management in the engineering of systems of systems more challenging\nthan managing risk in a traditional systems engineering project? The basic risk management\nprocess steps are the same. The challenge comes from implementing and managing the pro­\ncess steps across a large-scale, complex system of systems—one whose subordinate systems,\nmanagers, and stakeholders may be geographically dispersed, organizationally distributed,\nand may not have fully intersecting user needs.\n\nHow does the delivery of capability over time affect how risks are managed in a system\nof systems? The difficulty is in aligning or mapping identified risks to capabilities planned to\nbe delivered within a specified build by a specified time. Here, it is critically important that\nassessments are made as a function of which capabilities are affected, when these effects\noccur, and their impacts on users and stakeholders.\n\nLack of clearly defined system boundaries, management lines of responsibility, and\naccountability further challenge the management of risk in the engineering of systems of\nsystems. User and stakeholder acceptance of risk management, and their participation in the\nprocess, is essential for success.\n\nGiven the above, a program needs to establish an environment where the reporting of\nrisks and their potential consequences is encouraged and rewarded. Without this, there will\nbe an incomplete picture of risk. Risks that threaten the successful engineering of a system of\nsystems may become evident only when it is too late to effectively manage or mitigate them.\n\nFrequently a system of systems is planned and engineered to deliver capabilities through\na series of evolutionary builds. Risks can originate from different sources and threaten the\n\n\n-----\n\nsystem of systems at different times during their evolution. These risks and their sources\nshould be mapped to the capabilities they potentially affect, according to their planned deliv­\nery date. Assessments should be made of each risk’s potential impacts to planned capabilities,\nand whether they have collateral effects on dependent capabilities or technologies.\n\nIn most cases, the overall system-of-systems risk is not just a linear “roll-up” of its subor­\ndinate system-level risks. Rather, it is a combination of specific lower level individual system\nrisks that, when put together, have the potential to adversely impact the system of systems in\nways that do not equate to a simple roll-up of the system-level risks. The result is that some\nrisks will be important to the individual systems and be managed at that level, while others\nwill warrant the attention of system-of-systems engineering and management.\n\n###### Risk Management in Enterprise Engineering Programs\n\nRisk management of enterprise systems poses an even greater challenge than risk manage­\nment in systems-of-systems programs.\n\nEnterprise environments (e.g., the Internet) offer users ubiquitous, cross-boundary access\nto wide varieties of services, applications, and information repositories. Enterprise systems\nengineering is an emerging discipline. It encompasses and extends “traditional” systems\nengineering to create and evolve “webs” of systems and systems of systems that operate in\na network-centric way to deliver capabilities via services, data, and applications through an\ninterconnected network of information and communications technologies. This is an environ­\nment in which systems engineering is at its “water’s edge.”\n\nIn an enterprise, risk management is viewed as the integration of people, processes, and\ntools that together ensure the early and continuous identification and resolution of enterprise\nrisks. The goal is to provide decision makers an enterprise-wide understanding of risks, their\npotential consequences, interdependencies, and rippling effects within and beyond enterprise\n“boundaries.” Ultimately risk management aims to establish and maintain a holistic view of\nrisks across the enterprise, so capabilities and performance objectives are achieved via riskinformed resource and investment decisions.\n\nToday we are in the early stage of understanding how systems engineering, engineering\nmanagement, and social science methods weave together to create systems that “live” and\n“evolve” in enterprise environments.\n\n###### Requirements for Getting Risk Management Started\n �Senior leadership commitment and participation is required. �Stakeholder commitment and participation is required. �Risk management is made a program-wide priority and “enforced” as such throughout\n\nthe program’s life cycle.\n\n\n-----\n\n###### �Technical and program management disciplines are represented and engaged. Both pro­\n\ngram management and engineering specialties need to be communicating risk informa­\ntion and progress toward mitigation. Program management needs to identify contract­\ning, funding concerns, SEs need to engage across the team and identify risks, costs, and\npotential ramifications if the risk were to occur, as well as mitigation plans (actions to\nreduce the risk, and cost/resources needed to execute successfully).\n###### �Risk management is integrated into the program’s business processes and systems engi­\n\nneering plans. Examples include risk status included in management meetings and/or\nprogram reviews, risk mitigation plan actions tracked in schedules, and cost estimates\nreflective of risk exposure.\n\n###### The Risk Management Plan\n\nThe Risk Management Plan describes a process, such as the fundamental steps shown in\nFigure 1 in the preceding “Risk Management” topic article, that are intended to enable the\nengineering of a system that is accomplished within cost, delivered on time, and meets user\nneeds.\n\n###### Best Practices and Lessons Learned\n\n\nTwenty-one “musts.” In supporting both\n\nDepartment of Defense (DoD) and civilian agency\n\nprojects and programs, MITRE SEs have found the\n\nfollowing minimum conditions needed to initiate\n\nand continuously execute risk management suc­\n\ncessfully. With these, the program increases its\n\nchance of identifying risks early so the goals and\n\nobjectives are achieved [5].\n\n1. Risk management must be a priority for\n\nleadership and throughout the program’s\n\nmanagement levels. Maintain leader­\n\nship priority and open communication.\n\nTeams will not identify risks if they do\n\nnot perceive an open environment to\n\nshare risk information (messenger not\n\nshot) or management priority on want­\n\ning to know risk information (requested\n\nat program reviews and meetings), or if\n\n\nthey do not feel the information will be\n\nused to support management decisions\n\n(lip service, information not informative,\n\nteam members will not waste their time\n\nif the information is not used).\n\n2. Risk management must never be del­\n\negated to staff that lack authority.\n\n3. A formal and repeatable risk manage­\n\nment process must be present—one\n\nthat is balanced in complexity and data\n\nneeds, such that meaningful and action­\n\nable insights are produced with mini­\n\nmum burden.\n\n4. The management culture must encour­\n\nage and reward identifying risk by staff at\n\nall levels of program contribution.\n\n\n-----\n\n5. Program leadership must have the ability\n\nto regularly and quickly engage subject\n\nmatter experts.\n\n6. Risk management must be formally inte­\n\ngrated into program management.\n\n7. Participants must be trained in the\n\nprogram’s specific risk management\n\npractices and procedures.\n\n8. A risk management plan must be written\n\nwith its practices and procedures con­\n\nsistent with process training.\n\n9. Risk management execution must be\n\nshared among all stakeholders.\n\n10. Risks must be identified, assessed, and\n\nreviewed continuously—not just prior to\n\nmajor reviews.\n\n11. Risk considerations must be a central\n\nfocus of program reviews.\n\n12. Risk management working groups and\n\nreview boards must be rescheduled\n\nwhen conflicts arise with other program\n\nneeds.\n\n13. Risk mitigation plans must be developed,\n\nsuccess criteria defined, and their imple­\n\nmentation monitored relative to achiev­\n\ning success criteria outcomes.\n\n14. Risks must be assigned only to staff\n\nwith authority to implement mitigation\n\nactions and obligate resources.\n\n15. Risk management must never be\n\noutsourced.\n\n16. Risks that extend beyond traditional\n\nimpact dimensions of cost, schedule,\n\nand technical performance must be\n\nconsidered (e.g., programmatic, enter­\n\n\nprise, cross-program/cross-portfolio,\n\nand social, political, economic impacts).\n\n17. Technology maturity and its future readi­\n\nness must be understood.\n\n18. The adaptability of a program’s technol­\n\nogy to change in operational environ­\n\nments must be understood.\n\n19. Risks must be written clearly using the\n\nCondition-If-Then protocol.\n\n20. The nature and needs of the pro­\n\ngram must drive the design of the risk\n\nmanagement process with which a risk\n\nmanagement tool/database conforms.\n\n21. A risk management tool/database\n\nmust be maintained with current risk\n\nstatus information; preferably, employ\n\na tool/database that rapidly produces\n\n“dashboard-like” status reports for\n\nmanagement.\n\nIt is important for MITRE SEs as well as project\n\nand program leaders to keep these minimum\n\nconditions in mind, with each taking action appro­\n\npriate for their roles.\n\nGet top-level buy-in. MITRE SEs can help\n\ngain senior leadership support for risk manage­\n\nment by highlighting some of the engineering as\n\nwell as programmatic risks. MITRE SEs should\n\nprepare assessments of the impact that risks\n\ncould manifest and back them by facts and data\n\n(e.g., increased schedule due to more develop­\n\nment, increased costs, increased user training\n\nfor unique, technology-edge capabilities, and\n\npotential of risk that capabilities will not be used\n\nbecause they do not interoperate with legacy\n\nsystems). MITRE SEs can highlight the various\n\n\n-----\n\nrisk areas, present the pros and cons of alter­\n\nnative courses of mitigation actions (and their\n\nimpacts), and help the decision makers determine\n\nthe actual discriminators and residual impact\n\nof taking one action or another. In addition to\n\ndata-driven technical assessments, success in\n\ngetting top-level buy-in requires consideration of\n\npolitical, organizational/operational, and economic\n\nfactors as seen through the eyes of the senior\n\nleadership [6].\n\nGet stakeholder trust. Gain the trust of stake­\n\nholders by clearly basing risk reduction or\n\nacceptance recommendations on getting mission\n\ncapabilities to users.\n\nLeverage your peers. Someone at MITRE gener­\n\nally knows a lot about every risk management\n\ntopic imaginable. This includes technical, opera­\n\ntional, and programmatic dimensions of risks\n\nand mitigations. Bringing the company to bear is\n\nmore than a slogan—it is a technique to use, as\n\nrisks are determined, particularly in system-of\nsystems and enterprise programs. In all likeli­\n\nhood, MITRE is working other parts of these large\n\nproblems.\n\nThink horizontal. Emphasize cross-program\n\nor cross-organization participation in risk\n\n###### References and Resources\n\n\nidentification, assessment, and management.\n\nCross-team coordination and communication can\n\nbe particularly useful in risk management. All “-ili­\n\nties” (e.g., information assurance, security, logis­\n\ntics, software) should be represented in the risk\n\nreviews. Communication of risk information helps\n\nilluminate risks that have impact across organiza­\n\ntions and amplifies the benefits of mitigations that\n\nare shared.\n\nStay savvy in risk management processes and\n\ntools. Become the knowledgeable advisor in\n\navailable risk management processes and tools.\n\nMany government organizations have program\n\nmanagement offices that have defined risk man­\n\nagement processes, templates, and tools. These\n\nshould be used as a starting point to develop\n\nthe specific approach and plan for an individual\n\nproject or program. Make sure the government\n\nsponsors or customers have the current infor­\n\nmation about the risk management approach\n\nand plan required by their organizations, and\n\nassist them in complying with it. Assist the spon­\n\nsors or customers in determining the minimum\n\nset of activities for their particular program\n\nthat will produce an effective risk management\n\napproach and plan.\n\n\n1. National Institute of Standards and Technology, July 2002, Risk Management Guide for\n\n_Information Technology System, Special Publication 800-30, p. 1._\n\n[2. Project Management Institute, A Guide to the Project Management Body of Knowledge,](http://www.pmi.org/Pages/default.aspx)\n\n_(PMBOK Guide), Fourth Edition, ANSI/PMI 99-001-2008, pp. 273–312._\n\n[3. The MITRE Institute, September 1, 2007, “MITRE Systems Engineering (SE) Competency](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n\n[Model, Ver. 1,” pp. 10, 41–42.](http://www.mitre.org/work/systems_engineering/guide/10_0678_presentation.pdf)\n\n\n-----\n\n[4. International Council on Systems Engineering (INCOSE), January 2010, INCOSE Systems](http://www.incose.org/ProductsPubs/products/sehandbook.aspx)\n\n[Engineering Handbook, Ver. 3.2, INCOSE-TP-2003-002-03.2, pp. 213–225.](http://www.incose.org/ProductsPubs/products/sehandbook.aspx)\n\n5. Garvey, P. R., 2008, Analytical Methods for Risk Management: A Systems Engineering\n\n_Perspective, Chapman-Hall/CRC-Press, Taylor & Francis Group (UK), Boca Raton, London,_\nNew York, ISBN: 1584886374.\n\n6. Neugent, B. “Persuasion,” The MITRE Corporation.\n\n###### Additional References and Resources\n\nKossiakoff, A. and W. N. Sweet, 2003, Systems Engineering Principles and Practice, John\nWiley and Sons, Inc., pp. 98–106.\n\n[MITRE SEPO Risk Management Toolkit.](http://www.mitre.org/work/sepo/toolkits/risk/)\n\n\n-----\n\nDefinition: Risk identification is\n\n_the process of determining risks_\n\n_that could potentially prevent_\n\n_the program, enterprise, or_\n\n_investment from achieving its_\n\n_objectives. It includes docu­_\n\n_menting and communicating_\n\n_the concern._\n\nKeywords: risk, risk identifica­\n\n\nRISK MANAGEMENT\n###### Risk Identification\n\n\n_tion, risk management_ **MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) working on govern­\n\nment programs are expected to identify\n\nrisks that could impact the project and\n\nprogram. They are expected to write and\n\nreview risk statements that are clear, unam­\n\nbiguous, and supported by evidence [1].\n\n\n-----\n\n###### Background\n\nRisk identification is the critical first step of the risk management process depicted in Figure 1\nin the “Risk Management” topic article.\n\nThe objective of risk identification is the early and continuous identification of events that,\nif they occur, will have negative impacts on the project’s ability to achieve performance or\ncapability outcome goals. They may come from within the project or from external sources.\n\nThere are multiple types of risk assessments, including program risk assessments, risk\nassessments to support an investment decision, analysis of alternatives, and assessments of\noperational or cost uncertainty. Risk identification needs to match the type of assessment\nrequired to support risk-informed decision making. For an acquisition program, the first step\nis to identify the program goals and objectives, thus fostering a common understanding across\nthe team of what is needed for program success. This gives context and bounds the scope by\nwhich risks are identified and assessed.\n\n###### Identifying Risks in the Systems Engineering Program\n\nThere are multiple sources of risk. For risk identification, the project team should review the\nprogram scope, cost estimates, schedule (to include evaluation of the critical path), technical\nmaturity, key performance parameters, performance challenges, stakeholder expectations\nvs. current plan, external and internal dependencies, implementation challenges, integration,\ninteroperability, supportability, supply chain vulnerabilities, ability to handle threats, cost\ndeviations, test event expectations, safety, security, and more. In addition, historical data from\nsimilar projects, stakeholder interviews, and risk lists provide valuable insight into areas for\nconsideration of risk.\n\nRisk identification is an iterative process. As the program progresses, more informa­\ntion will be gained about the program (e.g., specific design), and the risk statement will be\nadjusted to reflect the current understanding. New risks will be identified as the project pro­\ngresses through the life cycle.\n\n###### Best Practices and Lessons Learned\n\n\nOperational risk. Understand the operational\n\nnature of the capabilities you are supporting and\n\nthe risk to the end users, their missions, and their\n\noperations of the capabilities. Understanding\n\nof the operational need/mission (see the SEG’s\n\nConcept Development topic) will help you appre­\n\nciate the gravity of risks and the impact they could\n\n\nhave to the end users. This is a critical part of\n\nrisk analysis—realizing the real-world impact that\n\ncan occur if a risk arises during operational use.\n\nTypically operational users are willing to accept\n\nsome level of risk if they are able to accomplish\n\ntheir mission (e.g., mission assurance), but you\n\nneed to help users to understand the risks they\n\n\n-----\n\nare accepting and to assess the options, bal­\n\nances, and alternatives available.\n\nTechnical maturity. Work with and leverage\n\nindustry and academia to understand the tech­\n\nnologies being considered for an effort and the\n\nlikely transition of the technology over time. One\n\napproach is to work with vendors under a non\ndisclosure agreement to understand the capabili­\n\nties and where they are going, so that the risk can\n\nbe assessed.\n\nNon-developmental items (NDI). NDI includes\n\ncommercial-off-the-shelf and government-off\nthe-shelf items. To manage risk, consider the via­\n\nbility of the NDI provider. Does the provider have\n\nmarket share? Does the provider have appropriate\n\nlongevity compared to its competitors? How does\n\nthe provider address capability problems and\n\nrelease fixes, etc.? What is the user base for the\n\nparticular NDI? Can the provider demonstrate the\n\nNDI, preferably in a setting similar to that of your\n\ncustomer? Can the government use the NDI to\n\ncreate a prototype? All of these factors will help\n\nassess the risk of the viability of the NDI and the\n\nprovider. Seek answers to these questions from\n\nother MITRE staff that have worked the area or\n\nhave used the NDI being assessed.\n\nAcquisition drivers. Emphasize critical capabil­\n\nity enablers, particularly those that have limited\n\nalternatives. Evaluate and determine the primary\n\ndrivers to an acquisition and emphasize their\n\nassociated risk in formulating risk mitigation rec­\n\nommendations. If a particular aspect of a capabil­\n\nity is not critical to its success, its risk should be\n\nassessed, but it need not be the primary focus of\n\nrisk management. For example, if there is risk to a\n\nproposed user interface, but the marketplace has\n\n\nnumerous alternatives, the success of the pro­\n\nposed approach is probably less critical to overall\n\nsuccess of the capability. On the other hand, an\n\ninformation security feature is likely to be critical.\n\nIf only one alternative approach satisfies the need,\n\nemphasis should be placed on it. Determine the\n\nprimary success drivers by evaluating needs and\n\ndesigns, and determining the alternatives that\n\nexist. Is a unique solution on the critical path to\n\nsuccess? Make sure critical path analyses include\n\nthe entire systems engineering cycle and not just\n\ndevelopment (i.e., system development, per se,\n\nmay be a “piece of cake,” but fielding in an active\n\noperational situation may be a major risk).\n\nUse capability evolution to manage risk. If\n\nparticular requirements are driving implementa­\n\ntion of capabilities that are high risk due to unique\n\ndevelopment, edge-of-the-envelope perfor­\n\nmance needs, etc., the requirements should be\n\ndiscussed with the users for their criticality. It may\n\nbe that the need could be postponed, and the\n\ndevelopment community should assess when it\n\nmight be satisfied in the future. Help users and\n\ndevelopers gauge how much risk (and schedule\n\nand cost impact) a particular capability should\n\nassume against the requirements to receive\n\nless risky capabilities sooner. In developing your\n\nrecommendations, consider technical feasibility\n\nand knowledge of related implementation suc­\n\ncesses and failures to assess the risk of imple­\n\nmenting now instead of in the future. In deferring\n\ncapabilities, take care not to fall into the trap of\n\npostponing ultimate failure by trading near-term\n\neasy successes for a future of multiple high-risk\n\nrequirements that may be essential to overall\n\nsuccess.\n\n\n-----\n\nKey performance parameters (KPPs). Work\n\nclosely with the users to establish KPPs. Overall\n\nrisk of program cancelation can be centered\n\non failure to meet KPPs. Work with the users to\n\nensure the parameters are responsive to mission\n\nneeds and technically feasible. The parameters\n\nshould not be so lenient that they can easily be\n\nmet, but not meet the mission need; nor should\n\nthey be so stringent that they cannot be met\n\nwithout an extensive effort or pushing technol­\n\nogy—either of which can put a program at risk.\n\nSeek results of past operations, experiments, per­\n\nformance assessments, and industry implemen­\n\ntations to help determine performance feasibility.\n\nExternal and internal dependencies. Having an\n\nenterprise perspective can help acquirers, pro­\n\ngram managers, developers, integrators, and users\n\nappreciate risk from dependencies of a devel­\n\nopment effort. With the emergence of service\noriented approaches, a program will become\n\nmore dependent on the availability and operation\n\nof services provided by others that they intend to\n\nuse in their program’s development efforts. Work\n\nwith the developers of services to ensure quality\n\nservices are being created, and thought has been\n\nput into the maintenance and evolution of those\n\nservices. Work with the development program\n\nstaff to assess the services that are available,\n\ntheir quality, and the risk that a program has in\n\nusing and relying on the service. Likewise, there\n\nis a risk associated with creating the service and\n\nnot using services from another enterprise effort.\n\nHelp determine the risks and potential benefits\n\nof creating a service internal to the development\n\nwith possibly a transition to the enterprise service\n\nat some future time.\n\n\nIntegration and interoperability (I&I). I&I will\n\nalmost always be a major risk factor. They are\n\nforms of dependencies in which the value of\n\nintegrating or interoperating has been judged to\n\noverride their inherent risks. Techniques such as\n\nenterprise federation architecting, composable\n\ncapabilities on demand, and design patterns can\n\nhelp the government plan and execute a route\n\nto navigate I&I risks. See the SEG’s Enterprise\n\nEngineering section for articles on techniques for\n\naddressing I&I associated risks.\n\nInformation security. Information security is a\n\nrisk in nearly every development. Some of this\n\nis due to the uniqueness of government needs\n\nand requirements in this area. Some of this is\n\nbecause of the inherent difficulties in counter­\n\ning cyber attacks. Creating defensive capabilities\n\nto cover the spectrum of attacks is challenging\n\nand risky. Help the government develop resiliency\n\napproaches (e.g., contingency plans, backup/\n\nrecovery). Enabling information sharing across\n\nsystems in coalition operations with international\n\npartners presents technical challenges and policy\n\nissues that translate into development risk. The\n\ninformation security issues associated with supply\n\nchain management are so broad and complex\n\nthat even maintaining rudimentary awareness of\n\nthe threats is a tremendous challenge.\n\nSkill level. The skill or experience level of the\n\ndevelopers, integrators, government, and other\n\nstakeholders can lead to risks. Be on the lookout\n\nfor insufficient skills and reach across the cor­\n\nporation to fill any gaps. In doing so, help educate\n\ngovernment team members at the same time you\n\nare bringing corporate skills and experience to\n\nbear.\n\n\n-----\n\nCost risks. Programs will typically create a gov­\n\nernment’s cost estimate that considers risk. As\n\nyou develop and refine the program’s technical\n\nand other risks, the associated cost estimates\n\nshould evolve, as well. Cost estimation is not a\n\none-time activity.\n\nHistorical information as a guide to risk iden­\n\ntification. Historical information from similar\n\ngovernment programs can provide valuable\n\ninsight into future risks. Seek out information\n\nabout operational challenges and risks in various\n\noperation lessons learned, after-action reports,\n\nexercise summaries, and experimentation results.\n\nCustomers often have repositories of these\n\nto access. Government leaders normally will\n\ncommunicate their strategic needs and chal­\n\nlenges. Understand and factor these into your\n\nassessment of the most important capabilities\n\nneeded by your customer and as a basis for risk\n\nassessments.\n\nHistorical data to help assess risk is frequently\n\navailable from the past performance assess­\n\nments and lessons learned of acquisition\n\nprograms and contractors. In many cases, MITRE\n\nstaff will assist the government in preparing per­\n\nformance information for a particular acquisition.\n\nThe AF has a Past Performance Evaluation Guide\n\n(PPEG) that identifies the type of information to\n\ncapture that can be used for future government\n\nsource selections [3]. This repository of informa­\n\ntion can help provide background information of\n\nprevious challenges and where they might arise\n\nagain—both for the particular type of devel­\n\nopment activity as well as with the particular\n\ncontractors.\n\n\nNumerous technical assessments for vendor\n\nproducts can be accessed to determine the\n\nrisk and viability of various products. One MITRE\n\nrepository of evaluations of tools is the Analyst’s\n\nToolshed [4] that contains guidance on and expe­\n\nrience with analytical tools. Using resources like\n\nthese and seeking others who have tried products\n\nand techniques in prototypes and experiments\n\ncan help assess the risks for a particular effort.\n\nHow to write a risk—a best practice [2]. A best\n\npractice protocol for writing a risk statement is\n\nthe Condition-If-Then construct. This protocol\n\napplies to risk management processes designed\n\nfor almost any environment. It is a recognition that\n\na risk, by its nature, is probabilistic and one that, if\n\nit occurs, has unwanted consequences.\n\nWhat is the Condition-If-Then construct?\n\n###### � [The ][Condition][ reflects what is known ]\n\ntoday. It is the root cause of the identified\n\nrisk event. Thus the Condition is an event\n\nthat has occurred, is presently occurring,\n\nor will occur with certainty. Risk events are\n\nfuture events that may occur because of\n\nthe Condition present.\n###### � [The ][If][ is the risk event associated with the ]\n\n_Condition present. It is critically important_\n\nto recognize the If and the Condition as\n\na dual. When examined jointly, there may\n\nbe ways to directly intervene or remedy\n\nthe risk event’s underlying root (Condition)\n\nsuch that the consequences from this\n\nevent, if it occurs, no longer threaten the\n\nproject. The If is the probabilistic portion\n\nof the risk statement.\n\n\n-----\n\nTHEN these are the\nconsequences\n\nConsequence\nEvent 111\n\nRoot e.g., Event B\nCause Subsystem will reveal\n\nunanticipated performance\nshortfalls.\n\nCurrent test plans are focused on the\n\nConsequence\n\ncomponents of the subsystem and not\n\nEvent 211\n\non the subsystem as a whole.\n\nThe full-up system will reveal\nunanticipated performance\n\nRisk Event 11 shortfalls.\n\nConsequence\n\nSubsystem will not be fully tested when\n\nEvent 311\n\nintegrated into the system for full-up\nsystem-level testing. Subsystem will have to incorpo\nrate late fixes to the tested\nsoftware baseline.\n\nConsequence\n\ne.g., Event A\n\nEvent 411\n\nSubsystem will have to accommodate unanticipated changes in\nsubsequent build hardware /\nsoftware requirements which\nwill affect development cost and\nschedules.\n\nConsequence\nEvent 511\n\nUser will not accept delivery of the\nsubsystem hardware / software\nwithout fixes.\n\nFigure 1. Writing a Risk—The Condition-If-Then Best Practice\n\n\n###### � [The ][Then][ is the consequence, or set of ]\n\nconsequences, that will impact the engi­\n\nneering system project if the risk event\n\noccurs.\n\nAn example of a Condition-If-Then construct is\n\nillustrated in Figure 1.\n\nEncourage teams to identify risks. The culture\n\nin some government projects and programs\n\n\ndiscourages the identification of risks. This may\n\narise because the risk management activities of\n\ntracking, monitoring, and mitigating the risks are\n\nseen as burdensome and unhelpful. In this situ­\n\nation, it can be useful to talk to the teams about\n\nthe benefits of identifying risks and the inability to\n\nmanage it all in your heads (e.g., determine prior­\n\nity, who needs to be involved, mitigation actions).\n\n\n-----\n\nAssist the government teams in executing a pro­\n\ncess that balances management investment with\n\nvalue to the outcomes of the project. In general, a\n\ngood balance is being achieved when the project\n\nscope, schedule, and cost targets are being met\n\nor successfully mitigated by action plans, and the\n\nproject team believes risk management activities\n\nprovide value to the project. Cross-team repre­\n\nsentation is a must; risks should not be identified\n\nby an individual, or strictly by the systems engi­\n\nneering team (review sources of risk above).\n\nConsider organizational and environmental\n\nfactors. Organizational, cultural, political, and\n\nother environmental factors, such as stakeholder\n\nsupport or organizational priorities, can pose as\n\nmuch or more risk to a project than technical\n\nfactors alone. These risks should be identified\n\nand actively mitigated throughout the life of the\n\nproject. Mitigation activities could include moni­\n\ntoring legislative mandates or emergency changes\n\nthat might affect the program or project mission,\n\norganizational changes that could affect user\n\nrequirements or capability usefulness, or changes\n\nin political support that could affect funding. In\n\neach case, consider the risk to the program and\n\nidentify action options for discussion with stake­\n\nholders. For more information, see the article” Risk\n\nMitigation Planning, Implementation, and Progress\n\nMonitoring.”\n\nInclude stakeholders in risk identification.\n\nProjects and programs usually have multiple\n\nstakeholders that bring various dimensions of risk\n\nto the outcomes. They include operators, who\n\nmight be overwhelmed with new systems; users,\n\nwho might not be properly trained or have fears\n\nfor their jobs; supervisors, who might not support\n\n\na new capability because it appears to dimin­\n\nish their authority; and policy makers, who are\n\nconcerned with legislative approval and cost. In\n\naddition, it is important to include all stakeholders,\n\nsuch as certification and accreditation authorities\n\nwho, if inadvertently overlooked, can pose major\n\nrisks later in the program. Stakeholders may be\n\nkeenly aware of various environmental factors,\n\nsuch as pending legislation or political program\n\nsupport that can pose risks to a project that are\n\nunknown to the government or MITRE project\n\nteam. Include stakeholders in the risk identifica­\n\ntion process to help surface these risks.\n\nWrite clear risk statements. Using the\n\n_Condition-If-Then format helps communicate_\n\nand evaluate a risk statement and develop a\n\nmitigation strategy. The root cause is the underly­\n\ning Condition that has introduced the risk (e.g.,\n\na design approach might be the cause), the If\n\nreflects the probability (e.g., probability assess­\n\nment that the If portion of the risk statement\n\nwere to occur), and the Then communicates the\n\nimpact to the program (e.g., increased resources\n\nto support testing, additional schedule, and\n\nconcern to meet performance). The mitigation\n\nstrategy is almost always better when based on a\n\nclearly articulated risk statement.\n\nExpect risk statement modifications as the\n\nrisk assessment and mitigation strategy is\n\ndeveloped. It is common to have risk statements\n\nrefined once the team evaluates the impact.\n\nWhen assessing and documenting the potential\n\nrisk impact (cost, schedule, technical, or time­\n\nframe), the understanding and statement of the\n\nrisk might change. For example, when assessing\n\na risk impact of software schedule slip, the risk\n\n\n-----\n\nstatement might be refined to include the need\nby date, and/or further clarification of impact (e.g.\n\nif the XYZ software is not delivered by March 2015,\n\nthen there will not be sufficient time to test the\n\ninterface exchanges prior to Limited User Test).\n\nDo not include the mitigation statement in the\n\nrisk statement. Be careful not to fall into the trap\n\nof having the mitigation statement introduced\n\ninto the risk statement. A risk is an uncertainty\n\nwith potential negative impact. Some jump quickly\n\nto the conclusion of mitigation of the risk and,\n\ninstead of identifying the risk that should be\n\nmitigated (with mitigation options identified), they\n\nidentify the risk as a suboptimal design approach.\n\nFor example, a risk statement might be: If the\n\ncontractor does not use XYZ for test, then the\n\n###### References and Resources\n\n\ntest will fail. The concern is really test sufficiency.\n\nIf the contractor does not conduct the test with\n\nmeasurable results for analysis, then the program\n\nmay not pass limited user test. Use of XYZ may be\n\na mitigation option to reduce the test sufficiency\n\nrisk.\n\nDo not jump to a mitigation strategy before\n\nassessing the risk probability and impact. A risk\n\nmay be refined or changed given further analy­\n\nsis, which might affect what the most efficient/\n\ndesired mitigation may be. Engineers often jump\n\nto the solution; it is best to move to the next step\n\ndiscussed in the “Risk Impact Assessment and\n\nPrioritization” article to decompose and under­\n\nstand the problem first. Ultimately this will lead to\n\na strategy that is closely aligned with the concern.\n\n\n[1. The MITRE Corproation, September 1, 2007, MITRE Systems Engineering (SE)](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n[Competency Model, Ver. 1, pp. 10, 40–41.](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n2. Garvey, P. R., 2008, Analytical Methods for Risk Management: A Systems Engineering\n\n_Perspective, Chapman-Hall/CRC-Press, Taylor & Francis Group (UK), Boca Raton, London,_\nNew York, ISBN: 1584886374.\n\n3. U.S. Air Force, January 2008, Air Force Past Performance Evaluation Guide (PPEG),\n\nIG5315.305(a).\n\n4. “Analysis Toolshed,” MITREpedia, accessed March 2, 2010.\n\n###### Additional References and Resources\n\nMITRE E520 Risk Analysis and Management Technical Team checklists, Risk Checks, Risk\nAnalysis and Management Documents.\n\nProject Management Institute, A Guide to the Project Management Body of Knowledge,\n(PMBOK Guide), Fourth Edition, ANSI/PMI 99-001-2008, pp. 273–312.\n\n[“Standard Process/Steps of Process, Step 2: Identify Risks and Hazards,” MITRE SEPO Risk](http://www.mitre.org/work/sepo/toolkits/risk/)\n[Management Toolkit.](http://www.mitre.org/work/sepo/toolkits/risk/)\n\n\n-----\n\nDefinition: Risk impact assess­\n\n_ment is the process of assess­_\n\n_ing the probabilities and_\n\n_consequences of risk events if_\n\n_they are realized. The results of_\n\n_this assessment are then used_\n\n_to prioritize risks to establish a_\n\n_most-to-least-critical impor­_\n\n_tance ranking. Ranking risks_\n\n_in terms of their criticality or_\n\n_importance provides insights_\n\n_to the project’s management_\n\n_on where resources may be_\n\n_needed to manage or mitigate_\n\n_the realization of high prob­_\n\n_ability/high consequence risk_\n\n_events._\n\nKeywords: risk, risk impact\n\n_assessment, risk management,_\n\n_risk prioritization_\n\n\nRISK MANAGEMENT\n###### Risk Impact Assessment and Prioritization\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) working on government\n\nprograms are expected to analyze risks with\n\nrespect to impact, probability, dependencies,\n\nand timeframes and to prioritize risks to facilitate\n\ndecision making by the sponsor or customers [1].\n\n\n-----\n\n###### Background\n\nRisk impact assessment and prioritization are the second and third steps of the process\ndepicted in Figure 1 in the Risk Management topic article [2].\n\n###### Risk Impact Assessment in the Systems Engineering Program\n\nIn this step, the impact each risk event could have on the project is assessed. Typically this\nassessment considers how the event could impact cost, schedule, or technical performance\nobjectives. Impacts are not limited to these criteria, however; political or economic conse­\nquences may also need to be considered. The probability (chance) each risk event will occur is\nalso assessed. This often involves the use of subjective probability assessment techniques, par­\nticularly if circumstances preclude a direct evaluation of the probability by objective methods\n(i.e., engineering analysis, modeling, and simulation). Chapters 2 and 4 of Garvey [2] discuss\nthe topic of subjective probability assessments, as well as criteria for assessing a risk event’s\nimpact or consequence to a project.\n\nAs part of the risk assessment, risk dependencies, interdependencies, and the timeframe\nof the potential impact (near-, mid-, or far-term) need to be identified. The MITRE-developed\nRiskNav® tool is an example of a tool that can help perform this assessment. For additional\ndetails, see the article “Risk Management Tools.”\n\nWhen assessing risk, it is important to match the assessment impact to the decision\nframework. For program management, risks are typically assessed against cost, schedule, and\ntechnical performance targets. Some programs may also include oversight and compliance, or\npolitical impacts. Garvey [2] provides an extensive set of rating scales for making these mul­\nticriteria assessments, as well as ways to combine them into an overall measure of impact or\nconsequence. These scales provide a consistent basis for determining risk impact levels across\ncost, schedule, performance, and other criteria considered important to the project. In addi­\ntion, the Risk Matrix tool can help evaluate these risks to particular programs (see the article\n“Risk Management Tools”). Performing POET (Political, Operational, Economic, Technical)\nand/or SWOT (Strengths, Weaknesses, Opportunities, and Threats) assessments can help\ndetermine the drivers of the risks. For more details on these analyses, see the Tools to Enable a\nComprehensive Viewpoint topic in the SEG’s Enterprise Engineering section.\n\nFor some programs or projects, the impacts of risk on enterprise or organizational goals\nand objectives are more meaningful to the managing organization. Risks are assessed against\nthe potential negative impact on enterprise goals. Using risk management tools for the enter­\nprise and its components can help with the consistency of risk determination. This consistency\nis similar to the scale example shown below, except that the assessment would be done at the\nenterprise level. Depending on the criticality of a component to enterprise success (e.g., risk\nof using commercial communications to support a military operation and the impact of the\n\n\n-----\n\nenterprise to mission success, versus risk of using commercial communications for peacetime\ntransportation of military equipment), the risks may be viewed differently at the enterprise\nlevel even when the solution sets are the same or similar.\n\nOne way management plans for engineering an enterprise is to create capability portfo­\nlios of technology programs and initiatives that, when synchronized, will deliver time-phased\ncapabilities that advance enterprise goals and mission outcomes. A capability portfolio is a timedynamic organizing construct to deliver capabilities across specified epochs; a capability can be\ndefined as the ability to achieve an effect to a standard under specified conditions using multiple\ncombinations of means and ways to perform a set of tasks [2]. With the introduction of capabil­\nity management, defining the impact of risk on functional or capability objectives may provide\nvaluable insights into what capability is at risk, and which risks could potentially significantly\nimpact the ability to achieve a capability and/or impact multiple capability areas.\n\nIn portfolio management, a set of investments is administered based on an overall goal(s),\ntiming, tolerance for risk, cost/price interdependencies, a budget, and changes in the relevant\nenvironment over time. These factors are generally applicable to the government acquisition\nenvironment (see the article “Portfolio Management” in the SEG’s Enterprise Engineering\nsection). For portfolio risk assessment, investment decision, or analysis of alternatives tasks,\nusing categories of risk area scales may be the most appropriate way to ensure each alterna­\ntive or option has considered all areas of risk. Risk areas may include advocacy, funding,\nresources, schedule and cost estimate confidence, technical maturity, ability to meet techni­\ncal performance, operational deployability, integration and interoperability, and complexity.\nScales are determined for each risk area, and each alternative is assessed against all catego­\nries. Risk assessment may also include operational consideration of threat and vulnerability.\nFor cost-risk analysis, the determination of uncertainty bounds is the risk assessment.\n\nWhen determining the appropriate risk assessment approach, it is important to consider\nthe information need. As a Probability of Occurrence example, the sample Program Risk\nManagement Assessment Scale in Table 1 and the Investment Risk Assessment Scale in Table\n2 are from MITRE’s systems engineering work with government sponsors or clients.\n\n###### Risk Prioritization in the Systems Engineering Program\n\nIn the risk prioritization step, the overall set of identified risk events, their impact assess­\nments, and their probabilities of occurrences are “processed” to derive a most-to-least-critical\nrank-order of identified risks. A major purpose of prioritizing risks is to form a basis for allo­\ncating resources.\n\nMultiple qualitative and quantitative techniques have been developed for risk impact\nassessment and prioritization. Qualitative techniques include analysis of probability and\nimpact, developing a probability and impact matrix, risk categorization, risk frequency\n\n\n-----\n\nTable 1. Sample Program Risk Management Assessment Scale\n\n1.00 Issue: 1 Certain to occur\n\n0.95-0.99 High: - 0.95 < 1 Extremely sure to occur\n\n0.85-0.95 High: - 0.85 <= 0.95 Almost sure to occur\n\n0.75-0.85 High: - 0.75 <=0.85 Very likely to occur\n\n0.65-0.75 High: - 0.65 <=0.75 Likely to occur\n\nSomewhat greater than an even\n0.55-0.65 Medium: - 0.55 <=0.65\nchance\n\n0.45-0.55 Medium: - 0.45 <=0.55 An even chance to occur\n\nSomewhat less than an even\n0.35-0.45 Medium: - 0.35 <=0.45\nchance\n\n0.25-0.35 Low: - 0.25 <=0.35 Not very likely to occur\n\n0.15-0.25 Low: - 0.15 <=0.25 Not likely to occur\n\n0.00-0.15 Low: - 0.00 <=0.15 Almost sure not to occur\n\nranking (risks with multiple impacts), and risk urgency assessment. Quantitative techniques\ninclude weighting of cardinal risk assessments of consequence, probability, and timeframe;\nprobability distributions; sensitivity analysis; expected monetary value analysis; and model­\ning and simulation. MITRE has developed the min- and max-average approaches (using a\nweighting scale more heavily weighting the max or min value). Expert judgment is involved in\nall of these techniques to identify potential impacts, define inputs, and interpret the data [3].\n\n[In addition, MITRE has developed the RiskNav® tool that assists managers in assessing and](http://www.mitre.org/work/sepo/toolkits/risk/ToolsTechniques/RiskNav.html)\nprioritizing program risks. RiskNav includes the ability to weight timeframe in the risk rank­\ning (e.g., how much time to react/potentially mitigate). RiskNav is used by a number of MITRE\nsponsors and customers. For details on RiskNav, see the article “Risk Management Tools.”\n\n###### Best Practices and Lessons Learned\n\n|1.00|Issue:|1|Certain to occur|\n|---|---|---|---|\n|0.95-0.99|High:|> 0.95 < 1|Extremely sure to occur|\n|0.85-0.95|High:|> 0.85 <= 0.95|Almost sure to occur|\n|0.75-0.85|High:|> 0.75 <=0.85|Very likely to occur|\n|0.65-0.75|High:|> 0.65 <=0.75|Likely to occur|\n|0.55-0.65|Medium:|> 0.55 <=0.65|Somewhat greater than an even chance|\n|0.45-0.55|Medium:|> 0.45 <=0.55|An even chance to occur|\n|0.35-0.45|Medium:|> 0.35 <=0.45|Somewhat less than an even chance|\n|0.25-0.35|Low:|> 0.25 <=0.35|Not very likely to occur|\n|0.15-0.25|Low:|> 0.15 <=0.25|Not likely to occur|\n|0.00-0.15|Low:|> 0.00 <=0.15|Almost sure not to occur|\n\n\nTailor the assessment criteria to the decision\n\nor project. When assessing risks, recommend\n\ntechniques and tools that are suitable for the\n\nanalysis. For example, if the project is an enter­\n\nprise management or organizational oversight\n\nproject, then risk impact might be most suitably\n\nassessed against goals in lieu of technical perfor­\n\nmance, cost, and schedule. If the assessment is to\n\n\ndetermine the risk of investment options, the risk\n\narea scale approach might be best suited. For an\n\nexample of application of risk management, see the\n\nCryptologic Systems Group’s Risk Management\n\nImplementation Guide [4].\n\nDocument the rationale for the assess­\n\nment of impact and probability. It is important\n\n\n-----\n\nTable 2. Sample Investment Risk Assessment Scale\n\n|Col1|Technical Maturity|Technical Performance|Integration/Interoperability|\n|---|---|---|---|\n|Details|Maturity of technologies associated with the alternative|Confidence in perfor­ mance expectations|This refers to Integration and Interoperability (I&I) issues as they affect the alternative’s abil­ ity to achieve its stated outcome. The extent that I&I is understood has been demonstrated. It is assumed I&I considerations associated.|\n|Low|Key technolo­ gies are ready and mature and require little/no effort in time to execute the alternative|There are no techni­ cal or performance expectations identi­ fied that will have any impact on achieving the stated outcome objectives expected from the alternative|For this alternative, I&I considerations are well understood. Most of the challenging con­ cerns have been resolved and/or successfully tested/demonstrated under representative or actual field conditions. As such, I&I consider­ ations are not expected to have severe nega­ tive impact on the ability of this alternative to achieve its stated objectives.|\n|Low med .|Key tech­ nologies are expected to be ready and mature in time to execute the alternative|Limited technical or performance expec­ tations identified that will have a minor impact on achieving the stated outcome objectives expected from the alternative|For this alternative, I&I considerations are very well understood. Some challenging concerns have not been resolved and/or successfully tested/demonstrated under representative or actual field conditions. As such, I&I consider­ ations are expected to have negligible impact on the ability of this alternative to achieve its stated objectives.|\n|Med.|Key technolo­ gies are not ready and mature and require moder­ ate effort to implement the alternative|Technical or perfor­ mance limitations have been identi­ fied that will have a moderate impact on achieving the stated outcome objectives expected from the alternative|For this alternative, I&I considerations are somewhat-borderline understood. Nearly all (including the most challenging concerns) have been resolved and/or successfully tested/ demonstrated under representative or actual field conditions. As such, I&I considerations are expected to have modest negative effects on the ability of this alternative to achieve its stated objectives.|\n|Med.- High|Key technolo­ gies are not ready and mature and require signif­i cant effort to implement the alternative|There are no techni­ cal or performance expectations identi­ fied that will have any impact on achieving the stated outcome objectives expected from the alternative|For this alternative, I&I considerations are somewhat-borderline understood. Nearly all (including the most challenging concerns) have been resolved and/or successfully tested/ demonstrated under representative or actual field conditions. As such, I&I considerations are expected to have significant negative effects on the ability of this alternative to achieve its stated objectives.|\n\n\n-----\n\n|High|Key technolo­ gies will not be ready and mature and will have a severe impact on the alternative|Major technical or performance issues have been identified that will have a severe impact on achieving the stated outcome objectives expected from the alternative|For this alternative, I&I considerations are not very well understood. Many challenging con­ cerns are not resolved and/or successfully tested/demonstrated under representative or actual field conditions. As such, I&I consid­ erations are expected to have severe nega­ tive effects on the ability of this alternative to achieve its stated objectives.|\n|---|---|---|---|\n|Cata­ strophic|Key technolo­ gies will not be available and there is no alternative|Serious technical or performance issues have been identi­ fied that will pre­ vent achieving any of the stated outcome objectives expected from the alternative|For this alternative, I&I considerations are show-stoppers with the respect to the abil­ ity of this alternative to achieve its stated objectives.|\n\n\nto document the justification or rationale for\n\neach risk impact assessment and probability of\n\noccurrence rating. If the conditions or environ­\n\nment change, the assessment might need to be\n\nrevisited. The rationale helps to communicate the\n\nsignificance of the risk. When using the invest­\n\nment assessment scale approach, the statement\n\nof risk is typically captured in the rationale.\n\nRecognize the role of systems engineering.\n\nRisk assessment and management are roles of\n\nsystems engineering, especially as projects and\n\nprograms become more complex and interde­\n\npendent. The judgments that are involved require\n\na breadth of knowledge of system characteristics\n\nand the constituent technologies beyond that\n\nof design specialists. In addition, the judgments\n\nof risk criticality are at the system and program\n\nlevels [5]. Risk cuts across the life cycle of systems\n\nengineering, and MITRE SEs should be prepared\n\nto address risk throughout—concept and require­\n\nments satisfaction, architectural level risks, design\n\nand development risks, training risks, fielding, and\n\n\nenvironment risks. MITRE SEs are encouraged to\n\nadvocate for SE involvement in risk assessment\n\nand management.\n\nTailor the prioritization approach to the deci­\n\nsion or project. Match the prioritizing algorithm,\n\ntechniques, and tools to the assessment need\n\n(e.g., needs could include time criticality as a\n\nprioritization factor, the ability to see capability at\n\nrisk, the need for a single risk score for the port­\n\nfolio, the ability to have insight into risks with mul­\n\ntiple impacts). Each risk area—threat, operations,\n\nprogrammatic, etc.—will have different priorities.\n\nTypically, there will be a priority to these areas\n\nthemselves—a major threat risk could be totally\n\nunacceptable and the effort may be abandoned.\n\nIf the threat risks are acceptable but the opera­\n\ntions cannot be effectively performed, then, again,\n\nthe effort may be abandoned. Be sure to consider\n\nthese various decisions and criticality to help the\n\ngovernment assess the priorities of mitigating the\n\nrisks that arise.\n\n\n-----\n\n[Consider MITRE’s RiskNav® tool. RiskNav might](http://www.mitre.org/work/sepo/toolkits/risk/ToolsTechniques/RiskNav.html)\n\nbe appropriate for assessing and prioritizing risks\n\non your government program. MITRE SEs have\n\nfound that having easy access to this well-tested\n\ntool and a support team sometimes encourages\n\ngovernment program teams to adopt a more\n\nrobust risk management process than they other­\n\nwise might have. See the article “Risk Management\n\nTools.”\n\n###### References and Resources\n\n\nConsider Monte Carlo simulations. Monte Carlo\n\nsimulations use probability distributions to assess\n\nthe likelihood of achieving particular outcomes,\n\nsuch as cost or completion date [3]. They have\n\nbeen used effectively on a number of MITRE\n\ngovernment programs to help the project teams\n\nassess schedule risk.\n\n\n[1. The MITRE Institute, September 1, 2007, “MITRE Systems Engineering (SE) Competency](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n[Model, Ver. 1,” pp. 11, 41–42.](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n2. Garvey, P. R., 2008, Analytical Methods for Risk Management: A Systems Engineering\n\n_Perspective, Chapman-Hall/CRC Press, Taylor & Francis Group (UK), Boca Raton, London,_\nNew York, ISBN: 1584886374.\n\n3. _A Guide to the Project Management Body of Knowledge, (PMBOK Guide), 4th Ed., ANSI/_\n\nPMI 99-001-2008, pp. 273–312.\n\n4. Cryptologic Systems Group, July 2007, CPSG Risk Management Implementation Guide, V. 1.2.\n\n5. Kossiakoff, A., and W. N. Sweet, 2003, Systems Engineering Principles and Practice, John\n\nWiley and Sons, Inc., pp. 98–106.\n\n###### Additional References and Resources\n\nGarvey, P. R., January 2000, Probability Methods for Cost Uncertainty Analysis: A Systems\n_Engineering Perspective, Chapman-Hall/CRC Press, Taylor & Francis Group (UK), Boca Raton,_\nLondon, New York, ISBN: 0824789660.\n\nInternational Council on Systems Engineering (INCOSE), January 2010, INCOSE Systems\n_Engineering Handbook, Ver.3.2, INCOSE-TP-2003-002-03.2, pp. 213–225._\n\nLavine, M., S. McBrien, J. Ruddy, and M. Yaphe, August 2006, Methodology for Assessing\nAlternative IT Architectures for Portfolio Decisions, MITRE Technical Report 06W0000057.\n\nMcMahon, C. and R. Henry, September 2009, “RiskAssessment Scales—Sept09.” MITRE SEPO,\n[Risk Management Toolkit.](http://www.mitre.org/work/sepo/toolkits/risk/)\n\nStevens, R., “Engineering and Acquiring Mega-Systems: Systems Engineering Challenge for\nthe XXI Century,” (aka. Megasystems Engineering), June 13, 2007, Presentation to ATEC\nProfessional Development Day.\n\n\n-----\n\nDefinition: Risk mitigation\n\n_planning is the process of_\n\n_developing options and actions_\n\n_to enhance opportunities_\n\n_and reduce threats to project_\n\n_objectives [1]. Risk mitigation_\n\n_implementation is the process_\n\n_of executing risk mitigation_\n\n_actions. Risk mitigation prog­_\n\n_ress monitoring includes track­_\n\n_ing identified risks, identifying_\n\n_new risks, and evaluating risk_\n\n_process effectiveness through­_\n\n_out the project [1]._\n\nKeywords: risk, risk manage­\n\n_ment, risk mitigation, risk_\n\n_mitigation implementation, risk_\n\n_mitigation planning, risk mitiga­_\n\n_tion progress monitoring_\n\n\nRISK MANAGEMENT\n###### Risk Mitigation Planning, Implementation, and Progress Monitoring\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) working on govern­\n\nment programs develop actionable risk mitiga­\n\ntion strategies and monitoring metrics, monitor\n\nimplementation of risk mitigation plans to ensure\n\nsuccessful project and program comple­\n\ntion, collaborate with the government team in\n\nconducting risk reviews across projects and\n\nprograms, and analyze metrics to determine\n\nongoing risk status and identify serious risks\n\nto elevate to the sponsor or customer [2].\n\n\n-----\n\n###### Background\n\nRisk mitigation planning, implementation, and progress monitoring are depicted in Figure 2\nin the Risk Management topic article. As part of an iterative process, the risk tracking tool is\nused to record the results of risk prioritization analysis (step 3) that provides input to both risk\nmitigation (step 4) and risk impact assessment (step 2).\n\nThe risk mitigation step involves development of mitigation plans designed to man­\nage, eliminate, or reduce risk to an acceptable level. Once a plan is implemented, it is\ncontinually monitored to assess its efficacy with the intent of revising the course of action\nif needed.\n\n\n###### Risk Mitigation Strategies\n\nGeneral guidelines for applying risk mitigation handling options are shown in Figure 2. These\noptions are based on the assessed combination of the probability of occurrence and severity of\nthe consequence for an identified risk. These guidelines are appropriate for many, but not all,\nprojects and programs.\n\n\nRisk mitigation handling options include:\n\n###### �Assume/Accept: Acknowledge the existence of a particular risk, and make a deliberate\n\ndecision to accept it without engaging in special efforts to control it. Approval of project\nor program leaders is required.\n###### �Avoid: Adjust program require­\n\n5\n\nments or constraints to eliminate\nor reduce the risk. This adjustment\ncould be accommodated by a change 4\nin funding, schedule, or technical\nrequirements. 3\n###### �Control: Implement actions to mini­\n\n\nmize the impact or likelihood of the\nrisk.\n###### �Transfer: Reassign organizational\n\naccountability, responsibility, and\nauthority to another stakeholder will­\ning to accept the risk.\n###### �Watch/Monitor: Monitor the envi­\n\nronment for changes that affect the\nnature and/or the impact of the risk.\nEach of these options requires devel­\noping a plan that is implemented and\n\n\n2\n\n1\n\n\n1 2 3 4 5\nConsequence\n\n\nAvoid\nControl\nTransfer\n\n\nAvoid\nControl\nTransfer\n\n\nWatch\nAssume\n\n\nFigure 1. Risk Mitigation Handling Options [3]\n\n\n-----\n\nmonitored for effectiveness. More information on handling options is discussed under\nBest Practices and Lessons Learned below.\n\nFrom a systems engineering perspective, common methods of risk reduction or mitigation\nwith identified program risks include the following, listed in order of increasing seriousness of\nthe risk [4]:\n\n1. Intensified technical and management reviews of the engineering process\n2. Special oversight of designated component engineering\n3. Special analysis and testing of critical design items\n4. Rapid prototyping and test feedback\n5. Consideration of relieving critical design requirements\n6. Initiation of fallback parallel developments\nWhen determining the method for risk mitigation, the MITRE SE can help the customer\nassess the performance, schedule, and cost impacts of one mitigation strategy over another.\nFor something like “parallel” development mitigation, MITRE SEs could help the govern­\nment determine whether the cost could more than double, while time might not be extended\nby much (e.g., double the cost for parallel effort, but also added cost for additional program\noffice and user engagement). For conducting rapid prototyping or changing operational\nrequirements, MITRE SEs can use knowledge in creating prototypes and using prototyping\nand experimenting (see the article “Special Considerations for Conditions of Uncertainty:\nPrototyping and Experimentation” and the Requirements Engineering topic) for projecting\nthe cost and time to conduct a prototype to help mitigate particular risks (e.g., requirements).\nImplementing more engineering reviews and special oversight and testing may require\nchanges to contractual agreements. MITRE systems engineers can help the government\nassess these (schedule and cost) by helping determine the basis of estimates for additional\ncontractor efforts and providing a reality check for these estimates. MITRE’s CASA (Center\nfor Acquisition and Systems Analysis) and the CCG (Center for Connected Government)\nInvestment Management practice department have experience and a knowledge base in many\ndevelopment activities across a wide spectrum of methods and can help with realistic assess­\nments of mitigation alternatives.\n\nFor related information, see the other articles in the SEG’s Risk Management topic area.\n\n###### Best Practices and Lessons Learned\n\n\nHandling Options\n\nAssume/Accept. Collaborate with the operational\n\nusers to create a collective understanding of risks\n\n\nand their implications. Risks can be character­\n\nized as impacting traditional cost, schedule, and\n\nperformance parameters. Risks should also be\n\ncharacterized as impact to mission performance\n\n\n-----\n\nresulting from reduced technical performance\n\nor capability. Develop an understanding of all\n\nthese impacts. Bringing users into the mission\n\nimpact characterization is particularly important\n\nto selecting which “assume/accept” option is ulti­\n\nmately chosen. Users will decide whether accept­\n\ning the consequences of a risk is acceptable.\n\nProvide the users with the vulnerabilities affecting\n\na risk, countermeasures that can be performed,\n\nand residual risk that may occur. Help the users\n\nunderstand the costs in terms of time and money.\n\nAvoid. Again, work with users to achieve a col­\n\nlective understanding of the implications of risks.\n\nProvide users with projections of schedule adjust­\n\nments needed to reduce risk associated with\n\ntechnology maturity or additional development\n\nto improve performance. Identify capabilities that\n\nwill be delayed and any impacts resulting from\n\ndependencies on other efforts. This information\n\nbetter enables users to interpret the operational\n\nimplications of an “avoid” option.\n\nControl. Help control risks by performing analy­\n\nses of various mitigation options. For example,\n\none option is to use a commercially available\n\ncapability instead of a contractor-developed one.\n\nIn developing options for controlling risk in your\n\nprogram, seek out potential solutions from similar\n\nrisk situations of other MITRE customers, industry,\n\nand academia. When considering a solution from\n\nanother organization, take special care in assess­\n\ning any architectural changes needed and their\n\nimplications.\n\nTransfer. Reassigning accountability, responsibil­\n\nity, or authority for a risk area to another organiza­\n\ntion can be a double-edged sword. It may make\n\nsense when the risk involves a narrow specialized\n\n\narea of expertise not normally found in program\n\noffices. But, transferring a risk to another orga­\n\nnization can result in dependencies and loss of\n\ncontrol that may have their own complications.\n\nPosition yourself and your customer to consider\n\na transfer option by acquiring and maintaining\n\nawareness of organizations within your customer\n\nspace that focus on specialized needs and their\n\nsolutions. Acquire this awareness as early in the\n\nprogram acquisition cycle as possible, when\n\ntransfer options are more easily implemented.\n\nWatch/monitor. Once a risk has been identified\n\nand a plan put in place to manage it, there can be\n\na tendency to adopt a “heads down” attitude, par­\n\nticularly if the execution of the mitigation appears\n\nto be operating on “cruise control.” Resist that\n\ninclination. Periodically revisit the basic assump­\n\ntions and premises of the risk. Scan the environ­\n\nment to see whether the situation has changed in\n\na way that affects the nature or impact of the risk.\n\nThe risk may have changed sufficiently so that the\n\ncurrent mitigation is ineffective and needs to be\n\nscrapped in favor of a different one. On the other\n\nhand, the risk may have diminished in a way that\n\nallows resources devoted to it to be redirected.\n\nDetermining Mitigation Plans\n\nUnderstand the users and their needs. The\n\nusers/operational decision makers will be the\n\ndecision authority for accepting and avoiding\n\nrisks. Maintain a close relationship with the user\n\ncommunity throughout the systems engineering\n\nlife cycle. Realize that mission accomplishment\n\nis paramount to the user community and accep­\n\ntance of residual risk should be firmly rooted in a\n\nmission decision.\n\n\n-----\n\nSeek out the experts and use them. Seek out\n\nthe experts within and outside MITRE. MITRE’s\n\ntechnical centers exist to provide support in their\n\nspecialty areas. They understand what’s feasible,\n\nwhat’s worked and been implemented, what’s\n\neasy, and what’s hard. They have the knowledge\n\nand experience essential to risk assessment in\n\ntheir area of expertise. Know our internal centers\n\nof excellence, cultivate relationships with them,\n\nand know when and how to use them.\n\nRecognize risks that recur. Identify and maintain\n\nawareness of the risks that are “always there”—\n\ninterfaces, dependencies, changes in needs,\n\nenvironment and requirements, information secu­\n\nrity, and gaps or holes in contractor and program\n\noffice skill sets. Help create an acceptance by\n\nthe government that these risks will occur and\n\nrecur and that plans for mitigation are needed up\n\nfront. Recommend various mitigation approaches,\n\nincluding adoption of an evolution strategy,\n\nprototyping, experimentation, engagement with\n\nbroader stakeholder community, and the like.\n\nEncourage risk taking. Given all that has been\n\nsaid in this article and its companions, this may\n\nappear to be an odd piece of advice. The point\n\nis that there are consequences of not taking\n\nrisks, some of which may be negative. Help the\n\ncustomer and users understand that reality and\n\nthe potential consequences of being overly timid\n\nand not taking certain risks in your program. An\n\nexample of a negative consequence for not taking\n\na risk when delivering a full capability is that an\n\nadversary might realize a gain against our opera­\n\ntional users. Risks are not defeats, but simply\n\nbumps in the road that need to be anticipated and\n\ndealt with.\n\n\nRecognize opportunities. Help the govern­\n\nment understand and see opportunities that may\n\narise from a risk. When considering alternatives\n\nfor managing a particular risk, be sure to assess\n\nwhether they provide an opportunistic advantage\n\nby improving performance, capacity, flexibility,\n\nor desirable attributes in other areas not directly\n\nassociated with the risk.\n\nEncourage deliberate consideration of mitiga­\n\ntion options. This piece of advice is good anytime,\n\nbut particularly when supporting a fast-paced,\n\nquick reaction government program that is jug­\n\ngling many competing priorities. Carefully ana­\n\nlyze mitigation options and encourage thorough\n\ndiscussion by the program team. This is the form\n\nof the wisdom “go slow to go fast.”\n\nNot all risks require mitigation plans. Risk events\n\nassessed as medium or high criticality should go\n\ninto risk mitigation planning and implementation.\n\nOn the other hand, consider whether some low\n\ncriticality risks might just be tracked and moni­\n\ntored on a watch list. Husband your risk-related\n\nresources.\n\nMitigation Plan Content\n\nDetermine the appropriate risk manager. The\n\nrisk manager is responsible for identifying and\n\nimplementing the risk mitigation plan. He or\n\nshe must have the knowledge, authority, and\n\nresources to implement the plan. Risk mitiga­\n\ntion activities will not be effective without an\n\nengaged risk manager. It may be necessary to\n\nengage higher levels in the customer organiza­\n\ntion to ensure the need for the risk manager\n\nis addressed. This can be difficult and usually\n\n\n-----\n\ninvolves engaging more senior levels of the MITRE\n\nteam as well.\n\nDevelop a high-level mitigation strategy. This\n\nis an overall approach to reduce the risk impact\n\nseverity and/or probability of occurrence. It could\n\naffect a number of risks and include, for example,\n\nincreasing staffing or reducing scope.\n\nIdentify actions and steps needed to imple­\n\nment the mitigation strategy. Ask these key\n\nquestions:\n\nWhat actions are needed?\n\n###### � [Make sure you have the right exit criteria ]\n\nfor each. For example, appropriate deci­\n\nsions, agreements, and actions resulting\n\nfrom a meeting would be required for exit,\n\nnot merely the fact that the meeting was\n\nheld.\n###### � [Look for evaluation, proof, and validation of ]\n\nmet criteria. Consider, for example, metrics\n\nor test events.\n###### � [Include only and all stakeholders relevant ]\n\nto the step, action, or decisions.\n\nWhen must actions be completed?\n\n###### � [Backward Planning: Evaluate the risk ]\n\nimpact and schedule of need for the suc­\n\ncessful completion of the program and\n\nevaluate test events, design consider­\n\nations, and more.\n###### � [Forward Planning: Determine the time ]\n\nneeded to complete each action step\n\nand when the expected completion date\n\nshould be.\n\n\n###### � [Evaluate key decision points and deter­]\n\nmine when a move to a contingency plan\n\nshould be taken.\n\nWho is the responsible action owner?\n\n###### � [What resources are required? Con­]\n\nsider, for example, additional funding or\n\ncollaboration.\n###### � [How will this action reduce the probability ]\n\nor severity of impact?\n\nDevelop a contingency plan (“fall back, plan B”)\n\nfor any high risk.\n\n###### � [Are cues and triggers identified to activate ]\n\ncontingency plans and risk reviews?\n###### � [Include decision point dates to move to ]\n\nfallback plans. The date to move must\n\nallow time to execute the contingency\n\nplan.\n\nEvaluate the status of each action. Determine\n\nwhen each action is expected to be completed\n\nsuccessfully.\n\nIntegrate plans into IMS and program manage­\n\nment baselines. Risk plans are integral to the\n\nprogram, not something apart from it.\n\nMonitoring Risk\n\nInclude risk monitoring as part of the program\n\nreview and manage continuously. Monitoring\n\nrisks should be a standard part of program\n\nreviews. At the same time, risks should be man­\n\naged continuously rather than just before a\n\nprogram review. Routinely review plans in man­\n\nagement meetings.\n\n\n-----\n\nReview and track risk mitigation actions\n\nfor progress. Determine when each action is\n\nexpected to be completed successfully.\n\nRefine and redefine strategies and action steps\n\nas needed.\n\n###### References and Resources\n\n\nRevisit risk analysis as plans and actions are\n\nsuccessfully completed. Are the risks burning\n\ndown? Evaluate impact to program critical path.\n\nRoutinely reassess the program’s risk exposure.\n\nEvaluate the current environment for new risks or\n\nmodification to existing risks.\n\n\n1. Project Management Institute, A Guide to the Project Management Body of Knowledge,\n\n_(PMBOK Guide), Fourth Edition, ANSI/PMI 99-001-2008, pp. 273–312._\n\n2. The MITRE Institute, September 1, 2007, MITRE Systems Engineering (SE) Competency\n\nModel, Ver. 1, pp. 10, 40–41.\n\n3. Garvey, P. R., 2008, Analytical Methods for Risk Management: A Systems Engineering\n\n_Perspective, Chapman-Hall/CRC-Press, Taylor & Francis Group (UK), Boca Raton, London,_\nNew York, ISBN: 1584886374.\n\n4. Kossiakoff, A. and W. N. Sweet, 2003, Systems Engineering Principles and Practice, John\n\nWiley and Sons, Inc., pp. 98–106.\n\n###### Additional References and Resources\n\nInternational Council on Systems Engineering (INCOSE), January 2010, INCOSE Systems\n_Engineering Handbook, Ver. 3.2, INCOSE-TP-2003-002-03.2, pp. 213–225._\n\n\n-----\n\nDefinition: Risk management\n\n_tools support the implementa­_\n\n_tion and execution of program_\n\n_risk management in systems_\n\n_engineering programs._\n\nKeywords: risk analysis tools,\n\n_risk management tools, risk_\n\n_tools_\n\n\nRISK MANAGEMENT\n###### Risk Management Tools\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) working on government\n\nprograms are expected to use risk analysis and\n\nmanagement tools to support risk manage­\n\nment efforts. MITRE SEs also are expected to\n\nunderstand the purpose, outputs, strengths,\n\nand limitations of the risk tool being used.\n\n\n-----\n\n###### Background\n\nRisk analysis and management tools serve multiple purposes and come in many shapes and\nsizes. Some risk analysis and management tools include those used for:\n\n###### �Strategic and Capability Risk Analysis: Focuses on identifying, analyzing, and priori­\n\ntizing risks to achieve strategic goals, objectives, and capabilities.\n###### �Threat Analysis: Focuses on identifying, analyzing, and prioritizing threats to mini­\n\nmize their impact on national security.\n###### �Investment and Portfolio Risk Analysis: Focuses on identifying, analyzing, and priori­\n\ntizing investments and possible alternatives based on risk.\n###### �Program Risk Management: Focuses on identifying, analyzing, prioritizing, and man­\n\naging risks to eliminate or minimize their impact on a program’s objectives and prob­\nability of success.\n###### �Cost Risk Analysis: Focuses on quantifying how technological and economic risks may\n\naffect a system’s cost. Applies probability methods to model, measure, and manage risk\nin the cost of engineering advanced systems.\nEach specialized risk analysis and management area has developed tools to support its\nobjectives with various levels of maturity. This article focuses on tools that support the imple­\nmentation and execution of program risk management.\n\n###### Selecting the Right Tool\n\nIt is important that the organization defines the risk analysis and management process before\nselecting a tool. Ultimately, the tool must support the process. Consider the following criteria\nwhen selecting a risk analysis and management tool:\n\n###### �Aligned to risk analysis objectives: Does the tool support the analysis that the organi­\n\nzation is trying to accomplish? Is the organization attempting to implement an ongoing\nrisk management process or conduct a one-time risk analysis?\n###### �Supports decision making: Does the tool provide the necessary information to support\n\ndecision making?\n###### �Accessibility: Is the tool accessible to all users and key stakeholders? Can the tool be\n\nlocated/hosted where all necessary personnel can access it?\n###### �Availability of data: Is data available for the tool’s analysis? �Level of detail: Is the tool detailed enough to support decision making? �Integration with other program management/systems engineering processes: Does\n\nthe tool support integration with other program management/systems engineering\nprocesses?\n\n\n-----\n\n###### Program Risk Management Tools\n\nIn program risk management, it is important to select a tool that supports the risk manage­\nment process steps outlined in Figure 1 in the preceding Risk Management topic article. See\nthe other articles in the SEG’s Risk Management topic area for additional information on each\nof the process steps. Many tools are available that support the implementation of program risk\nmanagement. Many tools also can be used to support the management of project, enterprise,\nand system-of-systems risks.\n\n###### MITRE Developed Tools\n\nRiskNav®\n\n[RiskNav® (RiskNav is a registered trademark of The MITRE Corporation) is a well-tested tool](http://www.mitre.org/work/sepo/toolkits/risk/ToolsTechniques/RiskNav.html)\ndeveloped by MITRE to facilitate the risk process and help program managers handle their\nrisk space. RiskNav lets you collect, analyze, prioritize, monitor, and visualize risk informa­\ntion in a collaborative fashion. This tool provides three dimensions of information graphi­\ncally—risk priority, probability, and mitigation/management status.\n\nRiskNav, originally produced for the U.S. government, is designed to capture, analyze,\nand display risks at a project or enterprise level. RiskNav is currently deployed throughout\nnumerous MITRE sponsors or clients.\n\nSince January 2005, the Technology Transfer Office at MITRE has licensed RiskNav\ntechnology to commercial companies. Current licensees include Sycamore.US, Inc. and NMR\nConsulting. The Technology Transfer Office will support the tool for contractor and other\ngovernment acquisition, and will ensure that proper licensing forms are obtained and signed\nby new users. There is no cost for government usage. This formal procedure is not needed if\nMITRE is hosting a risk management effort.\n\nRiskNav presents the risk space in tabular and graphical form. The tabular form, shown\nin Table 1, presents key information for each risk, and allows the risk space to be filtered and\nsorted to focus on the most important risks. The information in the tables and figures is artifi­\ncial and for illustrative purposes only. It does not represent real programs, past or present.\n\nRiskNav uses a weighted average model (Table 2) that computes an overall score for each\nidentified risk. The risk priority is a weighted average of the timeframe (how soon the risk will\noccur), probability of occurrence, and impact (cost, schedule, technical). This score provides a\nrank order of the risks from most critical to least critical. Formally, this scoring model origi­\nnates from the concept of linear utility, where more important risks get higher numbers, and\nthe gaps between the numbers correspond to the relative strengths of the differences.\n\nIn graphical form (Figure 1), RiskNav represents three key aspects of each risk in the\nrisk space—risk priority, probability, and the mitigation/management status. The data points\n\n\n-----\n\nTable 1. RiskNav Summaries of Key Risk Information\n\n\n**Risk Space:  <All Risks>**\n\n\n**Risk List | Reports**\n\n**Risk Space Filters:** **Edit | Defaults     Default Filters   Sort Field: Priority**\n\n**Show Details  |  Hide Categories**\n\n**5X5** **Mitigation** **Impact** **Risk**\n**Risk ID** **State** **Name** **Category** **Priority**\n\n**color** **status** **date** **manager**\n\nOrgani­ White\n\nMGT.001 High/0.89 16 Sep\nOpen zational Red (No plan)\nDescription Analysis 2008\n\ninterfaces Mitigation\n\nOpera­\n\nGround\n\ntional;\n\nOOPS.003 sampling Issue/0.84 Green 19 Jul Landes,\nOpen sub­ Red\nDescription collection Analysis Mitigation 2008 Maxine\n\nsystem;\n\nand analysis\n\ntechnical\n\nTechnology\n\nProposed Program­\n\nSE.016 readiness for High/0.81 Red 16 Nov Landes,\n\nor pending matic; Red\n\nDescription science pay­ Analysis Mitigation 2008 Maxine\n\nreview technical\n\nload Cis\n\nStakeholder\n\nOpen or\n\nPROG.001 and mis­ Program­ High/0.79 Red 02 Oct Landes,\n\nneeds Red\n\nDescription sion partner matic Analysis Mitigation 2008 Maxine\n\nreview\n\ncomplexity\n\nOpera­\n\nOPS.006 Balloon High/0.75 Yellow 07 Jul Ramirez,\nOpen tional; Red\nDescription inflation Analysis Mitigation 2008 Diego\n\nsubsystem\n\nWhite (No\n\nMGT.002 Program­ High/0.74 28 Aug Santos,\nOpen WBS Red status)\nDescription matic Analysis 2008 Andrea\n\nMitigation\n\nWhite\n\nMGT.003 Program­ 27 Jul\nProposed IMS Yellow High/0.72 (No plan)\nDescription matic 2008\n\nMitigation\n\nrepresent risks, and the color of a box indicates the status of the mitigation action (White:\nno plan; Red: plan not working; Yellow: may not work; Green: most likely successful; Blue:\ncompleted successfully; Black: actions will start at a later date). Data points can be selected to\nshow detailed risk information about the analysis, who is working the management actions,\nthe status, and other information.\n\nRiskNav also displays a 5x5 frequency chart (Figure 2) showing the number of risks in\neach square of a 5x5 matrix of probability versus consequence ranges. The Red cells contain\nthe highest priority risks. The Yellow and Green cells contain medium and low priority risks,\nrespectively. RiskNav incorporates an administrative capability that allows the chart’s prob­\nability and consequence ranges to be customized. Clicking on a cell provides a detailed list of\n\n|Risk ID|State|Name|Category|5X5 color|Priority|Mitigation status|Impact date|Risk manager|\n|---|---|---|---|---|---|---|---|---|\n|MGT.001 Description|Open|Organi­ zational interfaces||Red|High/0.89 Analysis|White (No plan) Mitigation|16 Sep 2008||\n|OOPS.003 Description|Open|Ground sampling collection and analysis|Opera­ tional; sub­ system; technical|Red|Issue/0.84 Analysis|Green Mitigation|19 Jul 2008|Landes, Maxine|\n|SE.016 Description|Proposed or pending review|Technology readiness for science pay­ load Cis|Program­ matic; technical|Red|High/0.81 Analysis|Red Mitigation|16 Nov 2008|Landes, Maxine|\n|PROG.001 Description|Open or needs review|Stakeholder and mis­ sion partner complexity|Program­ matic|Red|High/0.79 Analysis|Red Mitigation|02 Oct 2008|Landes, Maxine|\n|OPS.006 Description|Open|Balloon inflation|Opera­ tional; subsystem|Red|High/0.75 Analysis|Yellow Mitigation|07 Jul 2008|Ramirez, Diego|\n|MGT.002 Description|Open|WBS|Program­ matic|Red|High/0.74 Analysis|White (No status) Mitigation|28 Aug 2008|Santos, Andrea|\n|MGT.003 Description|Proposed|IMS|Program­ matic|Yellow|High/0.72|White (No plan) Mitigation|27 Jul 2008||\n\n\n-----\n\nTable 2. RiskNav Uses a Scoring Model to Prioritize Risks\n\n**Risk Analysis Inputs** **Computed Risk Scores**\n\nImpact Date: 16 Sep 2008 Risk Timeframe: Short-term / 0.99\n\nProbability: High / 0.90 Overall Risk Impact: High / 0.79\n\nCost Impact Rating: High / 0.83 Risk Consequence: High / 0.89\n\nSchedule Impact Rating: High / 0.83 Risk Priority: High / 0.89\n\nTechnical Impact Rating: High / 0.65 Risk Ranking: (Ranks “open” risks with priority > 0)\n\nCompliance & Oversight Impact Rating: High / 0.83 Rank in Program: 1 of 17\n\nRank in Organization: 1 of 4\n\nRank in Project: 1 of 2\n\nthe risks in that cell. The All Red, All Yellow, and All Green icons at the top of the chart can\nbe used to list risks in all cells of a particular color.\n\nRiskNav is a Web application that runs on a personal computer (PC) server, which can\nconcurrently be used as a client. Once installed, it is intended to run using Internet Explorer\nas the browser.\n\nBecause RiskNav is a Web application, its installation requires more experience than\nsimply installing a normal executable. A detailed installation guide is available to assist in\nthe installation process. However, it is assumed that the installer has expertise installing and\nconfiguring Windows Web-based software. To obtain more information about RiskNav, email\n[risknav@mitre.org.](mailto:risknav@mitre.org)\n\n###### Risk Matrix\n\nRisk Matrix is a software application that can help identify, prioritize, and manage key risks\non a program. MITRE created it a few years ago to support a risk assessment process devel­\noped by a MITRE DoD client. MITRE and the client have expanded and improved the original\nprocess, creating the Baseline Risk Assessment Process. Although the process and application\nwere developed for use by a specific client, these principles can be applied to most govern­\nment acquisition projects. (See Figure 3.)\n\nRisk Matrix (as well as more information on RiskNav and Risk Radar) is available in the\n[Systems Engineering Process Office (SEPO) Risk Management Toolkit. Although Risk Matrix](http://www.mitre.org/work/sepo/toolkits/risk/index.html)\nis available for public release, support is limited to downloadable online documentation.\n\n###### Commercial Tools\n\nMany commercial tools are available to support program risk management efforts. Risk man­\nagement tools most commonly used by the government are:\n\n###### �Risk Radar and Risk Radar Enterprise— American Systems �Active Risk Manager— Strategic Thought Group\n\n|Risk Analysis Inputs|Computed Risk Scores|\n|---|---|\n|Impact Date: 16 Sep 2008 Risk Timeframe: Short-term / 0.99 Probability: High / 0.90 Overall Risk Impact: High / 0.79 Cost Impact Rating: High / 0.83 Risk Consequence: High / 0.89 Schedule Impact Rating: High / 0.83 Risk Priority: High / 0.89 Technical Impact Rating: High / 0.65 Risk Ranking: (Ranks “open” risks with priority > 0) Compliance & Oversight Impact Rating: High / 0.83 Rank in Program: 1 of 17 Rank in Organization: 1 of 4 Rank in Project: 1 of 2||\n\n\n-----\n\n|G G Y Y|G|Col3|\n|---|---|---|\n|W R R W|Y G||\n|R Y Y WW R, G W|W W||\n|W|R G||\n\n\n-----\n\nAll 5 x 5 cells: Red\n\n\n1\n\n0.8\n\n\n0.6\n\n0.4\n\n\n0.2\n\n\n###### 1 1 PROG.001 mission partner Stakeholder and\n\n0 complexity\n\n0 0.2 0.4 0.6 0.8 1\n\n|Col1|1|1|3|3|\n|---|---|---|---|---|\n|||||1|\n||||2|6|\n||||4|1|\n||||1|1|\n\n\nRisk Consequence\n(Impact and Time Frame)\n\n\nTotal # Risks Plotted = 24\n\nNote: A risk will not be plotted if\nits analysis inputs are incomplete.\n\n|sk ID|sk Name|onseque|quenc obabil|ability tigation|\n|---|---|---|---|---|\n|R isk|R isk|C o n|P ro|M iti|\n|INF.003|Platform system integration|0.91|1.00|Yellow|\n|MGT.001|Organizational interfaces|0.89|0.90|White|\n|OPS.003|Ground sampling collection and analysis|0.84|1.00|Green|\n|SE.016|Technology readiness for science payload CIs|0.87|0.70|Red|\n|SE.015|Cannot handle real-time transmission rate|0.91|0.60|Red|\n|PROG.001|Stakeholder and mission partner complexity|0.73|0.90|Red|\n|SE.017|Balloon weight|0.89|0.50|White|\n|OPS.006|Balloon inflation|0.82|0.60|Yellow|\n|MGT.002|WBS|0.81|0.60|White|\n|OPS.009|Accurately controlling vehicle altitude in the Venusian environment|0.84|0.50|Yellow|\n|SE.005|Component single point of failure|0.81|0.50|Green|\n|APPS.001|SW testing environment|0.67|1.00|Yellow|\n\n\nFigure 2. 5x5 Frequency Chart to Identify High-Priority Risks\n\nBoth tools are Web-based applications that support all steps in the risk management process.\n\n\n###### Contractor Tools\n\nGovernment programs sometimes implement a combined government/contractor risk manage­\nment process that uses tools provided by the contractor. Multiple major government contractors\nhave developed in-house risk management applications. Many applications are comparable to\nMITRE and commercial tools available, and effectively support program risk management.\n\n\n-----\n\nFigure 3. Screenshot of Risk Matrix\n\n###### Customized Tools\n\nMany smaller programs use Microsoft Excel or Access customized risk management tools.\nSome customized solutions meet the tool selection criteria outlined above. This is important\nwhen considering a customized solution that meets the need of the program being supported.\n\n###### Best Practices and Lessons Learned\n\n\nFit the tool to the process or assessment\n\nneeded. There are many types of risk analysis\n\nand management tools available, including ones\n\nfor financial analysis, cost-risk uncertainty, and\n\ntraditional program management. Understand\n\nthe need of the program, reporting, analysis (e.g.,\n\nability to modify risk impact scales to reflect the\n\nneed), and accessibility (e.g., multiple user envi­\n\nronment), before selecting a tool. Do not let the\n\ntool drive the process.\n\n\nChange the tool if it does not support decision\n\nmaking and the process. As the risk process\n\nmatures and reporting needs evolve, it is impor­\n\ntant to change the risk management tool used to\n\nsupport this changed environment. The follow­\n\ning conditions could warrant a change in the risk\n\nmanagement tool:\n\n###### � [New reporting requirements:][ It is best ]\n\nto use a tool that matches reporting\n\nrequirements.\n\n\n-----\n\n###### � [Increase in level of mitigation detail ]\n\nneeded: Some tools capture only high\nlevel mitigation plans, whereas others\n\ncapture detailed plans with action steps\n\nand statuses.\n###### � [Team capacity unable to support tool:][ If ]\n\nthe tool is too burdensome, it is important\n\nto examine ways to streamline its use or\n\n###### References and Resources\n\n\nchange to another tool that better sup­\n\nports the program’s environment.\n\nMaximize access to the tool. It is important that\n\nthe widest cross-section of the team has access\n\nto the tool and is responsible for updates. This\n\nensures distribution of workload and ownership,\n\nand prevents bottlenecks in the process.\n\n\n1. Garvey, P. R., 2008, Analytical Methods for Risk Management: A Systems Engineering\n\n_Perspective, Chapman-Hall/CRC-Press, Taylor & Francis Group (UK), ISBN: 1584886374._\n\n###### Additional References and Resources\n\nGarvey, P. R., January 2000, Probability Methods for Cost Uncertainty Analysis: A Systems\n_Engineering Perspective, Chapman-Hall/CRC Press, Taylor & Francis Group (UK), ISBN:_\n0824789660.\n\nMITRE E520 Risk Analysis and Management Technical Team, “Risk Analysis and\nManagement Tools.”\n\n[Risk Process Guidance, SEPO Risk Management Toolkit.](http://www.mitre.org/work/sepo/toolkits/risk/index.html)\n\n\n-----\n\n##### Configuration Management\n\nDefinition: Configuration management is the application of sound program\n\n_practices to establish and maintain consistency of a product’s or system’s attri­_\n\n_butes with its requirements and evolving technical baseline over its life. It involves_\n\n_interaction among government and contractor program functions in an integrated_\n\n_product team environment. A configuration management process guides the sys­_\n\n_tem products, processes, and related documentation, and facilitates the develop­_\n\n_ment of open systems. Configuration management efforts result in a complete_\n\n_audit trail of plans, decisions, and design modifications [1]._\n\nKeywords: acquisition development program, program control configuration\n\n_management policy, program management_\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) should have a sound understanding of\n\nthe principles of configuration management (CM), how the development\n\norganization initiates configuration control, how the developer imple­\n\nments configuration management, and how the government sponsor\n\nor sustainment organization continues the configuration management\n\nfollowing product delivery. Because many of our programs place a sig­\n\nnificant amount of their configuration management effort on software\n\nconfiguration management and commercial hardware and software,\n\nthat will be the focus of this discussion; however, the configuration\n\n\n-----\n\nmanagement of developmental hardware deserves a similar discussion. Usually MITRE’s\nrole in the practice of configuration management is to ensure that good CM processes are\nin place, documented as part of a program Systems Engineering Plan and/or Life-cycle\nManagement Plan, and followed by all contractors and program office personnel. The imple­\nmentation of the process is not likely to be a MITRE role, unless there are special circum­\nstances (e.g., analysis of a CM breakdown). Issues such as the use of appropriate CM tools\nfor a development environment, application of automated system configuration monitoring,\nand the frequent conundrum of managing moving baselines are likely to be the focus of\nMITRE’s expertise for CM.\n\n###### Context\n\nWhy do we perform configuration management? One reason is that it is required on\nDepartment of Defense (DoD) [2], Federal Aviation Administration (FAA) [3], Internal\nRevenue Service (IRS) [4], and other formal organizationally run programs; behind those\nrequirements are the reasons and lessons learned. Once a program has been developed\nand deployed as a system, it is necessary to understand the baseline for reasons of sustain­\nability and affordability. Obviously it is very hard to maintain or upgrade something that\nis undefined and not controlled. We need to minimize unintended negative consequences\nand cost of changes that are not fully analyzed. We would prefer to have “interchangeable\nparts,” and standards and baselines that allow for ease of maintenance and interoperabil­\nity. Finally, there is the need for repeatable performance for operations, test, security, and\nsafety. Think of the ability to react to an Information Assurance Vulnerability Assessment\nwhen you do not know what could be affected in your system baseline or where the items\nare located. Think of your ability to specify a replacement system when you do not know\nwhat constitutes your system or configuration item baselines because those changes were\nnot tracked after the initial system deployment.\n\n###### Best Practices and Lessons Learned\n\n\nConsistent with the systems engineering life cycle,\n\nconfiguration management exists for the life of\n\na program and system. As part of initial program\n\nplanning, MITRE is involved in the establishment\n\nof systems engineering processes, one of which is\n\nconfiguration management.\n\nA plan is essential. A configuration manage­\n\nment plan is necessary for sound configuration\n\n\nmanagement practice. Include the following in\n\nthe plan:\n\n###### � [Configuration identification.][ Identify the ]\n\nthings to be managed and level of control\n\nat each level.\n\ny Identify all configuration items to be y\n\ncontrolled: user requirements docu­\nments, requirements specifications and\n\n\n-----\n\ntraceability, design artifacts, develop­\nment documents, software version\ndocuments, interface control docu­\nments, drawings and parts lists, test\nplans and procedures, test scripts, test\nresults, training materials; depending\non the type of program, you may also\nhave architecture products, data flows\nand network diagrams, simulation data,\ntest harness/modeling and simulations,\netc.\n\nyy Identify the level of detail of each to be\n\ncontrolled: system-of-systems, system,\nconfiguration item, component, item,\npart number, network asset, etc.\n\nyy Identify all baselines to be managed:\n\nuser requirements, system require­\nments, design, development, test, sus­\ntainment, experimentation, etc.\n\nyy Develop a schema or comply with\n\norganizational policy to provide unique\nidentifiers for each item.\n\nyy Determine the level of the configuration\n\nmanagement hierarchy (stakeholders)\nfor each identified “configuration item”\nto be approved (baselined).\n###### � [Configuration control.]\n\nyy Develop a closed-loop corrective\n\naction process to track all configuration\nitem changes to closure and inclusion in\nappropriate baseline documentation.\n\nyy Build or provide specifications to build\n\nwork products from the software\nconfiguration management system or\nphysical products from the hardware\nconfiguration management system.\n\nyy Purchase or develop tools for version\n\ncontrol of source code. This product\nshould provide version control tracking\nto the line of code level. Assure imple­\nmentation of an engineering release\n\n\nsystem to provide hardware version\ncontrol.\n###### � [Configuration status accounting.][ Publish ]\n\nperiodic reports describing the current\n\nconfiguration of each configuration item.\n\nThere should be a configuration version\n\ndescription document detailing each\n\nversion of software undergoing integra­\n\ntion, system, or acceptance test. There\n\nshould be a set of engineering drawings\n\ndetailing each developmental hardware\n\nitem undergoing integration and testing.\n\nCommercial hardware and software also\n\nneeds to be under configuration control\n\nduring integration and testing. Configura­\n\ntion status accounting applies to all fielded\n\nhardware, software, and other controlled\n\nassets during operations and maintenance\n\nfor the life of the system.\n###### � [Configuration audits.][ Perform periodic ]\n\nexaminations of operational baselines\n\nfor completeness (configuration verifica­\n\ntion audit). Prior to product delivery to\n\nthe sponsor, ensure successful comple­\n\ntion of a functional configuration audit to\n\nassure that the product meets its speci­\n\nfied requirements. Also conduct a physi­\n\ncal configuration audit to assure that the\n\nsuccessfully tested product matches the\n\ndocumentation.\n###### � [Accounting of requirements changes][ per ]\n\nmonth and changes processing time; also,\n\nthe number of defects that are open and\n\nclosed are metrics that may be used for\n\nconfiguration management.\n\n\n-----\n\nAutomate to manage complexity. If the program\n\nis sufficiently complex, identify and install an auto­\n\nmated tool to support the configuration man­\n\nagement tasks. Consider the other stakeholders\n\n(engineers/programmers, users, contractors,\n\ninterfacing systems, and sustainment organiza­\n\ntions) in the selection of any automated configu­\n\nration management tools.\n\n\nWork your plan. Implement and conduct the\n\nconfiguration management activities accord­\n\ning to the program’s configuration management\n\nplan.\n\nUse checklists. A basic checklist, such as the\n\none below, can assist in capturing the necessary\n\nefforts.\n\n\nTable 1. Checklist\n\n**Checklist Item**\n\nHave all items subject to configuration control been identified in the program\nplan?\n\nHas a closed loop change management system been established and\nimplemented?\n\nHas a government configuration control board been established for both\ndevelopment and sustainment?\n\nAre impact reviews performed to ensure that the proposed changes have not\ncomprised the performance, reliability, safety, or security of the system?\n\nDoes the developer’s CM create or release all baselines for internal use?\n\nDoes the developer’s CM create or release all baselines for delivery to the\ncustomer?\n\nAre records established and maintained describing configuration items?\n\nAre audits of CM activities performed to confirm that baselines and docu­\nments are accurate?\n\nDo sponsor, program office, primary developer team, and sustainment orga­\nnizations have CM systems and expertise? Are developers and managers\ntrained equivalently on CM?\n\nAre CM resources across development team interoperable and compatible\n(i.e., use of SourceSafe, CVS, CAD/CAM, Requirements Management, and\nSubversion may represent logistical issues if left unmanaged)?\n\nMITRE’s experience has shown that the CM process chosen for a particular program\nmay be a rigorous, serial process to tightly control the system baseline (i.e., for an aircraft\nwhere flight safety is paramount) or a more flexible process that allows for urgent modifica­\ntions and variable levels of approval. The challenge is to determine the process that best\n\n|Checklist Item|Col2|\n|---|---|\n|Have all items subject to configuration control been identified in the program plan?||\n|Has a closed loop change management system been established and implemented?||\n|Has a government configuration control board been established for both development and sustainment?||\n|Are impact reviews performed to ensure that the proposed changes have not comprised the performance, reliability, safety, or security of the system?||\n|Does the developer’s CM create or release all baselines for internal use?||\n|Does the developer’s CM create or release all baselines for delivery to the customer?||\n|Are records established and maintained describing configuration items?||\n|Are audits of CM activities performed to confirm that baselines and docu­ ments are accurate?||\n|Do sponsor, program office, primary developer team, and sustainment orga­ nizations have CM systems and expertise? Are developers and managers trained equivalently on CM?||\n|Are CM resources across development team interoperable and compatible (i.e., use of SourceSafe, CVS, CAD/CAM, Requirements Management, and Subversion may represent logistical issues if left unmanaged)?||\n\n\n-----\n\nmeets stakeholder needs as well as the acquisition/procurement/maintenance needs of the\nsystem program office. Depending on the level of complexity, the number of stakeholders,\nand the nature of the system (e.g., system-of-systems), see the Engineering InformationIntensive Enterprises topic in the SEG’s Enterprise Engineering section. Enterprise manage­\nment, evolutionary acquisition, and spiral development combined with sustainment introduce\nsome unique challenges in configuration management, because of the number of concurrent\ndevelopment and operational baselines that must be controlled and maintained for develop­\nment, test, experimentation, and operations. Understanding the complexity of the system will\nenable you to apply the appropriate CM process and the relationship between layers of the CM\nhierarchy. See the article “How to Control a Moving Baseline.”\n\nThe number and types of tools employed to assist in configuration management\nhave grown and changed according to technology and development techniques. Once\nthe list of items to be configuration controlled has been determined, assess the variety\nof tools appropriate to automate the management and control process (i.e., Dynamic\nObject-Oriented Requirements System tool for requirements management and traceabil­\nity, Deficiency Reporting Databases, Software Configuration Management tools, Network\ndiscovery tools, etc.). For additional guidance, see the article “Configuration Management\nTools.”\n\n###### References and Resources\n\n[1. Acquisition Community Connection, Defense Acquisition Guidebook, 4.2.3.1.6, accessed](https://acc.dau.mil/CommunityBrowser.aspx?id=332970)\n\nJanuary 25, 2010.\n\n[2. DoD, December 2008, DoDI 5000.02.](http://www.dtic.mil/whs/directives/corres/pdf/500002p.pdf)\n\n[3. FAA, August 7, 2008, FAA Order 1800.66, Configuration Management Policy.](http://www.faa.gov/about/office_org/headquarters_offices/ato/service_units/techops/atc_facilities/cm/files/cmcommunity.pdf)\n\n[4. Department of Treasury (IRS), July 1, 2007, Enterprise Life Cycle Guide, Process](http://www.irs.gov/irm/part2/irm_02-016-001.html)\n\n[Management and Improvement Policy.](http://www.irs.gov/irm/part2/irm_02-016-001.html)\n\n###### Additional References and Resources\n\n[ANSI/EIA-649-A, 2004, National Consensus Standard for Configuration Management.](https://acc.dau.mil/CommunityBrowser.aspx?id=32215)\n\nAssociation for Configuration and Data Management website, accessed January 25, 2010.\n\nCM Crossroads, “Configuration Management Yellow Pages,” accessed January 25, 2010.\n\nIEEE Standards Association, “IEEE Std 828-2005 IEEE Standard for Software Configuration\nManagement Plans,” accessed January 25, 2010.\n\nMIL-HDBK-61A, February 2001, Configuration Management Guidance, accessed January 25,\n2010.\n\n\n-----\n\nMITRE Center for Connected Government, CM Toolbox.\n\nMITRE Systems Engineering Practice Office, Configuration Management Toolkit.\n\nSoftware Technology Support Center, “Configuration Management,” accessed February 2,\n2010.\n\n[Ventimiglia, B., February 1998, “Effective Software Configuration Management,” CrossTalk.](http://www.crosstalkonline.org/storage/issue-archives/1998/199802/199802-Ventimiglia.pdf)\n\n\n-----\n\nDefinition: Configuration\n\n_Management (CM) is the appli­_\n\n_cation of sound practices to_\n\n_establish and maintain consis­_\n\n_tency of a product or system’s_\n\n_attributes with its requirements_\n\n_and evolving technical baseline_\n\n_over its life [1]._\n\nKeywords: CM, CM process,\n\n_configuration baseline, configu­_\n\n_ration management, modifi­_\n\n_cation management, moving_\n\n_baseline, UCN, urgent compel­_\n\n_ling need_\n\n\nCONFIGURATION MANAGEMENT\n###### How to Control a Moving Baseline\n\n**MITRE SE Roles and Expectations: Although**\n\nconfiguration management is not a primary focus\n\nof MITRE’s systems engineering, it is an integral\n\npart of the overall systems engineering job and\n\nis an area MITRE systems engineers (SEs) are\n\nexpected to understand sufficiently. SEs need\n\nto monitor and evaluate the CM efforts of oth­\n\ners and recommend changes when warranted.\n\nOne of the more challenging tasks for the SE\n\nof a particular program is to assure the applica­\n\ntion of sound CM practices in environments\n\nwhere change is pervasive and persistent.\n\nIncreasingly the types of programs that\n\nMITRE supports are becoming more complex and\n\nreliant on commercial technology. Many of our\n\ncustomers’ users are facing asymmetric threats\n\nand constantly changing mission environments.\n\n\n-----\n\nThe result is that the programs we work on have constantly changing baselines that need\nto be managed efficiently and effectively. This is most evident in programs that are already\nfielded and have substantial development efforts taking place in parallel. In these situations,\nthe government is responsible for the overall CM of the program (fielded baseline as well as\noversight of development efforts being done by industry).\n\n###### Best Practices and Lessons Learned\n\n\nCM is “paperwork”—the “right paperwork.”\n\nTracking and managing a fielded configuration\n\nbaseline is largely an exercise in documentation\n\nmanagement. The configuration baseline is an\n\namalgamation of various documents (hardware\n\ndrawings, software version descriptions docu­\n\nments, interface control/design documents,\n\ntechnical orders, etc.), and is normally managed as\n\nsite-specific configurations for fielded systems.\n\nDevelopment contractors and vendors maintain\n\nconfiguration control of their various software\n\nproducts and applications that make up the sys­\n\ntem, but the overall configuration baseline is at the\n\nsystem level and managed by the customer orga­\n\nnization or program office. From this perspective,\n\nconfiguration management really means main­\n\ntaining positive control over the paperwork that\n\ndescribes the configuration. The hard part is to\n\ndetermine what level of documentation provides\n\nthe most accurate representation of the configu­\n\nration. Experience has shown that the hardware\n\ndrawings, software version descriptions, technical\n\norders, and systems specifications provide the\n\nmost bang for the CM buck.\n\n“Shock treatment CM” can be useful—when\n\nused sparingly. CM normally works well up until\n\nprograms are fielded and delivered to operating\n\nlocations. After fielding and over time, CM can\n\nbreak down. Users at different operating locations\n\n\nhave an interest in the system doing specifically\n\nwhat they need, and needs between differing\n\nlocations can vary enough that the baselines\n\nbegin to diverge. This can also happen in develop­\n\nment programs when different versions of a sys­\n\ntem are delivered to different users with slightly\n\ndifferent baselines. This poses a fundamental\n\ndilemma and choice for CM. Is the goal to man­\n\nage a common core baseline with divergent parts\n\nfor different users and locations? Or is the goal\n\nto maintain a single baseline (for interoperability\n\nor standard operations and training reasons, for\n\nexample)? Either answer can be right. The prob­\n\nlem arises when there is no explicit discussion\n\nand decision. The usual consequence is that CM\n\nbreaks down because the different customers\n\nand locations start to assume they have control\n\nand modify the baseline themselves.\n\nHow can this situation be remedied? Sometimes\n\n“shock treatment” is the only way to re-assert CM\n\nauthority. Essentially, this is the threat of clos­\n\ning down an operation due to critical certifica­\n\ntion or security baselines not being upheld. It is\n\nan extreme measure, but if the operation of the\n\nsystem is critical enough to warrant consistent\n\ncertification of operations or security, it can be\n\na useful hammer. As an example, in one govern­\n\nment program, the Designated Accreditation\n\nAuthority (DAA) became aware of baseline control\n\n\n-----\n\ndeviations that resulted in concerns over the\n\nintegrity of the system. Sites (operating locations)\n\nwere put on notice by the DAA that their security\n\naccreditation would be jeopardized if they did\n\nnot adhere to the configuration management\n\ncontrols instituted by the program office. This\n\nstrict enforcement technique sent shock waves\n\nthroughout the user community, the sustainment\n\norganization, and the program office. Uncontrolled\n\nbaseline modifications soon became a thing of\n\nthe past. “Shock treatment” CM can be a useful\n\ntool to get a configuration back on track. But, it is\n\nnot an enduring solution to CM issues.\n\nCM—a balance between rigor and reality. CM\n\nprocesses tend to swing back and forth like a\n\npendulum in meticulousness, resources applied,\n\nand adherence to documented process. There\n\nare benefits from a highly disciplined CM pro­\n\ncess. However, an unintended consequence can\n\nbe delays in processing baseline changes and\n\nultimately in fielding modifications. When this hap­\n\npens, the phrase, “CM is slowing me down” may\n\nbecome common. Experience has shown that\n\nthe most effective CM processes strike a balance\n\nbetween sufficient standards and control pro­\n\ncesses and mission needs of the program/system.\n\nConsider the following factors when determin­\n\ning your CM process: life-cycle stage, opera­\n\ntional environment, acceptable risk, and mission\n\nrequirements. The volume of data maintained\n\nis not necessarily a good metric for measuring\n\nthe effectiveness of a CM process. Less can be\n\nbetter, especially if the quality of data meets your\n\n“good enough” threshold.\n\nGet the user invested in the CM process. The\n\nuser should be your most critical stakeholder and\n\n\nstrongest advocate in quality and disciplined CM.\n\nTheir early and active involvement throughout\n\nthe CM process is a must. Understandably, users\n\ntend to favor speed of execution, and they com­\n\nmonly consider CM as a burden to be managed\n\nby engineers. When operational users participate\n\nas members on modification teams, engineer­\n\ning reviews, and configuration control boards,\n\nthey become vested, resulting in a CM process\n\nowned by both the managing program office and\n\nthe user. Users can feel “stonewalled” when they\n\nask for a change that never appears to happen,\n\nif the process appears chaotic or inefficient, or is\n\njust not understood. An operational user properly\n\ninvested in CM is the best advocate or spokes­\n\nperson to the rest of the user community for a\n\ndisciplined CM process.\n\nTwo compelling examples come from an exist­\n\ning government program. In both cases, the\n\nuser representatives proved to be critical to the\n\ndevelopment and implementation of two major\n\nCM process changes for the system. The first\n\nwas the establishment of an urgent and com­\n\npelling need (UCN) process to facilitate rapid\n\nfielding of mission critical modifications. Working\n\ntogether, the systems engineers and the opera­\n\ntional users negotiated and designed a minimum\n\nacceptable CM (documentation and approval)\n\nprocess that met the “good enough” standards\n\nfor program management, engineering, and the\n\nusers. This process has stood the test of time,\n\nand has proven itself over several years of direct\n\nsupport to field operations. The second example\n\ninvolved the transition to a bulk release modifica­\n\ntion process for fielding modifications. The bulk\n\nrelease method significantly reduced the number\n\n\n-----\n\nof disruptions and logistical burdens at each site\n\nby combining non-time-critical major and minor\n\nmodifications into one integrated baseline release,\n\nwith the result that the overall time spent at the\n\nsites for modifications was reduced.\n\nComplexity of enterprise CM. Asserting CM\n\ncontrol over a small system with clearly defined\n\ninterfaces can be fairly straightforward. In general,\n\napplying good CM practices to that case is well\n\nunderstood. But as programs transition from\n\nbeing primarily “stovepipe” configurations to\n\nsystems that are part of an enterprise, the scope\n\nof their CM processes changes as well. Enterprise\n\nsystems have more stakeholders to involve in\n\nthe process, and they also tend to have more\n\nmoving parts, making CM particularly challenging.\n\nTeams responsible for coordinating modifications\n\nor new developments must now also manage\n\n###### References and Resources\n\n\nconfigurations that are part of and have implica­\n\ntions for enterprise capabilities like networking,\n\ninformation assurance, and data management.\n\nThis requires new perspectives on what needs to\n\nbe managed at what level (system or enterprise)\n\nand with what stakeholder involvement. Enterprise\n\nCM activities need to be appropriately resourced\n\nto meet the greater needs of an enterprise. For\n\nmore information on enterprise systems, see the\n\nSEG’s Enterprise Engineering section.\n\nSustaining and maintaining organizations are\n\ncritical CM stakeholders. Make sure the sustain­\n\nment or maintenance organization is on board\n\nand an equal partner in the CM process. Their\n\nunique vantage point helps ensure the sustain­\n\nability of the fielded baseline by identifying vanish­\n\ning vender items, provisioning, and a host of other\n\nlogistical considerations.\n\n\n1. Acquisition Community Connection, Defense Acquisition Guidebook, 4.2.3.1.6, accessed\n\nJanuary 25, 2010.\n\n###### Additional References and Resources\n\nAir Force Instruction 63-131, November 6, 2009, Modification Program Management.\n\nLeffingwell, D. and D. Widrig, 2003, “Managing Change,” Managing Software Requirements:\n_A Use Case Approach, Addison-Wesley Professional, Chapter 28._\n\n\n-----\n\nDefinition: Webster defines a\n\n_tool as “something regarded as_\n\n_necessary to the performance_\n\n_of one’s occupation or profes­_\n\n_sional task. [Words are the tools_\n\n_of my trade.]” Configuration_\n\n_Management (CM) tools_\n\n_come in several forms. For the_\n\n_systems engineers and their_\n\n_partners/sponsors/customers,_\n\n_these tools include best prac­_\n\n_tice methodologies, standards,_\n\n_documentation, managed envi­_\n\n_ronments, manual tools, auto­_\n\n_mated tools, and leadership_\n\n_skills. These require and enable_\n\n_discipline and rigor needed to_\n\n_plan, stand up, implement, and_\n\n_carry out CM successfully._\n\nKeywords: automated tools,\n\n_configuration management_\n\n_policy, program management_\n\n_plan, Statement of Work (SOW),_\n\n_tools_\n\n\nCONFIGURATION MANAGEMENT\n###### Configuration Management Tools\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to have a\n\nsound understanding of the principles of configu­\n\nration management; how program management\n\nand project management view configuration\n\nmanagement; how the development organization\n\ninitiates configuration control; how the developer\n\nimplements configuration management; and\n\nhow the government sponsor or sustainment\n\norganization continues the configuration man­\n\nagement following product delivery. MITRE SEs\n\nare generally involved in identifying requirements\n\nfor automated tools to support the CM process.\n\nRather than selecting specific automated CM\n\ntools, MITRE SEs need to begin with require­\n\nments that understand and address the roles of\n\ntechnical and non-technical elements of CM, to\n\n\n-----\n\ninclude documentation and the traditional software configuration management elements of\nhardware and software. To be successful, it is essential to understand CM. CM success is a\nfunction of leadership to insist on its implementation and use.\n\nCM is defined in the SEG’s Configuration Management topic. Within that context, it is\nimportant to note that CM is not all things to all people, nor is it program management. It is\na tool used by program management. It is not document management, but it is a partner tool\nused by document management. It is not requirements management nor engineering, but is a\ntool with important connections to requirements engineering activities and processes.\n\nAutomated CM tools can help:\n\n###### �Record, control, and correlate Configuration Items (CIs), Configuration Units (CUs), and\n\nConfiguration Components (CCs) within a number of individual baselines across the life\ncycle.\n###### �Identify and control baselines. �Track, control, manage, and report change requests for the baseline CIs, CCs, and CUs. �Track requirements from specification to testing. �Identify and control software versions. �Track hardware parts. �Enable rigorous compliance with a robust CM process. �Conduct Physical Configuration Audits (PCAs). �Facilitate conduct of Functional Configuration Audits.\n\n Best Practices and Lessons Learned\n\n\nStart at the beginning. Get top-down buy-in\n\non the value of CM. A successful CM program is\n\nsupported and enforced by leadership. Set or cite\n\na CM policy/directive that authorizes a high-level\n\nConfiguration Control Board (CCB) (e.g., Executive\n\nCCB) at the highest authority with links to higher\n\nand lower boards. Coordinate CM planning with\n\nrequirements development and management,\n\nquality assurance, process improvement, inde­\n\npendent validation and verification, and existing\n\nenterprise processing centers to ensure engage­\n\nment and integration of production stakeholders.\n\nEnsure the Program Management Plan contains\n\na program-level Configuration Management Plan.\n\n\nCoordinate with the acquisition organization\n\n(e.g., Contract Officer, Contract Officer Technical\n\nRepresentative Acquisition Advisor) to ensure\n\nadequate CM requirements are in the SOW. In\n\naddition to the standard CM requirements, the\n\nSOW should include formal CM audits of the con­\n\ntractors to measure compliance with the agency/\n\ncustomer CM policy, agency/customer regula­\n\ntions, etc.\n\nAudit early and often. Set standards early,\n\nand audit for compliance. Identify or establish\n\nagency/government/enterprise policy, plan,\n\npractice, procedures, and standards, includ­\n\ning naming and tagging conventions early. Audit\n\n\n-----\n\ninternally to ensure the Program Management\n\nOffice is following the policies and procedures.\n\nConsider an annual demonstration of contrac­\n\ntor alignment with Software Engineering Institute\n\nCapability Maturity Model Integrated (CMMI) CM,\n\nInformation Technology Infrastructure Library CM,\n\nuniform top-level CM processes—ISO-9001, and\n\nNational Consensus Standard for Configuration\n\nManagement (ANSI EIA 649). Consider using\n\na CMMI Practice Implementation Indicator\n\nDescription format, such as those used for CMMI\n\nassessments, and include conclusive evidence for\n\nthe demonstration. Schedule it annually.\n\nConsiderations for automated CM tool acqui­\n\nsition. First, ascertain the method of manage­\n\nment that is most significant for your project or\n\nsystem, and ensure the tools serve that purpose.\n\nNext, define the requirements. Automated CM\n\ntool requirements need to be identified before\n\nacquisition decisions are made. It is critical to\n\nestablish requirements for the automated CM tool\n\nby collecting available CM plans, policy, process,\n\nprocedure, and instructions, and meeting with\n\nthe relevant stakeholders. Be certain to include\n\nbusiness, user, contractor, and operations and\n\nmaintenance stakeholders to define the auto­\n\nmated CM tool requirements.\n\nThese requirements should include consider­\n\nations for the following:\n\n###### � [Requirements Management ] � [Document Management version control ] � [Controlled repositories ] � [Configuration Identification and control, ]\n\nincluding hardware, software, etc.\n###### � [Change Request processing and tracking ]\n\n\n###### � [Audit support ] � [Configuration Status Accounting Report­]\n\ning (CSAR)\n###### � [Baseline management (software, docu­]\n\nmentation, requirements, design, product,\n\nproduction)\n###### � [Software development check-out/]\n\ncheck-in\n###### � [CM of environments (development, test, ]\n\nproduct, production); may be multiples of\n\neach\n###### � [Multiplatform capabilities (personal com­]\n\nputer, local area network, Web, mainframe,\n\ndata centers, etc.)\n###### � [Release engineering of all types of Change ]\n\nRequests (CRs) (e.g., normal releases,\n\nroutine releases such as operating system\n\nand security updates and patches, break\n\nfixes, emergency)\n###### � [Transition CM tools into the sustainment ]\n\nactivities\n###### � [Automated CM tool within the approved ]\n\ntechnical reference model or fully justifi­\n\nable for a waiver.\n\nCM tool selection needs to include a discussion of\n\nthe selection criteria based on the requirements,\n\nevaluation of tools, and selection of tools.\n\n\n-----\n\n###### CM Lessons Learned and Pitfalls\n\n � [Depending on the level of support from ]\n\nthe program leadership and stakehold­\n\ners for CM, tools may not be included as\n\npart of the overall CM plan or planned\n\nfor acquisition. If automated tools are\n\nacquired, ensure that program leadership\n\nis aware of the need for planning short-,\n\nmid-, and long-term needs for installation,\n\nestablishing the baseline data, and training,\n\nupdating, securing, and maintaining the\n\ntools and the associated process and pro­\n\ncedures needed to use them effectively.\n###### � [Set expectations early. CM and configu­]\n\nration change control are all about CRs,\n\nregardless of what they are called, and\n\nthe impact of change on scope, cost, and\n\nschedule.\n###### � [Keep informal and formal communication ]\n\nopen with CM as an agenda topic in meet­\n\nings and gate reviews. Do not shoot the\n\nmessengers.\n###### � [Consider release management separate ]\n\nfrom CM. Do not assume that release\n\nmanagement can be done by the CM\n\norganization.\n###### � [Everyone may know CM, but training will ]\n\nbe needed to orient staff on how CM is\n\ndone in your particular organization.\n###### � [Coordinate with those responsible for ]\n\nbusiness continuity of operation and\n\ndisaster recovery. Some will assume that\n\nCM will provide the capability to restore\n\nthe entire system if the need arises. This\n\n\nis not a safe assumption, unless your CM\n\ntools are designed with this capability in\n\nmind.\n###### � [Identify, establish, maintain, and control ]\n\nthe necessary development, test, and\n\nproduction environments, including the\n\nautomated CM tools, hardware, software,\n\noperating system, security, access control,\n\nand supportive infrastructure.\n###### � [Contractors and periods of performance ]\n\nmay come and go. It is recommended\n\nthat the transition from one contractor to\n\nthe next include an inventory of base­\n\nlined hardware, software, documents, etc.\n\nPCAs on the departing contractor product\n\nshould establish what the new contractor\n\ninherits. Gap analysis should be performed\n\nto determine the delta and provide input\n\nto the contract closure activity prior to\n\nmaking final payments to the departing\n\ncontractor.\n\nConversely, if the above lessons are not applied,\n\nthe consequences can lead to CM failure.\n\nIndications that things are not going well include:\n\nleadership support is not evident, formal CCBs are\n\nnot chartered or recognized as change approval\n\nauthorities or do not function, “lanes in the road”\n\nare not defined and chaos reigns, attendance at\n\nCM meetings (CCBs, engineering/technical review\n\nboards, and impact assessment boards/teams)\n\ndeclines or is non-existent, and cross program/\n\nproject impacts are not identified by CM, but only\n\nwhen something breaks down.\n\n\n-----\n\n###### Automated CM Tools Lessons Learned\n\n � [Buying a tool will not establish an appro­]\n\npriate CM program for your effort.\n###### � [It is unlikely that a single automated CM ]\n\ntool can be all things to all stakeholders\n\nby integrating all required elements across\n\nall platforms. So-called commercial off\nthe-shelf “suites” of tools may not contain\n\nintegrated capabilities to suit the enter­\n\nprise. When they have the potential for\n\nintegration, often there may be a signifi­\n\ncant effort needed to adapt the products\n\nafter they are taken out of the box. What\n\nmay support software development with\n\ncheck-out/check-in features may be\n\nbundled within a “suite” of stand-alone\n\nautomated tools without any integration.\n\nTool administrators may only have the abil­\n\nity to export to a spreadsheet for report­\n\ning. Automated tools may control one area\n\nwell, but not be suited for other areas.\n###### � [The automated CM tools used within ]\n\nthe development and test environments\n\nmay not be compatible with those in the\n\nproduction environments. This may require\n\ndevelopment of semi-automated or\n\nmanual processes. In either event, security\n\nand firewall infrastructures may present\n\nadditional challenges.\n###### � [Automated CM tools may offer flexible ]\n\noptions, including the ability to design your\n\n###### References and Resources\n\n\nown change request (CR) form and flow.\n\nThis has inherent pros and cons. There is\n\noften an assumption by the acquirer that\n\nthe tool will deliver a CR process out of the\n\nbox such that no other development effort\n\nwill be needed. It is important to understand\n\nthat the capabilities delivered out of the box\n\nare directly impacted by the installation/cus­\n\ntomization of those tools. It is important to\n\nunderstand the need for development and\n\nadministration of the automated tools, and\n\nset expectations early on.\n###### � [When planning the acquisition of the auto­]\n\nmated CM tool, consider the initial and lon­\n\nger term costs, including licenses, and labor\n\ncost to install and develop it so it is usable\n\nfor your program. Plan for ongoing sys­\n\ntem administration, security, maintenance,\n\nbackup, and recovery as well as business\n\ncontinuity of operation and disaster recov­\n\nery. Consider the ability of the tool, and data\n\ncontained within the tool, to be transitioned\n\nfrom one contractor to another, which is\n\nsometimes the case when a program transi­\n\ntions from production to sustainment.\n###### � [Avoid an approach with tools that implies ]\n\nthe tool will establish the standard and\n\nsolve the problem. It is not best practice\n\nto say “Here is the solution. What is your\n\nproblem?”\n\n\nMITRE Center for Connected Government, CM Toolbox.\n\nMITRE Systems Engineering Practice Office, Configuration Management Toolkit.\n\n\n-----\n\n##### Integrated Logistics Support\n\nDefinition: Integrated logistics support (ILS) is the management and technical\n\n_process through which supportability and logistic support considerations are inte­_\n\n_grated into the design of a system or equipment and taken into account through­_\n\n_out its life cycle. It is the process by which all elements of logistic support are_\n\n_planned, acquired, tested, and provided in a timely and cost-effective manner [1]._\n\nKeywords: acquisition logistics, computer resources support, design interface, life\n_cycle cost, life-cycle logistics, maintenance planning, technical data_\n\n###### Context\n\nSupportability and life-cycle costs are generally driven by\n\ntechnical design that occurs during the development stage of\n\nthe acquisition process. The experience of the U.S. Department\n\nof Defense (DoD) has consistently shown that the last 5 percent\n\nincrease in performance adds a disproportionate increase in life\ncycle costs, generally due to the use of new and often unproven\n\ntechnologies and design, which drive up supportability costs. This\n\nincrease in costs is found regardless of the support plan (organic\n\nor contractor support) and is a significant cost factor in the total\n\nsustainment budget of government departments and agencies.\n\n\n-----\n\n###### MITRE SE Roles and Expectations\n\nMITRE staff are expected to understand the impact of technical decisions made during design\nand development on the usability and life-cycle support of the system. They are expected to\naccount for life-cycle logistics considerations as part of the systems engineering process.\n\n###### Introduction\n\nTraditionally, MITRE systems engineering support to acquisition programs has not strongly\nfocused on ILS. However, life-cycle costs are increasingly driven by engineering consider­\nations during the early stages of acquisition. As a consequence, there is an important causeand-effect relationship between systems engineering and ILS that systems engineers need to\nbe aware of and account for in their activities.\n\nIn addition to the general ILS best practices and lessons learned discussed below, this\ntopic contains two articles. The article “Reliability, Availability, and Maintainability” dis­\ncusses best practices and lessons learned in developing design attributes that have significant\nimpacts on the sustainment or total Life Cycle Costs (LCC) of a system. A companion article,\n“Affordability, Efficiency, and Effectiveness (AEE),” in the Systems Engineering Life-Cycle\nBuilding Blocks section also contains best practices and lessons learned for managing LCC.\nThe second article under this topic is an ILS subject of special interest. The article “Managing\nEnergy Efficiency” discusses engineering considerations of conserving energy resources dur­\ning production, operations, and disposal of systems. Impacts include not just environmental\nconcerns but also system performance parameters (e.g., range of host vehicle) and recurring\noperating costs. For example, engineering considerations of data centers are increasingly\nfocused on technical issues and cost concerns of the energy needed to run them.\n\n###### Best Practices and Lessons Learned\n\n\nThe computer resources working group.\n\nComputer resources support encompasses the\n\nfacilities, hardware, software, documentation,\n\nmanpower, and personnel needed to operate and\n\nsupport mission-critical computer hardware/\n\nsoftware systems. As the primary end item, sup­\n\nport equipment, and training devices all increase\n\nin complexity, more and more software is being\n\nused. The expense associated with the design\n\nand maintenance of software programs is so\n\nhigh that no one can afford to poorly manage this\n\n\nprocess. It is critical to establish some form of\n\ncomputer resource working group to accomplish\n\nthe necessary planning and management of com­\n\nputer resources support [2].\n\nThe impact of maintenance planning.\n\nMaintenance planning establishes maintenance\n\nconcepts and requirements for the life of the\n\nsystem. It includes, but is not limited to:\n\n###### � [Levels of repair ] � [Repair times ]\n\n\n-----\n\n###### � [Testability requirements ]\n � [Support equipment needs ] � [Manpower and skills required ] � [Facilities ] � [Interservice, organic, and contractor mix of ]\n\nrepair responsibility\n###### � [Site activation. ]\n\nThis element has a great impact on the planning,\n\ndevelopment, and acquisition of other logistics\n\nsupport elements. Taken together, these items\n\nconstitute a substantial portion of the recurring\n\ncost (and therefore life-cycle cost) of a procure­\n\nment. Another factor related to this, and one that\n\nshould be seriously considered, is energy use\n\nand efficiency. The rising cost of energy and its\n\nproportion within the overall recurring cost should\n\nbe managed proactively (refer to the article on\n\n“Managing Energy Efficiency” within this section of\n\nthe Guide).\n\nEarly consideration of manpower require\nments. Manpower and personnel involves\n\nthe identification and acquisition of person­\n\nnel (military and civilian) who have the skills and\n\ngrades required to operate, maintain, and support\n\nsystems over their lifetime. Early identification is\n\nessential. If the needed manpower requires add­\n\ning staff to an organization, a formalized process\n\nof identification and justification needs to be\n\nmade to higher authority. Add to this the neces­\n\nsity to train these staff, new and existing, in their\n\nrespective functions on the new system, and the\n\nseriousness of any delay in accomplishing this\n\nelement becomes apparent. In the case of user\n\nrequirements, manpower needs can, and in many\n\ncases do, ripple all the way back to recruiting\n\n\nquotas. Required maintenance skills should be\n\nconsidered during the design phase of the pro­\n\ngram; unique technology often requires unique\n\nskills for maintenance. Note that information\n\ntechnology expertise and information security\n\nskills can still be lacking in current organic main­\n\ntenance resources and may require investment in\n\nadequate training.\n\nEnsuring a supply support structure. Supply\n\nsupport consists of all the management actions,\n\nprocedures, and techniques necessary to deter­\n\nmine requirements to acquire, catalog, receive,\n\nstore, transfer, issue, and dispose of spares, repair\n\nparts, and supplies (including energy sources and\n\nwaste). In lay terms, this means having the right\n\nspares, repair parts, and supplies available, in the\n\nright quantities, at the right place, right time, and\n\nright price. The process includes provisioning\n\nfor initial support as well as acquiring, distribut­\n\ning, and replenishing inventories. An aircraft can\n\nbe grounded just as quickly for not having the oil\n\nto put in the engine as it can for not having the\n\nengine.\n\nEvaluating the supply chain. As stated above,\n\naccess to spare equipment and supplies is critical\n\nto the operation of the system delivered. Not only\n\nshould there be a logistics process in place to\n\nhandle spares and supplies, there also needs to\n\nbe assurance that the supply chain will continue\n\nor have alternate sources. Care should be taken\n\nto assess supply chains for continued viability:\n\navoidance of diminishing manufacturing supply,\n\nidentification of alternatives, and mission assur­\n\nance. An article “Supply Chain Risk Management”\n\nis in the SEG’s Enterprise Engineering section. (To\n\nlearn more about mission assurance, read the\n\n\n-----\n\narticle “Cyber Mission Assurance” in the SEG’s\n\nEnterprise Engineering section.)\n\nMinimizing unique support requirements.\n\nEnsure all the equipment (mobile or fixed, hard­\n\nware or software) required to support the opera­\n\ntion and maintenance of a system is identified\n\nand provided. This includes ground handling and\n\nmaintenance equipment, tools, metrology and\n\ncalibration equipment, manual and automatic\n\ntest equipment, modeling and simulation used for\n\ntesting, and any software debugging/monitoring\n\napplications necessary for software maintenance\n\nor modification. Acquisition programs should look\n\nto decrease the proliferation of unique support\n\nequipment into the inventory by minimizing the\n\ndevelopment of new support equipment and giv­\n\ning more attention to the use of existing govern­\n\nment or commercial equipment.\n\nThe benefits of pre-test training. A training plan\n\nand program should consist of all policy, pro­\n\ncesses, procedures, techniques, training devices,\n\nand equipment used to train all user personnel\n\nto acquire, operate, and support a system. This\n\nincludes individual and crew training; new equip­\n\nment training; and initial, formal, and on-the-job\n\ntraining. Although the greatest amount of training\n\nis accomplished just prior to fielding a system, in\n\nmost programs a large number of individuals must\n\nalso be trained during system development to\n\nsupport the system test and evaluation program.\n\nThis early training also provides a good oppor­\n\ntunity to flush out all training issues prior to the\n\nproduction and conduct of formal training.\n\nDesign interface. This is the relationship of\n\nlogistics-related design parameters to readiness\n\n\nand support resource requirements. Logistics\nrelated design parameters include:\n\n###### � [Reliability and maintainability ] � [Human factors ] � [System safety ] � [Survivability and vulnerability ] � [Hazardous material management ] � [Standardization and interoperability ] � [Energy management/efficiency ] � [Corrosion ] � [Nondestructive inspection ] � [Transportability. ]\n\nThese logistics-related design parameters are\n\nexpressed in operational terms rather than as\n\ninherent values, and specifically relate to system\n\nreadiness objectives and support costs. Design\n\ninterface really boils down to evaluating all facets\n\nof an acquisition, from design to support and\n\noperational concepts for logistical impacts, to the\n\nsystem itself and the logistics infrastructure.\n\nThe importance of technical data. The term\n\n“technical data” represents recorded information\n\nof a scientific or technical nature, regardless of\n\nform or character (such as manuals and drawings).\n\nComputer programs and related software are not\n\ntechnical data; documentation of computer pro­\n\ngrams and related software is. Technical manuals\n\nand engineering drawings are the most expensive\n\nand probably the most important data acquisi­\n\ntions made in support of a system, because they\n\nprovide the instructions for its operation and\n\nmaintenance. Generation and delivery of tech­\n\nnical data should be specified early on in the\n\nacquisition planning process (within requests for\n\n\n-----\n\nproposals and statements of work, etc.) to ensure\n\nconsideration and cost. Absence of techni­\n\ncal data adversely impacts the operations and\n\n###### More on Technical Data\n\n\nmaintenance of a system and may result in a sole\nsource situation for the developer.\n\n\nSince July 2006, a number of important Department of Defense (DoD) developments related to\ntechnical data rights have transpired, including:\n\n###### �Issuance of Secretary of the Air Force Memo, May 3, 2006, “Data Rights and Acquisition\n\nStrategy.” [3]\n###### �Issuance of GAO Report “Weapons Acquisition: DoD Should Strengthen Policies for\n\nAssessing Technical Data Needs to Support Weapon Systems, July 2006.” [4]\n###### �Passage of Congressional Language in PL 109-364, FY07 John Warner National Defense\n\nAuthorization Act. [5]\n###### �Issuance of USD AT&L Policy Memo “Data Management and Technical Data Rights,”\n\n[6] July 19, 2007, which requires program managers to assess long-term technical data\nrequirements for all ACAT I and II programs, regardless of the planned sustainment\napproach, and reflect that assessment in a data management strategy (DMS).\n###### �Data Management and Technical Data Rights wording will be added to the next update\n\nof the DoD Instruction 5000.2. [7]\n###### �Issuance of DFARS Interim Rule, September 6, 2007. [8] �Issuance of US Army ASA (ALT) policy memorandum “Data Management and Technical\n\nData Rights, April 1, 2008.” [9]\nTo encourage creative and well–thought-out development of data management strategies\n(DMS) (which may well include access rather than procurement of data), the DoD has cho­\nsen not to issue a standard DMS format/template. The cost of actual ownership of the data,\nincluding storage, maintenance, and revision, is high and DoD has found it more economical\nfor the provider to manage the data as a service through the life cycle of the system. However,\naccording to Office of the Secretary of Defense staff, at a minimum a DMS should address:\n\n###### �Specific data items required to be managed throughout the program’s life cycle �Design, manufacture, and sustainment of the system �Re-compete process for production, sustainment, or upgrade �Program’s approach to managing data during acquisition and sustainment (i.e., access,\n\ndelivery, format)\n###### �Contracting strategy for technical data and intellectual property rights �Any requirements/need for a priced option �Any unique circumstances.\n\n\n-----\n\nSee also the Interactive Electronic Technical Data (IETM) site [10] and the DAU Data\nManagement (DM) Community of Practice (CoP) [11] for further information.\n\n###### References and Resources\n\n[1. U.S. Department of Defense, 2005, Dictionary of Military and Associated Terms.](http://www.dtic.mil/doctrine/dod_dictionary/)\n\n2. “Computer Resources Support,” Acquisition Community Connection, accessed February 4,\n\n2010.\n\n[3. May 3, 2006 Secretary of the Air Force Memo, “Data Rights and Acquisition Strategy.”](https://acc.dau.mil/adl/en-US/32634/file/6199/SECAF%20Memo%20-%20Data%20Rights%20and%20Acquisition%20Strategy%20(3%20May%2006).pdf)\n\n4. GAO Report, July 2006, “Weapons Acquisition: DOD Should Strengthen Policies for\n\nAssessing Technical Data Needs to Support Weapon Systems.”\n\n5. PL 109-364, FY07 John Warner National Defense Authorization Act.\n\n[6. USD AT&L, July 19, 2007, Policy Memo, “Data Management and Technical Rights.”](https://acc.dau.mil/adl/en-US/158916/file/29612/USDATL%20USDATL%20DM%20and%20Tech%20Data%20Jul%2007.pdf)\n\n7. DoD Instruction 5000.2.\n\n8. DFARS Interim Rule Issued September 6, 2007.\n\n[9. US Army ASA (ALT) policy memorandum, April 1, 2008, “Data Management and](https://acc.dau.mil/CommunityBrowser.aspx?id=206953)\n\n[Technical Data Rights.”](https://acc.dau.mil/CommunityBrowser.aspx?id=206953)\n\n10. IETM, Interactive Electronic Technical Manuals, https://acc.dau.mil/ietm\n\n11. DAU Data Management (DM) Community of Practice (CoP), https://acc.dau.mil/\n\nCommunityBrowser.aspx?id=17647\n\n###### Additional References and Resources\n\nAssistant Secretary of Defense for Logistics and Materiel Readiness, Logistics and Materiel\nReadiness website, accessed February 4, 2010.\n\nDefense Acquisition University, accessed February 4, 2010.\n\n\n-----\n\nDefinition: Reliability, availabil­\n\n_ity, and maintainability (RAM_\n\n_or RMA) are system design_\n\n_attributes that have significant_\n\n_impacts on the sustainment_\n\n_or total life cycle costs (LCC)_\n\n_of a developed system. RAM_\n\n_attributes impact the ability to_\n\n_perform the intended mis­_\n\n_sion and affect overall mission_\n\n_success. Reliability is typically_\n\n_defined as the probability of_\n\n_zero failures over a defined time_\n\n_interval (or mission), whereas_\n\n_availability is the percentage_\n\n_of time a system is considered_\n\n_ready to use when tasked._\n\n_Maintainability is a measure_\n\n_of the ease and rapidity with_\n\n_which a system or equipment_\n\n_can be restored to operational_\n\n_status following a failure._\n\nKeywords: availability, main­\n\n_tainability, RAM, reliability, RMA_\n\n\nINTEGRATED LOGISTICS SUPPORT\n###### Reliability, Availability, and Maintainability\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to under­\n\nstand the purpose and role of reliability, availability,\n\nand maintainability (RAM) in the acquisition\n\nprocess, where it occurs in systems development,\n\nand the benefits of employing it. MITRE SEs are\n\nalso expected to understand and recommend\n\nwhen RAM is appropriate to a situation and if the\n\nprocess can be tailored to meet program needs.\n\nThey are expected to understand the technical\n\nrequirements for RAM as well as strategies and\n\nprocesses that encourage and facilitate active\n\nparticipation of end users and other stakeholders\n\nin the RAM process. They are expected to monitor\n\nand evaluate contractor RAM technical efforts\n\nand the acquisition program’s overall RAM pro­\n\ncesses and recommend changes when warranted.\n\n\n-----\n\n###### Background\n\nReliability is the wellspring for the other RAM system attributes of Availability and\nMaintainability. Reliability was first practiced in the early start-up days for the National\nAeronautics and Space Administration (NASA) when Robert Lusser, working with Dr.\nWernher von Braun’s rocketry program, developed what is known as “Lusser’s Law” [1].\nLusser’s Law states that that the reliability of any system is equal to the product of the reliabil­\nity of its components, or the so-called weakest link concept.\n\nThe term “reliability” is often used as an overarching concept that includes availability\nand maintainability. Reliability in its purest form is more concerned with the probability of\na failure occurring over a specified time interval, whereas availability is a measure of some­\nthing being in a state (mission capable) ready to be tasked (i.e., available). Maintainability is\nthe parameter concerned with how the system in use can be restored after a failure, while\nalso considering concepts like preventive maintenance and Built-In-Test (BIT), required\nmaintainer skill level, and support equipment. When dealing with the availability require­\nment, the maintainability requirement must also be invoked as some level of repair and\nrestoration to a mission-capable state must be included. One can see how logistics and logistic\nsupport strategies would also be closely related and be dependent variables at play in the\navailability requirement. This would take the form of sparing strategies, maintainer train­\ning, maintenance manuals, and identification of required support equipment. The linkage\nof RAM requirements and the dependencies associated with logistics support illustrates how\nthe RAM requirements have a direct impact on sustainment and overall LCC. In simple terms,\nRAM requirements are considered the upper level overarching requirements that are speci­\nfied at the overall system level. It is often necessary to decompose these upper level require­\nments into lower level design-related quantitative requirements such as Mean Time Between\nFailure/Critical Failure (MTBF or MTBCF) and Mean Time To Repair (MTTR). These lower\nlevel requirements are specified at the system level; however, they can be allocated to sub­\nsystems and assemblies. The most common allocation is made to the Line Replaceable Unit\n(LRU), which is the unit that has lowest level of repair at the field (often called organic) level\nof maintenance.\n\nMuch of this discussion has focused on hardware, but the complex systems used today\nare integrated solutions consisting of hardware and software. Because software performance\naffects the system RAM performance requirements, software must be addressed in the overall\nRAM requirements for the system. The wear or accumulated stress mechanisms that charac­\nterize hardware failures do not cause software failures. Instead, software exhibits behaviors\nthat operators perceive as a failure. It is critical that users, program offices, test community,\nand contractors agree early as to what constitutes a software failure. For example, software\n“malfunctions” are often recoverable with a reboot, and the time for reboot may be bounded\n\n\n-----\n\nbefore a software failure is declared. Another issue to consider is frequency of occurrence\neven if the software reboot recovers within the defined time window, as this will give an indi­\ncation of stability of the software. User perception of what constitutes a software failure will\nsurely be influenced by both the need to reboot and the frequency of “glitches” in the operat­\ning software.\n\nOne approach to assessing software “fitness” is to use a comprehensive model to deter­\nmine the current readiness of the software (at shipment) to meet customer requirements. Such\na model needs to address quantitative parameters (not just process elements). In addition,\nthe method should organize and streamline existing quality and reliability data into a simple\nmetric and visualization that are applicable across products and releases. A novel, quantita­\ntive software readiness criteria model [2] has recently been developed to support objective and\neffective decision making at product shipment. The model has been “socialized” in various\nforums and is being introduced to MITRE work programs for consideration and use on con­\ntractor software development processes for assessing maturity. The model offers:\n\n###### �An easy-to-understand composite index �The ability to set quantitative “pass” criteria from product requirements �Easy calculation from existing data �A meaningful, insightful visualization �Release-to-release comparisons �Product-to-product comparisons �A complete solution, incorporating almost all aspects of software development activities\nUsing this approach with development test data can measure the growth or maturity of a\nsoftware system along the following five dimensions:\n\n###### �Software Functionality �Operational Quality �Known Remaining Defects (defect density) �Testing Scope and Stability �Reliability\nFor greater detail, see ref. [2].\n\n###### Government Interest and Use\n\nMany U.S. government acquisition programs have recently put greater emphasis on reliability.\nThe Defense Science Board (DSB) performed a study on Developmental Test and Evaluation\n(DT&E) in May 2008 and published findings [3] that linked test suitability failures to a lack\nof a disciplined systems engineering approach that included reliability engineering. The\nDepartment of Defense (DoD) has been the initial proponent of systematic policy changes to\naddress these findings, but similar emphasis has been seen in the Department of Homeland\n\n\n-----\n\nSecurity (DHS) as many government agencies leverage DoD policies and processes in the\nexecution of their acquisition programs.\n\nAs evidenced above, the strongest and most recent government support for increased\nfocus on reliability comes from the DoD, which now requires most programs to integrate reli­\nability engineering with the systems engineering process and to institute reliability growth as\npart of the design and development phase [4]. The scope of reliability involvement is further\nexpanded by directing that reliability be addressed during the Analysis of Alternatives (AoA)\nprocess to map reliability impacts to system LCC outcomes [5]. The strongest policy directives\nhave come from the Chairman of the Joint Chiefs of Staff (CJCS) where a RAM-related sustain­\nment Key Performance Parameter (KPP) and supporting Key System Attributes (KSAs) have\nbeen mandated for most DoD programs [6]. Elevation of these RAM requirements to a KPP\nand supporting KSAs will bring greater focus and oversight, with programs not meeting these\nrequirements prone to reassessment and reevaluation and program modification.\n\n###### Best Practices and Lessons Learned [7] [8]\n\n\nSubject matter expertise matters. Acquisition\n\nprogram offices that employ RAM subject matter\n\nexperts (SMEs) tend to produce more consis­\n\ntent RAM requirements and better oversight of\n\ncontractor RAM processes and activities. The\n\nMITRE systems engineer has the opportunity to\n\n“reach back” to bring MITRE to bear by strategi­\n\ncally engaging MITRE-based RAM SMEs early in\n\nprograms.\n\nConsistent RAM requirements. The upper level\n\nRAM requirements should be consistent with\n\nthe lower level RAM input variables, which are\n\ntypically design related and called out in technical\n\nand performance specifications. A review of user\n\nrequirements and flow down of requirements to\n\na contractual specification document released\n\nwith a Request For Proposal (RFP) package must\n\nbe completed. If requirements are inconsistent or\n\nunrealistic, the program is placed at risk for RAM\n\nperformance before contract award.\n\n\nEnsure persistent, active engagement of all\n\nstakeholders. RAM is not a stand-alone spe­\n\ncialty called on to answer the mail in a crisis, but\n\nrather a key participant in the acquisition process.\n\nThe RAM discipline should be involved early in\n\nthe trade studies where performance, cost, and\n\nRAM should be part of any trade-space activ­\n\nity. The RAM SME needs to be part of require­\n\nments development with the user that draws\n\non a defined Concept of Operations (CONOPS)\n\nand what realistic RAM goals can be established\n\nfor the program. The RAM SME must be a core\n\nmember of several Integrated Product Teams\n\n(IPTs) during system design and development to\n\nestablish insight and a collaborative relationship\n\nwith the contractor team(s): RAM IPT, Systems\n\nEngineering IPT, and Logistics Support IPT.\n\nAdditionally, the RAM specialty should be part\n\nof the test and evaluation IPT to address RAM\n\ntest strategies (Reliability Growth, Qualification\n\ntests, Environmental testing, BIT testing, and\n\n\n-----\n\nMaintainability Demonstrations) while interfacing\n\nwith the contractor test teams and the govern­\n\nment operational test community.\n\nRemember—RAM is a risk reduction ­activity.\n\nRAM activities and engineering processes are\n\na risk mitigation activity used to ensure that\n\nperformance needs are achieved for mission\n\nsuccess and that the LCC are bounded and\n\npredictable. A system that performs as required\n\ncan be employed per the CONOPS, and sus­\n\ntainment costs can be budgeted with a low risk\n\nof cost overruns. Establish reliability Technical\n\nPerformance Measures (TPMs) that are reported\n\non during Program Management Reviews (PMRs)\n\nthroughout the design, development, and test\n\nphases of the program, and use these TPMs to\n\nmanage risk and mitigation activities.\n\nInstitute the Reliability Program Plan. The\n\nReliability (or RAM) Program Plan (RAMPP) is\n\nused to define the scope of RAM processes\n\nand activities to be used during the program. A\n\nprogram office RAMPP can be developed to help\n\nguide the contractor RAM process. The program\nlevel RAMPP will form the basis for the detailed\n\ncontractor RAMPP, which ties RAM activities and\n\ndeliverables to the Integrated Master Schedule\n\n(IMS).\n\nEmploy reliability prediction and modeling. Use\n\nreliability prediction and modeling to assess the\n\nrisk in meeting RAM requirements early in the\n\nprogram when a hardware/software architecture\n\nis formulated. Augment and refine the model later\n\nin the acquisition cycle, with design and test data\n\nduring those program phases.\n\n\nReliability testing. Be creative and use any test\n\nphase to gather data on reliability performance.\n\nEnsure that the contractor has planned for a\n\nFailure Review Board (FRB) and uses a robust\n\nFailure Reporting And Corrective Action System\n\n(FRACAS). When planning a reliability growth\n\ntest, realize that the actual calendar time will be\n\n50–100% more than the actual test time to allow\n\nfor root cause analysis and corrective action on\n\ndiscovered failure modes.\n\nDon’t forget the maintainability part of RAM.\n\nUse maintainability analysis to assess the design\n\nfor ease of maintenance, and collaborate with\n\nHuman Factors Engineering (HFE) SMEs to\n\nassess impacts to maintainers. Engage with the\n\nIntegrated Logistics Support (ILS) IPT to help craft\n\nthe maintenance strategy, and discuss levels\n\nof repair and sparing. Look for opportunities to\n\ngather maintainability and testability data during\n\nall test phases. Look at Fault Detection and Fault\n\nIsolation (FD/FI) coverage and impact on repair\n\ntime lines. Also consider and address software\n\nmaintenance activity in the field as patches,\n\nupgrades, and new software revisions are\n\ndeployed. Be aware that the ability to maintain the\n\nsoftware depends on the maintainer’s software\n\nand IT skill set and on the capability built into the\n\nmaintenance facility for software performance\n\nmonitoring tools. A complete maintenance picture\n\nincludes defining scheduled maintenance tasks\n\n(preventive maintenance) and assessing impacts\n\nto system availability.\n\nUnderstand reliability implications when using\n\nCOTS. Understand the operational environment\n\nand the COTS hardware design envelopes and\n\n\n-----\n\nimpact on reliability performance. Use Failure\n\nModes Effects Analysis (FMEA) techniques to\n\n###### References and Resources\n\n\nassess integration risk and characterize system\n\nbehavior during failure events.\n\n\n1. Military Handbook 338, Electronic Reliability Design Handbook, October 1998.\n\n2. Asthana, A., and J. Olivieri, Quantifying Software Reliability and Readiness, IEEE\n\n_Communications Quality Reliability (CQR) Proceedings, 2009._\n\n3. Report of the Defense Science Board Task Force on Developmental Test and Evaluation,\n\nMay 2008.\n\n4. Department of Defense, December 2008, Instruction Number 5000.02, Operation of the\n\nDefense Acquisition System.\n\n5. Department of Defense, June 2009, Reliability, Availability, Maintainability, and Cost\n\n_Rationale Report Manual_\n\n6. Department of Defense, January 2011, Manual for the Operation of the Joint Capabilities\n\n_Integration and Development System._\n\n7. [Department of Defense, August 2005, DoD Guide for Achieving Reliability, Availability,](http://www.acq.osd.mil/dte-trmc/)\n[and Maintainability.](http://www.acq.osd.mil/dte-trmc/)\n\n8. Reliability Information Analysis Center, Reliability Toolkit: Commercial Practices Edition.\n\n\n-----\n\nDefinition: Energy efficiency is\n\n_a measure of how well a system_\n\n_uses the available energy_\n\n_potential of its inputs. The goal_\n\n_of managing it is about using_\n\n_less energy to provide the same_\n\n_level of service, thus conserving_\n\n_our energy resources during_\n\n_production, operations, and dis­_\n\n_posal of systems. The impact_\n\n_goes beyond environmental_\n\n_concerns; it also has implica­_\n\n_tions on the range of vehicles,_\n\n_the cost of operations, and the_\n\n_future of a system._\n\nKeywords: efficiency, energy,\n\n_green, life-cycle costs,_\n\n_trade-off_\n\n\nINTEGRATED LOGISTICS SUPPORT\n###### Managing Energy Efficiency\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to be able\n\nto understand the impact of technical deci­\n\nsions on the energy efficiency of a system. They\n\nare expected to account for energy efficiency\n\nconsiderations across the system life cycle as\n\npart of the systems engineering process.\n\n\n-----\n\n###### Background and Motivation\n\nMany engineers have studied efficiency in the past and are familiar with the theoretical limits\nof a given thermodynamic cycle. Increased attention to energy efficiency has come about in\nthe last 50 years, primarily as a result of a continuing trend of energy crises throughout the\nworld. The first modern major crisis was in 1973. The oil crisis resulted not from a depletion\nof energy sources, but from political maneuvers that took the form of the Arab Oil Embargo.\nThe effects were sharply felt throughout the country, and resulted in the price of oil quadru­\npling [1], rationing, and long lines. Other crises occurred in 1979, associated with the Iranian\nRevolution, and in 1990, over the Iraqi invasion of Kuwait. Several times during this century,\nprice increases in oil were caused by reports of declining petroleum reserves, natural disasters\nsuch as Hurricane Katrina, political activities in other countries, and conflicts that did not\ndirectly involve the United States [2, 3]. Each of these demonstrates the importance of con­\nsidering all elements of POET (political, operational, economic, technical), or as expanded,\nTEAPOT (technically accurate, economically feasible, actionable, politically and culturally\ninsightful, operationally grounded, and timely) [4].\n\nComplicating the landscape is the fact that most energy today is produced by nonrenewable resources in the form of coal, fissile materials, petroleum, and natural gas. When\nproduction first begins with each material, the amount of material recovered for each unit of\nenergy is large. For example, in the mid-nineteenth century when oil production began, the\nlargest oil fields could recover 50 barrels of oil for every barrel used in extraction, refining,\nand transportation. Today that number is between one and five. Once it reaches one (one bar­\nrel of oil to make one barrel of oil), the only way to make the resource available for consump­\ntion is to use other energy sources to bring it to market. This situation will happen before the\nresource is physically exhausted. Energy resource production follows nearly a bell-shaped\ncurve called the Hubbert curve [5], and trails discovery by about 35 years. In the United\nStates, peak oil (the peak of the Hubbert curve) was reached in the 1970s and has resulted in\nthe country importing increasing amounts as the consumption continued to increase since\nthat time. For the world, peak oil is estimated to be 2020. As production falls, the production\nprocess gets more expensive and the price rises. The same happens for each natural resource.\nUnderstanding this sequence is important to understanding the push for increased energy\nefficiency.\n\nThe number of U.S. military bases overseas has declined over the past two decades, and\nit is getting increasingly difficult to gain access to countries to forward deploy forces and\nequipment. This trend and the desire for the U.S. military to minimize the risk to its forces\nhave resulted in programs that deliver weapons systems that operate from longer ranges. To\nachieve this, either an increased infrastructure of resupply systems must be put in place, or\nweapons systems must be more efficient to achieve the longer ranges needed to accomplish\n\n\n-----\n\nthe missions. As for the resupply chain, there is also a desire to reduce the forward footprint\nof deployed forces, including generators and the resupplied fuels (an issue now with complex\nweapon and computing systems and their power consumption).\n\n###### Government Interest and Use\n\nEnergy efficiency is a parameter that can potentially be adjusted during the development of\na system. It has a direct effect on the range of vehicles, as well as the upfront and recurring\noperating costs of systems. Secondary effects include minimum and maximum operating lim­\nits, and adverse effects on performance parameters like speed and dynamic control. In some\ndomains, increased energy efficiency generally means higher upfront costs with the savings\nrealized over the life cycle in the form of decreased energy bills.\n\nThe government uses energy to run its transportation fleets, surveillance and weapons\ndelivery systems, facilities, and information processing systems. Trends in data centers have\nshifted the predominate cost from the building itself to the operation of the building and,\nmore specifically, to obtaining the energy. The Green Grid [6, 7] has established a simple set\nof metrics that can be used to reduce the energy consumption for the data center. While they\nare very simplistic, such as the ratio of power used by IT equipment to the power entering\nthe facility, they give an indication of overall data center energy efficiency. Many organiza­\ntions have used these measurements as the tool to reduce the cost of operations. These include\nGoogle, Mass Mutual, Patagonia, and Kimberly-Clark [8, 9, 10, 11, 12]. To date, more work in\nthis area is being done in the commercial arena than in the government sector.\n\nThe government has established green standards for acquisitions, part of which mandate\nthe use of Energy Star™ and EPEAT™ [13, 14, 15]. There has been an increased use of the U.S.\nGreen Building Council LEED™ program in the acquisition of buildings. It is important to note\nthat these buildings must be maintained and consistently improved to keep the energy effi­\nciency of the building competitive. As with all green initiatives, the target continues to move\nover time [16, 17].\n\n###### Best Practices and Lessons Learned\n\n\nBe wary of advertised efficiency claims.\n\nPrograms such as Energy Star™, EPEAT™, LEED™,\n\nand Green Globes (international counterpart to\n\nLEED™) need to be carefully scrutinized for how\n\nthey gauge energy consumption. For example, a\n\nprinter rated by Energy Star™ can be expected to\n\nachieve the stated energy efficiency only when\n\n\noperated in exactly the same manner in which the\n\nrating was obtained. Generally the test conditions\n\nused are not provided. So, there is the need to be\n\naware of or find out what is being measured and\n\nhow it applies to your system’s situation [13, 14, 18,\n\n19, 20, 21].\n\n\n-----\n\nKnow your system’s operations. Thoroughly\n\nunderstand how the government intends to use\n\nthe system before evaluating its energy con­\n\nsumption profile and requirements. Understand\n\nthe level of energy management expertise in the\n\ncontractor and the maintenance organization\n\nbecause, although components of a system may\n\nhave energy-saving modes, they may be shipped\n\nin a configuration that does not enable the\n\nmodes. Also, look into whether operational, infor­\n\nmation security, or other policies or procedures\n\nprevent or impede their use [22]. As an example, in\n\nmany organizations it is common practice to push\n\nout software updates during the early morning\n\nhours. If a system is in a low power mode, it may\n\nnot respond to the push. To avoid this situation,\n\n###### References and Resources\n\n\nan organization may have policies that prevent\n\nsystems from entering a low power mode.\n\nTake measures. The only way to truly know what\n\nthe system is doing is to measure its performance.\n\nThe more data you have, the greater your ability\n\nto make informed decisions about your system’s\n\noperation, whether it be a weapons system, a\n\nbuilding, or a data center. With more knowledge,\n\nyou are better able to know where and how\n\nenergy is being used and recommend solutions to\n\nimprove energy efficiency [16, 23, 24].\n\nInvolve all stakeholders. Energy efficiency is an\n\nenterprise-level problem. In general, the organiza­\n\ntion paying for the energy is not the organization\n\nprocuring the system or facility, nor the organiza­\n\ntion using or making decisions about using the\n\nenergy. Be sure to involve all stakeholders [9, 17].\n\n\n1. CBC News in Depth, July 18, 2007, The Price of Oil, Marching to $100?, CBC News.\n\n2. Cooper P. J., July 8, 2006, “Record Oil Price sets the scene for $200 next year,”\n\n_AMEinfo.com._\n\n3. Mortished, Carl, August 30, 2005, “Hurricane Katrina whips oil price to a record high,”\n\n_The Times._\n\n4. The MITRE Corporation, CCG Quality Program, CEM Quality Handbook and Quality\n\nTools.\n\n5. Grove, N., June 1974, “Oil, the Dwindling Treasure,” National Geographic.\n\n6. “Guidelines for Energy-Efficient Datacenters,” The Green Grid, February 16, 2007.\n\n7. [The Green Grid, http://www.thegreengrid.org/, accessed February 17, 2014.](http://www.thegreengrid.com/)\n\n8. Anderson, S. F., November 2009, “Improving Data Center Efficiency,” World Energy\n\n_Engineering Congress (WEEC)._\n\n9. Belady, C., September 5, 2006, “How to Minimize Data Center Utility Bills,” E-Business\n\n_News._\n\n\n-----\n\n10. Dixon, S., November 2009, “Energy Management for Commercial Office Buildings: An\n\nOwner’s Perspective,” WEEC.\n\n11. McMahon, J., November 3, 2009, “Sustainable Distribution,” Sustainable Facility.\n\n12. Marklein, B. Richard, and J. Marin, November 2009, “Kimberly-Clark’s Energy\n\nManagement for 2015,” WEEC.\n\n13. Bush, G. W., January 24, 2007, Executive Order 13423, Strengthening Federal\n\nEnvironmental, Energy, and Transportation Management.\n\n[14. Energy Star, http://www.energystar.gov/, accessed February 17, 2014.](http://www.energystar.gov/)\n\n[15. EPEAT, http://www.epeat.net/, accessed February 14, 2014.](http://www.epeat.net/)\n\n16. Huppes, G., and M. Ishikawa, October 2005, “A Framework for Quantified Eco-efficiency\n\nAnalysis,” Journal of Industrial Ecology, vol. 9 no. 4, pp. 25–41.\n\n17. Myatt, B., September/October 2009, “Low-Cost Data Center Energy Efficiency Programs in\n\nAction,” Mission Critical, pp. 20–21.\n\n18. Boyd, Gale A., July 2005, “A Method for Measuring the Efficiency Gap between Average\n\nand Best Practice Energy Use, The ENERGY STAR Performance Indicator,” Journal of\n_Industrial Ecology, vol. 9 no. 3, pp. 51–65._\n\n19. DOE and EPA, Fuel Economy, (MPG and other information about vehicles), accessed\n\nFebruary 26, 2010.\n\n20. Green Building Initiative, Green Globes: the practical building rating system, accessed\n\nFebruary 26, 2010.\n\n21. U.S. Green Building Council (LEED) website, accessed February 26, 2010.\n\n22. Nordman, Bruce, Mary Ann Peitte, Kris Kinney, and Carrie Webber, January 1997,\n\nUser Guide to Power Management in PCs and Monitors, Lawrence Berkeley National\nLaboratory.\n\n[23. 80 PLUS Energy-Efficient Technology Solutions, http://www.plugloadsolutions.](http://www.plugloadsolutions.com/80PlusPowerSupplies.aspx)\n\n[com/80PlusPowerSupplies.aspx, accessed February 17, 2014.](http://www.plugloadsolutions.com/80PlusPowerSupplies.aspx)\n\n24. Group, J., November 2009, “Success Stories in Cutting Facility Energy Costs Using Electric\n\nSubmeters,” WEEC.\n\n###### Additional References and Resources\n\nRabiee, A., H. Shayanfar, and N. Amjady, Jan-Feb 2009, “Reactive Power Pricing,” IEEE Power\n_and Energy Magazine, vol. 7, issue 1, pp. 18–32._\n\nThe MITRE Corporation, “Integrated Logistics Support,” MITRE Systems Engineering\nCompetency Model, accessed February 26, 2010.\n\n\n-----\n\n##### Quality Assurance and\n Measurement\n\nDefinition: Quality assurance is “a planned and systematic means for assuring\n\n_management that the defined standards, practices, procedures, and methods_\n\n_of the process are applied.” “The purpose of [quality] measurement and analysis_\n\n_(MA) is to develop and sustain a measurement capability used to support_\n\n_management information needs [1].”_\n\nKeywords: continuous improvement, measurement, metrics, process improve­\n\n_ment, quality, standards_\n\n###### Context\n\nThere are multiple perspectives on both quality and its measurement\n\nthat depend on the stakeholder’s point of view. Knowledge of these\n\nperspectives is important when recommending quality or measure­\n\nment programs for a government organization.\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to be able to recom­\n\nmend how to establish a quality assurance program in the govern­\n\nment systems acquisition or government operational organization.\n\nThey are expected to be able to guide the establishment and\n\ndirection of quality assurance programs, conduct process and\n\nproduct reviews, and influence the resolution of corrective actions\n\nto ensure adherence to documented processes. MITRE SEs are\n\n\n-----\n\nexpected to be able to help develop measurement capabilities to monitor processes and\n­products [2].\n\n###### Perspectives on Quality\n\nSome of the perspectives on quality are as follows [3]:\n\n###### �Judgmental: When referred to as the transcendent definition of quality, it is both\n\nabsolute and universally recognizable, a mark of uncompromising standards and high\nachievement. You can’t really measure or assess it—you just know it when you see it.\nLexus and Ritz-Carlton are examples.\n###### �Product-based: In this view, quality is a function of a specific, measurable variable\n\nand differences in quality reflect differences in quantity of a product attribute, such as\nthreads per square inch or pixels per square inch. Bed sheets and LCD high-definition\ntelevisions are examples.\n###### �User-based: Quality is defined as fitness for intended use, or how well the product per­\n\nforms its intended function. If you want an off-road vehicle for camping, a Jeep might\nsuit your needs. If you want a luxury vehicle with lots of features, a Cadillac might bet­\nter suit your needs.\n###### �Value-based: From this perspective, a quality product is as useful as a competing prod­\n\nuct and sold at a lower price, or it offers greater usefulness or satisfaction at a compara­\nble price. If you had a choice between two “thumb drives” and one offered one gigabyte\nof storage for $39.95 and another offered two gigabytes of storage for $19.95, chances are\nyou would choose the latter.\n###### �Manufacturing-based: Quality is the desirable outcome of engineering and manufactur­\n\ning practice, or conformance to specifications. For Coca-Cola, quality is “about manu­\nfacturing a product that people can depend on every time they reach for it.”\n###### �Customer-drive: The American National Standards Institute and the American Society\n\nfor Quality (ASQ) define quality as “the totality of features and characteristics of a prod­\nuct or service that bears on its ability to satisfy given needs [4].” A popular extension of\nthis definition is “quality is meeting or exceeding customer expectations.”\n\n###### Quality Assurance Versus Quality Control\n\nThere is an important distinction between quality assurance (QA) and quality control (QC).\nASQ defines QA as “the planned and systematic activities implemented in a quality system so\nthat quality requirements for a product or service will be fulfilled.” ASQ defines QC as “the\nobservation techniques and activities used to fulfill requirements for quality [4].” Thus QA is a\nproactive, process-oriented activity whereas QC is a reactive, manufacturing-oriented activity.\n\n\n-----\n\nThe focus of QA is putting good processes in place so that the quality will be “built into” the\nproduct rather than trying to “inspect quality into” the finished product.\n\n###### Quality Standards and Guidance\n\nThe International Organization for Standardization (ISO) 9000 family introduces the concept\nof quality management, processes, certification, and continual improvement. ISO 9000 is the\ninternationally accepted standard for quality management. It looks at manufacturing and\ncustomer-based perspectives of quality. The ISO 9000:2000 family is built on eight quality\nmanagement principles: (1) Customer Focus, (2) Leadership, (3) Involvement of People, (4)\nProcess Approach, (5) System Approach to Management, (6) Continual Improvement, (7)\nFactual Approach to Decision Making, and (8) Mutually Beneficial Supplier Relationships [5].\n\nThe ISO 9001:2000 (the basis for ISO 9001 certification or registration) states, “This\nInternational Standard specifies requirements for a quality management system where an\norganization a) needs to demonstrate its ability to consistently provide a product that meets\ncustomer and applicable regulatory requirements, and b) aims to enhance customer satis­\nfaction through the effective application of the system, including processes for continual\nimprovement of the system and the assurance of conformity to customer and applicable regu­\nlatory requirements [6].” ISO 9001 registration is critical to securing and maintaining busi­\nness for both private and public sector contractors. The government recognizes ISO 9001:2000\nas a “higher level” quality requirement and may invoke it in the contract under the condi­\ntions stated in Federal Acquisition Regulation Part 46.202-4, Higher Level Contract Quality\nRequirements. In these situations, the government is more interested in the contractor’s qual­\nity management system (and its certification) than government inspection of a product under\ndevelopment.\n\nGovernment acquisition organizations rarely have an independent quality assurance\norganization that oversees the quality of the government’s work products or processes. The\nCapability Maturity Model Integrated for Acquisition (CMMI-ACQ) includes a process area\n(Product and Process Quality Assurance) that provides a set of goals and specific practices for\nquality assurance in an acquisition organization [7]. Both government and contractor develop­\nment organizations have a similar CMMI for Development (CMMI-DEV) process area [8].\n\nThe government occasionally introduces a quality or process improvement initiative that\nreceives emphasis for a while and is then overcome by events or forgotten. Several of these\npast initiatives include the “Suggestion Program,” “Zero Defects,” “Quality Circles,” and\n“Total Quality Management.” Some of the most recent initiatives are Department of Defense\n(DoD) Six Sigma, DoD-wide Continuous Process Improvement/Lean Six Sigma, and Air Force\nSmart Operations for the 21st Century [9, 10, 11]. Most of these initiatives deal with process\n\n\n-----\n\nimprovement by eliminating waste, streamlining the processes, or instituting a more efficient\nway to perform a required task that results in cost avoidance or cost savings.\n\n###### Perspectives on Measurement\n\nAll three CMMI models—CMMI-ACQ [12], CMMI for Services (CMMI-SVC) [13], and\nCMMI-DEV [14]—include a process area for Measurement and Analysis. “The purpose of\nMeasurement and Analysis (MA) is to develop and sustain a measurement capability that\nis used to support management information needs [14].” There are eight specific practices\nrecommended in the models: 1) Establish Measurement Objectives; 2) Specify Measures;\n3) Specify Data Collection and Storage Procedures; 4) Specify Analysis Procedures; 5)\nCollect Measurement Data; 6) Analyze Measurement Data; 7) Store Data and Results; and 8)\nCommunicate Results [14].\n\nFour categories of measurement are common to many acquisition programs in which\nMITRE is involved:\n\n1. The quantitative performance requirements of user requirements and system perfor­\n\nmance requirements are measured in the operational and development test programs.\nSuggestions on key performance parameters can be found in Enclosure B of the\nChairman of the Joint Chiefs of Staff Manual on the Operation of the Joint Capabilities\nIntegration and Development System [15].\n2. Technical Performance Measurement (TPM) monitors the developer’s progress in\n\nmeeting critical performance requirements over the life of the program where there is\na development risk. The concept is further explained on the Office of the Secretary of\nDefense (OSD) TPM website [16].\n3. Earned Value Management (EVM) monitors a developer’s cost and schedule perfor­\n\nmance in cost reimbursement development contracts. Additional information can be\nfound in the article “Earned Value Management” and in EIA-748A and the OSD EVM\nwebsite [17, 18].\n4. Process Metrics are associated with development processes like software development.\n\nA good approach to identifying the type of measurement needed and the proven met­\nrics that support that measurement can be found on the Practical Software and System\nMeasurement website [19].\nThere is a fifth category that may be involved if MITRE is assisting in developing perfor­\nmance-based logistics criteria for operations and maintenance efforts. OSD recommends five\nperformance parameters: 1) Operational Availability; 2) Operational Reliability; 3) Cost Per\nUnit Usage; 4) Logistics Footprint; and 5) Logistics Response Time. These are cited in an OSD\nMemo on the subject [20].\n\n\n-----\n\nFor additional information, refer to the articles “Acquisition Management Metrics” and\n“How to Develop a Measurement Capability.”\n\n###### Articles Under This Topic\n\nThe article “Establishing a Quality Assurance Program in the Systems Acquisition or\nGovernment Operational Organization” provides guidance on processes to assure that the\nright product is being built (customer-driven quality), that the product being built will meet\nits specified requirements (product-based quality), and that the product is suitable for its\nintended use (user-based quality).\n\nThe article “How to Conduct Process and Product Reviews Across Boundaries” provides\nguidance on assisting government and contractor organizations in documenting quality pro­\ncesses and work product specifications, and reviewing those processes and products.\n\n###### References and Resources\n\n[1. CMMI-DEV, Version 1.2.](http://www.sei.cmu.edu/publications/documents/06.reports/06tr008.html)\n\n2. MITRE Systems Engineering (SE) Competency Model, Version 1, September 1, 2007, The\n\nMITRE Institute, Section 3.7, Quality Assurance and Measurement, pp. 45-46.\n\n3. Evans, J. R. and W. M. Lindsay, 2008, Managing for Quality and Performance Excellence,\n\n7th Edition, Thomson, Southwestern.\n\n[4. The American Society for Quality (ASQ) website.](http://www.asq.org/learn-about-quality/quality-assurance-quality-control/overview/overview.html)\n\n5. _ISO 9000:2000, 2000. Quality Management Systems, Fundamentals and Vocabulary,_\n\nSecond Edition.\n\n6. _ISO 9001:2000, 2000, Quality Management Systems, Third Edition._\n\n7. [CMMI-ACQ, Version 1.2.](http://www.sei.cmu.edu/publications/documents/07.reports/07tr017.html)\n\n[8. CMMI-DEV, Version 1.2.](http://www.sei.cmu.edu/publications/documents/06.reports/06tr008.html)\n\n[9. Matchette, Daniel R., “Six Sigma for the DoD,” Defense AT&L Magazine, July-August](http://www.dau.mil/pubscats/PubsCats/atl/2006_07_08/july-aug_06.pdf)\n\n[2006, Vol. 35, No. 4, p. 19–21.](http://www.dau.mil/pubscats/PubsCats/atl/2006_07_08/july-aug_06.pdf)\n\n[10. DoD 5012.42, May 15, 2008, DoD-Wide Continuous Process Improvement (CPI)/Lean Six](http://www.dtic.mil/whs/directives/corres/pdf/501042p.pdf)\n\n[Sigma (LSS) Program.](http://www.dtic.mil/whs/directives/corres/pdf/501042p.pdf)\n\n[11. The Secretary of the Air Force and Chief of Staff of the Air Force, February 7, 2006, Air](https://acc.dau.mil/CommunityBrowser.aspx?id=140502&lang=en-US)\n\n[Force Smart Operations for the 21st Century CONOPS and Implementation Plan, Version 4.](https://acc.dau.mil/CommunityBrowser.aspx?id=140502&lang=en-US)\n\n[12. CMMI-ACQ, Version 1.2.](http://www.sei.cmu.edu/publications/documents/07.reports/07tr017.html)\n\n[13. CMMI-SVC, Version 1.2.](http://www.sei.cmu.edu/reports/09tr001.pdf)\n\n\n-----\n\n[14. CMMI-DEV, Version 1.2.](http://www.sei.cmu.edu/publications/documents/06.reports/06tr008.html)\n\n[15. CJCSM 3170.01C, Operation of the Joint Capabilities Integration and Development System.](http://www.dtic.mil/cjcs_directives/cdata/unlimit/m317001.pdf)\n\n[16. OSD Technical Performance Measurement website.](http://www.acq.osd.mil/evm/)\n\n17. _Earned Value Management, ANSI EIA-748A Standard (June 1998 ed.)._\n\n[18. OSD Earned Value Management website.](http://www.acq.osd.mil/evm/)\n\n[19. Practical Software and System Measurement website.](http://psmsc.com/SampleMeasures.asp)\n\n[20. OSD AT&L Memo, 16 Aug 2005, Performance Based Logistics: Purchasing Using](https://acc.dau.mil/CommunityBrowser.aspx?id=32574)\n\n[Performance Based Criteria.](https://acc.dau.mil/CommunityBrowser.aspx?id=32574)\n\n###### Additional References and Resources:\n\nMetzger, L., May 2009, Systems Engineering Quality at MITRE, The MITRE Corporation.\n\nThe MITRE Corporation, August 21, 2009, “Quality,” MITRE Project Leadership Handbook.\n\n\n-----\n\nDefinition: Quality Assurance\n\n_(QA) is “a planned and sys­_\n\n_tematic means for assuring_\n\n_management that the defined_\n\n_standards, practices, proce­_\n\n_dures, and methods of the_\n\n_process are applied [1].”_\n\nKeywords: continuous improve­\n\n_ment, process improvement,_\n\n\nQUALITY ASSURANCE AND MEASUREMENT\n###### Establishing a Quality Assurance Program in the Systems Acquisition or Government Operational Organization\n\n\n_quality, standards_ **MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to rec­\n\nommend how to establish a QA program in the\n\nsystems acquisition or the government opera­\n\ntional organization. They are expected to propose\n\nplans to resource, implement, and manage a QA\n\nprogram to enable a positive, preventive approach\n\nto managing the systems acquisition. They are\n\nexpected to participate in integrated teams to\n\ncreate directives and plans that establish QA\n\nstandards, processes, procedures, and tools [2].\n\n\n-----\n\n###### Background\n\nMITRE assists the government in preparing contract requirements for the acquisition of large\nsystems from major information technology contractors. With few exceptions, these contracts\nmust comply with mandatory provisions in the Federal Acquisition Regulation (FAR).\n\nThe definition of quality for government contracts is stated in the FAR Part 46.101:\n“Contract quality requirements means the technical requirements in the contract relating\nto the quality of the product or service and those contract clauses prescribing inspection,\nand other quality controls incumbent on the contractor, to assure that the product or service\nconforms to the contractual requirements.” Thus, the government contract interpretation of\nquality (control) in most contracts for major systems is the manufacturing-based perspective:\nconformance to specifications.\n\n###### MITRE’s Quality Guidelines and Standards\n\nMITRE’s QA efforts should focus on the use of appropriate processes to assure that the right\nproduct is being built (customer-driven quality), that the product being built will meet\nits specified requirements (product-based quality), and that the product is suitable for its\nintended use (user-based quality). This aligns with the view of systems engineering quality\nin the “Systems Engineering Quality at MITRE” white paper. This paper states, “1) Degree\nto which results of SE meet the higher level expectations for our FFRDCs [federally funded\nresearch and development centers]—resulting in usability and value for end recipients; 2)\nDegree to which results of SE [systems engineering] meet expectations of our immediate\ncustomers—service and performance [3].” MITRE’s QA efforts are particularly appropriate for\nthe design and development phases of the product, especially software and one or few-of-akind systems, rather than concentrating on quality control (QC) in the production phase. For\nadditional perspectives on quality, see the SEG’s Quality Assurance and Measurement topic.\n\n###### Best Practices and Lessons Learned\n\n\nUse project and portfolio reviews. The use of\n\nMITRE project reviews can provide project lead­\n\ners with additional perspectives and assistance\n\nfrom elsewhere in MITRE, when necessary. This\n\nis a form of leveraging the corporation. Portfolio\n\nreviews help maintain a focus on a sponsor’s most\n\nimportant problems and provide an opportunity\n\nfor cross-portfolio synergy among the projects in\n\nthe portfolio.\n\n\nEstablish watchlists. Watchlists in various forms\n\n(e.g., major issues, significant risks, external influ­\n\nences) provide a vehicle to keep project leaders\n\nfocused on issues that are likely to have a critical\n\nimpact on their programs. They also keep MITRE\n\nand senior government managers aware of proj­\n\nect issues that may require escalation. If done at\n\nan enterprise level, watchlists can keep individual\n\n\n-----\n\nprograms aware of enterprise issues where pro­\n\ngrams can make a difference.\n\nPerform Engineering Risk Assessments (ERAs).\n\nERAs are constructive engineering reviews that\n\nidentify and resolve issues or risks that might\n\npreclude program success. In the Department\n\nof Defense, ERAs are performed on Acquisition\n\nCategory II (ACAT II) and below programs. The\n\nERAs focus on solution appropriateness, SE\n\nprogress health, and SE process health. In doing\n\nso, the ERA considers all aspects of systems\n\nengineering in acquisition, including engineering to\n\nestablish sound technical baselines that support\n\nprogram planning and program cost estimation,\n\ntechnical resource planning, engineering manage­\n\nment methods and tools, engineering perfor­\n\nmance metrics, engineering basis of estimate\n\nand earned value management, system design\n\nappropriateness, system design for operational\n\neffectiveness (SDOE), and other areas. The ERA\n\nmethodology provides a tailorable framework for\n\nconducting ERAs to assist program managers and\n\nappropriate decision makers in preparation for\n\nmilestone decision and other reviews.\n\nPerform independent assessments.\n\nIndependent assessments can include Gold\n\nTeams, Blue Teams, Red Teams, Gray Beard\n\nVisits, or process assessments like the Standard\n\nCapability Maturity Model Integration Appraisal\n\nMethod for Process Improvement (SCAMPI). All\n\nof these assessments involve the use of external\n\nsubject matter experts to provide an objective\n\nopinion on the health of a program or organization\n\nand its processes. An independent assessment\n\ncan be used at any point in the program life cycle\n\nto provide insight into the progress and risks. For\n\n\nexample, the assessment may be used to provide\n\nan independent assessment of a preliminary\n\ndesign or an assessment of the product as it\n\nenters integration and test. Independent assess­\n\nments are typically proactive and intended to pro­\n\nvide an early look at potential problems that may\n\nbe on the horizon in time to take action and avoid\n\nadverse impact to the program. One of the best\n\nopportunities for an independent assessment is\n\nduring the management turnover of a program\n\nor organization. This provides the new manager\n\na documented assessment of the organization\n\nand a set of recommended improvements. The\n\nnew manager has the benefit of a documented\n\nassessment of the mistakes or improvements\n\nmade by prior management. For more informa­\n\ntion, see the SEG’s MITRE FFRDC Independent\n\nAssessments topic.\n\nConduct peer reviews of deliverables. Having\n\nan external peer review of formal deliverables\n\nensures that the delivered product makes sense\n\nand represents a MITRE position rather than an\n\nindividual’s opinion on a product provided to our\n\nsponsor. This is particularly important on small\n\nprojects that are located in a sponsor’s facility. It\n\nkeeps our staff objective in providing advice to our\n\nsponsors.\n\nPerform after-action reviews. After participat­\n\ning in the development of a critical deliverable or\n\nbriefing, or position for or with our sponsors, meet\n\nwith the MITRE participants and discuss what was\n\nexecuted well and what could have been better.\n\nThis allows us to continuously improve how we\n\nserve the customer and capture lessons learned\n\nfor others engaged in similar activities for their\n\nsponsors.\n\n\n-----\n\nIdentify key requirements. All requirements are\n\nnot created equal, although that may be the first\n\nresponse when you ask the “priority” question.\n\nWhether it is the key performance parameters in\n\nan operational requirements document, the criti­\n\ncal performance parameters in a system perfor­\n\nmance specification, or the evaluation factors for\n\naward in a request for proposal, identify the most\n\nimportant measures that will impact the final\n\ndecision.\n\nUse Technical Performance Measurement\n\n(TPM) in conjunction with risky performance\n\nrequirements only. If a given performance\n\nrequirement is within the state-of-the-art tech­\n\nnology, and there is little doubt that the developer\n\nwill be able to meet the requirement, do not use\n\nTPM. Focus on the “risky” performance require­\n\nments where it is important to monitor progress in\n\n“burning down” the risk.\n\nIdentify program risks and problem areas, and\n\nthen identify metrics that can help. Do not take\n\na “boilerplate” approach when specifying metrics\n\nto monitor your program. Identify the significant\n\n###### References and Resources\n\n\nprogrammatic and technical issues and risks on\n\nthe program, then select metrics that provide\n\ninsight into the handling or mitigation of the issues\n\nand risks.\n\nDo not identify all metrics at contract award,\n\nspecify them when you need them. As program\n\nissues and risks change as a function of time,\n\nthe metrics to monitor the handling or mitigation\n\nof the issues or risks should change as well. In\n\nthe front end of a program, developer ramp-up\n\nis something to monitor, but it disappears when\n\nthe program is staffed. In the testing phase of a\n\nprogram, defect density is important, but not par­\n\nticularly important in the front end of a program.\n\nDo not require measurement data, unless you\n\nhave an analysis capability. Do not implement\n\na set of metrics, unless you have staff with the\n\ncapability to analyze the resulting data and make\n\nprogram decisions. Two examples of data that\n\nis frequently requested, but there is no program\n\nstaffing qualified for analysis, are software metrics\n\nand Earned Value Management (EVM) data.\n\n\n[1. Software Engineering Institute, Carnegie Mellon, “CMMI-Development,” Version 1.2,](http://www.sei.cmu.edu/library/abstracts/reports/06tr008.cfm)\n\naccessed February 11, 2010.\n\n2. The MITRE Corporation, September 1, 2007, “MITRE Systems Engineering (SE)\n\nCompetency Model,” Version 1, Section 3.7, p. 45.\n\n3. Metzger, L., May 2009, “Systems Engineering Quality at MITRE.”\n\n###### Additional References and Resources\n\n[Acquisition Community Connection, August 16, 2005, Acting UDS/AT&L Policy Memo:](https://acc.dau.mil/CommunityBrowser.aspx?id=32574)\n[Performance Based Logistics: Purchasing Using Performance Based Criteria.](https://acc.dau.mil/CommunityBrowser.aspx?id=32574)\n\n\n-----\n\n[Acquisition Community Connection, February 7, 2006, Air Force Smart Operations for the 21st](https://acc.dau.mil/CommunityBrowser.aspx?id=140502)\n[Century CONOPS and Implementation Plan, Version 4.](https://acc.dau.mil/CommunityBrowser.aspx?id=140502)\n\n[CMMI Product Team, February 2009, “CMMI for Services, Ver. 1.2-CMMI-SVC, V1.2,” Software](http://www.sei.cmu.edu/reports/09tr001.pdf)\nEngineering Institute, accessed February 11, 2010.\n\n[Department of Defense Directive, May 15, 2008, “DoD-Wide Continuous Process Improvement](http://www.dtic.mil/whs/directives/corres/pdf/501042p.pdf)\n[(CPI)/Lean Six Sigma (LSS) Program,” DoD 5010.42.](http://www.dtic.mil/whs/directives/corres/pdf/501042p.pdf)\n\n“Earned Value Management,” June 1998, ANSI EIA-748A Standard.\n\nEvans, J. R., and W. M. Lindsay, 2008, Managing for Quality and Performance Excellence, 7th\nEd., Thomson South-western.\n\nInternational Organization for Standardization, 2000, ISO 9000:2000, Quality Management\n_Systems, Fundamentals and Vocabulary, Second Edition._\n\nInternational Organization for Standardization, 2000, ISO 9001:2000, Quality Management\n_Systems, 3rd Ed.._\n\n[Joint Chiefs of Staff, May 1, 2007, Operation of the Joint Capabilities Integration and](http://www.dtic.mil/cjcs_directives/cdata/unlimit/m317001.pdf)\n[Development System, CJCSM 3170.01C.](http://www.dtic.mil/cjcs_directives/cdata/unlimit/m317001.pdf)\n\n[OSD Earned Value Management, accessed February 15, 2010.](http://www.acq.osd.mil/evm/)\n\n[OSD Technical Performance Measurement, accessed February 15, 2010.](http://www.acq.osd.mil/evm/)\n\n[Practical Software and System Measurement, accessed February 15, 2010.](http://psmsc.com/SampleMeasures.asp)\n\n[Software Engineering Institute, Carnegie Mellon, “CMMI for Acquisition, Version 1.2,”](http://www.sei.cmu.edu/library/abstracts/reports/07tr017.cfm)\naccessed February 11, 2010.\n\n[The American Society for Quality (ASQ), accessed February 11, 2010.](http://asq.org/learn-about-quality/quality-assurance-quality-control/overview/overview.html)\n\nThe MITRE Corporation, August 2009, MITRE Project Leadership Handbook, “Quality.”\n\n\n-----\n\nDefinition: Process and Product\n\n_Quality Assurance are activities_\n\n_that provide staff and manage­_\n\n_ment with objective insight_\n\n_into processes and associated_\n\n_work products [1]. A Quality_\n\n_Management Process assures_\n\n_that products, services, and_\n\n_implementations of life-cycle_\n\n_processes meet organization_\n\n_quality objectives and achieve_\n\n_customer satisfaction [2]._\n\nKeywords: noncompliance,\n\n_objective evaluation, process,_\n\n_process description, process_\n\n_review, product review, qual­_\n\n_ity, quality assurance, quality_\n\n_management, work products_\n\n\nQUALITY ASSURANCE AND MEASUREMENT\n###### How to Conduct Process and Product Reviews Across Boundaries\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) conduct process\n\nand product reviews across boundaries in\n\nthe government systems acquisition and/or\n\noperational organizations. In this role, they assist\n\nboth government and contractor organiza­\n\ntions to document quality processes and work\n\nproduct specifications. To ensure adherence\n\nto documented processes and work product\n\nspecifications, MITRE SEs review government and\n\ncontractor quality processes and products and\n\ncontractor quality assurance programs; prioritize\n\nquality process improvement opportunities and\n\ncorrective actions; report to key decision mak­\n\ners the results of process and product reviews;\n\nand elevate high-priority corrective actions [3].\n\n\n-----\n\n###### Background\n\nWhen projects pay attention to the quality of their products and the processes that produce\nthem, they have a better chance of succeeding. When there is a government-to-contractor\nrelationship established through a contract, MITRE’s role needs to sharpen because the focus\nof our analysis and guidance then has two sides involved. We need to think through such\nsituations carefully to give customers our best advice. To do that, MITRE SEs should look to a\nwell-recognized common process framework like ISO/IEC 15288 [2] and established process\nimprovement models such as the Capability Maturity Model Integrated® (CMMI) [1] to ground\nour analysis and guidance and to get both sides on a firm process foundation.\n\nBoth the government and contractors need a common framework to improve communi­\ncation and cooperation among the parties that create, use, and manage modern systems so\nthey can work in an integrated, coherent fashion. ISO/IEC 15288 is an international standard\nthat provides this framework and covers the life cycle of human-made systems. This life\ncycle spans a system from idea conception to system retirement. ISO/IEC 15288 provides the\nprocesses for acquiring and supplying systems and for assessing and improving the life-cycle\nprocesses.\n\nISO/IEC 15288 defines outcomes that should result from the successful implementation of\na Quality Management Process:\n\n###### �Organization quality management policies and procedures are defined. �Organization quality objectives are defined. �Accountability and authority for quality management are defined. �The status of customer satisfaction is monitored. �Appropriate action is taken when quality objectives are not achieved.\nFurthermore, the standard defines certain activities and tasks an implementation is\nexpected to follow:\n\n###### �Plan quality management, which includes:\n\n     - Establishing quality management policies, standards, and procedures.\n\n     - Establishing organization quality management objectives based on business strategy\n\nfor customer satisfaction.\n\n     - Defining responsibilities and authority for implementation of quality management.\n###### �Assess quality management, which consists of:\n\n     - Assessing and reporting customer satisfaction.\n\n     - Conducting periodic reviews of project quality plans.\n\n     - Monitoring the status of quality improvements on products and services.\n###### �Perform quality management corrective action, which consists of:\n\n     - Planning corrective actions when quality management goals are not achieved.\n\n     - Implementing corrective actions and communicating results through the organization.\n\n\n-----\n\nAgain, both government and contractor processes should each perform, in some fash­\nion, these types of activities in their respective domains. The government has an additional\nresponsibility: to have insight into and oversight of the contractor’s quality management pro­\ncess to ensure it is in place, it is performing, and defects are identified and removed as a mat­\nter of daily practice. Otherwise, the government could be on the receiving end of poor-quality\nproducts that impact its commitments to its customers.\n\nBoth the government and contractors need to improve their processes. CMMI was\ndeveloped by a group of experts from industry, government, and the Software Engineering\nInstitute (SEI) at Carnegie Mellon University. It is a process improvement approach to software\nengineering and organizational development that provides organizations with the essential\nelements for effective process improvement to meet business goals. It can be used to guide\nprocess improvement across a project, a division, or an entire organization.\n\nCMMI’s Process and Product Quality Assurance Process Area supports the delivery of\nhigh-quality products and services by providing project staff and managers at all levels with\nappropriate visibility into, and feedback on, processes and associated work products through­\nout the life of the project. It establishes expectations that a project’s (or an organization’s)\nquality assurance process will objectively evaluate processes and work products so that when\nnon-conformance issues are identified, tracked, and communicated, resolution is ensured. For\nevery process, CMMI further establishes generic expectations, which include, for example, the\nestablishment of policy, plans, and monitoring. Having a common process improvement refer­\nence model gets both sides to think about process improvement in the same way, establishes a\ncommon framework and language, and promotes cooperation at any troublesome touch points\nbetween the two.\n\nFor related information, see the other articles in this Quality Assurance and Measurement\ntopic and those in the topic areas Continuous Process Improvement, Contractor Evaluation,\nand MITRE FFRDC Independent Assessments.\n\n###### Best Practices and Lessons Learned\n\n\nYes...quality is important for government work,\n\ntoo. Strive for the establishment of an indepen­\n\ndent, properly positioned quality management\n\nfunction on the government side. It needs to be\n\npositioned at a high enough level to have senior\n\nleadership’s attention and not be bullied by\n\nproject managers and midlevel management. To\n\n\nexpect quality assurance only on the contractor\n\nside is not good enough.\n\nUse standards and previous efforts in estab­\n\nlishing your quality capabilities. Don’t start from\n\nscratch in developing an organization’s quality\n\nmanagement process. Use a standard like ISO/IEC\n\n15288 for starters. Check with other government\n\norganizations and take a look at their quality office,\n\n\n-----\n\nand what policies, processes, and templates they\n\nuse. Stand on the shoulders of previous efforts\n\nand apply them to your situation.\n\nSet quality standards up front. Make sure there\n\nare organizational standards established for\n\nproject work process and products. It’s tough to\n\ncheck quality if you don’t have a standard to check\n\nagainst. Look to IEEE or your government depart­\n\nment or agency standards and tailor them for your\n\norganization. Check with the program’s prime\n\ncommercial contractor for examples, but remem­\n\nber the perspective from which those standards\n\nwere built...from the supplier side, so they’ll need\n\nto be adjusted.\n\nEnsure quality expectations are built into the\n\ncontracts/task orders. Build the expectation of\n\nquality into your contracts and/or task orders. If\n\nyou don’t state it, you’re not likely to get it. Expect\n\nproducts from the contractor will be checked\n\nagainst the agreed-on standard, and defects will\n\nbe identified and removed before the products\n\nare delivered. In the contract, make sure the gov­\n\nernment can periodically check defect records,\n\nprocess appraisal results, defect tracking data­\n\nbases, and process improvement plans to see if\n\nthey’re in place and actively being worked. Don’t\n\njust expect quality to magically appear from the\n\ncontractor. Many times it does not unless they\n\nknow the sponsor cares enough to be checking.\n\n###### References and Resources\n\n\nTrust...but verify. Once the contract arrange­\n\nment is up and running, the contractor’s quality\n\nmanagement function is operational, and trust is\n\nbeginning to solidify, the government may want to\n\nconsider only periodic reviews of contractor pro­\n\ncesses on a sampling basis. Objective evidence of\n\nactive reviews and defect resolution is key.\n\nHave a common strategy for improving the\n\nprocess. On both the government and contractor\n\nsides, there should be active process improve­\n\nment efforts in place that continuously look to\n\nmature the efficiency, effectiveness, and timeli­\n\nness of their quality assurance activities. It is\n\nhighly desirable that both sides agree on the same\n\nprocess improvement model, such as CMMI.\n\nBoth sides should have a common vocabulary\n\nand improvement and appraisal methods, which\n\npromote effective communication and collabora­\n\ntion opportunities to help speed performance\n\nimprovement initiatives.\n\nRemember the bottom line. MITRE SEs should\n\nactively encourage and promote quality manage­\n\nment processes and standards on both sides,\n\nproperly positioned in their management struc­\n\ntures, with a culture that encourages process\n\nimprovement to ultimately result in higher quality,\n\non-time, and useful systems.\n\n\n[1. CMMI Product Team, CMMI for Development, Ver. 1.2, CMU/SEI-2006-TR-008.](http://www.sei.cmu.edu/library/abstracts/reports/06tr008.cfm)\n\n[2. ISO/IEC 15288, IEEE Systems and Software Engineering—System Life Cycle Processes.](http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=43564)\n\n[3. The MITRE Corporation, “MITRE Systems Engineering (SE) Competency Model, Version](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n1,” September 1, 2007, p. 46.\n\n\n-----\n\n##### Continuous Process Improvement\n\nDefinition: A process is a set of steps to accomplish a defined purpose or pro­\n\n_duce a defined product or service. Continuous process improvement is the set of_\n\n_ongoing systems engineering and management activities used to select, tailor,_\n\n_implement, and assess the processes used to achieve an organization’s business_\n\n_goals. Continuous improvement is recognized as a component of modern quality_\n\n_management [1]._\n\nKeywords: continuous process improvement, plan-do-check-act cycle, process\n_based management, process improvement, process model, systems engineering_\n\n_processes_\n\n###### Context\n\nThe state of the art in system development management has evolved\n\nover the last few decades from basic concepts, practices, techniques,\n\nand tools borrowed from other disciplines to a relatively sophisticated\n\nsuite of training, guided experience, and performance evaluation using\n\nstructured collections of proven best practices. Experience has shown\n\nrepeatedly that careful planning, frequent, regular review by trained,\n\nqualified people, and meticulous control of product components as\n\nthey are developed, while not automatically sufficient by themselves, are\n\nnecessary to defining and fielding a complex product or system today.\n\nThe technology product and service industry as a whole has attempted\n\nnumerous times to define, document, and disseminate collections of\n\n\n-----\n\nsound practice and specifications of product quality. These have taken the form of standards,\nspecifications, methods, tools, books, and training and certification programs, among others.\n\n###### MITRE SE Roles and Expectations\n\nMITRE systems engineers (SEs) are expected to be able to collaborate with sponsors and\nclients to develop and influence the government’s approach to implementing and improving\nsystems engineering processes for the supported acquisition organization. They are expected\nto be able to draft policy, develop plans, and conduct maturity assessments for the technical\nand engineering processes. MITRE systems engineers are expected to be able to collaborate\nwith government and contractor organizations to implement, assess, and improve shared\nsystems engineering processes [1].\n\n###### A Four-Step Process\n\nDespite the ever changing, ever more sophisticated forms of delivery and media, success in\nmanaging the development and operation of complex technology-based systems is still based\non a well-executed “plan-do-check-act” cycle. It is founded on the quality control research of\nmathematician Dr. Walter A. Shewhart conducted in the United States during the 1940s, 50s,\nand 60s and broadened and elaborated by many others including, most notably, W. Edwards\nDeming [2, 3, 4, 5].\n\nSimply stated, the cycle is a four-step process used to control product quality during the\ndevelopment process. The steps are to: (1) Plan: determine what needs to be done, when, how,\nand by whom; (2) Do: carry out the plan, on a small scale first; (3) Check: analyze the results\nof carrying out the plan; and (4) Act: take appropriate steps to close the gap between planned\nand actual results. Then, repeat, starting at Step 1.\n\n“What needs to be done” is often expressed in the form of a process. Systems engineers\n(SEs) translate the concept of “small-scale first” into producing a prototype, a model, simu­\nlation, or mockup, or conducting a pilot project or trial run before producing the full-scale\nversion or initiating production. They build in regular review, measurement, and evaluation of\nthe resulting work products and the plans and processes used to build them. Then they act to\ntake corrective action as deviations from plans and expected results emerge—or as potential\ndeviation is predicted based on quantitative analysis of actual results against the background\nof prior experience.\n\n###### Process-based Management\n\nThis is process-based management. Using a systems engineering process-based approach,\nplanners, project managers, engineers, and other technical staff decompose the work of defin­\ning and building large, complex systems into more manageable, repeated cycles of these four\n\n\n-----\n\nsteps. Innovators and researchers are still looking for and proposing better approaches but, for\nnow, this is one of best we have found.\n\nProcesses may be thought of as generic templates for the components of specific plans.\nThey document the best way an organization knows how to do something. Mature organiza­\ntions manage and control them as they do other valuable tangible assets. Properly structured,\ndocumented processes clearly identify the work product or service to be produced or provided,\nalong with the inputs required, measurements that will be applied to determine compliance\nand quality, and any specific methods, tools, and training available. Entry and exit criteria\nindicate the conditions that prompt initiation of the process and those that help to determine\nwhen it is finished.\n\nSystems engineers select and sequence individual process descriptions to implement\nsystem development life-cycle models and corresponding work breakdown structures and to\norganize and tailor a technical approach to a particular project’s needs and circumstances.\nIf documented processes have been used, measured, and refined repeatedly—that is, if they\nhave been continuously improved—systems engineers and cost estimators should be able to\nascertain with some degree of confidence how long it will take to perform the processes again\nwith a given set of resources, requirements, and other constraints.\n\n###### Articles in This Topic\n\nThe article “Implementing and Improving Systems Engineering Processes for the Acquisition\nOrganization” provides guidance on commonly used systems engineering processes avail­\nable to assist MITRE SEs in developing organizational process policies and plans and con­\nducting maturity assessments. The article emphasizes that effective systems engineering\nefforts require both the government and contractor organizations to continuously mature and\nimprove processes.\n\nThe article “Matching Systems Engineering Process Improvement Frameworks/Solutions\nwith Customer Needs” provides guidance on working with the government to select and tailor\na process model appropriate to the task at hand. The article highlights two important process\nimprovement issues: working on the systems engineering/technology problem, and develop­\ning and executing a strategy to orchestrate associated organizational change.\n\nContinuous process improvement is closely associated with quality assurance and viewed\nby many as an aspect of it. For related information, see the SEG’s Quality Assurance and\nMeasurement topic.\n\n\n-----\n\n###### References and Resources\n\n[1. The MITRE Institute, September 1, 2007 “MITRE Systems Engineering (SE) Competency](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n[Model, Version 1,” pp. 47–48.](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n2. The Project Management Institute, 2008, A Guide to the Project Management Body of\n\n_Knowledge, (PMBOK Guide), 4th Ed., pp. 189–191._\n\n3. Kerzner, H., 2003, Project Management, 8th Ed., New York: John Wiley & Sons, Inc., pp.\n\n761–765.\n\n4. Shewhart, W. A., 1931, Economic Control of Quality of Manufactured Product, New York:\n\nD. Van Nostrand Company.\n\n5. Deming, W. E., August 2000, Out of the Crisis, Cambridge, MA: MIT Press.\n\n\n-----\n\nDefinition: Project manage­\n\n_ment and systems engineer­_\n\n_ing should be integrated into_\n\n_a seamless set of processes,_\n\n_plans, work products, reviews,_\n\n_and control events that are_\n\n_documented and continu­_\n\n_ously improved in an orderly,_\n\n_controlled manner, based on_\n\n_experience and lessons learned._\n\nKeywords: Capability Maturity\n\n_Model Integration (CMMI), con­_\n\n_tinuous process improvement,_\n\n_process, process improve­_\n\n_ment, process model, Standard_\n\n_CMMI Appraisal Method for_\n\n_Process Improvement (SCAMPI)_\n\n_appraisal, systems engineering_\n\n_processes_\n\n\nCONTINUOUS PROCESS IMPROVEMENT\n###### Implementing and Improving Systems Engineering Processes for the Acquisition Organization\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to support\n\nthe implementation and continuous improve­\n\nment of core and shared systems engineering\n\nprocesses. They develop process implementa­\n\ntion plans, including process improvement\n\ngoals, schedules, and estimated resources, and\n\nthey identify the need for, and often assist in,\n\nconducting process maturity assessments [1].\n\nThe MITRE SE’s role can vary from that of a\n\ntrusted advisor providing guidance to the govern­\n\nment on critical government and contractor work\n\nproducts, processes, methods, and tools to direct\n\ninvolvement in the development of strategies,\n\nplans, technical specifications, statements of\n\nwork, methods, tools, and commercial off-the\nshelf product evaluations and recommendations,\n\n\n-----\n\ncoaching, training, and decision support. The MITRE SE should assume a degree of owner­\nship for the effectiveness of the systems development process. The MITRE SE should assist the\ngovernment in achieving organizational performance goals, provide constructive feedback to\ndevelopment contractors in collaboration with the government, and help assure the quality,\nintegrity, and appropriateness of MITRE’s products and services.\n\n###### Background\n\nA process is a set of steps to accomplish a defined purpose or produce a defined product\nor service. The state-of-the-art technical aspects of systems development and management\nhave evolved over the past few decades from basic concepts, practices, techniques, and tools\nborrowed from other domains into a sophisticated, structured engineering discipline called\n“systems engineering.” Experience shows that supporting disciplined program and project\nmanagement with rigorously applied systems engineering is a steadfast approach to success­\nfully defining and managing the development and fielding of complex technology-based prod­\nucts and services. The most effective way to implement this strategy is by integrating project\nmanagement and systems engineering into a seamless set of processes, plans, work products,\nreviews, and control events that are documented and continuously improved in an orderly,\ncontrolled manner, based on experience and lessons learned.\n\nGovernment agencies typically obtain software-intensive systems, hardware, facilities,\nand operational support by issuing contracts for services from commercial contractors. The\ngovernment calls this approach “systems acquisition.” The government’s role is to define what\nit wants to acquire, how it will acquire it, and how to plan and manage the effort on behalf of\nits end-user organizations and operators. If commercial bidders hope to be awarded a contract,\nthey must demonstrate prior experience and success with the products and services sought by\nthe government as well as exhibit the required management, technical, and support services\nskills.\n\nThe government also has realized that it must have the capability to perform its role\neffectively and work in partnership with the selected contractor; the government sets the tone\nfor the partnership. What the acquisition organization does is just as important as what the\nsystem development contractor does. An immature, demanding, dysfunctional acquisition\nmanagement organization can render ineffective the practices and potential of a mature, highperforming contractor. When one or both parties perform inadequately, the entire develop­\nment process is impaired. Planning and documenting what each party will do, how each will\ndo it, and when each will do it is essential to success. Each party needs to keep activities and\nwork products up to date and synchronized. Plans and methods need to be refined as more is\nlearned about the nature and challenges inherent in the system or capability being built and\nits intended operating environment.\n\n\n-----\n\n###### Systems Engineering Process Improvement\n\nSystems engineering processes are documented in a variety of sources, including the\nInternational Council on Systems Engineering TP-2003-002-03.1 Systems Engineering\nHandbook, Institute of Electrical and Electronics Engineers (IEEE) Std. 15288-2008 Systems\nand Software Engineering—System Life Cycle Processes, and American National Standards\nInstitute (ANSI)/Energy Information Administration (EIA)-632-1999 Processes for Engineering\na System.\n\nMost of these references cite and implement project management, technical control,\nand systems engineering guidelines collected in the Carnegie Mellon Software Engineering\nInstitute’s CMMI for Development, Version 1.2, 2007. The CMMI is organized by vari­\nous categories: Process Areas, Maturity Levels, and formats. See the Process Areas in the\n“Engineering” category, primarily in Maturity Level 3 of the “staged” version of the model.\n\nCarnegie Mellon University’s IDEAL model (Initiating, Diagnosing, Establishing, Acting,\nand Learning) is a widely used method for project management and systems engineering pro­\ncess improvement. This model serves as a roadmap for initiating, planning, and implement­\ning improvement initiatives or projects. Process improvement professionals can apply it with\nthe CMMI and the SCAMPI. Note that the IDEAL model is a variation on Deming’s “Plan, Do,\nCheck, Act” cycle, which is discussed in the article “Continuous Process Improvement” [2].\n\nDepending on the need, several other prominent process improvement methods are\navailable. Examples include the International Standards Organization (ISO) 9000 Series, the\nInformation Technology Infrastructure Library, and the ISO/International Electro-technical\nCommission 15504 standard, also known as the Software Process Improvement and Capability\nDetermination method.\n\nSome government agencies have adopted “best practice” process models (e.g., Carnegie\nMellon Software Engineering Institute’s CMMI or various ANSI and IEEE standards) as\nguidelines for assessing a contractor’s system development capability and performance. Some\nagencies have gone further. They acknowledge that by adopting versions of these recognized\nframeworks and guidelines tailored to their respective roles and responsibilities, they can\ncontribute more to reducing the risks that are inherent in complex systems development and\ndeployment. In this type of operating environment, the MITRE SE can assess system design\nand development plans, processes, and actual activities against the requirements of the pro­\ncess model being used. The MITRE SE also can determine the level of compliance, identify\ngaps, and recommend remedial actions to the government and, indirectly, to development\ncontractors.\n\n\n-----\n\n###### Best Practices and Lessons Learned\n\nConsider adopting a de facto measurement\n\nstandard or benchmarking tool when your\n\norganization does not use a recognized process\n\nmodel. This best practice requires judgment. If\n\nyou tailor a de facto model, recommend incre­\n\nmental changes or additions to current practice\n\nthat are feasible without substantial impact on\n\nschedules and resources. Focus on recom­\n\nmendations that reduce specific acknowledged\n\nrisks or contribute to resolving or preventing the\n\nrecurrence of specific known issues. The process\n\nmodel should be used as a checklist and part of\n\nyour knowledge base, rather than as a binding\n\nstandard.\n\n###### References and Resources\n\n\nBase recommendations for process improve­\n\nment on a recognized process improvement\n\nframework. A structured improvement is most\n\neffective when based on a recognized process\n\nframework and on proven organizational change\n\nmanagement or organizational development\n\nmethods guided by trained, experienced orga­\n\nnizational change and process improvement\n\nprofessionals. See the article “Matching Systems\n\nEngineering Process Improvement Frameworks/\n\nSolutions with Customer Needs” for guidance on\n\nselecting an appropriate process model. Get help\n\nwith the intricacies of organizational change man­\n\nagement and process improvement if you or your\n\nteam have not already demonstrated mastery of\n\nthese methods and tools.\n\n\n[1. The MITRE Corporation, September 1, 2007, “MITRE Systems Engineering (SE)](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n[Competency Model, Version 1,” pp. 48-49, accessed February 10, 2010.](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n2. Deming, W. E., August 2000, Out of the Crisis, MIT Press, Cambridge, MA.\n\n###### Additional References and Resources\n\nChrissis, M. B, M. Konrad, and S. Schrum, 2007, CMMI, Second Edition, Guidelines for Process\n_Integration and Product Improvement, The SEI Series in Software Engineering, Boston, MA:_\nPearson Education, Inc., Addison- Wesley.\n\nChrissis, M. B., M. Konrad, and S. Schrum, 2003, CMMI: Guidelines for Process Integration and\n_Product Improvement, Boston, MA: Pearson Education, Inc., Addison-Wesley._\n\nMcFeeley, R., February 1996, IDEAL: A Users Guide for Software Process Improvement\n_Handbook, CMU/SEI-96-HB-001, Pittsburgh, PA: Software Engineering Institute, Carnegie_\nMellon University.\n\nShewhart, W. A., 1931, Economic Control of Quality of Manufactured Product, New York: D.\nVan Nostrand Company.\n\n\n-----\n\nDefinition: Frameworks that\n\n_enable systems engineering_\n\n_process improvement provide_\n\n_a basic conceptual structure to_\n\n_solve or address complex issues_\n\n_by designing, establishing,_\n\n_refining, and forcing adher­_\n\n_ence to a consistent design_\n\n_approach [1]._\n\nKeywords: business perfor­\n\n_mance model, improving effi­_\n\n_ciency, organizational maturity,_\n\n_process-driven management,_\n\n_process improvement, quality_\n\n_management, systems engi­_\n\n_neering best practice_\n\n\nCONTINUOUS PROCESS IMPROVEMENT\n###### Matching Systems Engineering Process Improvement Frameworks/ Solutions with Customer Needs\n\n**MITRE SE Roles and Expectations: MITRE**\n\nsystems engineers (SEs) are expected to col­\n\nlaborate with government and contractor organi­\n\nzations to select and tailor systems engineering\n\nprocess improvement models—e.g., Software\n\nProcess Improvement and Capability Determi­\n\nnation (SPICE), Software Engineering Institute\n\n(SEI) Ideal, Capability Maturity Model Integrated\n\n(CMMI), Lean Six Sigma, etc. These are used\n\nto modify, integrate, test, baseline, deploy, and\n\nmaintain systems engineering processes for the\n\ngovernment acquisition and/or contractor organi­\n\nzations [2]. SEs should be aware of the spectrum\n\nof choices for continuous process improvement\n\n(CPI) efforts, be able to form recommenda­\n\ntions about them, and assist in implementing a\n\nselected approach within their work environment.\n\n\n-----\n\n###### Background\n\nEach process improvement framework brings its own set of standards and strengths to\nsatisfy customer needs. Some, such as CMMI, Control Objectives for Information and related\nTechnology (COBIT), and Information Technology Infrastructure Library (ITIL) come from\na set of best practices. Others, such as Lean Six Sigma, consist of strategies or tools to iden­\ntify weaknesses and potential solutions. Because systems engineering process improve­\nment frameworks overlap, more than one framework may match the customer needs for a\nCPI effort. These frameworks are available for any effort; there are no exclusivity rights. For\nexample, the ITIL characterizes itself as “good practice” in IT service management. The SEI\nhas a CMMI for services [3].\n\nThere are standard sets of processes that provide templates and examples of key processes\nsuch as IEEE’s ISO/IEC 15288 [4]. The Six Sigma, Lean Manufacturing, Lean Six Sigma family\nof frameworks each contain tools to assess and improve processes, and are currently in use in\ngovernment organizations.\n\nCMMI has gained a great deal of popularity over the past few years. The Government\nAccountability Office (GAO) has been basing its oversight reviews on this framework and\nthe results are flowing back to the departments and agencies with references to CMMI best\npractices. As a result, some in government are taking the view that an organization aligned\nwith CMMI best practices and certified for its software development processes’ level of matu­\nrity at 2 or 3 will receive greater approval from the GAO and other oversight groups. This has\npromoted CMMI’s influence.\n\nLean Six Sigma is growing in popularity. It represents the joining of Six Sigma and Lean\nManufacturing. Six Sigma was heavily touted a few years ago in government and industry\nand is still used in some sectors because of the methodology’s success in eliminating defects.\nHowever, the downside was that it took too long and was too expensive. Lean Six Sigma, as\nthe name implies, is faster to complete and requires fewer resources. The combination of Six\nSigma and Lean tools and techniques is more effective and efficient and contains a richer\n­solution set.\n\nSelecting a framework may be based on factors that do not relate to the problem being\naddressed. Popularity of the framework can come into play. The background and experience\nof the individual leading the CPI effort can influence the approach. The customer may have\nsome predetermined ideas as well.\n\nMatching a process improvement framework/solution to the customer needs involves two\nissues: working on the systems engineering/technology problem, and developing and execut­\ning a strategy to orchestrate any associated organizational change. Any solution will require\nsome members of the organization to perform their duties differently. Continuous process\n\n\n-----\n\nimprovement often has a long-term, ongoing impact, as the processes are refined. Individuals\nin an organization executing CPI need to be comfortable with the change and embrace it.\n\nSome of the frameworks are concerned with “what” should be done and others focus on\n“how” it should be done. Frameworks such as CMMI are in the “what” category. For example,\nCMMI indicates the need for a requirements development process area. It involves establish­\ning customer and product requirements and analysis and validation. However, it does not\nprescribe elements such as what approach to use, the process model, or what rules to follow.\nFrameworks such as Six Sigma are in the “how” category. Six Sigma suggests appropriate\ntools to arrive at the right solution. Examples for requirements development include house of\nquality, voice of the customer, and affinity diagrams tools.\n\nThere is a high percentage of failure or slow progress in CPI efforts. In a 2007 quarterly\nreport, SEI reported that it takes on average 20 months to attain CMMI Level 2 and 19 months\nto attain CMMI Level 3. The variation of time to attain Level 2 and Level 3 is large. [5] Many\norganizations fail on their first attempt and have to restart before they are successful. This is a\nconsistent pattern with CMMI implementation. If there is frequent change in leadership at the\ntop of the organization, the risk for completion is higher because new leadership often brings\nnew direction.\n\n###### Best Practices and Lessons Learned\n\n\nConsider the path of least resistance. Be a prag­\n\nmatist but do not give up on principles. There are\n\nmany ways to meet customer needs. If you have\n\nbuilt a trusting relationship, you can guide the way\n\ncustomer needs are met through the appropriate\n\napproach. When more than one framework will do\n\nthe job, do not get hung up on which framework is\n\n“best” (e.g., ITIL [6] versus CMMI for Services [7]).\n\nIf there is a positive history of ITIL in the organiza­\n\ntion and it fills the need compared to a CMMI for\n\nservices solution, evaluate whether the benefits\n\nyou might gain outweigh the costs and difficulty\n\nof making the shift. When you are assessing\n\nalternative approaches, when the more difficult\n\npath is the only way to accomplish the client goals\n\ncompletely, then advise the client accordingly\n\n\nand include a clear and frank discussion of the\n\ndifficulties.\n\nCritically consider the customer’s true need.\n\nBeware of falling into the trap of investing time\n\ndefining a complete program-level set of policies,\n\ncharters, data repositories, metrics, processes,\n\nprocedures, etc., if the customer really only needs\n\nprocesses specific to each project and has no\n\ndesire to be certified at any level.\n\nOrganizational change is the difficult step.\n\nImplementing CPI normally involves a very dif­\n\nferent way of doing business in a continuously\n\nchanging environment. CPI may not be universally\n\nviewed as an improvement by those affected by\n\nthe change. Consider bringing in an organizational\n\nchange expert.\n\n\n-----\n\nCombine approaches. There may be strength in a\n\ncombined or hybrid CPI approach. As an example,\n\nthe CMMI framework is built on best practices\n\nfrom a variety of government and industry sources\n\nthat do a very good job of explaining “what” to do,\n\nbut do not provide guidance on “how” to do it.\n\nFor example, for a certain level of maturity, CMMI\n\nrequires a requirements development process.\n\nHowever, it does not define how to do that pro­\n\ncess. If you are looking for predefined processes,\n\nconsider ISO 9000. [8] If you are looking to create\n\nyour own, consider Lean Six Sigma and tools like\n\nvoice of the customer, affinity diagrams, or house\n\nof quality.\n\nGain and use upper management support. Elicit\n\nand gain upper management support to settle on\n\nthe right framework/solution for the organization\n\n###### Conclusion\n\n\nbefore attempting to implement it. This is crucial\n\nregardless of which framework is selected. Use a\n\ntop-down strategy to promote the CPI program.\n\nA bottom-up approach alone rarely results in a\n\nsuccessful outcome. Even if it is successful, the\n\nproject will usually take much longer.\n\nAvoid labeling processes as new. Embed the\n\nprocess improvement effort into the normal way\n\nthe organization conducts business. Avoid call­\n\ning attention to it by using the framework name.\n\nMake the process part of refining the organiza­\n\ntion’s customary system development life cycle.\n\nOtherwise, you risk creating the assumption that\n\nworkloads will be enlarged. Compliance is more\n\nlikely when those affected understand the change\n\nis merely a refinement to the work they are already\n\ndoing.\n\n\nThere are many reasons why an organization may not gain traction when adopting a CPI\nprogram. When an implementation falters, do not automatically assume it is due to the chosen\nframework or approach. Ask yourself whether there really is a compelling reason for CPI or if\nthere is a lack of engagement or buy-in by the customer. If so, it may be better to defer until\nanother time.\n\n###### References and Resources\n\n[1. Statemaster.com Encyclopedia, accessed February 12, 2010.](http://www.statemaster.com/encyclopedia/Framework)\n\n[2. The MITRE Corporation, “MITRE Systems Engineering (SE) Competency Model, Section](http://www.mitre.org/work/systems_engineering/guide/competency_model.html)\n\n3.8.”\n\n[3. CMMI Product Team, August 2006, CMMI® for Development, Version 1.2, Carnegie Mellon](http://www.sei.cmu.edu/reports/06tr008.pdf)\n\nSoftware Engineering Institute.\n\n[4. ISO/IEC 15288 IEEE, February 1, 2008, Systems and software engineering—System life](http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=43564)\n\n[cycle processes, 2nd ed..](http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=43564)\n\n[5. Software Engineering Institute (SEI), 2006, SEI Annual Report, Carnegie Mellon.](http://www.sei.cmu.edu/library/abstracts/annualreports/2006_SEI_AR.cfm)\n\n\n-----\n\n6. Bajada, S., February 2010, ITIL v3 Foundations Certification Training. This book and foun­\n\ndations courses for the Information Technology Instruction Library (ITIL) are available\nthrough The MITRE Institute’s SkillPort e-Learning site.\n\n7. [CMMI Product Team, February 2009, CMMI® for Services: Improving processes for better](http://www.sei.cmu.edu/reports/09tr001.pdf)\n[services, Version 1.2, Carnegie Mellon Software Engineering Institute.](http://www.sei.cmu.edu/reports/09tr001.pdf)\n\n[8. International Standards Organization (ISO), January 2009, ISO 9000.](http://www.iso.org/iso/iso_9000_selection_and_use-2009.pdf)\n\n\n-----\n\n###### Index\n\n A\n\nABC alternatives, 512\naccreditation, 463\nacquisition\n\nagile, 563–567\n“big bang”, 572\nevolutionary, 568–571, 574\nlife-cycle transition activities, 435\nperformance-based, 494\nacquisition life cycle, 544–545\nacquisition metrics, 502–508\nacquisition program planning, 491–542\nactual cost of work performed (ACWP),\n\n586–588\nAdvanced Cyber Threat (ACT), 168\nAdvanced Persistent Threat (APT), 177\nAdvisory Multi-step Process, 548–550\naffordability, efficiency, and effectiveness\n\n(AEE), 470–482\nagile acquisition, 563–567\nAgile Capability Mashup Environment\n\n(ACME), 132\nagility, 39–40\nanalysis, 32–33\nanalysis of alternatives (AOA), 281, 477, 493,\n\n496–501, 522, 537, 538\narchitecture, 569\n\ndevelopment, 334–340\nenterprise, 117\nfederated, 118\nfederated enterprise, 119\nresilient, 159, 163\n\n\narchitecture development approaches,\n\n334–340\nArchitecture Development Method (ADM)\n\nphases, 336\narchitecture framework, 327–333\n\ndetermining the right one, 329\nmodels and views, 328, 331–332\narchitectures federation, 116–123\nassessment\n\ncyber threat susceptibility, 175–183\nengineering risk, 683\nindependent, 257, 260–267, 683\noperational needs, 279–283\norganizational, 209–214\norganizational impact, 213\norganizational risk, 214\nprivacy impact, 149\nstakeholder, 220–224\navailability, 665\n\n###### B\n\nbaselines, 42, 89, 317, 361, 375, 498, 649–652\n“big-bang” acquisition, 561, 572–578\nbudgeted cost of work performed (BCWP),\n\n586–588\nbudgeted cost of work scheduled (BCWS),\n\n586–588\nBurke-Litwin Model of Organizational\n\nPerformance and Change, 210–212,\n216–217\nbusiness case analysis (BCA), 477, 537,\n\n540–541\nbusiness systems modernization (BSM), 449\n\n###### C\n\ncapability\n\nevolution, 614\n\n\n-----\n\nscope, 308\ncapability development document (CDD),\n\n281\nCapability Maturity Model Integration\n\n(CMMI), 258, 534, 677, 687, 688\ncapability portfolio management (CPM), 66\ncertification and accreditation (C&A),\n\n425–437\nprocess, 426\nchange management, 202\nchange process, 203–204\ncloud computing, 82, 88\ncode review, 192\nCommon Attack Pattern Enumeration and\n\nClassification (CAPEC), 177\nCommon Vulnerabilities and Exposures\n\n(CVE), 177\nCommon Weakness Enumeration, 194\nCommon Weakness Enumeration (CWE),\n\n177\ncommunication, 207, 217, 225–231, 384, 527,\n\n549, 566\nactivities, 228\nplan, 228\nroadmap, 229\ncommunities of interest (COIs), 242–247\ncommunities of practice (COPs), 242–247\ncompetitive prototyping (CP), 577, 593–603\ncomplexity, 36–45, 331, 646, 652\ncomplexity management, 343\ncompliance\n\nrole in federation, 120\nComposable Capabilities On Demand\n\n(CCOD), 130–136\ncomprehensive viewpoint, 27–30, 46–56\nconcept development, 275–278, 296\n\nfour activities of, 276\n\n\nconcept of operations (CONOPS), 284–289,\n\n290, 297, 320–321\ncritical components of, 287\nguidelines, 286\nin systems engineering applications,\n\n287\nobjectives, 286\nconfiguration\n\naudits, 645\ncontrol, 645\nidentification, 644\nmanagement checklist, 646\nmanagement (CM), 643–648\nmanagement plan, 644\nmanagement tools, 653–663\nstatus accounting, 645\ncontinuous process improvement, 690–702\nContract Data Requirements Listings\n\n(CDRLs), 423\ncontractor evaluation, 548–550, 576–578,\n\n579–584\nControl Objectives for Information and\n\nrelated Technology (COBIT), 93\ncost benefit analysis (CBA), 477, 537, 541\ncost estimation\n\nthree Rs of, 520–521\ncost performance index (CPI), 587–588\nCritical Asset Identification Process (CAIP),\n\n171\nCrown Jewels Analysis (CJA), 167–174, 185\ncyber asset failure, 170\ncyber mission assurance, 159–166\n\nengineering practices, 164\nCyber Mission Impact Assessment (CMIA),\n\n170\nCyber Risk Remediation Analysis (RRA),\n\n184–191\n\n\n-----\n\ncyber threat, 159\n\nthree levels of, 161\ncyber threat susceptibility analysis, 176\ncyber threat susceptibility assessment,\n\n175–183\n\n###### D\n\ndata, 83, 98–100\n\narchitecture, 101\nexchange, 100\nmining, 100\nrights to technical, 662\ntechnical, 661–663\nwarehouses, 100\ndatabase administration, 100\ndatabase management system (DBMS), 97,\n\n101–102\nDefense Critical Infrastructure Protection\n\n(DCIP) Program, 171\nDefense Information Assurance Certification\n\nand Accreditation (DIACAP) Process,\n178\nDepartment of Defense Architecture\n\nFramework, 325\ndesign depth, 365–367\ndesign patterns, 114, 124–129\ndoctrine, organization, training, logistics,\n\npersonnel, facilities (DOTLPF), 280–281,\n335, 478\nDoD Information Assurance Certification\n\nand Accreditation Process (DIACAP),\n427\nartifacts, 427–428\ninformation assurance controls, 429\nrobustness, 429\n\n\n###### E\n\nearned value management, 525, 577,\n\n585–592, 590–591, 678\neconomic analysis (EA), 537, 541\necosystem, 37–38\nenergy efficiency, 670–680\nengineering risk assessments (ERAs), 683\nenterprise\n\narchitectures (EAs), 117\ndefinition, 8\nengineering, 21\nplanning and management, 54–81\nprinciples, 50\nsystems engineering, 8–9\nenterprise governance, 238–256\nevolutionary acquisition, 568–571, 574\nexperimentation, 319–326\n\nthree stages of, 321\n\n###### F\n\nFair Information Practices (FIPs), 148–152\nFederal Acquisition Regulation (FAR), 11,\n\n258, 492, 544, 559, 560–562, 682\nFederal Enterprise Architecture (FEA), 50,\n\n534\nfederally funded research and development\n\ncenters (FFRDC), 11–17, 28, 257–259, 577\nfederated architecture, 118\nfederated enterprise, 51\nflow\n\ninformation, 40\nrequirements, 371\n\n###### G\n\n_Gang of Four, 125–126_\ngovernance, 246, 410\n\n\n-----\n\nenterprise, 58\ninformation technology (IT), 57–64\nGovernment Performance and Results Act\n\n(GPRA), 76\n\n###### H\n\nhigh-level conceptual definition (HLCD),\n\n296–303, 299\nprocess, 297–298\nholistic perspective, 492\n\n###### I\n\nindependent assessment, 257–259, 683\nIndependent verification and validation\n\n(IV&V), 464\nInformation and Communications\n\nTechnology (ICT), 396\ninformation and data management (IDM),\n\n97–107\ninformation assurance (IA), 425\n\ncharacteristics, 430\ncontrols, 429\ninformation security, 615\ninitial capabilities document (ICD), 281, 406\nintegrated logistics support, 658–674\nintegrated master plan (IMP), 524–527\nintegrated master schedule (IMS), 70,\n\n524–527\nintegration\n\nhorizontal, 378\ntesting approaches, 390–395\nvertical, 378\nintegration and interoperability (I&I),\n\n381–385, 387, 615\ncomplexities, 383\ngaps, 385\nsolution strategies, 386–389\n\n\nsystems engineering responsibilities,\n\n382\ninterface management, 396–404\n\nin an enterprise engineering service\noriented environment, 397–398\ninterfaces, 317, 355, 384, 396\nInternational Telecommunication Union\n\n(ITU), 109\ninteroperability\n\nsemantic, 387\nsyntactic, 387\ninvestment analyses, 536–547\nIT governance, 57–64, 88\nIT infrastructure engineering, 86–91\nIT Infrastructure Library (ITIL), 87, 93–95,\n\n534\nIT investment management (ITIM), 60\nIT service management, 87, 92–96\n\n###### K\n\nkey performance parameters (KPPs), 615,\n\n667\nknowledge management, 218\n\n###### L\n\nleadership, 250\nlevel-of-effort (LOE) method, 591\nlife-cycle cost estimation (LCCE), 518–523\nlogic model, 76–81\nloose coupling, 126–127, 131, 134, 322, 411\n\n###### M\n\nmaintainability, 665, 668\nmeasurement, 678\nmeasurement capability, 75–81\nmetadata, 106\nmetrics, 411\n\n\n-----\n\nacquisition metrics, 503\nEarned Value Management (EVM), 504\nsystems engineering–specific, 505\nmilestone reviews, 579–584\nmission assurance, 155–200\n\ncyber, 159–166\nengineering, 168\nMission Assurance Engineering (MAE), 168,\n\n175, 184\nmission threads, 438–444\nMITRE systems engineering, 11–17\nMITRE Value Impact Assessment, 49–51\nmobile IT, 91\nmodeling and simulation (M&S), 41, 403,\n\n409, 462, 529\nmodeling techniques, 338\nMonte Carlo simulation, 626\n\n###### N\n\nNational Infrastructure Protection Plan\n\n(NIPP), 170–171\nNational Telecommunications and\n\nInformation Administration (NTIA), 109\nnon-competitive procurement, 554\n\n###### O\n\nObject-oriented with Unified Modeling\n\nLanguage, 335\nOffice of Management and Budget (OMB),\n\n50, 60–61, 285, 446, 595\nopen source software (OSS), 137–146\noperational data, 100\noperational needs assessment, 279–283\noperational needs statement\n\nattributes of, 281–282\noperational requirements, 290–295\n\nchallenges, 292\n\n\ndocumentation, 293\nprocess, 291\nOperational Test and Evaluation (OT&E),\n\n423\noperations and maintenance (O&M),\n\n433–435\norganizational assessments, 209–214\norganizational change, 201–208\n\nprocess, 203\norganizational change management (OCM),\n\n202, 234\nOrganizational Impact Assessment (OIA),\n\n213\nOrganizational Risk Assessment (ORA), 214\norganizational transformation, 215–219\norganizational transition plan, 206\n\n###### P\n\npatterns\n\napplication, 344\narchitectural, 341–350\ndimensions of effective use, 345\nexpression, 343\npeer reviews, 416–418\nPerformance Assessment Rating Tool\n\n(PART), 76\nperformance engineering, 528–535\nperformance measurement, 76\nPOET, 47–49, 84, 671\npolicy analysis, 252–259\nPortfolio Analysis Machine (PALMA), 68\nportfolio management (PfM), 65–74\n\ncapability, 66\ninformation technology (IT), 66\nPost-Implementation Review (PIR, 445–450\nPrivacy Act of 1974, 149\nprivacy impact assessments (PIAs), 149\n\n\n-----\n\nprivacy systems engineering, 137–146\nProbability of Program Success (PoPS), 503\nprocess metrics, 678\nprocess reviews, 686–693\nproduct reviews, 686–693\nprogram acquisition strategy, 559–562\nproject development methodologies\n\nagile, 306\nspiral, 306\nwaterfall, 306\nprototypes, 403\nprototyping, 319–326\n\n###### Q\n\nquality, 375, 566, 675\n\nMITRE’s guidelines and standards for,\n\n682\nperspectives, 676\nstandards and guidance, 677\nquality assurance, 254, 675–689\n\nprogram, 681–685\nvs. quality control, 676\nquality control, 676\nquality management process, 687, 688\nquick reaction capabilities (QRCs), 407\n\n###### R\n\nradio frequency spectrum management,\n\n108–115\nreliability, 665, 668\nreliability, availability, maintainability\n\n(RAM), 664–669\nrequest for proposal (RFP), 235, 288, 363,\n\n543, 550, 551–562, 667\nrequirements\n\nanalyze and define, 314\nbaseline, 375\n\n\ncollecting and evaluating, 305\ncreep, 293, 375\ndocument, 310\nelicit, collect, and develop, 304–313\nmodel, 311\nperformance, 356\nprioritize, 311\nstatement, 315, 355\nsystem, 370–380\nsystem-level, 351–360\ntraceability, 349, 376\nrequirements analysis, 317\n\nmeasures associated with, 316\nrequirements engineering, 301–323\nRequirements Traceability Matrix, 373\nresilience, 163\nresistance, 206, 230, 236\nRESTful architectures, 134\nrisk, 458\n\nanalysis, 500\nidentification, 612–619, 616, 617, 618\nimpact assessment, 601, 620–626, 621\nmonitoring, 632\nprioritization, 601, 620, 622, 625\nreduction, 533\nstatement, 616, 618\ntaking, 631\nrisk-informed trade-offs, 599\nrisk management, 263, 599–642\n\napproach and plan, 604–611\nassessment, 623, 624\nin enterprise engineering programs,\n\n607\nin system-level programs, 605\nin system-of-systems programs, 606\nplan, 608\nprinciples, 601\n\n\n-----\n\nstarting requirements, 607\ntools, 601, 634–648\nRisk Management Toolkit, 638\nRisk Matrix, 638\nrisk mitigation, 601, 627\n\nplanning, 630, 631\nstrategies, 628\nRiskNav®, 621, 626, 636–637\nrisk remediation analysis (RRA), 185\nRisk-to-Mission Assessment Process\n\n(RiskMAP), 169\n\n###### S\n\nschedule performance index (SPI), 587–588\nsecure code review, 192–196\nservice-level agreement (SLA), 89, 399\nservice-oriented architecture (SOA), 372,\n\n409\nSimulation Conceptual Model, 463\nSoftware as a Service (SaaS), 88\nsource selection, 543–558, 549, 551–562, 591\nSpewak architecture process, 335\nstakeholder\n\nanalysis, 51\nanalysis process, 49\nassessment, 220–224\nengagement, 67\nmanagement, 220–224\nstakeholders, 67, 84, 220–224, 246, 300, 308,\n\n311, 322, 358, 498, 527, 596–597, 618, 652,\n667, 673\nstandards boards and bodies, 248–251, 534\nStep Architecture Process, 335\nstrategy\n\ncertification and accreditation (C&A),\n\n425–437\ntest and evaluation, 405–412\n\n\nsubsystems, 384\nsupply chain, 660\nsupply chain risk management (SCRM),\n\n197–208\nsynthesis, 32–33\nsystem architecture, 324–346\nsystem design, 347–377\n\nassessment, 376\ndepth, 365\nsystem-level requirements, 351–360\n\nchecklist, 354\ndevelopment of, 353\nlevel of detail, 357\nsystem requirements, 347, 370–380\nsystems engineering\n\ndefinition, 1\nfor mission assurance, 155–158\nlife cycle, 1–3, 147, 150, 270, 420\nprivacy, 137–146\nstrategies for uncertainty and complex­\n\nity, 36\nSystems Engineering (SE) Profiler, 49–51\nsystems integration, 378–401\nsystems-of-systems\n\ndefinition and examples, 3–5, 392\nperformance, 457\ntest and evaluation, 451–460\ntesting, 393\nvs. systems, 5–6\nsystems thinking, 29, 31–35, 560\n\n###### T\n\ntactics, techniques, and procedures (TTPS),\n\n176\nTEAPOT, 48–51, 671\ntechnical maturity, 509–513, 614\n\n\n-----\n\ntechnical performance measurement (TPM),\n\n678, 684\ntechnology hype cycle, 510\ntechnology planning, 514\ntechnology transition testing, 90\nT&E Master Plan, 423\ntest and evaluation (T&E), 402–432, 420\n\nphases, 415\nplans and procedures, 413\npurpose, 406\nsystem-of-systems, 451–460\ntesting, 414–418\ntest readiness review (TRR), 417\nThe Open Group Architecture Framework\n\n(TOGAF), 335\nthreat susceptibility analysis, 175\nThreat Susceptibility Matrix, 180–181\ntools, 29, 46–56\n\nanalysis, 47, 67\ncontrol, 69\nevaluation, 70\nexchange, transform, and load (ETL),\n\n100\nselection, 69\ntop-level system design, 361\n\nprocess, 364\ntraceability, 371, 374\ntrade-off analysis, 531, 533\ntransformation planning, 201–208\ntransition\n\nactivities, 435\nprocess, 434\nstrategies, 433\ntrust, 245, 294, 610\n\n###### U\n\nuncertainty, 36–45, 255, 301, 319\n\n\nunit tests, 416\nusability engineering, 233–235\nUS Department of Defense Architecture\n\nFramework (DoDAF), 335\nuse cases, 438–444\nuser adoption, 232–241\n\n###### V\n\nvalidation, 419–424, 463\nverification, 374, 419–424, 463\nverification and validation (V&V), 135,\n\n419–424\nof simulation models, 461–469\nverification, validation, and accreditation\n\n(VV&A)\nplan, 464\nV-model, 2, 271, 402–404\n\n###### W\n\nwork breakdown structure (WBS), 519, 553,\n\n556\nWorld Radiocommunication Conference\n\n(WRC), 109\n\n###### Z\n\nZachman framework, 325, 335\n\n\n-----\n\n-----\n\na wide range of systems engineering subjects—ideal for understanding the essentials of the discipline\n\nand for translating this guidance into practice in your own work environment. The online version of\n\nMITRE Systems Engineering Guide (on www.mitre.org) has been viewed by hundreds of thousands\n\nof people around the world. Here is what readers are saying:\n\n_The MITRE Corporation is_ “\u0007This guide should help the entire systems\n\n_a not-for-profit organization_ engineering community significantly.”\n\n_that operates federally funded_\n\n###### “\u0007This is simply excellent—it is a fantastic\n\n_research and development_\n\n###### step forward to empowering me and others\n\n_centers (FFRDCs). FFRDCs_\n\n###### like me ‘in the field.’ Well done!”\n\n_are unique organizations that_\n\n_assist the U.S. government with_ “\u0007The Systems Engineering Guide fills an important\n\n_scientific research and analysis,_ niche for systems engineering practitioners.”\n\n_development and acquisition,_\n\n_and systems engineering and_ “\u0007It is obvious that MITRE has put a significant\n\n###### amount of effort into the guide, and it is a valuable\n\n_integration. We’re proud to have_\n\n###### contribution to the systems engineering community.”\n\n_served the public interest for_\n\n_more than 50 years._\n\n###### “\u0007I will use the Systems Engineering Guide\n\n as a resource in teaching and research.”\n\nThe MITRE Corporation Covering more than 100 subjects, the guide’s articles are written by MITRE\n\n202 Burlington Road systems engineering practitioners with substantial experience in particular\nBedford, MA 01730-1420\n\nsubject areas. Each article identifies real-world problems that commonly\n\n(781) 271-2000\n\noccur in engineering systems and provides best practices for avoiding\n\n7515 Colshire Drive\n\nand mitigating them.\n\nMcLean, VA 22102-7539\n(703) 983-6000\n\nsegteam@mitre.org\nwww.mitre.org\n\n\n-----",
    "language": "EN",
    "sources": [
        {
            "id": "99fdc3ef-333d-48f5-a4a1-becd788c7b80",
            "created_at": "2022-10-25T15:28:29.802983Z",
            "updated_at": "2022-10-25T15:28:29.802983Z",
            "deleted_at": null,
            "name": "MITRE",
            "url": "https://github.com/mitre-attack/attack-stix-data",
            "description": "MITRE ATT&CK STIX Data",
            "reports": null
        }
    ],
    "references": [
        "https://www.mitre.org/sites/default/files/publications/se-guide-book-interactive.pdf"
    ],
    "report_names": [
        "se-guide-book-interactive.pdf"
    ],
    "threat_actors": [
        {
            "id": "f7d9b02d-d294-422b-adf7-4b3adfac9d9a",
            "created_at": "2022-10-25T16:07:23.392241Z",
            "updated_at": "2025-03-27T02:02:09.775925Z",
            "deleted_at": null,
            "main_name": "The Big Bang",
            "aliases": [],
            "source_name": "ETDA:The Big Bang",
            "tools": [
                "Micropsia"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "d90307b6-14a9-4d0b-9156-89e453d6eb13",
            "created_at": "2022-10-25T16:07:23.773944Z",
            "updated_at": "2025-03-27T02:02:09.974695Z",
            "deleted_at": null,
            "main_name": "Lead",
            "aliases": [
                "Casper",
                "TG-3279"
            ],
            "source_name": "ETDA:Lead",
            "tools": [
                "Agentemis",
                "BleDoor",
                "Cobalt Strike",
                "CobaltStrike",
                "RbDoor",
                "RibDoor",
                "Winnti",
                "cobeacon"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "77b28afd-8187-4917-a453-1d5a279cb5e4",
            "created_at": "2022-10-25T15:50:23.768278Z",
            "updated_at": "2025-03-27T02:00:55.5423Z",
            "deleted_at": null,
            "main_name": "Inception",
            "aliases": [
                "Inception Framework",
                "Cloud Atlas"
            ],
            "source_name": "MITRE:Inception",
            "tools": [
                "PowerShower",
                "VBShower",
                "LaZagne"
            ],
            "source_id": "MITRE",
            "reports": null
        },
        {
            "id": "33f527a5-a5da-496a-a48c-7807cc858c3e",
            "created_at": "2022-10-25T15:50:23.803657Z",
            "updated_at": "2025-03-27T02:00:55.550132Z",
            "deleted_at": null,
            "main_name": "PLATINUM",
            "aliases": [
                "PLATINUM"
            ],
            "source_name": "MITRE:PLATINUM",
            "tools": [
                "JPIN",
                "Dipsind",
                "adbupd"
            ],
            "source_id": "MITRE",
            "reports": null
        },
        {
            "id": "dfee8b2e-d6b9-4143-a0d9-ca39396dd3bf",
            "created_at": "2022-10-25T16:07:24.467088Z",
            "updated_at": "2025-03-27T02:02:10.241387Z",
            "deleted_at": null,
            "main_name": "Circles",
            "aliases": [],
            "source_name": "ETDA:Circles",
            "tools": [],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "aa73cd6a-868c-4ae4-a5b2-7cb2c5ad1e9d",
            "created_at": "2022-10-25T16:07:24.139848Z",
            "updated_at": "2025-03-27T02:02:10.120505Z",
            "deleted_at": null,
            "main_name": "Safe",
            "aliases": [],
            "source_name": "ETDA:Safe",
            "tools": [
                "DebugView",
                "LZ77",
                "OpenDoc",
                "SafeDisk",
                "TypeConfig",
                "UPXShell",
                "UsbDoc",
                "UsbExe"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "9198aefa-3da6-4605-bb52-923df20a7fce",
            "created_at": "2023-01-06T13:46:38.766848Z",
            "updated_at": "2025-03-27T02:00:02.913269Z",
            "deleted_at": null,
            "main_name": "The Big Bang",
            "aliases": [],
            "source_name": "MISPGALAXY:The Big Bang",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        },
        {
            "id": "7d8ef10e-1d7b-49a0-ab6e-f1dae465a1a4",
            "created_at": "2023-01-06T13:46:38.595679Z",
            "updated_at": "2025-03-27T02:00:02.869216Z",
            "deleted_at": null,
            "main_name": "PLATINUM",
            "aliases": [
                "TwoForOne",
                "G0068",
                "ATK33"
            ],
            "source_name": "MISPGALAXY:PLATINUM",
            "tools": [],
            "source_id": "MISPGALAXY",
            "reports": null
        },
        {
            "id": "e61c46f7-88a1-421a-9fed-0cfe2eeb820a",
            "created_at": "2022-10-25T16:07:24.061767Z",
            "updated_at": "2025-03-27T02:02:10.094583Z",
            "deleted_at": null,
            "main_name": "Platinum",
            "aliases": [
                "ATK 33",
                "Operation EasternRoppels",
                "TwoForOne"
            ],
            "source_name": "ETDA:Platinum",
            "tools": [
                "AMTsol",
                "Adupib",
                "Adupihan",
                "Dipsind",
                "DvDupdate.dll",
                "JPIN",
                "LOLBAS",
                "LOLBins",
                "Living off the Land",
                "RedPepper",
                "RedSalt",
                "Titanium",
                "adbupd",
                "psinstrc.ps1"
            ],
            "source_id": "ETDA",
            "reports": null
        },
        {
            "id": "75108fc1-7f6a-450e-b024-10284f3f62bb",
            "created_at": "2024-11-01T02:00:52.756877Z",
            "updated_at": "2025-03-27T02:00:55.544216Z",
            "deleted_at": null,
            "main_name": "Play",
            "aliases": null,
            "source_name": "MITRE:Play",
            "tools": [
                "Nltest",
                "AdFind",
                "PsExec",
                "Wevtutil",
                "Cobalt Strike",
                "Playcrypt",
                "Mimikatz"
            ],
            "source_id": "MITRE",
            "reports": null
        }
    ],
    "ts_created_at": 1666716499,
    "ts_updated_at": 1743041785,
    "ts_creation_date": 1399642905,
    "ts_modification_date": 1400240397,
    "files": {
        "pdf": "https://archive.orkl.eu/aff2aa7c9402dd04739cebd6abd32f5cac462c47.pdf",
        "text": "https://archive.orkl.eu/aff2aa7c9402dd04739cebd6abd32f5cac462c47.txt",
        "img": "https://archive.orkl.eu/aff2aa7c9402dd04739cebd6abd32f5cac462c47.jpg"
    }
}